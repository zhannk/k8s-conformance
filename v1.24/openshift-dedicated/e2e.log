I0118 22:14:46.425377      22 e2e.go:129] Starting e2e run "b690fb47-651e-40b7-8fa6-91d8030e242f" on Ginkgo node 1
{"msg":"Test Suite starting","total":356,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1674080086 - Will randomize all specs
Will run 356 of 6965 specs

Jan 18 22:14:48.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 22:14:48.724: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 18 22:14:48.747: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 18 22:14:48.762: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 18 22:14:48.762: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Jan 18 22:14:48.762: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 18 22:14:48.764: INFO: e2e test version: v1.24.0
Jan 18 22:14:48.764: INFO: kube-apiserver version: v1.24.0+9546431
Jan 18 22:14:48.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 22:14:48.767: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:14:48.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename statefulset
W0118 22:14:48.787575      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jan 18 22:14:48.787: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4051
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4051
W0118 22:14:48.815337      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4051
Jan 18 22:14:48.821: INFO: Found 0 stateful pods, waiting for 1
Jan 18 22:14:58.826: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 18 22:14:58.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-4051 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:14:58.977: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:14:58.977: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:14:58.977: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:14:58.980: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 18 22:15:08.983: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:15:08.983: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:15:09.001: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999846s
Jan 18 22:15:10.003: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996677324s
Jan 18 22:15:11.005: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994536287s
Jan 18 22:15:12.009: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991448937s
Jan 18 22:15:13.012: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988257146s
Jan 18 22:15:14.015: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.985254045s
Jan 18 22:15:15.018: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.982250877s
Jan 18 22:15:16.021: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.979255212s
Jan 18 22:15:17.025: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.975255802s
Jan 18 22:15:18.027: INFO: Verifying statefulset ss doesn't scale past 1 for another 972.826663ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4051
Jan 18 22:15:19.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-4051 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:15:19.160: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 22:15:19.160: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:15:19.160: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:15:19.163: INFO: Found 1 stateful pods, waiting for 3
Jan 18 22:15:29.168: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:15:29.168: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:15:29.168: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan 18 22:15:39.166: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:15:39.166: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:15:39.166: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 18 22:15:39.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-4051 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:15:39.300: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:15:39.300: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:15:39.300: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:15:39.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-4051 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:15:39.405: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:15:39.405: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:15:39.405: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:15:39.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-4051 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:15:39.524: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:15:39.524: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:15:39.524: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:15:39.524: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:15:39.526: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 18 22:15:49.534: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:15:49.534: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:15:49.534: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:15:49.544: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999982s
Jan 18 22:15:50.547: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997520325s
Jan 18 22:15:51.551: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993365271s
Jan 18 22:15:52.554: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990688869s
Jan 18 22:15:53.556: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.987720411s
Jan 18 22:15:54.559: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.98517914s
Jan 18 22:15:55.562: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982593123s
Jan 18 22:15:56.566: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.978639124s
Jan 18 22:15:57.568: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.975843583s
Jan 18 22:15:58.572: INFO: Verifying statefulset ss doesn't scale past 3 for another 973.245106ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4051
Jan 18 22:15:59.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-4051 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:15:59.708: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 22:15:59.708: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:15:59.708: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:15:59.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-4051 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:15:59.837: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 22:15:59.837: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:15:59.837: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:15:59.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-4051 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:15:59.984: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 22:15:59.984: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:15:59.984: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:15:59.984: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 22:16:10.001: INFO: Deleting all statefulset in ns statefulset-4051
Jan 18 22:16:10.003: INFO: Scaling statefulset ss to 0
Jan 18 22:16:10.011: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:16:10.013: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 18 22:16:10.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4051" for this suite.

• [SLOW TEST:81.269 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":356,"completed":1,"skipped":27,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:16:10.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3413
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3413
STEP: creating replication controller externalsvc in namespace services-3413
I0118 22:16:10.122406      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3413, replica count: 2
I0118 22:16:13.173784      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 22:16:16.174652      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jan 18 22:16:16.199: INFO: Creating new exec pod
Jan 18 22:16:20.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-3413 exec execpodmjshz -- /bin/sh -x -c nslookup nodeport-service.services-3413.svc.cluster.local'
Jan 18 22:16:20.409: INFO: stderr: "+ nslookup nodeport-service.services-3413.svc.cluster.local\n"
Jan 18 22:16:20.409: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nnodeport-service.services-3413.svc.cluster.local\tcanonical name = externalsvc.services-3413.svc.cluster.local.\nName:\texternalsvc.services-3413.svc.cluster.local\nAddress: 172.30.158.0\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3413, will wait for the garbage collector to delete the pods
Jan 18 22:16:20.469: INFO: Deleting ReplicationController externalsvc took: 4.953883ms
Jan 18 22:16:20.569: INFO: Terminating ReplicationController externalsvc pods took: 100.297257ms
Jan 18 22:16:22.991: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 18 22:16:23.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3413" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:12.983 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":356,"completed":2,"skipped":31,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:16:23.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override command
W0118 22:16:23.087637      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:16:23.087: INFO: Waiting up to 5m0s for pod "client-containers-219167cc-70fb-4fcb-92e4-8a634210c0d5" in namespace "containers-8578" to be "Succeeded or Failed"
Jan 18 22:16:23.091: INFO: Pod "client-containers-219167cc-70fb-4fcb-92e4-8a634210c0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.280409ms
Jan 18 22:16:25.094: INFO: Pod "client-containers-219167cc-70fb-4fcb-92e4-8a634210c0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006601s
Jan 18 22:16:27.111: INFO: Pod "client-containers-219167cc-70fb-4fcb-92e4-8a634210c0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024083062s
Jan 18 22:16:29.115: INFO: Pod "client-containers-219167cc-70fb-4fcb-92e4-8a634210c0d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027542136s
STEP: Saw pod success
Jan 18 22:16:29.115: INFO: Pod "client-containers-219167cc-70fb-4fcb-92e4-8a634210c0d5" satisfied condition "Succeeded or Failed"
Jan 18 22:16:29.117: INFO: Trying to get logs from node ip-10-0-211-217.ec2.internal pod client-containers-219167cc-70fb-4fcb-92e4-8a634210c0d5 container agnhost-container: <nil>
STEP: delete the pod
Jan 18 22:16:29.149: INFO: Waiting for pod client-containers-219167cc-70fb-4fcb-92e4-8a634210c0d5 to disappear
Jan 18 22:16:29.151: INFO: Pod client-containers-219167cc-70fb-4fcb-92e4-8a634210c0d5 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 18 22:16:29.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8578" for this suite.

• [SLOW TEST:6.139 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","total":356,"completed":3,"skipped":41,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:16:29.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 22:16:30.196: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4c787e91-85df-43b5-b2f5-3794187378a1" in namespace "downward-api-9836" to be "Succeeded or Failed"
Jan 18 22:16:30.198: INFO: Pod "downwardapi-volume-4c787e91-85df-43b5-b2f5-3794187378a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.293822ms
Jan 18 22:16:32.202: INFO: Pod "downwardapi-volume-4c787e91-85df-43b5-b2f5-3794187378a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006451462s
Jan 18 22:16:34.205: INFO: Pod "downwardapi-volume-4c787e91-85df-43b5-b2f5-3794187378a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008955491s
Jan 18 22:16:36.208: INFO: Pod "downwardapi-volume-4c787e91-85df-43b5-b2f5-3794187378a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012192671s
STEP: Saw pod success
Jan 18 22:16:36.208: INFO: Pod "downwardapi-volume-4c787e91-85df-43b5-b2f5-3794187378a1" satisfied condition "Succeeded or Failed"
Jan 18 22:16:36.210: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-4c787e91-85df-43b5-b2f5-3794187378a1 container client-container: <nil>
STEP: delete the pod
Jan 18 22:16:36.230: INFO: Waiting for pod downwardapi-volume-4c787e91-85df-43b5-b2f5-3794187378a1 to disappear
Jan 18 22:16:36.232: INFO: Pod downwardapi-volume-4c787e91-85df-43b5-b2f5-3794187378a1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 18 22:16:36.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9836" for this suite.

• [SLOW TEST:7.081 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":4,"skipped":56,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:16:36.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:16:36.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-587 version'
Jan 18 22:16:36.307: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 18 22:16:36.307: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.0\", GitCommit:\"4ce5a8954017644c5420bae81d72b09b735c21f0\", GitTreeState:\"clean\", BuildDate:\"2022-05-03T13:46:05Z\", GoVersion:\"go1.18.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.4\nServer Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.0+9546431\", GitCommit:\"0a57f1f59bda75ea2cf13d9f3b4ac5d202134f2d\", GitTreeState:\"clean\", BuildDate:\"2022-07-08T19:55:26Z\", GoVersion:\"go1.18.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 22:16:36.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-587" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":356,"completed":5,"skipped":58,"failed":0}
SSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:16:36.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 18 22:16:36.678: INFO: starting watch
STEP: patching
STEP: updating
Jan 18 22:16:36.688: INFO: waiting for watch events with expected annotations
Jan 18 22:16:36.688: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 22:16:36.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-3772" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":356,"completed":6,"skipped":61,"failed":0}
S
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:16:36.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:16:36.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-1891
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:188
Jan 18 22:16:40.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-7576" for this suite.
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 18 22:16:40.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1891" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":356,"completed":7,"skipped":62,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:16:40.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 18 22:16:41.026: INFO: PodSpec: initContainers in spec.initContainers
Jan 18 22:17:29.269: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7af2ce66-7dfa-44f4-9f59-2719c6caca45", GenerateName:"", Namespace:"init-container-8114", SelfLink:"", UID:"92721ba9-f349-43ea-bf0e-ab611fb4a087", ResourceVersion:"149325", Generation:0, CreationTimestamp:time.Date(2023, time.January, 18, 22, 16, 41, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"25993521"}, Annotations:map[string]string{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.12.16/23\"],\"mac_address\":\"0a:58:0a:80:0c:10\",\"gateway_ips\":[\"10.128.12.1\"],\"ip_address\":\"10.128.12.16/23\",\"gateway_ip\":\"10.128.12.1\"}}", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.12.16\"\n    ],\n    \"mac\": \"0a:58:0a:80:0c:10\",\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.12.16\"\n    ],\n    \"mac\": \"0a:58:0a:80:0c:10\",\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 22, 16, 41, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002dd7770), Subresource:""}, v1.ManagedFieldsEntry{Manager:"ip-10-0-176-170", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 22, 16, 41, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002dd77a0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 22, 16, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002dd77d0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 22, 16, 45, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002dd7800), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-4828f", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc002ddf840), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4828f", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0013fe180), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4828f", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0013fe300), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.7", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4828f", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0013fe120), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00348d948), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-219-147.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001c4df10), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00348d9f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00348da10)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00348da2c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00348da40), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002772f40), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 16, 41, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 16, 41, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 16, 41, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 16, 41, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.219.147", PodIP:"10.128.12.16", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.128.12.16"}}, StartTime:time.Date(2023, time.January, 18, 22, 16, 41, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00066a000)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://47d8b8ad80880ff41c0e66830317e1068b915b8821483eb05d442318b5da23ef", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002ddf920), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002ddf8c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.7", ImageID:"", ContainerID:"", Started:(*bool)(0xc00348dacf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 18 22:17:29.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8114" for this suite.

• [SLOW TEST:48.283 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":356,"completed":8,"skipped":78,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:17:29.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating all guestbook components
Jan 18 22:17:29.308: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 18 22:17:29.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-817 create -f -'
Jan 18 22:17:29.672: INFO: stderr: ""
Jan 18 22:17:29.672: INFO: stdout: "service/agnhost-replica created\n"
Jan 18 22:17:29.672: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 18 22:17:29.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-817 create -f -'
Jan 18 22:17:29.949: INFO: stderr: ""
Jan 18 22:17:29.949: INFO: stdout: "service/agnhost-primary created\n"
Jan 18 22:17:29.949: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 18 22:17:29.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-817 create -f -'
Jan 18 22:17:30.275: INFO: stderr: ""
Jan 18 22:17:30.275: INFO: stdout: "service/frontend created\n"
Jan 18 22:17:30.275: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.36
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 18 22:17:30.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-817 create -f -'
Jan 18 22:17:30.541: INFO: stderr: ""
Jan 18 22:17:30.541: INFO: stdout: "deployment.apps/frontend created\n"
Jan 18 22:17:30.541: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.36
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 18 22:17:30.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-817 create -f -'
Jan 18 22:17:30.835: INFO: stderr: ""
Jan 18 22:17:30.835: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 18 22:17:30.835: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.36
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 18 22:17:30.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-817 create -f -'
Jan 18 22:17:32.939: INFO: stderr: ""
Jan 18 22:17:32.939: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jan 18 22:17:32.939: INFO: Waiting for all frontend pods to be Running.
Jan 18 22:17:37.991: INFO: Waiting for frontend to serve content.
Jan 18 22:17:37.999: INFO: Trying to add a new entry to the guestbook.
Jan 18 22:17:38.015: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan 18 22:17:38.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-817 delete --grace-period=0 --force -f -'
Jan 18 22:17:38.096: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 22:17:38.096: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jan 18 22:17:38.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-817 delete --grace-period=0 --force -f -'
Jan 18 22:17:38.221: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 22:17:38.221: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 18 22:17:38.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-817 delete --grace-period=0 --force -f -'
Jan 18 22:17:38.312: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 22:17:38.312: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 18 22:17:38.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-817 delete --grace-period=0 --force -f -'
Jan 18 22:17:38.382: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 22:17:38.382: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 18 22:17:38.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-817 delete --grace-period=0 --force -f -'
Jan 18 22:17:38.435: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 22:17:38.435: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 18 22:17:38.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-817 delete --grace-period=0 --force -f -'
Jan 18 22:17:38.482: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 22:17:38.482: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 22:17:38.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-817" for this suite.

• [SLOW TEST:9.216 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:340
    should create and stop a working application  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":356,"completed":9,"skipped":79,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:17:38.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 18 22:17:46.581: INFO: DNS probes using dns-3051/dns-test-654df5fc-f7b2-42e0-8a0e-7bccb545b030 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 18 22:17:46.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3051" for this suite.

• [SLOW TEST:8.106 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":356,"completed":10,"skipped":88,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:17:46.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0118 22:17:46.671725      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:17:46.671: INFO: Waiting up to 5m0s for pod "downwardapi-volume-71b039bd-8ed3-4d88-9ddd-a0f295aef0aa" in namespace "projected-720" to be "Succeeded or Failed"
Jan 18 22:17:46.674: INFO: Pod "downwardapi-volume-71b039bd-8ed3-4d88-9ddd-a0f295aef0aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.796502ms
Jan 18 22:17:48.686: INFO: Pod "downwardapi-volume-71b039bd-8ed3-4d88-9ddd-a0f295aef0aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014730918s
Jan 18 22:17:50.693: INFO: Pod "downwardapi-volume-71b039bd-8ed3-4d88-9ddd-a0f295aef0aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021511383s
Jan 18 22:17:52.702: INFO: Pod "downwardapi-volume-71b039bd-8ed3-4d88-9ddd-a0f295aef0aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030544917s
STEP: Saw pod success
Jan 18 22:17:52.702: INFO: Pod "downwardapi-volume-71b039bd-8ed3-4d88-9ddd-a0f295aef0aa" satisfied condition "Succeeded or Failed"
Jan 18 22:17:52.705: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-71b039bd-8ed3-4d88-9ddd-a0f295aef0aa container client-container: <nil>
STEP: delete the pod
Jan 18 22:17:52.719: INFO: Waiting for pod downwardapi-volume-71b039bd-8ed3-4d88-9ddd-a0f295aef0aa to disappear
Jan 18 22:17:52.721: INFO: Pod downwardapi-volume-71b039bd-8ed3-4d88-9ddd-a0f295aef0aa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 18 22:17:52.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-720" for this suite.

• [SLOW TEST:6.133 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":11,"skipped":144,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:17:52.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-421
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-421
Jan 18 22:17:52.812: INFO: Found 0 stateful pods, waiting for 1
Jan 18 22:18:02.818: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Jan 18 22:18:02.833: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Jan 18 22:18:02.842: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Jan 18 22:18:02.843: INFO: Observed &StatefulSet event: ADDED
Jan 18 22:18:02.843: INFO: Found Statefulset ss in namespace statefulset-421 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 22:18:02.843: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Jan 18 22:18:02.843: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 22:18:02.849: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Jan 18 22:18:02.850: INFO: Observed &StatefulSet event: ADDED
Jan 18 22:18:02.850: INFO: Observed Statefulset ss in namespace statefulset-421 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 22:18:02.850: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 22:18:02.850: INFO: Deleting all statefulset in ns statefulset-421
Jan 18 22:18:02.852: INFO: Scaling statefulset ss to 0
Jan 18 22:18:12.866: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:18:12.869: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 18 22:18:12.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-421" for this suite.

• [SLOW TEST:20.159 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":356,"completed":12,"skipped":160,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:18:12.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 18 22:18:13.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1486" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":356,"completed":13,"skipped":180,"failed":0}

------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:18:13.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5963.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5963.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5963.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5963.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 18 22:18:17.093: INFO: DNS probes using dns-5963/dns-test-3e0369e8-c063-4bdc-82a5-0ca242e02fad succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 18 22:18:17.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5963" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","total":356,"completed":14,"skipped":180,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:18:17.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jan 18 22:18:17.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 22:18:25.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 22:18:57.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6516" for this suite.

• [SLOW TEST:40.622 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":356,"completed":15,"skipped":181,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:18:57.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 18 22:18:58.789: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:19:00.794: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:19:02.795: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:19:04.795: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 18 22:19:04.814: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:19:06.822: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:19:08.820: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 18 22:19:08.831: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 22:19:08.833: INFO: Pod pod-with-prestop-http-hook still exists
Jan 18 22:19:10.834: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 22:19:10.840: INFO: Pod pod-with-prestop-http-hook still exists
Jan 18 22:19:12.833: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 22:19:12.838: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 18 22:19:12.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-653" for this suite.

• [SLOW TEST:15.126 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":356,"completed":16,"skipped":208,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:19:12.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:19:13.913: INFO: The status of Pod server-envvars-257600d6-525a-4aed-ac21-0db877bbf4de is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:19:15.919: INFO: The status of Pod server-envvars-257600d6-525a-4aed-ac21-0db877bbf4de is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:19:17.919: INFO: The status of Pod server-envvars-257600d6-525a-4aed-ac21-0db877bbf4de is Running (Ready = true)
Jan 18 22:19:17.945: INFO: Waiting up to 5m0s for pod "client-envvars-49b1ca1f-9e39-4ff0-b130-f673d8b6c7c0" in namespace "pods-9926" to be "Succeeded or Failed"
Jan 18 22:19:17.947: INFO: Pod "client-envvars-49b1ca1f-9e39-4ff0-b130-f673d8b6c7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.122575ms
Jan 18 22:19:19.954: INFO: Pod "client-envvars-49b1ca1f-9e39-4ff0-b130-f673d8b6c7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008784939s
Jan 18 22:19:21.962: INFO: Pod "client-envvars-49b1ca1f-9e39-4ff0-b130-f673d8b6c7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01692109s
Jan 18 22:19:23.967: INFO: Pod "client-envvars-49b1ca1f-9e39-4ff0-b130-f673d8b6c7c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022090551s
STEP: Saw pod success
Jan 18 22:19:23.967: INFO: Pod "client-envvars-49b1ca1f-9e39-4ff0-b130-f673d8b6c7c0" satisfied condition "Succeeded or Failed"
Jan 18 22:19:23.970: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod client-envvars-49b1ca1f-9e39-4ff0-b130-f673d8b6c7c0 container env3cont: <nil>
STEP: delete the pod
Jan 18 22:19:23.992: INFO: Waiting for pod client-envvars-49b1ca1f-9e39-4ff0-b130-f673d8b6c7c0 to disappear
Jan 18 22:19:23.994: INFO: Pod client-envvars-49b1ca1f-9e39-4ff0-b130-f673d8b6c7c0 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 18 22:19:23.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9926" for this suite.

• [SLOW TEST:11.142 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":356,"completed":17,"skipped":242,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:19:24.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jan 18 22:19:24.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 22:19:32.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 22:20:04.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9692" for this suite.

• [SLOW TEST:40.481 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":356,"completed":18,"skipped":247,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:20:04.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7045
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating stateful set ss in namespace statefulset-7045
W0118 22:20:04.522080      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7045
Jan 18 22:20:04.530: INFO: Found 0 stateful pods, waiting for 1
Jan 18 22:20:14.533: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 18 22:20:14.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-7045 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:20:14.667: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:20:14.667: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:20:14.667: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:20:14.670: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 18 22:20:24.673: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:20:24.673: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:20:24.688: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Jan 18 22:20:24.688: INFO: ss-0  ip-10-0-219-147.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:04 +0000 UTC  }]
Jan 18 22:20:24.688: INFO: 
Jan 18 22:20:24.688: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 18 22:20:25.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997071137s
Jan 18 22:20:26.696: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991579261s
Jan 18 22:20:27.700: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988070777s
Jan 18 22:20:28.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985056426s
Jan 18 22:20:29.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982068078s
Jan 18 22:20:30.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973118546s
Jan 18 22:20:31.723: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964840251s
Jan 18 22:20:32.729: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.961042413s
Jan 18 22:20:33.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.864353ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7045
Jan 18 22:20:34.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-7045 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:20:34.867: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 22:20:34.867: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:20:34.867: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:20:34.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-7045 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:20:35.003: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 18 22:20:35.003: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:20:35.003: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:20:35.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-7045 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:20:35.110: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 18 22:20:35.110: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:20:35.110: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:20:35.116: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:20:35.116: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:20:35.116: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 18 22:20:35.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-7045 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:20:35.236: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:20:35.236: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:20:35.236: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:20:35.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-7045 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:20:35.342: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:20:35.342: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:20:35.342: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:20:35.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-7045 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:20:35.448: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:20:35.448: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:20:35.448: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:20:35.448: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:20:35.450: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 18 22:20:45.460: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:20:45.460: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:20:45.460: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:20:45.471: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Jan 18 22:20:45.471: INFO: ss-0  ip-10-0-219-147.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:04 +0000 UTC  }]
Jan 18 22:20:45.471: INFO: ss-1  ip-10-0-211-217.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:24 +0000 UTC  }]
Jan 18 22:20:45.471: INFO: ss-2  ip-10-0-128-7.ec2.internal    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:24 +0000 UTC  }]
Jan 18 22:20:45.471: INFO: 
Jan 18 22:20:45.471: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 18 22:20:46.476: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Jan 18 22:20:46.476: INFO: ss-0  ip-10-0-219-147.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:04 +0000 UTC  }]
Jan 18 22:20:46.476: INFO: ss-1  ip-10-0-211-217.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:24 +0000 UTC  }]
Jan 18 22:20:46.476: INFO: ss-2  ip-10-0-128-7.ec2.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:20:24 +0000 UTC  }]
Jan 18 22:20:46.476: INFO: 
Jan 18 22:20:46.476: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 18 22:20:47.480: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992482975s
Jan 18 22:20:48.486: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987492901s
Jan 18 22:20:49.491: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98204815s
Jan 18 22:20:50.496: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.97764571s
Jan 18 22:20:51.501: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.972643232s
Jan 18 22:20:52.504: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.967500126s
Jan 18 22:20:53.509: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.964486015s
Jan 18 22:20:54.514: INFO: Verifying statefulset ss doesn't scale past 0 for another 959.486833ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7045
Jan 18 22:20:55.520: INFO: Scaling statefulset ss to 0
Jan 18 22:20:55.532: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 22:20:55.533: INFO: Deleting all statefulset in ns statefulset-7045
Jan 18 22:20:55.535: INFO: Scaling statefulset ss to 0
Jan 18 22:20:55.544: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:20:55.546: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 18 22:20:55.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7045" for this suite.

• [SLOW TEST:51.077 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":356,"completed":19,"skipped":300,"failed":0}
SSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:20:55.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan 18 22:20:55.585: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 22:21:55.803: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:21:55.809: INFO: Starting informer...
STEP: Starting pods...
Jan 18 22:21:56.037: INFO: Pod1 is running on ip-10-0-219-147.ec2.internal. Tainting Node
Jan 18 22:22:00.262: INFO: Pod2 is running on ip-10-0-219-147.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jan 18 22:22:06.239: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 18 22:22:26.282: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:188
Jan 18 22:22:26.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5759" for this suite.

• [SLOW TEST:90.797 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":356,"completed":20,"skipped":308,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:22:26.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 18 22:22:26.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7204" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":356,"completed":21,"skipped":319,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:22:26.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-75fbb07e-29bf-44df-9e55-a5871655efb6
STEP: Creating a pod to test consume secrets
Jan 18 22:22:26.662: INFO: Waiting up to 5m0s for pod "pod-secrets-d5a25d5c-ffa7-417d-b497-95be6e1da855" in namespace "secrets-8260" to be "Succeeded or Failed"
Jan 18 22:22:26.671: INFO: Pod "pod-secrets-d5a25d5c-ffa7-417d-b497-95be6e1da855": Phase="Pending", Reason="", readiness=false. Elapsed: 8.702053ms
Jan 18 22:22:28.684: INFO: Pod "pod-secrets-d5a25d5c-ffa7-417d-b497-95be6e1da855": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021932355s
Jan 18 22:22:30.689: INFO: Pod "pod-secrets-d5a25d5c-ffa7-417d-b497-95be6e1da855": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026308672s
Jan 18 22:22:32.696: INFO: Pod "pod-secrets-d5a25d5c-ffa7-417d-b497-95be6e1da855": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033786757s
STEP: Saw pod success
Jan 18 22:22:32.696: INFO: Pod "pod-secrets-d5a25d5c-ffa7-417d-b497-95be6e1da855" satisfied condition "Succeeded or Failed"
Jan 18 22:22:32.698: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-secrets-d5a25d5c-ffa7-417d-b497-95be6e1da855 container secret-volume-test: <nil>
STEP: delete the pod
Jan 18 22:22:32.729: INFO: Waiting for pod pod-secrets-d5a25d5c-ffa7-417d-b497-95be6e1da855 to disappear
Jan 18 22:22:32.731: INFO: Pod pod-secrets-d5a25d5c-ffa7-417d-b497-95be6e1da855 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 18 22:22:32.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8260" for this suite.
STEP: Destroying namespace "secret-namespace-1718" for this suite.

• [SLOW TEST:6.287 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":356,"completed":22,"skipped":374,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:22:32.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 18 22:22:32.822: INFO: Waiting up to 5m0s for pod "pod-06b41f4d-ec41-4c56-a749-6678ed9fa2e4" in namespace "emptydir-800" to be "Succeeded or Failed"
Jan 18 22:22:32.829: INFO: Pod "pod-06b41f4d-ec41-4c56-a749-6678ed9fa2e4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.399119ms
Jan 18 22:22:34.836: INFO: Pod "pod-06b41f4d-ec41-4c56-a749-6678ed9fa2e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013549479s
Jan 18 22:22:36.843: INFO: Pod "pod-06b41f4d-ec41-4c56-a749-6678ed9fa2e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020769723s
Jan 18 22:22:38.848: INFO: Pod "pod-06b41f4d-ec41-4c56-a749-6678ed9fa2e4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025535594s
Jan 18 22:22:40.853: INFO: Pod "pod-06b41f4d-ec41-4c56-a749-6678ed9fa2e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.030946471s
STEP: Saw pod success
Jan 18 22:22:40.853: INFO: Pod "pod-06b41f4d-ec41-4c56-a749-6678ed9fa2e4" satisfied condition "Succeeded or Failed"
Jan 18 22:22:40.855: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-06b41f4d-ec41-4c56-a749-6678ed9fa2e4 container test-container: <nil>
STEP: delete the pod
Jan 18 22:22:40.871: INFO: Waiting for pod pod-06b41f4d-ec41-4c56-a749-6678ed9fa2e4 to disappear
Jan 18 22:22:40.873: INFO: Pod pod-06b41f4d-ec41-4c56-a749-6678ed9fa2e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 18 22:22:40.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-800" for this suite.

• [SLOW TEST:8.121 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":23,"skipped":391,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:22:40.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:22:40.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4149
W0118 22:22:40.916585      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0118 22:22:40.916759      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4149, replica count: 1
I0118 22:22:41.967755      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 22:22:42.968815      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 22:22:43.969819      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 22:22:44.970196      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:22:45.082: INFO: Created: latency-svc-gdsmh
Jan 18 22:22:45.086: INFO: Got endpoints: latency-svc-gdsmh [16.207033ms]
Jan 18 22:22:45.121: INFO: Created: latency-svc-zcz79
Jan 18 22:22:45.121: INFO: Created: latency-svc-gjvxk
Jan 18 22:22:45.121: INFO: Got endpoints: latency-svc-zcz79 [33.201396ms]
Jan 18 22:22:45.125: INFO: Got endpoints: latency-svc-gjvxk [37.511046ms]
Jan 18 22:22:45.147: INFO: Created: latency-svc-qkqzt
Jan 18 22:22:45.165: INFO: Created: latency-svc-4d768
Jan 18 22:22:45.165: INFO: Got endpoints: latency-svc-qkqzt [77.165701ms]
Jan 18 22:22:45.165: INFO: Created: latency-svc-sjtz4
Jan 18 22:22:45.174: INFO: Got endpoints: latency-svc-sjtz4 [87.053211ms]
Jan 18 22:22:45.174: INFO: Created: latency-svc-qmfb2
Jan 18 22:22:45.174: INFO: Got endpoints: latency-svc-4d768 [87.561751ms]
Jan 18 22:22:45.178: INFO: Got endpoints: latency-svc-qmfb2 [90.346583ms]
Jan 18 22:22:45.182: INFO: Created: latency-svc-pbdbr
Jan 18 22:22:45.193: INFO: Got endpoints: latency-svc-pbdbr [105.601394ms]
Jan 18 22:22:45.282: INFO: Created: latency-svc-hw8cr
Jan 18 22:22:45.283: INFO: Created: latency-svc-kn9ll
Jan 18 22:22:45.283: INFO: Created: latency-svc-8vfqb
Jan 18 22:22:45.284: INFO: Created: latency-svc-hdxrb
Jan 18 22:22:45.284: INFO: Created: latency-svc-5bdsw
Jan 18 22:22:45.285: INFO: Created: latency-svc-8k4qz
Jan 18 22:22:45.285: INFO: Created: latency-svc-rc5bc
Jan 18 22:22:45.285: INFO: Created: latency-svc-568c8
Jan 18 22:22:45.285: INFO: Created: latency-svc-7xlq5
Jan 18 22:22:45.286: INFO: Created: latency-svc-brr9m
Jan 18 22:22:45.286: INFO: Created: latency-svc-2xr8w
Jan 18 22:22:45.286: INFO: Created: latency-svc-vk72b
Jan 18 22:22:45.286: INFO: Created: latency-svc-v95rz
Jan 18 22:22:45.290: INFO: Got endpoints: latency-svc-hw8cr [96.867345ms]
Jan 18 22:22:45.292: INFO: Created: latency-svc-74t84
Jan 18 22:22:45.293: INFO: Got endpoints: latency-svc-kn9ll [171.914404ms]
Jan 18 22:22:45.293: INFO: Created: latency-svc-5fhzw
Jan 18 22:22:45.296: INFO: Got endpoints: latency-svc-8vfqb [209.360359ms]
Jan 18 22:22:45.299: INFO: Got endpoints: latency-svc-5fhzw [173.599927ms]
Jan 18 22:22:45.299: INFO: Got endpoints: latency-svc-74t84 [211.829206ms]
Jan 18 22:22:45.302: INFO: Got endpoints: latency-svc-2xr8w [214.713449ms]
Jan 18 22:22:45.304: INFO: Got endpoints: latency-svc-vk72b [129.999194ms]
Jan 18 22:22:45.308: INFO: Got endpoints: latency-svc-rc5bc [221.221132ms]
Jan 18 22:22:45.310: INFO: Got endpoints: latency-svc-hdxrb [136.072957ms]
Jan 18 22:22:45.323: INFO: Got endpoints: latency-svc-568c8 [145.713669ms]
Jan 18 22:22:45.323: INFO: Got endpoints: latency-svc-5bdsw [236.178604ms]
Jan 18 22:22:45.324: INFO: Got endpoints: latency-svc-7xlq5 [236.277162ms]
Jan 18 22:22:45.324: INFO: Got endpoints: latency-svc-v95rz [236.577384ms]
Jan 18 22:22:45.324: INFO: Got endpoints: latency-svc-brr9m [236.955214ms]
Jan 18 22:22:45.328: INFO: Got endpoints: latency-svc-8k4qz [162.800283ms]
Jan 18 22:22:45.331: INFO: Created: latency-svc-hj5xs
Jan 18 22:22:45.334: INFO: Created: latency-svc-fc2q7
Jan 18 22:22:45.337: INFO: Got endpoints: latency-svc-hj5xs [46.929558ms]
Jan 18 22:22:45.346: INFO: Got endpoints: latency-svc-fc2q7 [53.197254ms]
Jan 18 22:22:45.353: INFO: Created: latency-svc-q5h9b
Jan 18 22:22:45.358: INFO: Created: latency-svc-rbpvj
Jan 18 22:22:45.371: INFO: Got endpoints: latency-svc-rbpvj [72.024639ms]
Jan 18 22:22:45.371: INFO: Got endpoints: latency-svc-q5h9b [74.291342ms]
Jan 18 22:22:45.380: INFO: Created: latency-svc-87f7h
Jan 18 22:22:45.393: INFO: Got endpoints: latency-svc-87f7h [94.762084ms]
Jan 18 22:22:45.395: INFO: Created: latency-svc-l2bm6
Jan 18 22:22:45.404: INFO: Created: latency-svc-4ppc6
Jan 18 22:22:45.407: INFO: Got endpoints: latency-svc-l2bm6 [104.587225ms]
Jan 18 22:22:45.407: INFO: Got endpoints: latency-svc-4ppc6 [102.50765ms]
Jan 18 22:22:45.408: INFO: Created: latency-svc-26k5f
Jan 18 22:22:45.419: INFO: Got endpoints: latency-svc-26k5f [111.032668ms]
Jan 18 22:22:45.429: INFO: Created: latency-svc-k894m
Jan 18 22:22:45.430: INFO: Created: latency-svc-nxqpq
Jan 18 22:22:45.437: INFO: Got endpoints: latency-svc-k894m [126.618297ms]
Jan 18 22:22:45.442: INFO: Created: latency-svc-k4pjl
Jan 18 22:22:45.444: INFO: Got endpoints: latency-svc-nxqpq [120.979114ms]
Jan 18 22:22:45.449: INFO: Got endpoints: latency-svc-k4pjl [125.503763ms]
Jan 18 22:22:45.452: INFO: Created: latency-svc-2vgl4
Jan 18 22:22:45.454: INFO: Got endpoints: latency-svc-2vgl4 [130.662901ms]
Jan 18 22:22:45.467: INFO: Created: latency-svc-7z5kx
Jan 18 22:22:45.471: INFO: Got endpoints: latency-svc-7z5kx [146.668663ms]
Jan 18 22:22:45.479: INFO: Created: latency-svc-pg2pb
Jan 18 22:22:45.481: INFO: Got endpoints: latency-svc-pg2pb [156.770409ms]
Jan 18 22:22:45.482: INFO: Created: latency-svc-gvbvk
Jan 18 22:22:45.488: INFO: Created: latency-svc-h8hzd
Jan 18 22:22:45.489: INFO: Got endpoints: latency-svc-gvbvk [160.79566ms]
Jan 18 22:22:45.497: INFO: Got endpoints: latency-svc-h8hzd [159.907275ms]
Jan 18 22:22:45.505: INFO: Created: latency-svc-4z9n2
Jan 18 22:22:45.512: INFO: Got endpoints: latency-svc-4z9n2 [165.873057ms]
Jan 18 22:22:45.518: INFO: Created: latency-svc-frgff
Jan 18 22:22:45.524: INFO: Got endpoints: latency-svc-frgff [153.161512ms]
Jan 18 22:22:45.530: INFO: Created: latency-svc-6t2q5
Jan 18 22:22:45.532: INFO: Got endpoints: latency-svc-6t2q5 [160.811802ms]
Jan 18 22:22:45.532: INFO: Created: latency-svc-2g5tn
Jan 18 22:22:45.539: INFO: Got endpoints: latency-svc-2g5tn [145.435898ms]
Jan 18 22:22:45.543: INFO: Created: latency-svc-8h2rq
Jan 18 22:22:45.552: INFO: Got endpoints: latency-svc-8h2rq [145.604408ms]
Jan 18 22:22:45.553: INFO: Created: latency-svc-5n7hk
Jan 18 22:22:45.559: INFO: Got endpoints: latency-svc-5n7hk [151.798693ms]
Jan 18 22:22:45.569: INFO: Created: latency-svc-6r6dd
Jan 18 22:22:45.569: INFO: Created: latency-svc-5f67x
Jan 18 22:22:45.573: INFO: Got endpoints: latency-svc-5f67x [154.098675ms]
Jan 18 22:22:45.576: INFO: Got endpoints: latency-svc-6r6dd [139.644315ms]
Jan 18 22:22:45.581: INFO: Created: latency-svc-8drrs
Jan 18 22:22:45.589: INFO: Got endpoints: latency-svc-8drrs [145.17165ms]
Jan 18 22:22:45.599: INFO: Created: latency-svc-w598v
Jan 18 22:22:45.604: INFO: Created: latency-svc-wg225
Jan 18 22:22:45.613: INFO: Got endpoints: latency-svc-w598v [164.545733ms]
Jan 18 22:22:45.617: INFO: Created: latency-svc-q9jhr
Jan 18 22:22:45.621: INFO: Got endpoints: latency-svc-wg225 [166.86569ms]
Jan 18 22:22:45.630: INFO: Got endpoints: latency-svc-q9jhr [159.227447ms]
Jan 18 22:22:45.638: INFO: Created: latency-svc-74kw5
Jan 18 22:22:45.647: INFO: Created: latency-svc-csgd8
Jan 18 22:22:45.648: INFO: Got endpoints: latency-svc-74kw5 [166.401024ms]
Jan 18 22:22:45.656: INFO: Created: latency-svc-v55cg
Jan 18 22:22:45.659: INFO: Created: latency-svc-n8kps
Jan 18 22:22:45.666: INFO: Got endpoints: latency-svc-v55cg [169.259677ms]
Jan 18 22:22:45.669: INFO: Got endpoints: latency-svc-csgd8 [180.785518ms]
Jan 18 22:22:45.670: INFO: Got endpoints: latency-svc-n8kps [157.909708ms]
Jan 18 22:22:45.678: INFO: Created: latency-svc-b9sdj
Jan 18 22:22:45.686: INFO: Got endpoints: latency-svc-b9sdj [162.240877ms]
Jan 18 22:22:45.885: INFO: Created: latency-svc-cqvnw
Jan 18 22:22:45.887: INFO: Created: latency-svc-pqd2t
Jan 18 22:22:45.887: INFO: Created: latency-svc-twpl4
Jan 18 22:22:45.887: INFO: Created: latency-svc-lckpc
Jan 18 22:22:45.888: INFO: Created: latency-svc-2mtdk
Jan 18 22:22:45.888: INFO: Created: latency-svc-qcb25
Jan 18 22:22:45.889: INFO: Created: latency-svc-6r8rh
Jan 18 22:22:45.889: INFO: Created: latency-svc-2lfhq
Jan 18 22:22:45.889: INFO: Created: latency-svc-d4vck
Jan 18 22:22:45.889: INFO: Created: latency-svc-xct5f
Jan 18 22:22:45.889: INFO: Created: latency-svc-cxgrg
Jan 18 22:22:45.889: INFO: Created: latency-svc-k4628
Jan 18 22:22:45.889: INFO: Got endpoints: latency-svc-cqvnw [219.597598ms]
Jan 18 22:22:45.889: INFO: Created: latency-svc-qcvhg
Jan 18 22:22:45.890: INFO: Created: latency-svc-vd6gv
Jan 18 22:22:45.890: INFO: Created: latency-svc-695vx
Jan 18 22:22:45.900: INFO: Got endpoints: latency-svc-d4vck [326.328991ms]
Jan 18 22:22:45.900: INFO: Got endpoints: latency-svc-qcvhg [252.331743ms]
Jan 18 22:22:45.900: INFO: Got endpoints: latency-svc-pqd2t [368.303991ms]
Jan 18 22:22:45.900: INFO: Got endpoints: latency-svc-twpl4 [361.119018ms]
Jan 18 22:22:45.900: INFO: Got endpoints: latency-svc-vd6gv [347.727855ms]
Jan 18 22:22:45.915: INFO: Got endpoints: latency-svc-xct5f [229.065868ms]
Jan 18 22:22:45.921: INFO: Created: latency-svc-flsrc
Jan 18 22:22:45.928: INFO: Got endpoints: latency-svc-2mtdk [314.696572ms]
Jan 18 22:22:45.928: INFO: Got endpoints: latency-svc-cxgrg [261.959272ms]
Jan 18 22:22:45.929: INFO: Got endpoints: latency-svc-695vx [307.301875ms]
Jan 18 22:22:45.929: INFO: Got endpoints: latency-svc-6r8rh [370.195315ms]
Jan 18 22:22:45.935: INFO: Got endpoints: latency-svc-2lfhq [358.498339ms]
Jan 18 22:22:45.942: INFO: Got endpoints: latency-svc-k4628 [352.052162ms]
Jan 18 22:22:45.952: INFO: Created: latency-svc-gzxdk
Jan 18 22:22:45.952: INFO: Got endpoints: latency-svc-flsrc [62.290121ms]
Jan 18 22:22:45.952: INFO: Got endpoints: latency-svc-lckpc [282.540006ms]
Jan 18 22:22:45.952: INFO: Got endpoints: latency-svc-qcb25 [321.797476ms]
Jan 18 22:22:45.961: INFO: Created: latency-svc-9cw2j
Jan 18 22:22:45.965: INFO: Got endpoints: latency-svc-gzxdk [65.354489ms]
Jan 18 22:22:45.971: INFO: Created: latency-svc-l48bn
Jan 18 22:22:45.974: INFO: Got endpoints: latency-svc-9cw2j [73.921299ms]
Jan 18 22:22:45.985: INFO: Got endpoints: latency-svc-l48bn [85.405868ms]
Jan 18 22:22:45.996: INFO: Created: latency-svc-jnl9h
Jan 18 22:22:45.999: INFO: Got endpoints: latency-svc-jnl9h [98.862313ms]
Jan 18 22:22:46.002: INFO: Created: latency-svc-ktrwv
Jan 18 22:22:46.015: INFO: Got endpoints: latency-svc-ktrwv [114.511571ms]
Jan 18 22:22:46.015: INFO: Created: latency-svc-5k6qh
Jan 18 22:22:46.018: INFO: Got endpoints: latency-svc-5k6qh [102.68826ms]
Jan 18 22:22:46.022: INFO: Created: latency-svc-xl5l7
Jan 18 22:22:46.028: INFO: Got endpoints: latency-svc-xl5l7 [99.696836ms]
Jan 18 22:22:46.038: INFO: Created: latency-svc-bgj8b
Jan 18 22:22:46.048: INFO: Got endpoints: latency-svc-bgj8b [119.449395ms]
Jan 18 22:22:46.052: INFO: Created: latency-svc-gwb7r
Jan 18 22:22:46.061: INFO: Created: latency-svc-bk4j8
Jan 18 22:22:46.064: INFO: Got endpoints: latency-svc-gwb7r [135.777647ms]
Jan 18 22:22:46.071: INFO: Got endpoints: latency-svc-bk4j8 [136.103822ms]
Jan 18 22:22:46.077: INFO: Created: latency-svc-cdkm2
Jan 18 22:22:46.091: INFO: Got endpoints: latency-svc-cdkm2 [155.165651ms]
Jan 18 22:22:46.094: INFO: Created: latency-svc-fkm9w
Jan 18 22:22:46.102: INFO: Got endpoints: latency-svc-fkm9w [160.34583ms]
Jan 18 22:22:46.107: INFO: Created: latency-svc-69gtt
Jan 18 22:22:46.114: INFO: Got endpoints: latency-svc-69gtt [162.362664ms]
Jan 18 22:22:46.118: INFO: Created: latency-svc-sfbrl
Jan 18 22:22:46.131: INFO: Created: latency-svc-xqhqg
Jan 18 22:22:46.131: INFO: Got endpoints: latency-svc-sfbrl [179.376659ms]
Jan 18 22:22:46.138: INFO: Got endpoints: latency-svc-xqhqg [185.230474ms]
Jan 18 22:22:46.142: INFO: Created: latency-svc-95dfs
Jan 18 22:22:46.151: INFO: Created: latency-svc-zl9hj
Jan 18 22:22:46.153: INFO: Got endpoints: latency-svc-95dfs [187.805834ms]
Jan 18 22:22:46.155: INFO: Got endpoints: latency-svc-zl9hj [180.722257ms]
Jan 18 22:22:46.159: INFO: Created: latency-svc-t22q9
Jan 18 22:22:46.180: INFO: Created: latency-svc-fjc64
Jan 18 22:22:46.180: INFO: Got endpoints: latency-svc-fjc64 [180.514916ms]
Jan 18 22:22:46.180: INFO: Got endpoints: latency-svc-t22q9 [194.539147ms]
Jan 18 22:22:46.180: INFO: Created: latency-svc-t7zfq
Jan 18 22:22:46.184: INFO: Got endpoints: latency-svc-t7zfq [169.950669ms]
Jan 18 22:22:46.185: INFO: Created: latency-svc-6wkc2
Jan 18 22:22:46.198: INFO: Got endpoints: latency-svc-6wkc2 [180.027096ms]
Jan 18 22:22:46.198: INFO: Created: latency-svc-hbv4c
Jan 18 22:22:46.205: INFO: Got endpoints: latency-svc-hbv4c [176.958364ms]
Jan 18 22:22:46.208: INFO: Created: latency-svc-r9kj5
Jan 18 22:22:46.217: INFO: Got endpoints: latency-svc-r9kj5 [168.562739ms]
Jan 18 22:22:46.227: INFO: Created: latency-svc-v695r
Jan 18 22:22:46.233: INFO: Created: latency-svc-gvqkh
Jan 18 22:22:46.235: INFO: Got endpoints: latency-svc-v695r [170.305044ms]
Jan 18 22:22:46.241: INFO: Got endpoints: latency-svc-gvqkh [169.89536ms]
Jan 18 22:22:46.246: INFO: Created: latency-svc-rrwkw
Jan 18 22:22:46.262: INFO: Got endpoints: latency-svc-rrwkw [171.254483ms]
Jan 18 22:22:46.264: INFO: Created: latency-svc-5nrdr
Jan 18 22:22:46.277: INFO: Got endpoints: latency-svc-5nrdr [174.847733ms]
Jan 18 22:22:46.296: INFO: Created: latency-svc-2wf4s
Jan 18 22:22:46.296: INFO: Got endpoints: latency-svc-2wf4s [181.428564ms]
Jan 18 22:22:46.306: INFO: Created: latency-svc-66gsz
Jan 18 22:22:46.308: INFO: Got endpoints: latency-svc-66gsz [176.791067ms]
Jan 18 22:22:46.308: INFO: Created: latency-svc-wpbtr
Jan 18 22:22:46.318: INFO: Got endpoints: latency-svc-wpbtr [180.249962ms]
Jan 18 22:22:46.325: INFO: Created: latency-svc-wclf8
Jan 18 22:22:46.327: INFO: Got endpoints: latency-svc-wclf8 [174.470631ms]
Jan 18 22:22:46.342: INFO: Created: latency-svc-8tntd
Jan 18 22:22:46.342: INFO: Got endpoints: latency-svc-8tntd [186.859725ms]
Jan 18 22:22:46.342: INFO: Created: latency-svc-pdfvq
Jan 18 22:22:46.348: INFO: Got endpoints: latency-svc-pdfvq [167.945154ms]
Jan 18 22:22:46.351: INFO: Created: latency-svc-6svs9
Jan 18 22:22:46.358: INFO: Got endpoints: latency-svc-6svs9 [177.635419ms]
Jan 18 22:22:46.365: INFO: Created: latency-svc-9nqlm
Jan 18 22:22:46.376: INFO: Got endpoints: latency-svc-9nqlm [191.616421ms]
Jan 18 22:22:46.378: INFO: Created: latency-svc-cl5kd
Jan 18 22:22:46.388: INFO: Got endpoints: latency-svc-cl5kd [190.118594ms]
Jan 18 22:22:46.391: INFO: Created: latency-svc-r5gx4
Jan 18 22:22:46.400: INFO: Got endpoints: latency-svc-r5gx4 [194.989431ms]
Jan 18 22:22:46.402: INFO: Created: latency-svc-wtrrx
Jan 18 22:22:46.411: INFO: Created: latency-svc-56d8z
Jan 18 22:22:46.418: INFO: Got endpoints: latency-svc-wtrrx [201.932955ms]
Jan 18 22:22:46.419: INFO: Got endpoints: latency-svc-56d8z [184.08046ms]
Jan 18 22:22:46.422: INFO: Created: latency-svc-s295x
Jan 18 22:22:46.433: INFO: Got endpoints: latency-svc-s295x [191.737155ms]
Jan 18 22:22:46.436: INFO: Created: latency-svc-n5hvw
Jan 18 22:22:46.447: INFO: Got endpoints: latency-svc-n5hvw [184.836481ms]
Jan 18 22:22:46.450: INFO: Created: latency-svc-xjwxf
Jan 18 22:22:46.458: INFO: Got endpoints: latency-svc-xjwxf [181.19412ms]
Jan 18 22:22:46.465: INFO: Created: latency-svc-gzr4t
Jan 18 22:22:46.472: INFO: Got endpoints: latency-svc-gzr4t [175.657696ms]
Jan 18 22:22:46.475: INFO: Created: latency-svc-nrp7q
Jan 18 22:22:46.485: INFO: Got endpoints: latency-svc-nrp7q [176.934829ms]
Jan 18 22:22:46.491: INFO: Created: latency-svc-cf9wt
Jan 18 22:22:46.491: INFO: Got endpoints: latency-svc-cf9wt [173.033265ms]
Jan 18 22:22:46.500: INFO: Created: latency-svc-fc7bt
Jan 18 22:22:46.507: INFO: Got endpoints: latency-svc-fc7bt [179.644434ms]
Jan 18 22:22:46.507: INFO: Created: latency-svc-f8xvw
Jan 18 22:22:46.514: INFO: Got endpoints: latency-svc-f8xvw [172.573337ms]
Jan 18 22:22:46.521: INFO: Created: latency-svc-2xzkz
Jan 18 22:22:46.524: INFO: Got endpoints: latency-svc-2xzkz [175.870066ms]
Jan 18 22:22:46.530: INFO: Created: latency-svc-zjhnd
Jan 18 22:22:46.531: INFO: Created: latency-svc-hfdxk
Jan 18 22:22:46.535: INFO: Got endpoints: latency-svc-hfdxk [158.791981ms]
Jan 18 22:22:46.541: INFO: Created: latency-svc-68nsp
Jan 18 22:22:46.543: INFO: Got endpoints: latency-svc-zjhnd [185.783598ms]
Jan 18 22:22:46.549: INFO: Got endpoints: latency-svc-68nsp [160.847852ms]
Jan 18 22:22:46.552: INFO: Created: latency-svc-xkntg
Jan 18 22:22:46.556: INFO: Got endpoints: latency-svc-xkntg [156.447225ms]
Jan 18 22:22:46.560: INFO: Created: latency-svc-gb7mm
Jan 18 22:22:46.569: INFO: Got endpoints: latency-svc-gb7mm [150.435739ms]
Jan 18 22:22:46.574: INFO: Created: latency-svc-7jjgr
Jan 18 22:22:46.580: INFO: Created: latency-svc-mrztz
Jan 18 22:22:46.582: INFO: Got endpoints: latency-svc-7jjgr [163.474196ms]
Jan 18 22:22:46.587: INFO: Got endpoints: latency-svc-mrztz [153.645731ms]
Jan 18 22:22:46.592: INFO: Created: latency-svc-qh5wt
Jan 18 22:22:46.605: INFO: Created: latency-svc-9tgnz
Jan 18 22:22:46.605: INFO: Got endpoints: latency-svc-9tgnz [146.337911ms]
Jan 18 22:22:46.605: INFO: Got endpoints: latency-svc-qh5wt [158.075872ms]
Jan 18 22:22:46.611: INFO: Created: latency-svc-79z2l
Jan 18 22:22:46.619: INFO: Created: latency-svc-9jzdp
Jan 18 22:22:46.625: INFO: Got endpoints: latency-svc-79z2l [153.280315ms]
Jan 18 22:22:46.637: INFO: Got endpoints: latency-svc-9jzdp [151.749413ms]
Jan 18 22:22:46.644: INFO: Created: latency-svc-8f9vz
Jan 18 22:22:46.657: INFO: Got endpoints: latency-svc-8f9vz [166.188931ms]
Jan 18 22:22:46.657: INFO: Created: latency-svc-gnzlq
Jan 18 22:22:46.667: INFO: Got endpoints: latency-svc-gnzlq [159.729388ms]
Jan 18 22:22:46.670: INFO: Created: latency-svc-d2rh7
Jan 18 22:22:46.683: INFO: Got endpoints: latency-svc-d2rh7 [168.745479ms]
Jan 18 22:22:46.686: INFO: Created: latency-svc-5mkvf
Jan 18 22:22:46.694: INFO: Got endpoints: latency-svc-5mkvf [170.49418ms]
Jan 18 22:22:46.705: INFO: Created: latency-svc-8mc2t
Jan 18 22:22:46.705: INFO: Created: latency-svc-psvg2
Jan 18 22:22:46.709: INFO: Got endpoints: latency-svc-8mc2t [174.112879ms]
Jan 18 22:22:46.729: INFO: Got endpoints: latency-svc-psvg2 [185.891601ms]
Jan 18 22:22:46.730: INFO: Created: latency-svc-wp4b2
Jan 18 22:22:46.741: INFO: Got endpoints: latency-svc-wp4b2 [192.38451ms]
Jan 18 22:22:46.790: INFO: Created: latency-svc-gbgnl
Jan 18 22:22:46.830: INFO: Got endpoints: latency-svc-gbgnl [273.812487ms]
Jan 18 22:22:46.906: INFO: Created: latency-svc-5lbkk
Jan 18 22:22:46.956: INFO: Created: latency-svc-ls782
Jan 18 22:22:46.957: INFO: Created: latency-svc-zbscb
Jan 18 22:22:46.957: INFO: Got endpoints: latency-svc-zbscb [370.246072ms]
Jan 18 22:22:46.957: INFO: Got endpoints: latency-svc-5lbkk [388.364432ms]
Jan 18 22:22:46.958: INFO: Got endpoints: latency-svc-ls782 [375.00686ms]
Jan 18 22:22:46.958: INFO: Created: latency-svc-ckr5k
Jan 18 22:22:46.958: INFO: Got endpoints: latency-svc-ckr5k [352.866293ms]
Jan 18 22:22:46.958: INFO: Created: latency-svc-9rbmv
Jan 18 22:22:46.958: INFO: Got endpoints: latency-svc-9rbmv [352.77216ms]
Jan 18 22:22:46.995: INFO: Created: latency-svc-l6lws
Jan 18 22:22:46.996: INFO: Got endpoints: latency-svc-l6lws [370.445333ms]
Jan 18 22:22:46.996: INFO: Created: latency-svc-mxwgk
Jan 18 22:22:46.997: INFO: Created: latency-svc-6vztw
Jan 18 22:22:46.997: INFO: Created: latency-svc-8x7gk
Jan 18 22:22:46.998: INFO: Created: latency-svc-qgq26
Jan 18 22:22:46.998: INFO: Created: latency-svc-8rdhv
Jan 18 22:22:46.998: INFO: Created: latency-svc-8tp54
Jan 18 22:22:46.998: INFO: Created: latency-svc-5b4mw
Jan 18 22:22:46.998: INFO: Created: latency-svc-xj295
Jan 18 22:22:46.998: INFO: Created: latency-svc-p8rt9
Jan 18 22:22:46.998: INFO: Created: latency-svc-6c492
Jan 18 22:22:46.998: INFO: Created: latency-svc-jmhr4
Jan 18 22:22:46.998: INFO: Got endpoints: latency-svc-6c492 [40.387387ms]
Jan 18 22:22:46.998: INFO: Got endpoints: latency-svc-8rdhv [303.883679ms]
Jan 18 22:22:47.000: INFO: Got endpoints: latency-svc-mxwgk [363.50915ms]
Jan 18 22:22:47.121: INFO: Got endpoints: latency-svc-6vztw [463.400469ms]
Jan 18 22:22:47.121: INFO: Got endpoints: latency-svc-8x7gk [453.601568ms]
Jan 18 22:22:47.121: INFO: Got endpoints: latency-svc-qgq26 [437.321464ms]
Jan 18 22:22:47.121: INFO: Got endpoints: latency-svc-xj295 [365.574191ms]
Jan 18 22:22:47.121: INFO: Got endpoints: latency-svc-8tp54 [412.069631ms]
Jan 18 22:22:47.121: INFO: Got endpoints: latency-svc-5b4mw [391.884261ms]
Jan 18 22:22:47.121: INFO: Got endpoints: latency-svc-p8rt9 [264.920091ms]
Jan 18 22:22:47.191: INFO: Created: latency-svc-pphj5
Jan 18 22:22:47.261: INFO: Created: latency-svc-n5gws
Jan 18 22:22:47.261: INFO: Created: latency-svc-s45jp
Jan 18 22:22:47.261: INFO: Created: latency-svc-jtw7g
Jan 18 22:22:47.261: INFO: Got endpoints: latency-svc-s45jp [139.968799ms]
Jan 18 22:22:47.261: INFO: Got endpoints: latency-svc-n5gws [139.885088ms]
Jan 18 22:22:47.253: INFO: Got endpoints: latency-svc-pphj5 [295.430505ms]
Jan 18 22:22:47.213: INFO: Got endpoints: latency-svc-jmhr4 [255.384955ms]
Jan 18 22:22:47.213: INFO: Created: latency-svc-fh89b
Jan 18 22:22:47.421: INFO: Got endpoints: latency-svc-fh89b [463.312062ms]
Jan 18 22:22:47.213: INFO: Created: latency-svc-rqgjm
Jan 18 22:22:47.435: INFO: Got endpoints: latency-svc-rqgjm [477.172034ms]
Jan 18 22:22:47.225: INFO: Created: latency-svc-xv68n
Jan 18 22:22:47.460: INFO: Got endpoints: latency-svc-xv68n [462.277096ms]
Jan 18 22:22:47.225: INFO: Created: latency-svc-r7bds
Jan 18 22:22:47.539: INFO: Got endpoints: latency-svc-r7bds [538.745468ms]
Jan 18 22:22:47.225: INFO: Created: latency-svc-dwnjn
Jan 18 22:22:47.539: INFO: Got endpoints: latency-svc-dwnjn [418.516448ms]
Jan 18 22:22:47.229: INFO: Created: latency-svc-4npkc
Jan 18 22:22:47.540: INFO: Got endpoints: latency-svc-4npkc [544.007897ms]
Jan 18 22:22:47.296: INFO: Created: latency-svc-rtwhn
Jan 18 22:22:47.540: INFO: Got endpoints: latency-svc-rtwhn [418.572809ms]
Jan 18 22:22:47.296: INFO: Created: latency-svc-nvq6v
Jan 18 22:22:47.540: INFO: Got endpoints: latency-svc-nvq6v [418.678195ms]
Jan 18 22:22:47.296: INFO: Created: latency-svc-cvwfq
Jan 18 22:22:47.540: INFO: Got endpoints: latency-svc-cvwfq [419.054967ms]
Jan 18 22:22:47.296: INFO: Got endpoints: latency-svc-jtw7g [174.922042ms]
Jan 18 22:22:47.332: INFO: Created: latency-svc-fbcjx
Jan 18 22:22:47.540: INFO: Got endpoints: latency-svc-fbcjx [419.326472ms]
Jan 18 22:22:47.332: INFO: Created: latency-svc-k492d
Jan 18 22:22:47.541: INFO: Got endpoints: latency-svc-k492d [279.638404ms]
Jan 18 22:22:47.373: INFO: Created: latency-svc-kjsrt
Jan 18 22:22:47.541: INFO: Got endpoints: latency-svc-kjsrt [276.334844ms]
Jan 18 22:22:47.373: INFO: Created: latency-svc-4bflc
Jan 18 22:22:47.541: INFO: Got endpoints: latency-svc-4bflc [279.707814ms]
Jan 18 22:22:47.491: INFO: Created: latency-svc-h5b2v
Jan 18 22:22:47.541: INFO: Got endpoints: latency-svc-h5b2v [106.40033ms]
Jan 18 22:22:47.514: INFO: Created: latency-svc-dkxrt
Jan 18 22:22:47.541: INFO: Got endpoints: latency-svc-dkxrt [148.972557ms]
Jan 18 22:22:47.539: INFO: Created: latency-svc-9ztsr
Jan 18 22:22:47.541: INFO: Got endpoints: latency-svc-9ztsr [120.623441ms]
Jan 18 22:22:47.539: INFO: Created: latency-svc-7jxmh
Jan 18 22:22:47.542: INFO: Got endpoints: latency-svc-7jxmh [81.098445ms]
Jan 18 22:22:47.560: INFO: Created: latency-svc-dv99v
Jan 18 22:22:47.562: INFO: Got endpoints: latency-svc-dv99v [22.573395ms]
Jan 18 22:22:47.564: INFO: Created: latency-svc-cmvrl
Jan 18 22:22:47.574: INFO: Got endpoints: latency-svc-cmvrl [35.007914ms]
Jan 18 22:22:47.581: INFO: Created: latency-svc-rbwpk
Jan 18 22:22:47.583: INFO: Created: latency-svc-7k49t
Jan 18 22:22:47.585: INFO: Got endpoints: latency-svc-rbwpk [45.324279ms]
Jan 18 22:22:47.587: INFO: Got endpoints: latency-svc-7k49t [47.973421ms]
Jan 18 22:22:47.590: INFO: Created: latency-svc-s58g4
Jan 18 22:22:47.597: INFO: Created: latency-svc-wpjc4
Jan 18 22:22:47.600: INFO: Got endpoints: latency-svc-s58g4 [60.477608ms]
Jan 18 22:22:47.604: INFO: Got endpoints: latency-svc-wpjc4 [64.251692ms]
Jan 18 22:22:47.615: INFO: Created: latency-svc-lmsq6
Jan 18 22:22:47.615: INFO: Got endpoints: latency-svc-lmsq6 [74.629344ms]
Jan 18 22:22:47.617: INFO: Created: latency-svc-xkc78
Jan 18 22:22:47.627: INFO: Got endpoints: latency-svc-xkc78 [86.168586ms]
Jan 18 22:22:47.629: INFO: Created: latency-svc-q8pqj
Jan 18 22:22:47.634: INFO: Got endpoints: latency-svc-q8pqj [93.236205ms]
Jan 18 22:22:47.637: INFO: Created: latency-svc-hv7gz
Jan 18 22:22:47.641: INFO: Got endpoints: latency-svc-hv7gz [99.718861ms]
Jan 18 22:22:47.647: INFO: Created: latency-svc-vkw2z
Jan 18 22:22:47.651: INFO: Created: latency-svc-bmhqq
Jan 18 22:22:47.652: INFO: Got endpoints: latency-svc-vkw2z [111.153108ms]
Jan 18 22:22:47.665: INFO: Created: latency-svc-8b7xg
Jan 18 22:22:47.665: INFO: Got endpoints: latency-svc-bmhqq [124.747748ms]
Jan 18 22:22:47.665: INFO: Created: latency-svc-7nlls
Jan 18 22:22:47.671: INFO: Got endpoints: latency-svc-7nlls [130.155636ms]
Jan 18 22:22:47.672: INFO: Got endpoints: latency-svc-8b7xg [130.951194ms]
Jan 18 22:22:47.686: INFO: Created: latency-svc-c26rp
Jan 18 22:22:47.689: INFO: Got endpoints: latency-svc-c26rp [147.942886ms]
Jan 18 22:22:47.689: INFO: Created: latency-svc-27zrg
Jan 18 22:22:47.693: INFO: Got endpoints: latency-svc-27zrg [130.299159ms]
Jan 18 22:22:47.707: INFO: Created: latency-svc-6nc89
Jan 18 22:22:47.715: INFO: Created: latency-svc-xldqs
Jan 18 22:22:47.719: INFO: Got endpoints: latency-svc-6nc89 [144.465706ms]
Jan 18 22:22:47.722: INFO: Got endpoints: latency-svc-xldqs [137.15034ms]
Jan 18 22:22:47.732: INFO: Created: latency-svc-xwxpz
Jan 18 22:22:47.736: INFO: Created: latency-svc-sr9sp
Jan 18 22:22:47.744: INFO: Got endpoints: latency-svc-xwxpz [156.941193ms]
Jan 18 22:22:47.750: INFO: Got endpoints: latency-svc-sr9sp [149.810415ms]
Jan 18 22:22:47.753: INFO: Created: latency-svc-54747
Jan 18 22:22:47.759: INFO: Created: latency-svc-24gbf
Jan 18 22:22:47.767: INFO: Got endpoints: latency-svc-54747 [162.07218ms]
Jan 18 22:22:47.772: INFO: Got endpoints: latency-svc-24gbf [156.955056ms]
Jan 18 22:22:47.781: INFO: Created: latency-svc-bz96z
Jan 18 22:22:47.785: INFO: Created: latency-svc-txpcv
Jan 18 22:22:47.786: INFO: Got endpoints: latency-svc-bz96z [159.780855ms]
Jan 18 22:22:47.795: INFO: Got endpoints: latency-svc-txpcv [160.758647ms]
Jan 18 22:22:47.795: INFO: Latencies: [22.573395ms 33.201396ms 35.007914ms 37.511046ms 40.387387ms 45.324279ms 46.929558ms 47.973421ms 53.197254ms 60.477608ms 62.290121ms 64.251692ms 65.354489ms 72.024639ms 73.921299ms 74.291342ms 74.629344ms 77.165701ms 81.098445ms 85.405868ms 86.168586ms 87.053211ms 87.561751ms 90.346583ms 93.236205ms 94.762084ms 96.867345ms 98.862313ms 99.696836ms 99.718861ms 102.50765ms 102.68826ms 104.587225ms 105.601394ms 106.40033ms 111.032668ms 111.153108ms 114.511571ms 119.449395ms 120.623441ms 120.979114ms 124.747748ms 125.503763ms 126.618297ms 129.999194ms 130.155636ms 130.299159ms 130.662901ms 130.951194ms 135.777647ms 136.072957ms 136.103822ms 137.15034ms 139.644315ms 139.885088ms 139.968799ms 144.465706ms 145.17165ms 145.435898ms 145.604408ms 145.713669ms 146.337911ms 146.668663ms 147.942886ms 148.972557ms 149.810415ms 150.435739ms 151.749413ms 151.798693ms 153.161512ms 153.280315ms 153.645731ms 154.098675ms 155.165651ms 156.447225ms 156.770409ms 156.941193ms 156.955056ms 157.909708ms 158.075872ms 158.791981ms 159.227447ms 159.729388ms 159.780855ms 159.907275ms 160.34583ms 160.758647ms 160.79566ms 160.811802ms 160.847852ms 162.07218ms 162.240877ms 162.362664ms 162.800283ms 163.474196ms 164.545733ms 165.873057ms 166.188931ms 166.401024ms 166.86569ms 167.945154ms 168.562739ms 168.745479ms 169.259677ms 169.89536ms 169.950669ms 170.305044ms 170.49418ms 171.254483ms 171.914404ms 172.573337ms 173.033265ms 173.599927ms 174.112879ms 174.470631ms 174.847733ms 174.922042ms 175.657696ms 175.870066ms 176.791067ms 176.934829ms 176.958364ms 177.635419ms 179.376659ms 179.644434ms 180.027096ms 180.249962ms 180.514916ms 180.722257ms 180.785518ms 181.19412ms 181.428564ms 184.08046ms 184.836481ms 185.230474ms 185.783598ms 185.891601ms 186.859725ms 187.805834ms 190.118594ms 191.616421ms 191.737155ms 192.38451ms 194.539147ms 194.989431ms 201.932955ms 209.360359ms 211.829206ms 214.713449ms 219.597598ms 221.221132ms 229.065868ms 236.178604ms 236.277162ms 236.577384ms 236.955214ms 252.331743ms 255.384955ms 261.959272ms 264.920091ms 273.812487ms 276.334844ms 279.638404ms 279.707814ms 282.540006ms 295.430505ms 303.883679ms 307.301875ms 314.696572ms 321.797476ms 326.328991ms 347.727855ms 352.052162ms 352.77216ms 352.866293ms 358.498339ms 361.119018ms 363.50915ms 365.574191ms 368.303991ms 370.195315ms 370.246072ms 370.445333ms 375.00686ms 388.364432ms 391.884261ms 412.069631ms 418.516448ms 418.572809ms 418.678195ms 419.054967ms 419.326472ms 437.321464ms 453.601568ms 462.277096ms 463.312062ms 463.400469ms 477.172034ms 538.745468ms 544.007897ms]
Jan 18 22:22:47.795: INFO: 50 %ile: 167.945154ms
Jan 18 22:22:47.795: INFO: 90 %ile: 370.195315ms
Jan 18 22:22:47.795: INFO: 99 %ile: 538.745468ms
Jan 18 22:22:47.795: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:188
Jan 18 22:22:47.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4149" for this suite.

• [SLOW TEST:6.935 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":356,"completed":24,"skipped":414,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:22:47.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-0031346d-587f-4198-9a1a-a7b89eb26978
STEP: Creating a pod to test consume secrets
Jan 18 22:22:48.858: INFO: Waiting up to 5m0s for pod "pod-secrets-0a4c0351-e55f-4dac-992b-51b51646cc85" in namespace "secrets-6386" to be "Succeeded or Failed"
Jan 18 22:22:48.863: INFO: Pod "pod-secrets-0a4c0351-e55f-4dac-992b-51b51646cc85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.13329ms
Jan 18 22:22:50.867: INFO: Pod "pod-secrets-0a4c0351-e55f-4dac-992b-51b51646cc85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008959527s
Jan 18 22:22:52.878: INFO: Pod "pod-secrets-0a4c0351-e55f-4dac-992b-51b51646cc85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019484194s
Jan 18 22:22:54.895: INFO: Pod "pod-secrets-0a4c0351-e55f-4dac-992b-51b51646cc85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036341974s
STEP: Saw pod success
Jan 18 22:22:54.895: INFO: Pod "pod-secrets-0a4c0351-e55f-4dac-992b-51b51646cc85" satisfied condition "Succeeded or Failed"
Jan 18 22:22:54.905: INFO: Trying to get logs from node ip-10-0-211-217.ec2.internal pod pod-secrets-0a4c0351-e55f-4dac-992b-51b51646cc85 container secret-volume-test: <nil>
STEP: delete the pod
Jan 18 22:22:54.951: INFO: Waiting for pod pod-secrets-0a4c0351-e55f-4dac-992b-51b51646cc85 to disappear
Jan 18 22:22:54.958: INFO: Pod pod-secrets-0a4c0351-e55f-4dac-992b-51b51646cc85 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 18 22:22:54.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6386" for this suite.

• [SLOW TEST:7.157 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":25,"skipped":432,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:22:54.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 22:22:55.953: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 22:22:57.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 22, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 22, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 22, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 22, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 22:23:01.063: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:23:01.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4853-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 22:23:04.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3294" for this suite.
STEP: Destroying namespace "webhook-3294-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:9.442 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":356,"completed":26,"skipped":439,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:23:04.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a suspended cronjob
W0118 22:23:04.474605      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 18 22:28:04.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-344" for this suite.

• [SLOW TEST:300.088 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":356,"completed":27,"skipped":457,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:28:04.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5698
STEP: creating service affinity-clusterip in namespace services-5698
STEP: creating replication controller affinity-clusterip in namespace services-5698
I0118 22:28:04.582386      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5698, replica count: 3
I0118 22:28:07.633428      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 22:28:10.634784      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:28:10.641: INFO: Creating new exec pod
Jan 18 22:28:15.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-5698 exec execpod-affinitydf8l9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 18 22:28:15.781: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 18 22:28:15.781: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:28:15.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-5698 exec execpod-affinitydf8l9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.248.165 80'
Jan 18 22:28:15.888: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.248.165 80\nConnection to 172.30.248.165 80 port [tcp/http] succeeded!\n"
Jan 18 22:28:15.888: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:28:15.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-5698 exec execpod-affinitydf8l9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.248.165:80/ ; done'
Jan 18 22:28:16.047: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.248.165:80/\n"
Jan 18 22:28:16.047: INFO: stdout: "\naffinity-clusterip-pqmxg\naffinity-clusterip-pqmxg\naffinity-clusterip-pqmxg\naffinity-clusterip-pqmxg\naffinity-clusterip-pqmxg\naffinity-clusterip-pqmxg\naffinity-clusterip-pqmxg\naffinity-clusterip-pqmxg\naffinity-clusterip-pqmxg\naffinity-clusterip-pqmxg\naffinity-clusterip-pqmxg\naffinity-clusterip-pqmxg\naffinity-clusterip-pqmxg\naffinity-clusterip-pqmxg\naffinity-clusterip-pqmxg\naffinity-clusterip-pqmxg"
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Received response from host: affinity-clusterip-pqmxg
Jan 18 22:28:16.047: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-5698, will wait for the garbage collector to delete the pods
Jan 18 22:28:16.120: INFO: Deleting ReplicationController affinity-clusterip took: 4.613356ms
Jan 18 22:28:16.224: INFO: Terminating ReplicationController affinity-clusterip pods took: 103.984174ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 18 22:28:18.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5698" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:14.270 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":28,"skipped":483,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:28:18.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 22:28:19.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-788c8053-5bec-4eee-9492-13a1d04adea8" in namespace "projected-9924" to be "Succeeded or Failed"
Jan 18 22:28:19.859: INFO: Pod "downwardapi-volume-788c8053-5bec-4eee-9492-13a1d04adea8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.926137ms
Jan 18 22:28:21.864: INFO: Pod "downwardapi-volume-788c8053-5bec-4eee-9492-13a1d04adea8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008728494s
Jan 18 22:28:23.871: INFO: Pod "downwardapi-volume-788c8053-5bec-4eee-9492-13a1d04adea8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015719904s
Jan 18 22:28:25.877: INFO: Pod "downwardapi-volume-788c8053-5bec-4eee-9492-13a1d04adea8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022251232s
STEP: Saw pod success
Jan 18 22:28:25.877: INFO: Pod "downwardapi-volume-788c8053-5bec-4eee-9492-13a1d04adea8" satisfied condition "Succeeded or Failed"
Jan 18 22:28:25.880: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-788c8053-5bec-4eee-9492-13a1d04adea8 container client-container: <nil>
STEP: delete the pod
Jan 18 22:28:25.902: INFO: Waiting for pod downwardapi-volume-788c8053-5bec-4eee-9492-13a1d04adea8 to disappear
Jan 18 22:28:25.903: INFO: Pod downwardapi-volume-788c8053-5bec-4eee-9492-13a1d04adea8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 18 22:28:25.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9924" for this suite.

• [SLOW TEST:7.137 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":29,"skipped":491,"failed":0}
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:28:25.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-rrwrb in namespace proxy-9226
I0118 22:28:25.963750      22 runners.go:193] Created replication controller with name: proxy-service-rrwrb, namespace: proxy-9226, replica count: 1
I0118 22:28:27.014753      22 runners.go:193] proxy-service-rrwrb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 22:28:28.015815      22 runners.go:193] proxy-service-rrwrb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 22:28:29.016535      22 runners.go:193] proxy-service-rrwrb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0118 22:28:30.016878      22 runners.go:193] proxy-service-rrwrb Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:28:30.023: INFO: setup took 4.089215223s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 18 22:28:30.030: INFO: (0) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 6.937423ms)
Jan 18 22:28:30.030: INFO: (0) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 7.232746ms)
Jan 18 22:28:30.030: INFO: (0) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 7.229515ms)
Jan 18 22:28:30.030: INFO: (0) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 7.150874ms)
Jan 18 22:28:30.031: INFO: (0) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 7.229967ms)
Jan 18 22:28:30.031: INFO: (0) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 7.32223ms)
Jan 18 22:28:30.031: INFO: (0) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 7.510783ms)
Jan 18 22:28:30.031: INFO: (0) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 7.501589ms)
Jan 18 22:28:30.031: INFO: (0) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 7.574373ms)
Jan 18 22:28:30.031: INFO: (0) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 7.507047ms)
Jan 18 22:28:30.031: INFO: (0) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 7.531886ms)
Jan 18 22:28:30.034: INFO: (0) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 10.749208ms)
Jan 18 22:28:30.034: INFO: (0) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 10.869209ms)
Jan 18 22:28:30.034: INFO: (0) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 10.72238ms)
Jan 18 22:28:30.037: INFO: (0) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 13.497862ms)
Jan 18 22:28:30.037: INFO: (0) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 13.67624ms)
Jan 18 22:28:30.040: INFO: (1) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 2.788992ms)
Jan 18 22:28:30.040: INFO: (1) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 3.187474ms)
Jan 18 22:28:30.040: INFO: (1) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 3.168905ms)
Jan 18 22:28:30.040: INFO: (1) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 3.342316ms)
Jan 18 22:28:30.040: INFO: (1) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 3.262024ms)
Jan 18 22:28:30.040: INFO: (1) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.340843ms)
Jan 18 22:28:30.041: INFO: (1) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 3.777663ms)
Jan 18 22:28:30.041: INFO: (1) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.734371ms)
Jan 18 22:28:30.041: INFO: (1) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 3.707358ms)
Jan 18 22:28:30.041: INFO: (1) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 3.781179ms)
Jan 18 22:28:30.041: INFO: (1) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 4.081198ms)
Jan 18 22:28:30.041: INFO: (1) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 4.115102ms)
Jan 18 22:28:30.043: INFO: (1) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 5.923756ms)
Jan 18 22:28:30.043: INFO: (1) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 6.375357ms)
Jan 18 22:28:30.044: INFO: (1) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 6.830276ms)
Jan 18 22:28:30.044: INFO: (1) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 6.979589ms)
Jan 18 22:28:30.047: INFO: (2) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 2.750139ms)
Jan 18 22:28:30.047: INFO: (2) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 2.732413ms)
Jan 18 22:28:30.047: INFO: (2) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 2.981051ms)
Jan 18 22:28:30.047: INFO: (2) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 2.9824ms)
Jan 18 22:28:30.047: INFO: (2) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 2.81701ms)
Jan 18 22:28:30.047: INFO: (2) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 3.25256ms)
Jan 18 22:28:30.047: INFO: (2) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 3.170752ms)
Jan 18 22:28:30.047: INFO: (2) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.32468ms)
Jan 18 22:28:30.048: INFO: (2) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 3.532237ms)
Jan 18 22:28:30.048: INFO: (2) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 3.817339ms)
Jan 18 22:28:30.048: INFO: (2) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 4.043191ms)
Jan 18 22:28:30.048: INFO: (2) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 3.92057ms)
Jan 18 22:28:30.048: INFO: (2) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 4.27909ms)
Jan 18 22:28:30.048: INFO: (2) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 4.268531ms)
Jan 18 22:28:30.048: INFO: (2) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 4.419755ms)
Jan 18 22:28:30.050: INFO: (2) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 6.210958ms)
Jan 18 22:28:30.053: INFO: (3) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 2.550485ms)
Jan 18 22:28:30.053: INFO: (3) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 2.863192ms)
Jan 18 22:28:30.053: INFO: (3) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 2.812041ms)
Jan 18 22:28:30.053: INFO: (3) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 3.177837ms)
Jan 18 22:28:30.054: INFO: (3) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 3.027765ms)
Jan 18 22:28:30.054: INFO: (3) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 3.301552ms)
Jan 18 22:28:30.054: INFO: (3) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 3.658482ms)
Jan 18 22:28:30.054: INFO: (3) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.819126ms)
Jan 18 22:28:30.054: INFO: (3) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 3.842195ms)
Jan 18 22:28:30.054: INFO: (3) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.868127ms)
Jan 18 22:28:30.055: INFO: (3) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 4.246789ms)
Jan 18 22:28:30.055: INFO: (3) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 4.33795ms)
Jan 18 22:28:30.055: INFO: (3) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 4.587487ms)
Jan 18 22:28:30.055: INFO: (3) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 4.666045ms)
Jan 18 22:28:30.055: INFO: (3) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 4.719969ms)
Jan 18 22:28:30.055: INFO: (3) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 4.867497ms)
Jan 18 22:28:30.058: INFO: (4) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 2.672133ms)
Jan 18 22:28:30.058: INFO: (4) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 2.656072ms)
Jan 18 22:28:30.058: INFO: (4) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 2.424173ms)
Jan 18 22:28:30.059: INFO: (4) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 2.834282ms)
Jan 18 22:28:30.059: INFO: (4) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 3.315198ms)
Jan 18 22:28:30.059: INFO: (4) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 2.941731ms)
Jan 18 22:28:30.059: INFO: (4) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.250807ms)
Jan 18 22:28:30.059: INFO: (4) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.035392ms)
Jan 18 22:28:30.059: INFO: (4) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 3.665464ms)
Jan 18 22:28:30.059: INFO: (4) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 3.057474ms)
Jan 18 22:28:30.060: INFO: (4) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 3.967142ms)
Jan 18 22:28:30.060: INFO: (4) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 4.020983ms)
Jan 18 22:28:30.060: INFO: (4) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 4.259925ms)
Jan 18 22:28:30.060: INFO: (4) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 4.381879ms)
Jan 18 22:28:30.060: INFO: (4) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 4.091751ms)
Jan 18 22:28:30.060: INFO: (4) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 4.595492ms)
Jan 18 22:28:30.063: INFO: (5) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 2.409438ms)
Jan 18 22:28:30.063: INFO: (5) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 2.521441ms)
Jan 18 22:28:30.063: INFO: (5) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 2.758626ms)
Jan 18 22:28:30.063: INFO: (5) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 2.802304ms)
Jan 18 22:28:30.063: INFO: (5) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 3.017189ms)
Jan 18 22:28:30.064: INFO: (5) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 3.036347ms)
Jan 18 22:28:30.064: INFO: (5) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.016211ms)
Jan 18 22:28:30.064: INFO: (5) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.291341ms)
Jan 18 22:28:30.064: INFO: (5) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 3.438084ms)
Jan 18 22:28:30.065: INFO: (5) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 4.439793ms)
Jan 18 22:28:30.065: INFO: (5) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 4.352872ms)
Jan 18 22:28:30.065: INFO: (5) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 4.53271ms)
Jan 18 22:28:30.065: INFO: (5) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 4.459677ms)
Jan 18 22:28:30.065: INFO: (5) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 4.527397ms)
Jan 18 22:28:30.065: INFO: (5) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 4.816959ms)
Jan 18 22:28:30.065: INFO: (5) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 4.892065ms)
Jan 18 22:28:30.068: INFO: (6) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 2.842326ms)
Jan 18 22:28:30.069: INFO: (6) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 3.115168ms)
Jan 18 22:28:30.069: INFO: (6) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.156287ms)
Jan 18 22:28:30.069: INFO: (6) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 3.35114ms)
Jan 18 22:28:30.069: INFO: (6) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 3.103958ms)
Jan 18 22:28:30.069: INFO: (6) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 3.384447ms)
Jan 18 22:28:30.069: INFO: (6) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.560512ms)
Jan 18 22:28:30.069: INFO: (6) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 3.742911ms)
Jan 18 22:28:30.069: INFO: (6) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 3.831563ms)
Jan 18 22:28:30.069: INFO: (6) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.833342ms)
Jan 18 22:28:30.069: INFO: (6) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 3.743138ms)
Jan 18 22:28:30.070: INFO: (6) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 4.076711ms)
Jan 18 22:28:30.070: INFO: (6) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 4.429835ms)
Jan 18 22:28:30.070: INFO: (6) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 4.525046ms)
Jan 18 22:28:30.070: INFO: (6) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 4.810942ms)
Jan 18 22:28:30.071: INFO: (6) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 4.787842ms)
Jan 18 22:28:30.074: INFO: (7) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 3.126244ms)
Jan 18 22:28:30.074: INFO: (7) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.729356ms)
Jan 18 22:28:30.074: INFO: (7) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.685437ms)
Jan 18 22:28:30.075: INFO: (7) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.829658ms)
Jan 18 22:28:30.075: INFO: (7) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 3.905727ms)
Jan 18 22:28:30.075: INFO: (7) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 4.558644ms)
Jan 18 22:28:30.075: INFO: (7) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 4.460672ms)
Jan 18 22:28:30.075: INFO: (7) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 4.454282ms)
Jan 18 22:28:30.075: INFO: (7) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 4.597517ms)
Jan 18 22:28:30.075: INFO: (7) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 4.765313ms)
Jan 18 22:28:30.076: INFO: (7) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 4.672224ms)
Jan 18 22:28:30.076: INFO: (7) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 4.719191ms)
Jan 18 22:28:30.077: INFO: (7) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 6.100629ms)
Jan 18 22:28:30.077: INFO: (7) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 6.136844ms)
Jan 18 22:28:30.077: INFO: (7) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 6.308148ms)
Jan 18 22:28:30.077: INFO: (7) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 6.664139ms)
Jan 18 22:28:30.080: INFO: (8) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 2.954436ms)
Jan 18 22:28:30.081: INFO: (8) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 3.016745ms)
Jan 18 22:28:30.081: INFO: (8) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 3.146954ms)
Jan 18 22:28:30.081: INFO: (8) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 3.014086ms)
Jan 18 22:28:30.081: INFO: (8) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.280431ms)
Jan 18 22:28:30.081: INFO: (8) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 3.040594ms)
Jan 18 22:28:30.081: INFO: (8) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 3.312774ms)
Jan 18 22:28:30.081: INFO: (8) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 3.206009ms)
Jan 18 22:28:30.081: INFO: (8) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.352562ms)
Jan 18 22:28:30.081: INFO: (8) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.365901ms)
Jan 18 22:28:30.081: INFO: (8) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 3.838862ms)
Jan 18 22:28:30.082: INFO: (8) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 4.145742ms)
Jan 18 22:28:30.082: INFO: (8) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 3.998794ms)
Jan 18 22:28:30.082: INFO: (8) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 4.010236ms)
Jan 18 22:28:30.082: INFO: (8) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 4.338423ms)
Jan 18 22:28:30.083: INFO: (8) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 5.12955ms)
Jan 18 22:28:30.085: INFO: (9) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 2.140516ms)
Jan 18 22:28:30.086: INFO: (9) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 2.932161ms)
Jan 18 22:28:30.086: INFO: (9) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 3.30969ms)
Jan 18 22:28:30.086: INFO: (9) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 3.206352ms)
Jan 18 22:28:30.086: INFO: (9) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.045472ms)
Jan 18 22:28:30.086: INFO: (9) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 2.748432ms)
Jan 18 22:28:30.086: INFO: (9) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 2.957719ms)
Jan 18 22:28:30.086: INFO: (9) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 2.90583ms)
Jan 18 22:28:30.086: INFO: (9) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 3.084397ms)
Jan 18 22:28:30.087: INFO: (9) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.033247ms)
Jan 18 22:28:30.087: INFO: (9) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 4.100685ms)
Jan 18 22:28:30.087: INFO: (9) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 4.32507ms)
Jan 18 22:28:30.088: INFO: (9) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 4.233529ms)
Jan 18 22:28:30.088: INFO: (9) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 4.361511ms)
Jan 18 22:28:30.088: INFO: (9) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 4.502899ms)
Jan 18 22:28:30.088: INFO: (9) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 4.752326ms)
Jan 18 22:28:30.090: INFO: (10) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 1.882845ms)
Jan 18 22:28:30.093: INFO: (10) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 4.326653ms)
Jan 18 22:28:30.093: INFO: (10) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 4.206974ms)
Jan 18 22:28:30.093: INFO: (10) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 4.355312ms)
Jan 18 22:28:30.093: INFO: (10) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 4.482205ms)
Jan 18 22:28:30.093: INFO: (10) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 4.594586ms)
Jan 18 22:28:30.094: INFO: (10) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 5.409741ms)
Jan 18 22:28:30.094: INFO: (10) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 5.474987ms)
Jan 18 22:28:30.094: INFO: (10) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 5.371974ms)
Jan 18 22:28:30.094: INFO: (10) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 5.489487ms)
Jan 18 22:28:30.094: INFO: (10) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 5.174603ms)
Jan 18 22:28:30.094: INFO: (10) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 5.911561ms)
Jan 18 22:28:30.094: INFO: (10) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 5.965221ms)
Jan 18 22:28:30.094: INFO: (10) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 5.59208ms)
Jan 18 22:28:30.094: INFO: (10) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 5.740153ms)
Jan 18 22:28:30.094: INFO: (10) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 5.804894ms)
Jan 18 22:28:30.097: INFO: (11) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 2.614644ms)
Jan 18 22:28:30.097: INFO: (11) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 2.985166ms)
Jan 18 22:28:30.097: INFO: (11) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 3.091691ms)
Jan 18 22:28:30.098: INFO: (11) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 3.152365ms)
Jan 18 22:28:30.098: INFO: (11) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.152757ms)
Jan 18 22:28:30.098: INFO: (11) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 3.102696ms)
Jan 18 22:28:30.098: INFO: (11) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 3.273799ms)
Jan 18 22:28:30.098: INFO: (11) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.217491ms)
Jan 18 22:28:30.098: INFO: (11) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.246483ms)
Jan 18 22:28:30.098: INFO: (11) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.330895ms)
Jan 18 22:28:30.099: INFO: (11) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 4.151654ms)
Jan 18 22:28:30.099: INFO: (11) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 4.186759ms)
Jan 18 22:28:30.099: INFO: (11) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 4.427037ms)
Jan 18 22:28:30.099: INFO: (11) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 4.468043ms)
Jan 18 22:28:30.099: INFO: (11) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 4.610341ms)
Jan 18 22:28:30.099: INFO: (11) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 4.747469ms)
Jan 18 22:28:30.102: INFO: (12) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.072633ms)
Jan 18 22:28:30.103: INFO: (12) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 3.198066ms)
Jan 18 22:28:30.103: INFO: (12) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 3.442768ms)
Jan 18 22:28:30.103: INFO: (12) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 3.684251ms)
Jan 18 22:28:30.103: INFO: (12) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 3.567189ms)
Jan 18 22:28:30.103: INFO: (12) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 3.637237ms)
Jan 18 22:28:30.103: INFO: (12) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.574677ms)
Jan 18 22:28:30.103: INFO: (12) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.546633ms)
Jan 18 22:28:30.104: INFO: (12) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 4.27916ms)
Jan 18 22:28:30.104: INFO: (12) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 4.304872ms)
Jan 18 22:28:30.104: INFO: (12) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 4.389707ms)
Jan 18 22:28:30.104: INFO: (12) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 4.669861ms)
Jan 18 22:28:30.104: INFO: (12) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 4.743357ms)
Jan 18 22:28:30.104: INFO: (12) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 5.008812ms)
Jan 18 22:28:30.105: INFO: (12) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 5.145404ms)
Jan 18 22:28:30.105: INFO: (12) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 5.212623ms)
Jan 18 22:28:30.107: INFO: (13) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 2.473598ms)
Jan 18 22:28:30.107: INFO: (13) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 2.677989ms)
Jan 18 22:28:30.107: INFO: (13) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 2.539474ms)
Jan 18 22:28:30.108: INFO: (13) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 2.991926ms)
Jan 18 22:28:30.108: INFO: (13) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.061777ms)
Jan 18 22:28:30.108: INFO: (13) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.333164ms)
Jan 18 22:28:30.108: INFO: (13) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 3.430016ms)
Jan 18 22:28:30.108: INFO: (13) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.743553ms)
Jan 18 22:28:30.109: INFO: (13) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 3.964828ms)
Jan 18 22:28:30.109: INFO: (13) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 3.753055ms)
Jan 18 22:28:30.109: INFO: (13) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 4.021332ms)
Jan 18 22:28:30.109: INFO: (13) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 4.004112ms)
Jan 18 22:28:30.111: INFO: (13) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 5.82973ms)
Jan 18 22:28:30.111: INFO: (13) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 5.930445ms)
Jan 18 22:28:30.111: INFO: (13) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 5.959073ms)
Jan 18 22:28:30.111: INFO: (13) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 6.550699ms)
Jan 18 22:28:30.113: INFO: (14) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 2.17528ms)
Jan 18 22:28:30.114: INFO: (14) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 2.650474ms)
Jan 18 22:28:30.114: INFO: (14) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 2.624151ms)
Jan 18 22:28:30.114: INFO: (14) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 2.987178ms)
Jan 18 22:28:30.114: INFO: (14) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 2.971191ms)
Jan 18 22:28:30.114: INFO: (14) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.143416ms)
Jan 18 22:28:30.115: INFO: (14) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 3.207227ms)
Jan 18 22:28:30.115: INFO: (14) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.217721ms)
Jan 18 22:28:30.115: INFO: (14) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 3.237537ms)
Jan 18 22:28:30.115: INFO: (14) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 3.196647ms)
Jan 18 22:28:30.115: INFO: (14) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 3.879602ms)
Jan 18 22:28:30.116: INFO: (14) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 4.159496ms)
Jan 18 22:28:30.116: INFO: (14) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 4.194713ms)
Jan 18 22:28:30.116: INFO: (14) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 5.048449ms)
Jan 18 22:28:30.116: INFO: (14) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 4.870723ms)
Jan 18 22:28:30.117: INFO: (14) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 5.138121ms)
Jan 18 22:28:30.120: INFO: (15) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.141231ms)
Jan 18 22:28:30.120: INFO: (15) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 3.340864ms)
Jan 18 22:28:30.120: INFO: (15) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 3.283922ms)
Jan 18 22:28:30.120: INFO: (15) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 3.66579ms)
Jan 18 22:28:30.120: INFO: (15) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.383065ms)
Jan 18 22:28:30.121: INFO: (15) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 3.705489ms)
Jan 18 22:28:30.121: INFO: (15) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.847525ms)
Jan 18 22:28:30.121: INFO: (15) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 4.015878ms)
Jan 18 22:28:30.121: INFO: (15) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 4.280563ms)
Jan 18 22:28:30.121: INFO: (15) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.993231ms)
Jan 18 22:28:30.121: INFO: (15) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 4.177132ms)
Jan 18 22:28:30.121: INFO: (15) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 4.286437ms)
Jan 18 22:28:30.121: INFO: (15) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 4.757121ms)
Jan 18 22:28:30.123: INFO: (15) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 5.941051ms)
Jan 18 22:28:30.123: INFO: (15) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 6.210494ms)
Jan 18 22:28:30.123: INFO: (15) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 6.162793ms)
Jan 18 22:28:30.127: INFO: (16) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 3.894562ms)
Jan 18 22:28:30.127: INFO: (16) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.634992ms)
Jan 18 22:28:30.127: INFO: (16) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 4.169027ms)
Jan 18 22:28:30.127: INFO: (16) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 4.015106ms)
Jan 18 22:28:30.127: INFO: (16) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 4.038657ms)
Jan 18 22:28:30.127: INFO: (16) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.962984ms)
Jan 18 22:28:30.127: INFO: (16) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 4.066291ms)
Jan 18 22:28:30.128: INFO: (16) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 4.410323ms)
Jan 18 22:28:30.128: INFO: (16) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 4.058838ms)
Jan 18 22:28:30.128: INFO: (16) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 4.650588ms)
Jan 18 22:28:30.128: INFO: (16) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 5.08322ms)
Jan 18 22:28:30.128: INFO: (16) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 5.163395ms)
Jan 18 22:28:30.129: INFO: (16) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 5.161089ms)
Jan 18 22:28:30.129: INFO: (16) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 5.316811ms)
Jan 18 22:28:30.129: INFO: (16) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 5.482575ms)
Jan 18 22:28:30.129: INFO: (16) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 5.447345ms)
Jan 18 22:28:30.132: INFO: (17) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 2.918997ms)
Jan 18 22:28:30.133: INFO: (17) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 2.887057ms)
Jan 18 22:28:30.133: INFO: (17) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 2.90236ms)
Jan 18 22:28:30.133: INFO: (17) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 2.853724ms)
Jan 18 22:28:30.134: INFO: (17) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 4.107908ms)
Jan 18 22:28:30.134: INFO: (17) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 3.919046ms)
Jan 18 22:28:30.134: INFO: (17) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 4.722898ms)
Jan 18 22:28:30.135: INFO: (17) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 4.510559ms)
Jan 18 22:28:30.135: INFO: (17) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 4.600537ms)
Jan 18 22:28:30.135: INFO: (17) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 5.204621ms)
Jan 18 22:28:30.135: INFO: (17) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 4.365204ms)
Jan 18 22:28:30.135: INFO: (17) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 4.700424ms)
Jan 18 22:28:30.136: INFO: (17) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 5.521293ms)
Jan 18 22:28:30.136: INFO: (17) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 5.89767ms)
Jan 18 22:28:30.138: INFO: (17) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 7.50093ms)
Jan 18 22:28:30.138: INFO: (17) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 7.990136ms)
Jan 18 22:28:30.141: INFO: (18) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 3.163219ms)
Jan 18 22:28:30.141: INFO: (18) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 3.28165ms)
Jan 18 22:28:30.142: INFO: (18) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.428837ms)
Jan 18 22:28:30.142: INFO: (18) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.078522ms)
Jan 18 22:28:30.142: INFO: (18) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 3.196934ms)
Jan 18 22:28:30.142: INFO: (18) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.403455ms)
Jan 18 22:28:30.142: INFO: (18) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 3.286482ms)
Jan 18 22:28:30.142: INFO: (18) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 3.537545ms)
Jan 18 22:28:30.142: INFO: (18) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.464931ms)
Jan 18 22:28:30.144: INFO: (18) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 5.846838ms)
Jan 18 22:28:30.144: INFO: (18) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 5.528131ms)
Jan 18 22:28:30.144: INFO: (18) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 5.873032ms)
Jan 18 22:28:30.144: INFO: (18) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 5.79964ms)
Jan 18 22:28:30.144: INFO: (18) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 6.068167ms)
Jan 18 22:28:30.146: INFO: (18) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 7.243018ms)
Jan 18 22:28:30.146: INFO: (18) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 7.335838ms)
Jan 18 22:28:30.149: INFO: (19) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">... (200; 2.472573ms)
Jan 18 22:28:30.149: INFO: (19) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 3.029589ms)
Jan 18 22:28:30.149: INFO: (19) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:160/proxy/: foo (200; 2.730676ms)
Jan 18 22:28:30.149: INFO: (19) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:443/proxy/tlsrewritem... (200; 2.654577ms)
Jan 18 22:28:30.149: INFO: (19) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29/proxy/rewriteme">test</a> (200; 3.33071ms)
Jan 18 22:28:30.150: INFO: (19) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 2.970835ms)
Jan 18 22:28:30.150: INFO: (19) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname2/proxy/: bar (200; 3.896844ms)
Jan 18 22:28:30.150: INFO: (19) /api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/: <a href="/api/v1/namespaces/proxy-9226/pods/proxy-service-rrwrb-l7m29:1080/proxy/rewriteme">test<... (200; 3.54388ms)
Jan 18 22:28:30.150: INFO: (19) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:460/proxy/: tls baz (200; 3.379867ms)
Jan 18 22:28:30.150: INFO: (19) /api/v1/namespaces/proxy-9226/pods/https:proxy-service-rrwrb-l7m29:462/proxy/: tls qux (200; 4.106243ms)
Jan 18 22:28:30.150: INFO: (19) /api/v1/namespaces/proxy-9226/pods/http:proxy-service-rrwrb-l7m29:162/proxy/: bar (200; 3.576501ms)
Jan 18 22:28:30.151: INFO: (19) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname1/proxy/: tls baz (200; 4.188922ms)
Jan 18 22:28:30.151: INFO: (19) /api/v1/namespaces/proxy-9226/services/http:proxy-service-rrwrb:portname1/proxy/: foo (200; 4.745813ms)
Jan 18 22:28:30.151: INFO: (19) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname1/proxy/: foo (200; 4.470619ms)
Jan 18 22:28:30.151: INFO: (19) /api/v1/namespaces/proxy-9226/services/proxy-service-rrwrb:portname2/proxy/: bar (200; 4.498676ms)
Jan 18 22:28:30.151: INFO: (19) /api/v1/namespaces/proxy-9226/services/https:proxy-service-rrwrb:tlsportname2/proxy/: tls qux (200; 4.886016ms)
STEP: deleting ReplicationController proxy-service-rrwrb in namespace proxy-9226, will wait for the garbage collector to delete the pods
Jan 18 22:28:30.210: INFO: Deleting ReplicationController proxy-service-rrwrb took: 5.447125ms
Jan 18 22:28:30.310: INFO: Terminating ReplicationController proxy-service-rrwrb pods took: 100.174483ms
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jan 18 22:28:32.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9226" for this suite.

• [SLOW TEST:6.909 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":356,"completed":30,"skipped":498,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:28:32.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1574
[It] should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 18 22:28:32.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-2190 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 18 22:28:32.909: INFO: stderr: ""
Jan 18 22:28:32.909: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jan 18 22:28:37.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-2190 get pod e2e-test-httpd-pod -o json'
Jan 18 22:28:38.011: INFO: stderr: ""
Jan 18 22:28:38.011: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.ovn.org/pod-networks\": \"{\\\"default\\\":{\\\"ip_addresses\\\":[\\\"10.128.12.39/23\\\"],\\\"mac_address\\\":\\\"0a:58:0a:80:0c:27\\\",\\\"gateway_ips\\\":[\\\"10.128.12.1\\\"],\\\"ip_address\\\":\\\"10.128.12.39/23\\\",\\\"gateway_ip\\\":\\\"10.128.12.1\\\"}}\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"ovn-kubernetes\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.128.12.39\\\"\\n    ],\\n    \\\"mac\\\": \\\"0a:58:0a:80:0c:27\\\",\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"ovn-kubernetes\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.128.12.39\\\"\\n    ],\\n    \\\"mac\\\": \\\"0a:58:0a:80:0c:27\\\",\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-01-18T22:28:32Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2190\",\n        \"resourceVersion\": \"173891\",\n        \"uid\": \"ef3b9834-1570-4034-8673-c1fece292680\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-xc66s\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-219-147.ec2.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c37,c4\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-xc66s\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:28:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:28:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:28:35Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:28:32Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://811053b3e02416d29c918c2e60ac301d2d55f010cc6ab6b1dfa270bee27df02e\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-18T22:28:35Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.219.147\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.128.12.39\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.128.12.39\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-18T22:28:32Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 18 22:28:38.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-2190 replace -f -'
Jan 18 22:28:40.507: INFO: stderr: ""
Jan 18 22:28:40.507: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1578
Jan 18 22:28:40.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-2190 delete pods e2e-test-httpd-pod'
Jan 18 22:28:42.757: INFO: stderr: ""
Jan 18 22:28:42.757: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 22:28:42.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2190" for this suite.

• [SLOW TEST:9.952 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1571
    should update a single-container pod's image  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":356,"completed":31,"skipped":542,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:28:42.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:28:43.821: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-676683c7-f63b-4d9a-8388-819174640718" in namespace "security-context-test-9205" to be "Succeeded or Failed"
Jan 18 22:28:43.824: INFO: Pod "busybox-privileged-false-676683c7-f63b-4d9a-8388-819174640718": Phase="Pending", Reason="", readiness=false. Elapsed: 2.984518ms
Jan 18 22:28:45.831: INFO: Pod "busybox-privileged-false-676683c7-f63b-4d9a-8388-819174640718": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010324577s
Jan 18 22:28:47.835: INFO: Pod "busybox-privileged-false-676683c7-f63b-4d9a-8388-819174640718": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014135671s
Jan 18 22:28:49.840: INFO: Pod "busybox-privileged-false-676683c7-f63b-4d9a-8388-819174640718": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018918305s
Jan 18 22:28:49.840: INFO: Pod "busybox-privileged-false-676683c7-f63b-4d9a-8388-819174640718" satisfied condition "Succeeded or Failed"
Jan 18 22:28:49.845: INFO: Got logs for pod "busybox-privileged-false-676683c7-f63b-4d9a-8388-819174640718": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 18 22:28:49.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9205" for this suite.

• [SLOW TEST:7.081 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:234
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":32,"skipped":556,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:28:49.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
W0118 22:28:49.911287      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:28:49.911: INFO: Waiting up to 5m0s for pod "pod-8817711a-af6a-4c72-b7f1-381ea64bc61b" in namespace "emptydir-4772" to be "Succeeded or Failed"
Jan 18 22:28:49.913: INFO: Pod "pod-8817711a-af6a-4c72-b7f1-381ea64bc61b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.108274ms
Jan 18 22:28:51.920: INFO: Pod "pod-8817711a-af6a-4c72-b7f1-381ea64bc61b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009554351s
Jan 18 22:28:53.924: INFO: Pod "pod-8817711a-af6a-4c72-b7f1-381ea64bc61b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013114478s
Jan 18 22:28:55.931: INFO: Pod "pod-8817711a-af6a-4c72-b7f1-381ea64bc61b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01973827s
STEP: Saw pod success
Jan 18 22:28:55.931: INFO: Pod "pod-8817711a-af6a-4c72-b7f1-381ea64bc61b" satisfied condition "Succeeded or Failed"
Jan 18 22:28:55.933: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-8817711a-af6a-4c72-b7f1-381ea64bc61b container test-container: <nil>
STEP: delete the pod
Jan 18 22:28:55.951: INFO: Waiting for pod pod-8817711a-af6a-4c72-b7f1-381ea64bc61b to disappear
Jan 18 22:28:55.953: INFO: Pod pod-8817711a-af6a-4c72-b7f1-381ea64bc61b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 18 22:28:55.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4772" for this suite.

• [SLOW TEST:6.109 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":33,"skipped":559,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:28:55.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 22:28:56.009: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 22:29:56.233: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:29:56.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:29:56.294: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jan 18 22:29:56.297: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:188
Jan 18 22:29:56.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6600" for this suite.
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 18 22:29:56.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1535" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:60.468 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":356,"completed":34,"skipped":561,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:29:56.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:29:57.487: INFO: The status of Pod busybox-host-aliases2fa4762f-4564-4baa-9a23-95e982954915 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:29:59.489: INFO: The status of Pod busybox-host-aliases2fa4762f-4564-4baa-9a23-95e982954915 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:30:01.490: INFO: The status of Pod busybox-host-aliases2fa4762f-4564-4baa-9a23-95e982954915 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 18 22:30:01.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4600" for this suite.

• [SLOW TEST:5.078 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox Pod with hostAliases
  test/e2e/common/node/kubelet.go:139
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":35,"skipped":571,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:30:01.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0118 22:30:01.552719      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 18 22:30:05.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7490" for this suite.
•{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":356,"completed":36,"skipped":606,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:30:05.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1412
STEP: creating an pod
Jan 18 22:30:05.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3547 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.36 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 18 22:30:05.699: INFO: stderr: ""
Jan 18 22:30:05.699: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for log generator to start.
Jan 18 22:30:05.699: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 18 22:30:05.699: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3547" to be "running and ready, or succeeded"
Jan 18 22:30:05.707: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.122976ms
Jan 18 22:30:07.711: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011856022s
Jan 18 22:30:09.715: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.016272716s
Jan 18 22:30:09.715: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 18 22:30:09.715: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jan 18 22:30:09.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3547 logs logs-generator logs-generator'
Jan 18 22:30:09.770: INFO: stderr: ""
Jan 18 22:30:09.770: INFO: stdout: "I0118 22:30:08.349118       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/glt 238\nI0118 22:30:08.548979       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/tnzr 361\nI0118 22:30:08.749790       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/d6hv 469\nI0118 22:30:08.949020       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/w8r2 525\nI0118 22:30:09.149312       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/4rn 208\nI0118 22:30:09.349620       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/gbp 431\nI0118 22:30:09.549923       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/cls 279\nI0118 22:30:09.749220       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pv8k 527\n"
STEP: limiting log lines
Jan 18 22:30:09.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3547 logs logs-generator logs-generator --tail=1'
Jan 18 22:30:09.818: INFO: stderr: ""
Jan 18 22:30:09.818: INFO: stdout: "I0118 22:30:09.749220       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pv8k 527\n"
Jan 18 22:30:09.818: INFO: got output "I0118 22:30:09.749220       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pv8k 527\n"
STEP: limiting log bytes
Jan 18 22:30:09.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3547 logs logs-generator logs-generator --limit-bytes=1'
Jan 18 22:30:09.870: INFO: stderr: ""
Jan 18 22:30:09.870: INFO: stdout: "I"
Jan 18 22:30:09.870: INFO: got output "I"
STEP: exposing timestamps
Jan 18 22:30:09.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3547 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 18 22:30:09.924: INFO: stderr: ""
Jan 18 22:30:09.924: INFO: stdout: "2023-01-18T22:30:09.749262210Z I0118 22:30:09.749220       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pv8k 527\n"
Jan 18 22:30:09.924: INFO: got output "2023-01-18T22:30:09.749262210Z I0118 22:30:09.749220       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pv8k 527\n"
STEP: restricting to a time range
Jan 18 22:30:12.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3547 logs logs-generator logs-generator --since=1s'
Jan 18 22:30:12.477: INFO: stderr: ""
Jan 18 22:30:12.477: INFO: stdout: "I0118 22:30:11.559337       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/8bc 500\nI0118 22:30:11.749614       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/bk6 422\nI0118 22:30:11.949904       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/d9rl 559\nI0118 22:30:12.149200       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/bjdn 446\nI0118 22:30:12.349385       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/p6h 306\n"
Jan 18 22:30:12.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3547 logs logs-generator logs-generator --since=24h'
Jan 18 22:30:12.532: INFO: stderr: ""
Jan 18 22:30:12.532: INFO: stdout: "I0118 22:30:08.349118       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/glt 238\nI0118 22:30:08.548979       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/tnzr 361\nI0118 22:30:08.749790       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/d6hv 469\nI0118 22:30:08.949020       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/w8r2 525\nI0118 22:30:09.149312       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/4rn 208\nI0118 22:30:09.349620       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/gbp 431\nI0118 22:30:09.549923       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/cls 279\nI0118 22:30:09.749220       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pv8k 527\nI0118 22:30:09.949523       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/ghvs 217\nI0118 22:30:10.149823       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/hbx 425\nI0118 22:30:10.349064       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/8dfc 524\nI0118 22:30:10.549379       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/6d9h 501\nI0118 22:30:10.749682       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/569 437\nI0118 22:30:10.948924       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/2tf2 486\nI0118 22:30:11.149226       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/9lqv 250\nI0118 22:30:11.349392       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/gxsg 337\nI0118 22:30:11.559337       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/8bc 500\nI0118 22:30:11.749614       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/bk6 422\nI0118 22:30:11.949904       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/d9rl 559\nI0118 22:30:12.149200       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/bjdn 446\nI0118 22:30:12.349385       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/p6h 306\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1417
Jan 18 22:30:12.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3547 delete pod logs-generator'
Jan 18 22:30:13.957: INFO: stderr: ""
Jan 18 22:30:13.957: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 22:30:13.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3547" for this suite.

• [SLOW TEST:8.385 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1409
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":356,"completed":37,"skipped":671,"failed":0}
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:30:13.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 18 22:30:14.082: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:30:16.089: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:30:18.089: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 18 22:30:18.111: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:30:20.117: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 18 22:30:20.138: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 22:30:20.151: INFO: Pod pod-with-poststart-http-hook still exists
Jan 18 22:30:22.152: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 22:30:22.156: INFO: Pod pod-with-poststart-http-hook still exists
Jan 18 22:30:24.155: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 22:30:24.173: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 18 22:30:24.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5991" for this suite.

• [SLOW TEST:10.325 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":356,"completed":38,"skipped":671,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:30:24.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 22:30:24.978: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 22:30:26.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 30, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 30, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 30, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 30, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 22:30:30.003: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 22:30:40.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7617" for this suite.
STEP: Destroying namespace "webhook-7617-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:15.847 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":356,"completed":39,"skipped":674,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:30:40.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jan 18 22:30:40.369: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jan 18 22:30:40.390: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 18 22:30:40.390: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
W0118 22:30:40.460186      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:30:40.463: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 18 22:30:40.463: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jan 18 22:30:40.485: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 18 22:30:40.485: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jan 18 22:30:47.549: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:188
Jan 18 22:30:47.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-5803" for this suite.

• [SLOW TEST:7.261 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":356,"completed":40,"skipped":682,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:30:47.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 18 22:30:48.639: INFO: The status of Pod annotationupdatea333302b-3138-4cb0-a6b3-64b03d51378a is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:30:50.646: INFO: The status of Pod annotationupdatea333302b-3138-4cb0-a6b3-64b03d51378a is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:30:52.646: INFO: The status of Pod annotationupdatea333302b-3138-4cb0-a6b3-64b03d51378a is Running (Ready = true)
Jan 18 22:30:53.175: INFO: Successfully updated pod "annotationupdatea333302b-3138-4cb0-a6b3-64b03d51378a"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 18 22:30:57.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7236" for this suite.

• [SLOW TEST:9.623 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":41,"skipped":711,"failed":0}
SSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:30:57.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0118 22:30:57.281402      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 18 22:30:57.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9800" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","total":356,"completed":42,"skipped":715,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:30:57.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod test-webserver-a008c912-ccd6-4f65-b867-41e63ff3448b in namespace container-probe-5776
Jan 18 22:31:02.464: INFO: Started pod test-webserver-a008c912-ccd6-4f65-b867-41e63ff3448b in namespace container-probe-5776
STEP: checking the pod's current state and verifying that restartCount is present
Jan 18 22:31:02.466: INFO: Initial restart count of pod test-webserver-a008c912-ccd6-4f65-b867-41e63ff3448b is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 18 22:35:03.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5776" for this suite.

• [SLOW TEST:245.860 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":43,"skipped":734,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:35:03.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:35:03.307: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 18 22:35:03.317: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:35:03.317: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Jan 18 22:35:03.361: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:35:03.361: INFO: Node ip-10-0-211-217.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:35:04.366: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:35:04.366: INFO: Node ip-10-0-211-217.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:35:05.366: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:35:05.366: INFO: Node ip-10-0-211-217.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:35:06.366: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:35:06.366: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 18 22:35:06.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:35:06.389: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 18 22:35:07.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:35:07.394: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 18 22:35:07.407: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:35:07.407: INFO: Node ip-10-0-211-217.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:35:08.413: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:35:08.413: INFO: Node ip-10-0-211-217.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:35:09.411: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:35:09.411: INFO: Node ip-10-0-211-217.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:35:10.411: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:35:10.412: INFO: Node ip-10-0-211-217.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:35:11.412: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:35:11.412: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9791, will wait for the garbage collector to delete the pods
Jan 18 22:35:11.474: INFO: Deleting DaemonSet.extensions daemon-set took: 6.277158ms
Jan 18 22:35:11.575: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.855025ms
Jan 18 22:35:13.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:35:13.786: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 22:35:13.789: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"187186"},"items":null}

Jan 18 22:35:13.793: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"187187"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 18 22:35:13.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9791" for this suite.

• [SLOW TEST:10.636 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":356,"completed":44,"skipped":786,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:35:13.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 18 22:35:13.915: INFO: Waiting up to 5m0s for pod "pod-ebb773a7-6ebf-43b6-9a56-84daa5d169d2" in namespace "emptydir-8944" to be "Succeeded or Failed"
Jan 18 22:35:13.918: INFO: Pod "pod-ebb773a7-6ebf-43b6-9a56-84daa5d169d2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.392119ms
Jan 18 22:35:15.924: INFO: Pod "pod-ebb773a7-6ebf-43b6-9a56-84daa5d169d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00892429s
Jan 18 22:35:17.927: INFO: Pod "pod-ebb773a7-6ebf-43b6-9a56-84daa5d169d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012706148s
Jan 18 22:35:19.937: INFO: Pod "pod-ebb773a7-6ebf-43b6-9a56-84daa5d169d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021876707s
STEP: Saw pod success
Jan 18 22:35:19.937: INFO: Pod "pod-ebb773a7-6ebf-43b6-9a56-84daa5d169d2" satisfied condition "Succeeded or Failed"
Jan 18 22:35:19.939: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-ebb773a7-6ebf-43b6-9a56-84daa5d169d2 container test-container: <nil>
STEP: delete the pod
Jan 18 22:35:19.972: INFO: Waiting for pod pod-ebb773a7-6ebf-43b6-9a56-84daa5d169d2 to disappear
Jan 18 22:35:19.974: INFO: Pod pod-ebb773a7-6ebf-43b6-9a56-84daa5d169d2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 18 22:35:19.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8944" for this suite.

• [SLOW TEST:6.145 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":45,"skipped":800,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:35:19.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 18 22:35:21.033: INFO: Waiting up to 5m0s for pod "downward-api-44e8dacc-b2e9-4fdb-8f47-f7ec519000ab" in namespace "downward-api-7106" to be "Succeeded or Failed"
Jan 18 22:35:21.038: INFO: Pod "downward-api-44e8dacc-b2e9-4fdb-8f47-f7ec519000ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.771523ms
Jan 18 22:35:23.042: INFO: Pod "downward-api-44e8dacc-b2e9-4fdb-8f47-f7ec519000ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009496433s
Jan 18 22:35:25.053: INFO: Pod "downward-api-44e8dacc-b2e9-4fdb-8f47-f7ec519000ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019698778s
Jan 18 22:35:27.058: INFO: Pod "downward-api-44e8dacc-b2e9-4fdb-8f47-f7ec519000ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024667082s
STEP: Saw pod success
Jan 18 22:35:27.058: INFO: Pod "downward-api-44e8dacc-b2e9-4fdb-8f47-f7ec519000ab" satisfied condition "Succeeded or Failed"
Jan 18 22:35:27.060: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downward-api-44e8dacc-b2e9-4fdb-8f47-f7ec519000ab container dapi-container: <nil>
STEP: delete the pod
Jan 18 22:35:27.075: INFO: Waiting for pod downward-api-44e8dacc-b2e9-4fdb-8f47-f7ec519000ab to disappear
Jan 18 22:35:27.077: INFO: Pod downward-api-44e8dacc-b2e9-4fdb-8f47-f7ec519000ab no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 18 22:35:27.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7106" for this suite.

• [SLOW TEST:7.103 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":356,"completed":46,"skipped":811,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:35:27.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pods
Jan 18 22:35:28.124: INFO: created test-pod-1
Jan 18 22:35:28.136: INFO: created test-pod-2
Jan 18 22:35:28.147: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running
Jan 18 22:35:28.147: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7964' to be running and ready
Jan 18 22:35:28.157: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:35:28.157: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:35:28.157: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:35:28.157: INFO: 0 / 3 pods in namespace 'pods-7964' are running and ready (0 seconds elapsed)
Jan 18 22:35:28.157: INFO: expected 0 pod replicas in namespace 'pods-7964', 0 are Running and Ready.
Jan 18 22:35:28.157: INFO: POD         NODE                          PHASE    GRACE  CONDITIONS
Jan 18 22:35:28.157: INFO: test-pod-1  ip-10-0-219-147.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC  }]
Jan 18 22:35:28.157: INFO: test-pod-2  ip-10-0-211-217.ec2.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC  }]
Jan 18 22:35:28.157: INFO: test-pod-3                                Pending         []
Jan 18 22:35:28.157: INFO: 
Jan 18 22:35:30.166: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:35:30.166: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:35:30.166: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:35:30.166: INFO: 0 / 3 pods in namespace 'pods-7964' are running and ready (2 seconds elapsed)
Jan 18 22:35:30.166: INFO: expected 0 pod replicas in namespace 'pods-7964', 0 are Running and Ready.
Jan 18 22:35:30.166: INFO: POD         NODE                          PHASE    GRACE  CONDITIONS
Jan 18 22:35:30.166: INFO: test-pod-1  ip-10-0-219-147.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC  }]
Jan 18 22:35:30.166: INFO: test-pod-2  ip-10-0-211-217.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC  }]
Jan 18 22:35:30.167: INFO: test-pod-3  ip-10-0-219-147.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:35:28 +0000 UTC  }]
Jan 18 22:35:30.167: INFO: 
Jan 18 22:35:32.165: INFO: 3 / 3 pods in namespace 'pods-7964' are running and ready (4 seconds elapsed)
Jan 18 22:35:32.165: INFO: expected 0 pod replicas in namespace 'pods-7964', 0 are Running and Ready.
STEP: waiting for all pods to be deleted
Jan 18 22:35:32.185: INFO: Pod quantity 3 is different from expected quantity 0
Jan 18 22:35:33.193: INFO: Pod quantity 3 is different from expected quantity 0
Jan 18 22:35:34.190: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 18 22:35:35.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7964" for this suite.

• [SLOW TEST:8.115 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":356,"completed":47,"skipped":836,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:35:35.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 18 22:35:41.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4565" for this suite.

• [SLOW TEST:6.099 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":356,"completed":48,"skipped":859,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:35:41.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-3e677ce8-833b-4e2f-a7c7-089c274b1d2a
STEP: Creating a pod to test consume secrets
Jan 18 22:35:41.380: INFO: Waiting up to 5m0s for pod "pod-secrets-cb754390-223e-44d7-aba6-6cd18ebb1f72" in namespace "secrets-6336" to be "Succeeded or Failed"
Jan 18 22:35:41.385: INFO: Pod "pod-secrets-cb754390-223e-44d7-aba6-6cd18ebb1f72": Phase="Pending", Reason="", readiness=false. Elapsed: 5.075731ms
Jan 18 22:35:43.394: INFO: Pod "pod-secrets-cb754390-223e-44d7-aba6-6cd18ebb1f72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014392473s
Jan 18 22:35:45.401: INFO: Pod "pod-secrets-cb754390-223e-44d7-aba6-6cd18ebb1f72": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021356165s
Jan 18 22:35:47.408: INFO: Pod "pod-secrets-cb754390-223e-44d7-aba6-6cd18ebb1f72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02818382s
STEP: Saw pod success
Jan 18 22:35:47.408: INFO: Pod "pod-secrets-cb754390-223e-44d7-aba6-6cd18ebb1f72" satisfied condition "Succeeded or Failed"
Jan 18 22:35:47.410: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-secrets-cb754390-223e-44d7-aba6-6cd18ebb1f72 container secret-volume-test: <nil>
STEP: delete the pod
Jan 18 22:35:47.425: INFO: Waiting for pod pod-secrets-cb754390-223e-44d7-aba6-6cd18ebb1f72 to disappear
Jan 18 22:35:47.427: INFO: Pod pod-secrets-cb754390-223e-44d7-aba6-6cd18ebb1f72 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 18 22:35:47.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6336" for this suite.

• [SLOW TEST:6.134 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":49,"skipped":870,"failed":0}
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:35:47.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:35:47.466: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-20133b5e-1326-4948-a998-41a25a5e239f
STEP: Creating secret with name s-test-opt-upd-d5983faf-fd73-4c3f-a830-8c10ddd0101a
STEP: Creating the pod
Jan 18 22:35:47.519: INFO: The status of Pod pod-projected-secrets-fb6a3658-a53e-4665-975a-595296604d03 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:35:49.523: INFO: The status of Pod pod-projected-secrets-fb6a3658-a53e-4665-975a-595296604d03 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:35:51.526: INFO: The status of Pod pod-projected-secrets-fb6a3658-a53e-4665-975a-595296604d03 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-20133b5e-1326-4948-a998-41a25a5e239f
STEP: Updating secret s-test-opt-upd-d5983faf-fd73-4c3f-a830-8c10ddd0101a
STEP: Creating secret with name s-test-opt-create-856cee04-1ba1-4f9f-9fbb-63cccf1ca73a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 18 22:35:53.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7859" for this suite.

• [SLOW TEST:6.168 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":50,"skipped":870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:35:53.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ForbidConcurrent cronjob
W0118 22:35:53.635390      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 18 22:41:01.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9438" for this suite.

• [SLOW TEST:308.065 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":356,"completed":51,"skipped":905,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:41:01.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:41:01.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jan 18 22:41:11.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 create -f -'
Jan 18 22:41:12.501: INFO: stderr: ""
Jan 18 22:41:12.501: INFO: stdout: "e2e-test-crd-publish-openapi-2576-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 18 22:41:12.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 delete e2e-test-crd-publish-openapi-2576-crds test-cr'
Jan 18 22:41:12.577: INFO: stderr: ""
Jan 18 22:41:12.577: INFO: stdout: "e2e-test-crd-publish-openapi-2576-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 18 22:41:12.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 apply -f -'
Jan 18 22:41:13.865: INFO: stderr: ""
Jan 18 22:41:13.865: INFO: stdout: "e2e-test-crd-publish-openapi-2576-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 18 22:41:13.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 delete e2e-test-crd-publish-openapi-2576-crds test-cr'
Jan 18 22:41:13.945: INFO: stderr: ""
Jan 18 22:41:13.945: INFO: stdout: "e2e-test-crd-publish-openapi-2576-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 18 22:41:13.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-2054 explain e2e-test-crd-publish-openapi-2576-crds'
Jan 18 22:41:15.522: INFO: stderr: ""
Jan 18 22:41:15.522: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2576-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 22:41:24.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2054" for this suite.

• [SLOW TEST:22.841 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":356,"completed":52,"skipped":942,"failed":0}
SSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:41:24.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-ce1eac1e-36b0-440c-b5c6-381b44b89d2c
STEP: Creating a pod to test consume secrets
W0118 22:41:24.558760      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:41:24.558: INFO: Waiting up to 5m0s for pod "pod-secrets-fb9a5971-d0b5-423c-91ea-0d46cde91e9c" in namespace "secrets-7411" to be "Succeeded or Failed"
Jan 18 22:41:24.560: INFO: Pod "pod-secrets-fb9a5971-d0b5-423c-91ea-0d46cde91e9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015567ms
Jan 18 22:41:26.563: INFO: Pod "pod-secrets-fb9a5971-d0b5-423c-91ea-0d46cde91e9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004882571s
Jan 18 22:41:28.567: INFO: Pod "pod-secrets-fb9a5971-d0b5-423c-91ea-0d46cde91e9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008310667s
Jan 18 22:41:30.571: INFO: Pod "pod-secrets-fb9a5971-d0b5-423c-91ea-0d46cde91e9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012415559s
STEP: Saw pod success
Jan 18 22:41:30.571: INFO: Pod "pod-secrets-fb9a5971-d0b5-423c-91ea-0d46cde91e9c" satisfied condition "Succeeded or Failed"
Jan 18 22:41:30.582: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-secrets-fb9a5971-d0b5-423c-91ea-0d46cde91e9c container secret-env-test: <nil>
STEP: delete the pod
Jan 18 22:41:30.618: INFO: Waiting for pod pod-secrets-fb9a5971-d0b5-423c-91ea-0d46cde91e9c to disappear
Jan 18 22:41:30.620: INFO: Pod pod-secrets-fb9a5971-d0b5-423c-91ea-0d46cde91e9c no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 18 22:41:30.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7411" for this suite.

• [SLOW TEST:6.120 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":356,"completed":53,"skipped":945,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:41:30.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:41:30.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 18 22:41:31.726: INFO: The status of Pod pod-exec-websocket-75483b52-73b9-45fb-842e-4206ad173a6f is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:41:33.730: INFO: The status of Pod pod-exec-websocket-75483b52-73b9-45fb-842e-4206ad173a6f is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:41:35.730: INFO: The status of Pod pod-exec-websocket-75483b52-73b9-45fb-842e-4206ad173a6f is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 18 22:41:35.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4001" for this suite.

• [SLOW TEST:5.198 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":356,"completed":54,"skipped":958,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:41:35.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:41:35.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 22:41:42.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3803" for this suite.

• [SLOW TEST:6.744 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":356,"completed":55,"skipped":965,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:41:42.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 22:41:43.210: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 22:41:45.230: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 41, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 41, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 41, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 41, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 22:41:48.242: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 22:41:48.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3870" for this suite.
STEP: Destroying namespace "webhook-3870-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.806 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":356,"completed":56,"skipped":971,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:41:48.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:188
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 18 22:41:48.437: INFO: starting watch
STEP: patching
STEP: updating
Jan 18 22:41:48.470: INFO: waiting for watch events with expected annotations
Jan 18 22:41:48.470: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:188
Jan 18 22:41:48.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-6332" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":356,"completed":57,"skipped":991,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:41:48.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating server pod server in namespace prestop-9178
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9178
STEP: Deleting pre-stop pod
Jan 18 22:42:02.821: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:188
Jan 18 22:42:02.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9178" for this suite.

• [SLOW TEST:14.214 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":356,"completed":58,"skipped":1029,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:42:02.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-4389
STEP: creating service affinity-clusterip-transition in namespace services-4389
STEP: creating replication controller affinity-clusterip-transition in namespace services-4389
W0118 22:42:02.904784      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "affinity-clusterip-transition" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-clusterip-transition" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-clusterip-transition" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-clusterip-transition" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0118 22:42:02.904849      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4389, replica count: 3
I0118 22:42:05.956081      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 22:42:08.957755      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:42:08.962: INFO: Creating new exec pod
Jan 18 22:42:13.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-4389 exec execpod-affinity9vw4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 18 22:42:14.118: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 18 22:42:14.118: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:42:14.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-4389 exec execpod-affinity9vw4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.135.240 80'
Jan 18 22:42:14.238: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.135.240 80\nConnection to 172.30.135.240 80 port [tcp/http] succeeded!\n"
Jan 18 22:42:14.238: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:42:14.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-4389 exec execpod-affinity9vw4g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.135.240:80/ ; done'
Jan 18 22:42:14.416: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n"
Jan 18 22:42:14.416: INFO: stdout: "\naffinity-clusterip-transition-7vqcf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-7vqcf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-4lqs5\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-4lqs5\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-7vqcf\naffinity-clusterip-transition-7vqcf\naffinity-clusterip-transition-4lqs5\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-4lqs5\naffinity-clusterip-transition-mg5gf"
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-7vqcf
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-7vqcf
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-4lqs5
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-4lqs5
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-7vqcf
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-7vqcf
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-4lqs5
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-4lqs5
Jan 18 22:42:14.416: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-4389 exec execpod-affinity9vw4g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.135.240:80/ ; done'
Jan 18 22:42:14.607: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.135.240:80/\n"
Jan 18 22:42:14.607: INFO: stdout: "\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf\naffinity-clusterip-transition-mg5gf"
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Received response from host: affinity-clusterip-transition-mg5gf
Jan 18 22:42:14.607: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4389, will wait for the garbage collector to delete the pods
Jan 18 22:42:14.676: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.652299ms
Jan 18 22:42:14.777: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.658888ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 18 22:42:17.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4389" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:14.461 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":59,"skipped":1031,"failed":0}
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:42:17.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jan 18 22:42:17.407: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:42:19.411: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:42:21.413: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:42:23.410: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.200.13 on the node which pod1 resides and expect scheduled
Jan 18 22:42:23.427: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:42:25.430: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:42:27.430: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.200.13 but use UDP protocol on the node which pod2 resides
Jan 18 22:42:27.444: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:42:29.447: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:42:31.447: INFO: The status of Pod pod3 is Running (Ready = true)
Jan 18 22:42:31.458: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:42:33.462: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jan 18 22:42:33.465: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.200.13 http://127.0.0.1:54323/hostname] Namespace:hostport-4357 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:42:33.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 22:42:33.465: INFO: ExecWithOptions: Clientset creation
Jan 18 22:42:33.465: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-4357/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.200.13+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.200.13, port: 54323
Jan 18 22:42:33.556: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.200.13:54323/hostname] Namespace:hostport-4357 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:42:33.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 22:42:33.556: INFO: ExecWithOptions: Clientset creation
Jan 18 22:42:33.556: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-4357/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.200.13%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.200.13, port: 54323 UDP
Jan 18 22:42:33.648: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.200.13 54323] Namespace:hostport-4357 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:42:33.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 22:42:33.649: INFO: ExecWithOptions: Clientset creation
Jan 18 22:42:33.649: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-4357/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+10.0.200.13+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:188
Jan 18 22:42:38.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-4357" for this suite.

• [SLOW TEST:21.449 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":356,"completed":60,"skipped":1031,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:42:38.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-0a332d28-704e-4939-8749-5092345ac396
STEP: Creating a pod to test consume secrets
W0118 22:42:38.823401      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:42:38.823: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8534c744-1d2e-41c4-be30-7da9b458af0e" in namespace "projected-132" to be "Succeeded or Failed"
Jan 18 22:42:38.827: INFO: Pod "pod-projected-secrets-8534c744-1d2e-41c4-be30-7da9b458af0e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.514203ms
Jan 18 22:42:40.832: INFO: Pod "pod-projected-secrets-8534c744-1d2e-41c4-be30-7da9b458af0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009359896s
Jan 18 22:42:42.835: INFO: Pod "pod-projected-secrets-8534c744-1d2e-41c4-be30-7da9b458af0e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012187003s
Jan 18 22:42:44.837: INFO: Pod "pod-projected-secrets-8534c744-1d2e-41c4-be30-7da9b458af0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01445579s
STEP: Saw pod success
Jan 18 22:42:44.837: INFO: Pod "pod-projected-secrets-8534c744-1d2e-41c4-be30-7da9b458af0e" satisfied condition "Succeeded or Failed"
Jan 18 22:42:44.840: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-projected-secrets-8534c744-1d2e-41c4-be30-7da9b458af0e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 18 22:42:44.856: INFO: Waiting for pod pod-projected-secrets-8534c744-1d2e-41c4-be30-7da9b458af0e to disappear
Jan 18 22:42:44.858: INFO: Pod pod-projected-secrets-8534c744-1d2e-41c4-be30-7da9b458af0e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 18 22:42:44.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-132" for this suite.

• [SLOW TEST:6.113 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":61,"skipped":1039,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:42:44.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 18 22:42:50.981: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 18 22:42:50.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2280" for this suite.

• [SLOW TEST:6.144 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":62,"skipped":1054,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:42:51.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-29efa8a3-9c91-4e7c-924f-246e17c26f32
STEP: Creating a pod to test consume configMaps
W0118 22:42:51.055797      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:42:51.055: INFO: Waiting up to 5m0s for pod "pod-configmaps-985b300c-4e07-4876-b228-b8ef4e15b3a4" in namespace "configmap-7589" to be "Succeeded or Failed"
Jan 18 22:42:51.061: INFO: Pod "pod-configmaps-985b300c-4e07-4876-b228-b8ef4e15b3a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.045051ms
Jan 18 22:42:53.065: INFO: Pod "pod-configmaps-985b300c-4e07-4876-b228-b8ef4e15b3a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009345758s
Jan 18 22:42:55.068: INFO: Pod "pod-configmaps-985b300c-4e07-4876-b228-b8ef4e15b3a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012365768s
Jan 18 22:42:57.071: INFO: Pod "pod-configmaps-985b300c-4e07-4876-b228-b8ef4e15b3a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015126046s
STEP: Saw pod success
Jan 18 22:42:57.071: INFO: Pod "pod-configmaps-985b300c-4e07-4876-b228-b8ef4e15b3a4" satisfied condition "Succeeded or Failed"
Jan 18 22:42:57.073: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-configmaps-985b300c-4e07-4876-b228-b8ef4e15b3a4 container agnhost-container: <nil>
STEP: delete the pod
Jan 18 22:42:57.089: INFO: Waiting for pod pod-configmaps-985b300c-4e07-4876-b228-b8ef4e15b3a4 to disappear
Jan 18 22:42:57.092: INFO: Pod pod-configmaps-985b300c-4e07-4876-b228-b8ef4e15b3a4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 18 22:42:57.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7589" for this suite.

• [SLOW TEST:6.090 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":63,"skipped":1062,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:42:57.101: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: reading a file in the container
Jan 18 22:43:01.161: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2680 pod-service-account-073f4e8c-5bfb-4bba-9adb-ac25808c022e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan 18 22:43:01.300: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2680 pod-service-account-073f4e8c-5bfb-4bba-9adb-ac25808c022e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan 18 22:43:01.404: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2680 pod-service-account-073f4e8c-5bfb-4bba-9adb-ac25808c022e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 18 22:43:01.518: INFO: Got root ca configmap in namespace "svcaccounts-2680"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 18 22:43:01.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2680" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":356,"completed":64,"skipped":1082,"failed":0}

------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:43:01.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:43:01.569: INFO: Endpoints addresses: [10.0.164.47 10.0.176.161 10.0.176.170] , ports: [6443]
Jan 18 22:43:01.569: INFO: EndpointSlices addresses: [10.0.164.47 10.0.176.161 10.0.176.170] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 18 22:43:01.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8108" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":356,"completed":65,"skipped":1082,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:43:01.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jan 18 22:43:01.651: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Jan 18 22:43:01.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7258" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":356,"completed":66,"skipped":1093,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:43:01.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 18 22:43:01.755: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 18 22:43:01.758: INFO: starting watch
STEP: patching
STEP: updating
Jan 18 22:43:01.781: INFO: waiting for watch events with expected annotations
Jan 18 22:43:01.781: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:188
Jan 18 22:43:01.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-307" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":356,"completed":67,"skipped":1134,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:43:01.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 18 22:43:04.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3302" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":356,"completed":68,"skipped":1149,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:43:04.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 22:43:04.797: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a991f8da-40d8-4308-b0cc-219091e103c8" in namespace "projected-9044" to be "Succeeded or Failed"
Jan 18 22:43:04.801: INFO: Pod "downwardapi-volume-a991f8da-40d8-4308-b0cc-219091e103c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.156361ms
Jan 18 22:43:06.804: INFO: Pod "downwardapi-volume-a991f8da-40d8-4308-b0cc-219091e103c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006482404s
Jan 18 22:43:08.807: INFO: Pod "downwardapi-volume-a991f8da-40d8-4308-b0cc-219091e103c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009435342s
Jan 18 22:43:10.816: INFO: Pod "downwardapi-volume-a991f8da-40d8-4308-b0cc-219091e103c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019091254s
STEP: Saw pod success
Jan 18 22:43:10.817: INFO: Pod "downwardapi-volume-a991f8da-40d8-4308-b0cc-219091e103c8" satisfied condition "Succeeded or Failed"
Jan 18 22:43:10.819: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-a991f8da-40d8-4308-b0cc-219091e103c8 container client-container: <nil>
STEP: delete the pod
Jan 18 22:43:10.833: INFO: Waiting for pod downwardapi-volume-a991f8da-40d8-4308-b0cc-219091e103c8 to disappear
Jan 18 22:43:10.835: INFO: Pod downwardapi-volume-a991f8da-40d8-4308-b0cc-219091e103c8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 18 22:43:10.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9044" for this suite.

• [SLOW TEST:6.095 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":69,"skipped":1154,"failed":0}
SSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:43:10.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 18 22:43:14.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4981" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":356,"completed":70,"skipped":1157,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:43:14.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 22:43:15.954: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 22:43:17.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 43, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 43, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 43, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 43, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 22:43:20.978: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 22:43:21.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5118" for this suite.
STEP: Destroying namespace "webhook-5118-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.231 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":356,"completed":71,"skipped":1160,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:43:21.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 22:43:21.282: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba9b0ff6-dc81-4676-b56c-1f5fd153b67e" in namespace "projected-4578" to be "Succeeded or Failed"
Jan 18 22:43:21.286: INFO: Pod "downwardapi-volume-ba9b0ff6-dc81-4676-b56c-1f5fd153b67e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.472255ms
Jan 18 22:43:23.290: INFO: Pod "downwardapi-volume-ba9b0ff6-dc81-4676-b56c-1f5fd153b67e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008326492s
Jan 18 22:43:25.293: INFO: Pod "downwardapi-volume-ba9b0ff6-dc81-4676-b56c-1f5fd153b67e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011103524s
STEP: Saw pod success
Jan 18 22:43:25.293: INFO: Pod "downwardapi-volume-ba9b0ff6-dc81-4676-b56c-1f5fd153b67e" satisfied condition "Succeeded or Failed"
Jan 18 22:43:25.295: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-ba9b0ff6-dc81-4676-b56c-1f5fd153b67e container client-container: <nil>
STEP: delete the pod
Jan 18 22:43:25.310: INFO: Waiting for pod downwardapi-volume-ba9b0ff6-dc81-4676-b56c-1f5fd153b67e to disappear
Jan 18 22:43:25.312: INFO: Pod downwardapi-volume-ba9b0ff6-dc81-4676-b56c-1f5fd153b67e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 18 22:43:25.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4578" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":72,"skipped":1179,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:43:25.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
W0118 22:43:25.402291      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:43:25.402: INFO: Waiting up to 5m0s for pod "downward-api-233162a4-c932-4c21-8bfa-9cf7aa6984b8" in namespace "downward-api-9809" to be "Succeeded or Failed"
Jan 18 22:43:25.404: INFO: Pod "downward-api-233162a4-c932-4c21-8bfa-9cf7aa6984b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.747399ms
Jan 18 22:43:27.406: INFO: Pod "downward-api-233162a4-c932-4c21-8bfa-9cf7aa6984b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00400355s
Jan 18 22:43:29.409: INFO: Pod "downward-api-233162a4-c932-4c21-8bfa-9cf7aa6984b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007030969s
Jan 18 22:43:31.413: INFO: Pod "downward-api-233162a4-c932-4c21-8bfa-9cf7aa6984b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011485602s
STEP: Saw pod success
Jan 18 22:43:31.413: INFO: Pod "downward-api-233162a4-c932-4c21-8bfa-9cf7aa6984b8" satisfied condition "Succeeded or Failed"
Jan 18 22:43:31.415: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downward-api-233162a4-c932-4c21-8bfa-9cf7aa6984b8 container dapi-container: <nil>
STEP: delete the pod
Jan 18 22:43:31.427: INFO: Waiting for pod downward-api-233162a4-c932-4c21-8bfa-9cf7aa6984b8 to disappear
Jan 18 22:43:31.429: INFO: Pod downward-api-233162a4-c932-4c21-8bfa-9cf7aa6984b8 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 18 22:43:31.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9809" for this suite.

• [SLOW TEST:6.116 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":356,"completed":73,"skipped":1202,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:43:31.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0118 22:43:37.532106      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0118 22:43:37.532267      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 22:43:37.532: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 18 22:43:37.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9692" for this suite.

• [SLOW TEST:6.110 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":356,"completed":74,"skipped":1306,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:43:37.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 18 22:43:37.660: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:37.660: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:37.660: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:37.663: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:43:37.663: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:43:38.684: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:38.684: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:38.684: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:38.747: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:43:38.747: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:43:39.706: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:39.706: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:39.706: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:39.725: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:43:39.725: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:43:40.694: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:40.694: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:40.694: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:40.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:43:40.699: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:43:41.669: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:41.669: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:41.670: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:41.672: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:43:41.694: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:43:42.667: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:42.667: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:42.667: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:42.671: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:43:42.671: INFO: Node ip-10-0-200-13.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:43:43.668: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:43.668: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:43.668: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:43.670: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 18 22:43:43.670: INFO: Node ip-10-0-200-13.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:43:44.667: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:44.667: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:44.667: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:44.670: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 18 22:43:44.670: INFO: Node ip-10-0-200-13.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:43:45.678: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:45.678: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:45.678: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:43:45.691: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 18 22:43:45.691: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
STEP: Getting /status
Jan 18 22:43:45.703: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Jan 18 22:43:45.721: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Jan 18 22:43:45.724: INFO: Observed &DaemonSet event: ADDED
Jan 18 22:43:45.724: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:43:45.724: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:43:45.724: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:43:45.725: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:43:45.725: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:43:45.725: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:43:45.725: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:43:45.725: INFO: Found daemon set daemon-set in namespace daemonsets-2722 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 22:43:45.725: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Jan 18 22:43:45.737: INFO: Observed &DaemonSet event: ADDED
Jan 18 22:43:45.737: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:43:45.737: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:43:45.738: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:43:45.738: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:43:45.738: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:43:45.738: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:43:45.738: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:43:45.738: INFO: Observed daemon set daemon-set in namespace daemonsets-2722 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 22:43:45.738: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 22:43:45.738: INFO: Found daemon set daemon-set in namespace daemonsets-2722 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 18 22:43:45.738: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2722, will wait for the garbage collector to delete the pods
Jan 18 22:43:45.814: INFO: Deleting DaemonSet.extensions daemon-set took: 4.606438ms
Jan 18 22:43:45.916: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.119296ms
Jan 18 22:43:48.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:43:48.118: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 22:43:48.120: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"207374"},"items":null}

Jan 18 22:43:48.122: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"207374"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 18 22:43:48.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2722" for this suite.

• [SLOW TEST:10.598 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":356,"completed":75,"skipped":1307,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:43:48.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:43:48.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-9029 create -f -'
Jan 18 22:43:50.405: INFO: stderr: ""
Jan 18 22:43:50.405: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 18 22:43:50.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-9029 create -f -'
Jan 18 22:43:52.738: INFO: stderr: ""
Jan 18 22:43:52.738: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 18 22:43:53.743: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:43:53.743: INFO: Found 1 / 1
Jan 18 22:43:53.743: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 18 22:43:53.748: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:43:53.748: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 22:43:53.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-9029 describe pod agnhost-primary-c8pmz'
Jan 18 22:43:53.808: INFO: stderr: ""
Jan 18 22:43:53.808: INFO: stdout: "Name:         agnhost-primary-c8pmz\nNamespace:    kubectl-9029\nPriority:     0\nNode:         ip-10-0-219-147.ec2.internal/10.0.219.147\nStart Time:   Wed, 18 Jan 2023 22:43:50 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  k8s.ovn.org/pod-networks:\n                {\"default\":{\"ip_addresses\":[\"10.128.12.88/23\"],\"mac_address\":\"0a:58:0a:80:0c:58\",\"gateway_ips\":[\"10.128.12.1\"],\"ip_address\":\"10.128.12.88/...\n              k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"ovn-kubernetes\",\n                    \"interface\": \"eth0\",\n                    \"ips\": [\n                        \"10.128.12.88\"\n                    ],\n                    \"mac\": \"0a:58:0a:80:0c:58\",\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"ovn-kubernetes\",\n                    \"interface\": \"eth0\",\n                    \"ips\": [\n                        \"10.128.12.88\"\n                    ],\n                    \"mac\": \"0a:58:0a:80:0c:58\",\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           10.128.12.88\nIPs:\n  IP:           10.128.12.88\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://aa6f9b52250b412dfd92f2c09e6fb5ce96eff9285dac431a9ef39956b4e53487\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.36\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 18 Jan 2023 22:43:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pzz28 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-pzz28:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       3s    default-scheduler  Successfully assigned kubectl-9029/agnhost-primary-c8pmz to ip-10-0-219-147.ec2.internal by ip-10-0-176-161\n  Normal  AddedInterface  1s    multus             Add eth0 [10.128.12.88/23] from ovn-kubernetes\n  Normal  Pulled          1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.36\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Jan 18 22:43:53.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-9029 describe rc agnhost-primary'
Jan 18 22:43:53.871: INFO: stderr: ""
Jan 18 22:43:53.871: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9029\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.36\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-c8pmz\n"
Jan 18 22:43:53.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-9029 describe service agnhost-primary'
Jan 18 22:43:53.927: INFO: stderr: ""
Jan 18 22:43:53.927: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9029\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.30.0.52\nIPs:               172.30.0.52\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.128.12.88:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 18 22:43:53.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-9029 describe node ip-10-0-128-7.ec2.internal'
Jan 18 22:43:54.475: INFO: stderr: ""
Jan 18 22:43:54.475: INFO: stdout: "Name:               ip-10-0-128-7.ec2.internal\nRoles:              infra,worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=r5.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-128-7.ec2.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io=infra\n                    node-role.kubernetes.io/infra=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=r5.xlarge\n                    node.openshift.io/os_id=rhcos\n                    topology.ebs.csi.aws.com/zone=us-east-1a\n                    topology.kubernetes.io/region=us-east-1\n                    topology.kubernetes.io/zone=us-east-1a\nAnnotations:        cloud.network.openshift.io/egress-ipconfig:\n                      [{\"interface\":\"eni-010e853cf354cb872\",\"ifaddr\":{\"ipv4\":\"10.0.128.0/17\"},\"capacity\":{\"ipv4\":14,\"ipv6\":15}}]\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-0abfe3af21c04dd6c\"}\n                    k8s.ovn.org/host-addresses: [\"10.0.128.7\"]\n                    k8s.ovn.org/l3-gateway-config:\n                      {\"default\":{\"mode\":\"shared\",\"interface-id\":\"br-ex_ip-10-0-128-7.ec2.internal\",\"mac-address\":\"02:a3:9c:91:ae:55\",\"ip-addresses\":[\"10.0.128....\n                    k8s.ovn.org/node-chassis-id: 52724080-26ac-4555-b2d7-40245661c309\n                    k8s.ovn.org/node-gateway-router-lrp-ifaddr: {\"ipv4\":\"100.64.0.9/16\"}\n                    k8s.ovn.org/node-mgmt-port-mac-address: ae:4c:ac:9d:28:d3\n                    k8s.ovn.org/node-primary-ifaddr: {\"ipv4\":\"10.0.128.7/17\"}\n                    k8s.ovn.org/node-subnets: {\"default\":\"10.128.14.0/23\"}\n                    machine.openshift.io/machine: openshift-machine-api/sdcicd-cncf-l8h7s-infra-us-east-1a-f5bmn\n                    machineconfiguration.openshift.io/controlPlaneTopology: HighlyAvailable\n                    machineconfiguration.openshift.io/currentConfig: rendered-worker-af0537f98f02ab1d77a12933e3698455\n                    machineconfiguration.openshift.io/desiredConfig: rendered-worker-af0537f98f02ab1d77a12933e3698455\n                    machineconfiguration.openshift.io/desiredDrain: uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455\n                    machineconfiguration.openshift.io/lastAppliedDrain: uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    managed.openshift.com/customlabels: node-role.kubernetes.io,node-role.kubernetes.io/infra\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 18 Jan 2023 21:27:09 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-128-7.ec2.internal\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 18 Jan 2023 22:43:46 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 18 Jan 2023 22:43:43 +0000   Wed, 18 Jan 2023 21:36:02 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 18 Jan 2023 22:43:43 +0000   Wed, 18 Jan 2023 21:36:02 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 18 Jan 2023 22:43:43 +0000   Wed, 18 Jan 2023 21:36:02 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 18 Jan 2023 22:43:43 +0000   Wed, 18 Jan 2023 21:36:02 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.128.7\n  Hostname:     ip-10-0-128-7.ec2.internal\n  InternalDNS:  ip-10-0-128-7.ec2.internal\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         4\n  ephemeral-storage:           314037228Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      32443760Ki\n  pods:                        250\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         3920m\n  ephemeral-storage:           289416708846\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      29423880765440m\n  pods:                        250\nSystem Info:\n  Machine ID:                             ec21c658414c8d2d06336e7d6a9b3742\n  System UUID:                            ec21c658-414c-8d2d-0633-6e7d6a9b3742\n  Boot ID:                                c7eefbc8-7dc5-4eb8-9fc3-8763853c85cf\n  Kernel Version:                         4.18.0-372.19.1.el8_6.x86_64\n  OS Image:                               Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa)\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8\n  Kubelet Version:                        v1.24.0+9546431\n  Kube-Proxy Version:                     v1.24.0+9546431\nProviderID:                               aws:///us-east-1a/i-0abfe3af21c04dd6c\nNon-terminated Pods:                      (28 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  openshift-addon-operator                addon-operator-manager-c8c4859bf-84wp8                     200m (5%)     200m (5%)   330Mi (1%)       630Mi (2%)     67m\n  openshift-addon-operator                addon-operator-webhooks-569bd4cfc5-dz6nk                   100m (2%)     200m (5%)   30Mi (0%)        50Mi (0%)      70m\n  openshift-cluster-csi-drivers           aws-ebs-csi-driver-node-mq4qf                              30m (0%)      0 (0%)      150Mi (0%)       0 (0%)         76m\n  openshift-cluster-node-tuning-operator  tuned-l9d9s                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         76m\n  openshift-dns                           dns-default-btls8                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         37m\n  openshift-dns                           node-resolver-vhz2m                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         76m\n  openshift-image-registry                node-ca-6cvvj                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         76m\n  openshift-ingress-canary                ingress-canary-qw8v9                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         75m\n  openshift-ingress                       router-default-7cff97cd98-dqgwn                            100m (2%)     0 (0%)      256Mi (0%)       0 (0%)         70m\n  openshift-machine-config-operator       machine-config-daemon-htf69                                40m (1%)      0 (0%)      100Mi (0%)       0 (0%)         76m\n  openshift-monitoring                    alertmanager-main-1                                        9m (0%)       0 (0%)      120Mi (0%)       0 (0%)         70m\n  openshift-monitoring                    kube-state-metrics-76877575d5-ptnh8                        4m (0%)       0 (0%)      110Mi (0%)       0 (0%)         67m\n  openshift-monitoring                    node-exporter-spmbc                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         76m\n  openshift-monitoring                    openshift-state-metrics-59fc669d8d-v25vz                   3m (0%)       0 (0%)      72Mi (0%)        0 (0%)         67m\n  openshift-monitoring                    prometheus-adapter-cf64f7f46-7bf79                         1m (0%)       0 (0%)      40Mi (0%)        0 (0%)         70m\n  openshift-monitoring                    prometheus-k8s-1                                           75m (1%)      0 (0%)      1104Mi (3%)      0 (0%)         70m\n  openshift-monitoring                    prometheus-operator-76957bb5bd-2pztb                       6m (0%)       0 (0%)      165Mi (0%)       0 (0%)         67m\n  openshift-monitoring                    prometheus-operator-admission-webhook-577cc9c956-d6fkv     5m (0%)       0 (0%)      30Mi (0%)        0 (0%)         70m\n  openshift-monitoring                    sre-dns-latency-exporter-8dptc                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  openshift-monitoring                    telemeter-client-6bb748465-zttx9                           3m (0%)       0 (0%)      70Mi (0%)        0 (0%)         67m\n  openshift-monitoring                    thanos-querier-57f44c5498-xjsz4                            15m (0%)      0 (0%)      92Mi (0%)        0 (0%)         70m\n  openshift-multus                        multus-additional-cni-plugins-5lmsf                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         76m\n  openshift-multus                        multus-znqj6                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         76m\n  openshift-multus                        network-metrics-daemon-ffdlk                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         76m\n  openshift-network-diagnostics           network-check-target-xwp8p                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         76m\n  openshift-ovn-kubernetes                ovnkube-node-mkzt4                                         50m (1%)      0 (0%)      660Mi (2%)       0 (0%)         76m\n  openshift-security                      splunkforwarder-ds-fbjlj                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-034419193cd64c26-dwhpp    0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests      Limits\n  --------                    --------      ------\n  cpu                         795m (20%)    400m (10%)\n  memory                      3797Mi (13%)  680Mi (2%)\n  ephemeral-storage           0 (0%)        0 (0%)\n  hugepages-1Gi               0 (0%)        0 (0%)\n  hugepages-2Mi               0 (0%)        0 (0%)\n  attachable-volumes-aws-ebs  0             0\nEvents:\n  Type     Reason                     Age                From                 Message\n  ----     ------                     ----               ----                 -------\n  Normal   RegisteredNode             76m                node-controller      Node ip-10-0-128-7.ec2.internal event: Registered Node ip-10-0-128-7.ec2.internal in Controller\n  Normal   NodeDone                   76m                machineconfigdaemon  Setting node ip-10-0-128-7.ec2.internal, currentConfig rendered-worker-9028fcf7eb04cf237e03b24eab95852a to Done\n  Normal   Uncordon                   76m                machineconfigdaemon  Update completed for config rendered-worker-9028fcf7eb04cf237e03b24eab95852a and node has been uncordoned\n  Normal   ConfigDriftMonitorStarted  76m                machineconfigdaemon  Config Drift Monitor started, watching against rendered-worker-9028fcf7eb04cf237e03b24eab95852a\n  Normal   RegisteredNode             75m                node-controller      Node ip-10-0-128-7.ec2.internal event: Registered Node ip-10-0-128-7.ec2.internal in Controller\n  Normal   ConfigDriftMonitorStopped  70m                machineconfigdaemon  Config Drift Monitor stopped\n  Normal   Cordon                     70m                machineconfigdaemon  Cordoned node to apply update\n  Normal   Drain                      70m                machineconfigdaemon  Draining node to update config.\n  Normal   NodeNotSchedulable         70m                kubelet              Node ip-10-0-128-7.ec2.internal status is now: NodeNotSchedulable\n  Normal   RegisteredNode             70m                node-controller      Node ip-10-0-128-7.ec2.internal event: Registered Node ip-10-0-128-7.ec2.internal in Controller\n  Normal   NodeNotReady               68m                node-controller      Node ip-10-0-128-7.ec2.internal status is now: NodeNotReady\n  Normal   NodeHasSufficientPID       67m (x2 over 67m)  kubelet              Node ip-10-0-128-7.ec2.internal status is now: NodeHasSufficientPID\n  Normal   NodeReady                  67m                kubelet              Node ip-10-0-128-7.ec2.internal status is now: NodeReady\n  Normal   NodeNotSchedulable         67m                kubelet              Node ip-10-0-128-7.ec2.internal status is now: NodeNotSchedulable\n  Warning  Rebooted                   67m                kubelet              Node ip-10-0-128-7.ec2.internal has been rebooted, boot id: c7eefbc8-7dc5-4eb8-9fc3-8763853c85cf\n  Normal   Starting                   67m                kubelet              Starting kubelet.\n  Normal   NodeAllocatableEnforced    67m                kubelet              Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory    67m (x2 over 67m)  kubelet              Node ip-10-0-128-7.ec2.internal status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure      67m (x2 over 67m)  kubelet              Node ip-10-0-128-7.ec2.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeDone                   67m                machineconfigdaemon  Setting node ip-10-0-128-7.ec2.internal, currentConfig rendered-worker-af0537f98f02ab1d77a12933e3698455 to Done\n  Normal   ConfigDriftMonitorStarted  67m                machineconfigdaemon  Config Drift Monitor started, watching against rendered-worker-af0537f98f02ab1d77a12933e3698455\n  Normal   Uncordon                   67m                machineconfigdaemon  Update completed for config rendered-worker-af0537f98f02ab1d77a12933e3698455 and node has been uncordoned\n  Normal   NodeSchedulable            67m                kubelet              Node ip-10-0-128-7.ec2.internal status is now: NodeSchedulable\n  Normal   RegisteredNode             63m                node-controller      Node ip-10-0-128-7.ec2.internal event: Registered Node ip-10-0-128-7.ec2.internal in Controller\n"
Jan 18 22:43:54.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-9029 describe namespace kubectl-9029'
Jan 18 22:43:54.529: INFO: stderr: ""
Jan 18 22:43:54.529: INFO: stdout: "Name:         kubectl-9029\nLabels:       e2e-framework=kubectl\n              e2e-run=b690fb47-651e-40b7-8fa6-91d8030e242f\n              kubernetes.io/metadata.name=kubectl-9029\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c43,c17\n              openshift.io/sa.scc.supplemental-groups: 1001840000/10000\n              openshift.io/sa.scc.uid-range: 1001840000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 22:43:54.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9029" for this suite.

• [SLOW TEST:6.391 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1110
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":356,"completed":76,"skipped":1311,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:43:54.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
W0118 22:43:54.600315      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 18 22:44:00.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8020" for this suite.

• [SLOW TEST:6.210 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":77,"skipped":1328,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:44:00.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:188
Jan 18 22:44:01.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7997" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":356,"completed":78,"skipped":1338,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:44:01.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod with failed condition
STEP: updating the pod
Jan 18 22:46:02.726: INFO: Successfully updated pod "var-expansion-12ea65d7-39ff-44e8-8fef-21b1c22869b4"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jan 18 22:46:04.732: INFO: Deleting pod "var-expansion-12ea65d7-39ff-44e8-8fef-21b1c22869b4" in namespace "var-expansion-2111"
Jan 18 22:46:04.737: INFO: Wait up to 5m0s for pod "var-expansion-12ea65d7-39ff-44e8-8fef-21b1c22869b4" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 18 22:46:36.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2111" for this suite.

• [SLOW TEST:155.593 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":356,"completed":79,"skipped":1361,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:46:36.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-8bbf5c00-f1ec-4ed8-9153-3ce4ac6e80c2
STEP: Creating a pod to test consume configMaps
W0118 22:46:36.836802      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:46:36.836: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b1b434d-2b98-4163-be5c-9970192e20f4" in namespace "configmap-1386" to be "Succeeded or Failed"
Jan 18 22:46:36.848: INFO: Pod "pod-configmaps-9b1b434d-2b98-4163-be5c-9970192e20f4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.723996ms
Jan 18 22:46:38.862: INFO: Pod "pod-configmaps-9b1b434d-2b98-4163-be5c-9970192e20f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025355854s
Jan 18 22:46:40.865: INFO: Pod "pod-configmaps-9b1b434d-2b98-4163-be5c-9970192e20f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028516044s
Jan 18 22:46:42.868: INFO: Pod "pod-configmaps-9b1b434d-2b98-4163-be5c-9970192e20f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031282085s
STEP: Saw pod success
Jan 18 22:46:42.868: INFO: Pod "pod-configmaps-9b1b434d-2b98-4163-be5c-9970192e20f4" satisfied condition "Succeeded or Failed"
Jan 18 22:46:42.870: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-configmaps-9b1b434d-2b98-4163-be5c-9970192e20f4 container agnhost-container: <nil>
STEP: delete the pod
Jan 18 22:46:42.891: INFO: Waiting for pod pod-configmaps-9b1b434d-2b98-4163-be5c-9970192e20f4 to disappear
Jan 18 22:46:42.893: INFO: Pod pod-configmaps-9b1b434d-2b98-4163-be5c-9970192e20f4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 18 22:46:42.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1386" for this suite.

• [SLOW TEST:6.149 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":80,"skipped":1370,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:46:42.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
W0118 22:46:42.939931      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:46:42.943: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 22:46:47.948: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Jan 18 22:46:47.951: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Jan 18 22:46:47.957: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Jan 18 22:46:47.958: INFO: Observed &ReplicaSet event: ADDED
Jan 18 22:46:47.958: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:46:47.958: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:46:47.958: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:46:47.958: INFO: Found replicaset test-rs in namespace replicaset-653 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 22:46:47.959: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Jan 18 22:46:47.959: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 22:46:47.967: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Jan 18 22:46:47.968: INFO: Observed &ReplicaSet event: ADDED
Jan 18 22:46:47.968: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:46:47.968: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:46:47.968: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:46:47.968: INFO: Observed replicaset test-rs in namespace replicaset-653 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 22:46:47.968: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:46:47.968: INFO: Found replicaset test-rs in namespace replicaset-653 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 18 22:46:47.968: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 18 22:46:47.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-653" for this suite.

• [SLOW TEST:5.074 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":356,"completed":81,"skipped":1371,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:46:47.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0118 22:46:48.061110      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 18 22:46:50.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2063" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","total":356,"completed":82,"skipped":1381,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:46:50.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-51e81240-961f-4a58-af1f-d524619435ee in namespace container-probe-4580
Jan 18 22:46:55.120: INFO: Started pod liveness-51e81240-961f-4a58-af1f-d524619435ee in namespace container-probe-4580
STEP: checking the pod's current state and verifying that restartCount is present
Jan 18 22:46:55.122: INFO: Initial restart count of pod liveness-51e81240-961f-4a58-af1f-d524619435ee is 0
Jan 18 22:47:13.155: INFO: Restart count of pod container-probe-4580/liveness-51e81240-961f-4a58-af1f-d524619435ee is now 1 (18.03328367s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 18 22:47:13.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4580" for this suite.

• [SLOW TEST:23.097 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":83,"skipped":1400,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:47:13.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
W0118 22:47:13.211434      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jan 18 22:47:17.727: INFO: Successfully updated pod "adopt-release-4g986"
STEP: Checking that the Job readopts the Pod
Jan 18 22:47:17.727: INFO: Waiting up to 15m0s for pod "adopt-release-4g986" in namespace "job-9158" to be "adopted"
Jan 18 22:47:17.729: INFO: Pod "adopt-release-4g986": Phase="Running", Reason="", readiness=true. Elapsed: 1.864022ms
Jan 18 22:47:19.734: INFO: Pod "adopt-release-4g986": Phase="Running", Reason="", readiness=true. Elapsed: 2.006787691s
Jan 18 22:47:19.734: INFO: Pod "adopt-release-4g986" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jan 18 22:47:20.251: INFO: Successfully updated pod "adopt-release-4g986"
STEP: Checking that the Job releases the Pod
Jan 18 22:47:20.251: INFO: Waiting up to 15m0s for pod "adopt-release-4g986" in namespace "job-9158" to be "released"
Jan 18 22:47:20.253: INFO: Pod "adopt-release-4g986": Phase="Running", Reason="", readiness=true. Elapsed: 2.004129ms
Jan 18 22:47:22.257: INFO: Pod "adopt-release-4g986": Phase="Running", Reason="", readiness=true. Elapsed: 2.005419237s
Jan 18 22:47:22.257: INFO: Pod "adopt-release-4g986" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 18 22:47:22.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9158" for this suite.

• [SLOW TEST:9.091 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":356,"completed":84,"skipped":1403,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:47:22.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-projected-jdnr
STEP: Creating a pod to test atomic-volume-subpath
Jan 18 22:47:22.338: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-jdnr" in namespace "subpath-3845" to be "Succeeded or Failed"
Jan 18 22:47:22.345: INFO: Pod "pod-subpath-test-projected-jdnr": Phase="Pending", Reason="", readiness=false. Elapsed: 7.110972ms
Jan 18 22:47:24.349: INFO: Pod "pod-subpath-test-projected-jdnr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010772136s
Jan 18 22:47:26.351: INFO: Pod "pod-subpath-test-projected-jdnr": Phase="Running", Reason="", readiness=true. Elapsed: 4.012988753s
Jan 18 22:47:28.354: INFO: Pod "pod-subpath-test-projected-jdnr": Phase="Running", Reason="", readiness=true. Elapsed: 6.015915149s
Jan 18 22:47:30.358: INFO: Pod "pod-subpath-test-projected-jdnr": Phase="Running", Reason="", readiness=true. Elapsed: 8.020248476s
Jan 18 22:47:32.361: INFO: Pod "pod-subpath-test-projected-jdnr": Phase="Running", Reason="", readiness=true. Elapsed: 10.023020245s
Jan 18 22:47:34.364: INFO: Pod "pod-subpath-test-projected-jdnr": Phase="Running", Reason="", readiness=true. Elapsed: 12.026631005s
Jan 18 22:47:36.368: INFO: Pod "pod-subpath-test-projected-jdnr": Phase="Running", Reason="", readiness=true. Elapsed: 14.029856988s
Jan 18 22:47:38.371: INFO: Pod "pod-subpath-test-projected-jdnr": Phase="Running", Reason="", readiness=true. Elapsed: 16.032870433s
Jan 18 22:47:40.375: INFO: Pod "pod-subpath-test-projected-jdnr": Phase="Running", Reason="", readiness=true. Elapsed: 18.036803695s
Jan 18 22:47:42.378: INFO: Pod "pod-subpath-test-projected-jdnr": Phase="Running", Reason="", readiness=true. Elapsed: 20.039883328s
Jan 18 22:47:44.380: INFO: Pod "pod-subpath-test-projected-jdnr": Phase="Running", Reason="", readiness=true. Elapsed: 22.042480599s
Jan 18 22:47:46.384: INFO: Pod "pod-subpath-test-projected-jdnr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.045792396s
STEP: Saw pod success
Jan 18 22:47:46.384: INFO: Pod "pod-subpath-test-projected-jdnr" satisfied condition "Succeeded or Failed"
Jan 18 22:47:46.386: INFO: Trying to get logs from node ip-10-0-211-217.ec2.internal pod pod-subpath-test-projected-jdnr container test-container-subpath-projected-jdnr: <nil>
STEP: delete the pod
Jan 18 22:47:46.404: INFO: Waiting for pod pod-subpath-test-projected-jdnr to disappear
Jan 18 22:47:46.406: INFO: Pod pod-subpath-test-projected-jdnr no longer exists
STEP: Deleting pod pod-subpath-test-projected-jdnr
Jan 18 22:47:46.406: INFO: Deleting pod "pod-subpath-test-projected-jdnr" in namespace "subpath-3845"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 18 22:47:46.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3845" for this suite.

• [SLOW TEST:24.151 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","total":356,"completed":85,"skipped":1404,"failed":0}
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:47:46.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 22:47:46.462: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 22:48:46.759: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Jan 18 22:48:46.797: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 18 22:48:46.812: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 18 22:48:46.843: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 18 22:48:46.858: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 18 22:48:46.886: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 18 22:48:46.899: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jan 18 22:48:46.924: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jan 18 22:48:46.938: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Jan 18 22:48:46.968: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Jan 18 22:48:46.980: INFO: Created pod: pod4-1-sched-preemption-medium-priority
Jan 18 22:48:47.017: INFO: Created pod: pod5-0-sched-preemption-medium-priority
Jan 18 22:48:47.030: INFO: Created pod: pod5-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 18 22:49:05.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7890" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:78.954 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":356,"completed":86,"skipped":1404,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:49:05.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 18 22:49:05.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6493" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":356,"completed":87,"skipped":1407,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:49:05.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-c78cc0cb-d840-4e88-b7b4-8e396b7ba42d
STEP: Creating a pod to test consume configMaps
Jan 18 22:49:05.665: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d391a716-c715-493e-a2f3-c4fc4f6a4650" in namespace "projected-7789" to be "Succeeded or Failed"
Jan 18 22:49:05.678: INFO: Pod "pod-projected-configmaps-d391a716-c715-493e-a2f3-c4fc4f6a4650": Phase="Pending", Reason="", readiness=false. Elapsed: 12.520959ms
Jan 18 22:49:07.681: INFO: Pod "pod-projected-configmaps-d391a716-c715-493e-a2f3-c4fc4f6a4650": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015988589s
Jan 18 22:49:09.684: INFO: Pod "pod-projected-configmaps-d391a716-c715-493e-a2f3-c4fc4f6a4650": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019241147s
Jan 18 22:49:11.773: INFO: Pod "pod-projected-configmaps-d391a716-c715-493e-a2f3-c4fc4f6a4650": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.107562545s
STEP: Saw pod success
Jan 18 22:49:11.773: INFO: Pod "pod-projected-configmaps-d391a716-c715-493e-a2f3-c4fc4f6a4650" satisfied condition "Succeeded or Failed"
Jan 18 22:49:11.775: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-projected-configmaps-d391a716-c715-493e-a2f3-c4fc4f6a4650 container agnhost-container: <nil>
STEP: delete the pod
Jan 18 22:49:11.791: INFO: Waiting for pod pod-projected-configmaps-d391a716-c715-493e-a2f3-c4fc4f6a4650 to disappear
Jan 18 22:49:11.794: INFO: Pod pod-projected-configmaps-d391a716-c715-493e-a2f3-c4fc4f6a4650 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 18 22:49:11.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7789" for this suite.

• [SLOW TEST:6.240 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":88,"skipped":1443,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:49:11.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 18 22:49:42.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-76" for this suite.

• [SLOW TEST:30.268 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":356,"completed":89,"skipped":1459,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:49:42.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 18 22:49:42.818: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Jan 18 22:49:44.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 49, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 49, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 49, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 49, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-677b6dd845\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 22:49:47.845: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:49:47.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 22:49:50.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3390" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:9.011 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":356,"completed":90,"skipped":1482,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:49:51.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 22:49:52.018: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 22:49:54.028: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 49, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 49, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 49, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 49, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 22:49:57.041: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the crd webhook via the AdmissionRegistration API
Jan 18 22:49:57.062: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook
Jan 18 22:49:57.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 22:49:57.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7926" for this suite.
STEP: Destroying namespace "webhook-7926-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.172 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":356,"completed":91,"skipped":1485,"failed":0}
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:49:57.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Pod with a static label
W0118 22:49:57.377445      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: watching for Pod to be ready
Jan 18 22:49:57.378: INFO: observed Pod pod-test in namespace pods-9382 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 18 22:49:57.378: INFO: observed Pod pod-test in namespace pods-9382 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:49:57 +0000 UTC  }]
Jan 18 22:49:57.393: INFO: observed Pod pod-test in namespace pods-9382 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:49:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:49:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:49:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:49:57 +0000 UTC  }]
Jan 18 22:49:57.484: INFO: observed Pod pod-test in namespace pods-9382 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:49:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:49:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:49:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:49:57 +0000 UTC  }]
Jan 18 22:50:00.514: INFO: observed Pod pod-test in namespace pods-9382 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:49:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:49:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:49:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:49:57 +0000 UTC  }]
Jan 18 22:50:01.534: INFO: Found Pod pod-test in namespace pods-9382 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:49:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:50:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:50:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:49:57 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jan 18 22:50:01.568: INFO: observed event type MODIFIED
Jan 18 22:50:03.542: INFO: observed event type MODIFIED
Jan 18 22:50:04.543: INFO: observed event type MODIFIED
Jan 18 22:50:04.550: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 18 22:50:04.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9382" for this suite.

• [SLOW TEST:7.311 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":356,"completed":92,"skipped":1485,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:50:04.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 22:50:04.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2040" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":356,"completed":93,"skipped":1488,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:50:04.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 22:50:04.662: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 22:51:04.890: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Jan 18 22:51:04.925: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 18 22:51:04.938: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 18 22:51:04.966: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 18 22:51:04.986: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 18 22:51:05.014: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 18 22:51:05.026: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jan 18 22:51:05.054: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jan 18 22:51:05.077: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Jan 18 22:51:05.104: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Jan 18 22:51:05.115: INFO: Created pod: pod4-1-sched-preemption-medium-priority
Jan 18 22:51:05.146: INFO: Created pod: pod5-0-sched-preemption-medium-priority
Jan 18 22:51:05.162: INFO: Created pod: pod5-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
W0118 22:51:09.202288      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "critical-pod" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "critical-pod" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "critical-pod" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "critical-pod" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 18 22:51:17.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8317" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:72.770 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":356,"completed":94,"skipped":1523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:51:17.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
W0118 22:51:17.464632      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:51:17.464: INFO: Waiting up to 5m0s for pod "pod-128d89cd-43d0-4735-a068-4c05ebf78489" in namespace "emptydir-6854" to be "Succeeded or Failed"
Jan 18 22:51:17.467: INFO: Pod "pod-128d89cd-43d0-4735-a068-4c05ebf78489": Phase="Pending", Reason="", readiness=false. Elapsed: 2.638975ms
Jan 18 22:51:19.469: INFO: Pod "pod-128d89cd-43d0-4735-a068-4c05ebf78489": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005196318s
Jan 18 22:51:21.473: INFO: Pod "pod-128d89cd-43d0-4735-a068-4c05ebf78489": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008479338s
Jan 18 22:51:23.510: INFO: Pod "pod-128d89cd-43d0-4735-a068-4c05ebf78489": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045289167s
STEP: Saw pod success
Jan 18 22:51:23.510: INFO: Pod "pod-128d89cd-43d0-4735-a068-4c05ebf78489" satisfied condition "Succeeded or Failed"
Jan 18 22:51:23.513: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-128d89cd-43d0-4735-a068-4c05ebf78489 container test-container: <nil>
STEP: delete the pod
Jan 18 22:51:23.595: INFO: Waiting for pod pod-128d89cd-43d0-4735-a068-4c05ebf78489 to disappear
Jan 18 22:51:23.613: INFO: Pod pod-128d89cd-43d0-4735-a068-4c05ebf78489 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 18 22:51:23.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6854" for this suite.

• [SLOW TEST:6.255 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":95,"skipped":1553,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:51:23.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 18 22:51:23.841: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7310  b04dd46a-cd46-48e6-98f7-05bee14a1bfb 223443 0 2023-01-18 22:51:23 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2023-01-18 22:51:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:51:23.841: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7310  b04dd46a-cd46-48e6-98f7-05bee14a1bfb 223446 0 2023-01-18 22:51:23 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2023-01-18 22:51:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 18 22:51:23.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7310" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":356,"completed":96,"skipped":1554,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:51:23.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 18 22:51:30.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-741" for this suite.

• [SLOW TEST:6.308 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":356,"completed":97,"skipped":1610,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:51:30.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 18 22:51:30.237: INFO: Waiting up to 5m0s for pod "pod-2908d97e-4f1a-4628-be8d-fc84b46d58c8" in namespace "emptydir-9283" to be "Succeeded or Failed"
Jan 18 22:51:30.239: INFO: Pod "pod-2908d97e-4f1a-4628-be8d-fc84b46d58c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.944495ms
Jan 18 22:51:32.243: INFO: Pod "pod-2908d97e-4f1a-4628-be8d-fc84b46d58c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005723082s
Jan 18 22:51:34.249: INFO: Pod "pod-2908d97e-4f1a-4628-be8d-fc84b46d58c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011215052s
Jan 18 22:51:36.252: INFO: Pod "pod-2908d97e-4f1a-4628-be8d-fc84b46d58c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014989553s
STEP: Saw pod success
Jan 18 22:51:36.252: INFO: Pod "pod-2908d97e-4f1a-4628-be8d-fc84b46d58c8" satisfied condition "Succeeded or Failed"
Jan 18 22:51:36.254: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-2908d97e-4f1a-4628-be8d-fc84b46d58c8 container test-container: <nil>
STEP: delete the pod
Jan 18 22:51:36.268: INFO: Waiting for pod pod-2908d97e-4f1a-4628-be8d-fc84b46d58c8 to disappear
Jan 18 22:51:36.270: INFO: Pod pod-2908d97e-4f1a-4628-be8d-fc84b46d58c8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 18 22:51:36.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9283" for this suite.

• [SLOW TEST:6.114 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":98,"skipped":1627,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:51:36.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 22:51:36.899: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 22:51:38.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 51, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 51, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 51, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 51, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 22:51:41.919: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 22:51:41.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7320" for this suite.
STEP: Destroying namespace "webhook-7320-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.796 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":356,"completed":99,"skipped":1653,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:51:42.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching services
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 18 22:51:42.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6697" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":356,"completed":100,"skipped":1658,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:51:42.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-18a5910b-644f-4cb9-9548-0a990ab68665
STEP: Creating a pod to test consume configMaps
Jan 18 22:51:43.272: INFO: Waiting up to 5m0s for pod "pod-configmaps-f418d555-d7a5-4674-ab66-dd3fecfaf764" in namespace "configmap-8017" to be "Succeeded or Failed"
Jan 18 22:51:43.274: INFO: Pod "pod-configmaps-f418d555-d7a5-4674-ab66-dd3fecfaf764": Phase="Pending", Reason="", readiness=false. Elapsed: 1.847002ms
Jan 18 22:51:45.278: INFO: Pod "pod-configmaps-f418d555-d7a5-4674-ab66-dd3fecfaf764": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005660822s
Jan 18 22:51:47.282: INFO: Pod "pod-configmaps-f418d555-d7a5-4674-ab66-dd3fecfaf764": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00929994s
Jan 18 22:51:49.285: INFO: Pod "pod-configmaps-f418d555-d7a5-4674-ab66-dd3fecfaf764": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012575829s
STEP: Saw pod success
Jan 18 22:51:49.285: INFO: Pod "pod-configmaps-f418d555-d7a5-4674-ab66-dd3fecfaf764" satisfied condition "Succeeded or Failed"
Jan 18 22:51:49.287: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-configmaps-f418d555-d7a5-4674-ab66-dd3fecfaf764 container agnhost-container: <nil>
STEP: delete the pod
Jan 18 22:51:49.310: INFO: Waiting for pod pod-configmaps-f418d555-d7a5-4674-ab66-dd3fecfaf764 to disappear
Jan 18 22:51:49.312: INFO: Pod pod-configmaps-f418d555-d7a5-4674-ab66-dd3fecfaf764 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 18 22:51:49.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8017" for this suite.

• [SLOW TEST:7.099 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":101,"skipped":1665,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:51:49.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 18 22:51:49.372: INFO: Waiting up to 5m0s for pod "downward-api-5e9fc997-8a76-4abe-ae56-d51f58586d0e" in namespace "downward-api-6996" to be "Succeeded or Failed"
Jan 18 22:51:49.393: INFO: Pod "downward-api-5e9fc997-8a76-4abe-ae56-d51f58586d0e": Phase="Pending", Reason="", readiness=false. Elapsed: 21.254812ms
Jan 18 22:51:51.395: INFO: Pod "downward-api-5e9fc997-8a76-4abe-ae56-d51f58586d0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023636283s
Jan 18 22:51:53.399: INFO: Pod "downward-api-5e9fc997-8a76-4abe-ae56-d51f58586d0e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027204288s
Jan 18 22:51:55.403: INFO: Pod "downward-api-5e9fc997-8a76-4abe-ae56-d51f58586d0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031095962s
STEP: Saw pod success
Jan 18 22:51:55.403: INFO: Pod "downward-api-5e9fc997-8a76-4abe-ae56-d51f58586d0e" satisfied condition "Succeeded or Failed"
Jan 18 22:51:55.405: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downward-api-5e9fc997-8a76-4abe-ae56-d51f58586d0e container dapi-container: <nil>
STEP: delete the pod
Jan 18 22:51:55.417: INFO: Waiting for pod downward-api-5e9fc997-8a76-4abe-ae56-d51f58586d0e to disappear
Jan 18 22:51:55.419: INFO: Pod downward-api-5e9fc997-8a76-4abe-ae56-d51f58586d0e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 18 22:51:55.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6996" for this suite.

• [SLOW TEST:6.106 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":356,"completed":102,"skipped":1693,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:51:55.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 18 22:52:06.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-897" for this suite.

• [SLOW TEST:11.209 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":356,"completed":103,"skipped":1723,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:52:06.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 18 22:52:17.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4695" for this suite.

• [SLOW TEST:11.088 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":356,"completed":104,"skipped":1731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:52:17.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on node default medium
W0118 22:52:17.788524      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:52:17.788: INFO: Waiting up to 5m0s for pod "pod-ea19d8f7-73c4-425b-bb7d-2bf016425b7f" in namespace "emptydir-5314" to be "Succeeded or Failed"
Jan 18 22:52:17.794: INFO: Pod "pod-ea19d8f7-73c4-425b-bb7d-2bf016425b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.308212ms
Jan 18 22:52:19.798: INFO: Pod "pod-ea19d8f7-73c4-425b-bb7d-2bf016425b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009439002s
Jan 18 22:52:21.801: INFO: Pod "pod-ea19d8f7-73c4-425b-bb7d-2bf016425b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012620354s
Jan 18 22:52:23.803: INFO: Pod "pod-ea19d8f7-73c4-425b-bb7d-2bf016425b7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014903401s
STEP: Saw pod success
Jan 18 22:52:23.803: INFO: Pod "pod-ea19d8f7-73c4-425b-bb7d-2bf016425b7f" satisfied condition "Succeeded or Failed"
Jan 18 22:52:23.805: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-ea19d8f7-73c4-425b-bb7d-2bf016425b7f container test-container: <nil>
STEP: delete the pod
Jan 18 22:52:23.819: INFO: Waiting for pod pod-ea19d8f7-73c4-425b-bb7d-2bf016425b7f to disappear
Jan 18 22:52:23.821: INFO: Pod pod-ea19d8f7-73c4-425b-bb7d-2bf016425b7f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 18 22:52:23.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5314" for this suite.

• [SLOW TEST:6.106 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":105,"skipped":1786,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:52:23.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-a608d6ad-576b-48a5-9759-7d88d6fcc865
STEP: Creating a pod to test consume secrets
W0118 22:52:23.896205      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:52:23.896: INFO: Waiting up to 5m0s for pod "pod-secrets-cf34f5b3-aa80-4494-a34a-d88441444dfa" in namespace "secrets-9476" to be "Succeeded or Failed"
Jan 18 22:52:23.910: INFO: Pod "pod-secrets-cf34f5b3-aa80-4494-a34a-d88441444dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 14.304299ms
Jan 18 22:52:25.913: INFO: Pod "pod-secrets-cf34f5b3-aa80-4494-a34a-d88441444dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01693593s
Jan 18 22:52:27.916: INFO: Pod "pod-secrets-cf34f5b3-aa80-4494-a34a-d88441444dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020047761s
Jan 18 22:52:29.919: INFO: Pod "pod-secrets-cf34f5b3-aa80-4494-a34a-d88441444dfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023243292s
STEP: Saw pod success
Jan 18 22:52:29.919: INFO: Pod "pod-secrets-cf34f5b3-aa80-4494-a34a-d88441444dfa" satisfied condition "Succeeded or Failed"
Jan 18 22:52:29.921: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-secrets-cf34f5b3-aa80-4494-a34a-d88441444dfa container secret-volume-test: <nil>
STEP: delete the pod
Jan 18 22:52:29.941: INFO: Waiting for pod pod-secrets-cf34f5b3-aa80-4494-a34a-d88441444dfa to disappear
Jan 18 22:52:29.945: INFO: Pod pod-secrets-cf34f5b3-aa80-4494-a34a-d88441444dfa no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 18 22:52:29.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9476" for this suite.

• [SLOW TEST:6.129 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":106,"skipped":1792,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:52:29.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 18 22:52:30.232: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Jan 18 22:52:32.239: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 52, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 52, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 52, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 52, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-677b6dd845\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 22:52:35.251: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:52:35.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 22:52:38.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6057" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:8.433 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":356,"completed":107,"skipped":1795,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:52:38.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:52:38.529: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 18 22:52:38.540: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:38.540: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:38.540: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:38.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:52:38.543: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:52:39.549: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:39.549: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:39.549: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:39.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:52:39.551: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:52:40.547: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:40.547: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:40.547: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:40.550: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:52:40.550: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:52:41.548: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:41.548: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:41.548: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:41.550: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 18 22:52:41.550: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:52:42.548: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:42.548: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:42.548: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:42.550: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 18 22:52:42.550: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 18 22:52:42.577: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:42.577: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:42.577: INFO: Wrong image for pod: daemon-set-f2zq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:42.577: INFO: Wrong image for pod: daemon-set-klctg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:42.577: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:42.577: INFO: Wrong image for pod: daemon-set-zvh97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:42.581: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:42.581: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:42.581: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:43.584: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:43.584: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:43.584: INFO: Wrong image for pod: daemon-set-f2zq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:43.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:43.584: INFO: Wrong image for pod: daemon-set-zvh97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:43.588: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:43.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:43.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:44.584: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:44.584: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:44.584: INFO: Wrong image for pod: daemon-set-f2zq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:44.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:44.584: INFO: Pod daemon-set-sflmn is not available
Jan 18 22:52:44.584: INFO: Wrong image for pod: daemon-set-zvh97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:44.589: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:44.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:44.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:45.584: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:45.584: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:45.584: INFO: Wrong image for pod: daemon-set-f2zq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:45.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:45.584: INFO: Pod daemon-set-sflmn is not available
Jan 18 22:52:45.584: INFO: Wrong image for pod: daemon-set-zvh97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:45.588: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:45.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:45.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:46.584: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:46.584: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:46.584: INFO: Wrong image for pod: daemon-set-f2zq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:46.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:46.584: INFO: Pod daemon-set-sflmn is not available
Jan 18 22:52:46.584: INFO: Wrong image for pod: daemon-set-zvh97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:46.591: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:46.591: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:46.591: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:47.584: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:47.584: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:47.584: INFO: Wrong image for pod: daemon-set-f2zq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:47.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:47.584: INFO: Pod daemon-set-sflmn is not available
Jan 18 22:52:47.584: INFO: Wrong image for pod: daemon-set-zvh97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:47.588: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:47.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:47.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:48.585: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:48.585: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:48.585: INFO: Wrong image for pod: daemon-set-f2zq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:48.585: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:48.585: INFO: Pod daemon-set-sflmn is not available
Jan 18 22:52:48.585: INFO: Wrong image for pod: daemon-set-zvh97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:48.589: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:48.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:48.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:49.584: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:49.584: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:49.584: INFO: Wrong image for pod: daemon-set-f2zq2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:49.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:49.584: INFO: Pod daemon-set-sflmn is not available
Jan 18 22:52:49.584: INFO: Wrong image for pod: daemon-set-zvh97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:49.590: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:49.590: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:49.590: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:50.584: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:50.584: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:50.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:50.584: INFO: Wrong image for pod: daemon-set-zvh97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:50.589: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:50.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:50.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:51.584: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:51.584: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:51.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:51.584: INFO: Wrong image for pod: daemon-set-zvh97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:51.588: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:51.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:51.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:52.585: INFO: Pod daemon-set-8mcw8 is not available
Jan 18 22:52:52.585: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:52.585: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:52.585: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:52.585: INFO: Wrong image for pod: daemon-set-zvh97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:52.588: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:52.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:52.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:53.585: INFO: Pod daemon-set-8mcw8 is not available
Jan 18 22:52:53.585: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:53.585: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:53.585: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:53.585: INFO: Wrong image for pod: daemon-set-zvh97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:53.588: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:53.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:53.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:54.584: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:54.584: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:54.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:54.587: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:54.587: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:54.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:55.585: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:55.585: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:55.585: INFO: Pod daemon-set-gczxz is not available
Jan 18 22:52:55.585: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:55.589: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:55.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:55.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:56.584: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:56.584: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:56.584: INFO: Pod daemon-set-gczxz is not available
Jan 18 22:52:56.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:56.588: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:56.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:56.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:57.584: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:57.584: INFO: Wrong image for pod: daemon-set-b5j4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:57.584: INFO: Pod daemon-set-gczxz is not available
Jan 18 22:52:57.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:57.587: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:57.587: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:57.587: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:58.584: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:58.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:58.589: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:58.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:58.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:59.584: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:59.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:52:59.588: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:59.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:52:59.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:00.597: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:53:00.649: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:53:00.662: INFO: Pod daemon-set-v6lw4 is not available
Jan 18 22:53:00.698: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:00.720: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:00.741: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:01.585: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:53:01.585: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:53:01.585: INFO: Pod daemon-set-v6lw4 is not available
Jan 18 22:53:01.589: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:01.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:01.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:02.584: INFO: Wrong image for pod: daemon-set-977zz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:53:02.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:53:02.584: INFO: Pod daemon-set-v6lw4 is not available
Jan 18 22:53:02.588: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:02.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:02.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:03.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:53:03.588: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:03.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:03.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:04.584: INFO: Pod daemon-set-mhhbk is not available
Jan 18 22:53:04.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:53:04.589: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:04.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:04.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:05.585: INFO: Pod daemon-set-mhhbk is not available
Jan 18 22:53:05.585: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:53:05.589: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:05.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:05.589: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:06.584: INFO: Pod daemon-set-mhhbk is not available
Jan 18 22:53:06.584: INFO: Wrong image for pod: daemon-set-mws6s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 22:53:06.588: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:06.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:06.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:07.588: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:07.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:07.588: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:08.584: INFO: Pod daemon-set-tm97l is not available
Jan 18 22:53:08.590: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:08.590: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:08.590: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 18 22:53:08.596: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:08.596: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:08.596: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:08.600: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 18 22:53:08.600: INFO: Node ip-10-0-211-217.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:53:09.605: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:09.605: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:09.605: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:09.608: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 18 22:53:09.608: INFO: Node ip-10-0-211-217.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:53:10.604: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:10.604: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:10.604: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:10.607: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 18 22:53:10.607: INFO: Node ip-10-0-211-217.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:53:11.604: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:11.604: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:11.604: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:53:11.607: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 18 22:53:11.607: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7472, will wait for the garbage collector to delete the pods
Jan 18 22:53:11.674: INFO: Deleting DaemonSet.extensions daemon-set took: 4.82149ms
Jan 18 22:53:11.774: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.838859ms
Jan 18 22:53:14.478: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:53:14.478: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 22:53:14.480: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"228437"},"items":null}

Jan 18 22:53:14.482: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"228437"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 18 22:53:14.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7472" for this suite.

• [SLOW TEST:36.113 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":356,"completed":108,"skipped":1824,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:53:14.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 18 22:53:14.573: INFO: Waiting up to 5m0s for pod "pod-dd5e914e-9ff6-4f32-95e0-6bdfad725396" in namespace "emptydir-3010" to be "Succeeded or Failed"
Jan 18 22:53:14.576: INFO: Pod "pod-dd5e914e-9ff6-4f32-95e0-6bdfad725396": Phase="Pending", Reason="", readiness=false. Elapsed: 3.021148ms
Jan 18 22:53:16.578: INFO: Pod "pod-dd5e914e-9ff6-4f32-95e0-6bdfad725396": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005555351s
Jan 18 22:53:18.581: INFO: Pod "pod-dd5e914e-9ff6-4f32-95e0-6bdfad725396": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008160338s
Jan 18 22:53:20.586: INFO: Pod "pod-dd5e914e-9ff6-4f32-95e0-6bdfad725396": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013186111s
STEP: Saw pod success
Jan 18 22:53:20.586: INFO: Pod "pod-dd5e914e-9ff6-4f32-95e0-6bdfad725396" satisfied condition "Succeeded or Failed"
Jan 18 22:53:20.588: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-dd5e914e-9ff6-4f32-95e0-6bdfad725396 container test-container: <nil>
STEP: delete the pod
Jan 18 22:53:20.605: INFO: Waiting for pod pod-dd5e914e-9ff6-4f32-95e0-6bdfad725396 to disappear
Jan 18 22:53:20.608: INFO: Pod pod-dd5e914e-9ff6-4f32-95e0-6bdfad725396 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 18 22:53:20.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3010" for this suite.

• [SLOW TEST:6.108 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":109,"skipped":1830,"failed":0}
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:53:20.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 22:53:20.636: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 22:53:20.646: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 22:53:20.680: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-128-7.ec2.internal before test
Jan 18 22:53:20.754: INFO: addon-operator-manager-c8c4859bf-84wp8 from openshift-addon-operator started at 2023-01-18 21:36:25 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.754: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:53:20.754: INFO: 	Container metrics-relay-server ready: true, restart count 0
Jan 18 22:53:20.754: INFO: addon-operator-webhooks-569bd4cfc5-dz6nk from openshift-addon-operator started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.754: INFO: 	Container webhook ready: true, restart count 0
Jan 18 22:53:20.754: INFO: osd-delete-ownerrefs-serviceaccounts-27901297-slzq7 from openshift-backplane-srep started at 2023-01-18 21:37:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.754: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 18 22:53:20.754: INFO: aws-ebs-csi-driver-node-mq4qf from openshift-cluster-csi-drivers started at 2023-01-18 21:27:18 +0000 UTC (3 container statuses recorded)
Jan 18 22:53:20.754: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 22:53:20.754: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 22:53:20.754: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 22:53:20.754: INFO: tuned-l9d9s from openshift-cluster-node-tuning-operator started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.754: INFO: 	Container tuned ready: true, restart count 1
Jan 18 22:53:20.754: INFO: dns-default-btls8 from openshift-dns started at 2023-01-18 22:06:24 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.754: INFO: 	Container dns ready: true, restart count 0
Jan 18 22:53:20.754: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:53:20.754: INFO: node-resolver-vhz2m from openshift-dns started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.754: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 22:53:20.754: INFO: node-ca-6cvvj from openshift-image-registry started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.754: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 22:53:20.754: INFO: ingress-canary-qw8v9 from openshift-ingress-canary started at 2023-01-18 21:28:03 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.754: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 22:53:20.754: INFO: router-default-7cff97cd98-dqgwn from openshift-ingress started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.754: INFO: 	Container router ready: true, restart count 0
Jan 18 22:53:20.754: INFO: machine-config-daemon-htf69 from openshift-machine-config-operator started at 2023-01-18 21:27:18 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.754: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 22:53:20.754: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 22:53:20.754: INFO: osd-patch-subscription-source-27901320-gz2jm from openshift-marketplace started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.754: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 18 22:53:20.754: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (6 container statuses recorded)
Jan 18 22:53:20.754: INFO: 	Container alertmanager ready: true, restart count 0
Jan 18 22:53:20.754: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 18 22:53:20.754: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 22:53:20.754: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:53:20.754: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 18 22:53:20.754: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 22:53:20.754: INFO: kube-state-metrics-76877575d5-ptnh8 from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (3 container statuses recorded)
Jan 18 22:53:20.754: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 18 22:53:20.754: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 18 22:53:20.754: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 18 22:53:20.754: INFO: node-exporter-spmbc from openshift-monitoring started at 2023-01-18 21:27:18 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.754: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.754: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 22:53:20.755: INFO: openshift-state-metrics-59fc669d8d-v25vz from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (3 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jan 18 22:53:20.755: INFO: prometheus-adapter-cf64f7f46-7bf79 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 18 22:53:20.755: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (6 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container prometheus ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 18 22:53:20.755: INFO: prometheus-operator-76957bb5bd-2pztb from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 18 22:53:20.755: INFO: prometheus-operator-admission-webhook-577cc9c956-d6fkv from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 22:53:20.755: INFO: sre-dns-latency-exporter-8dptc from openshift-monitoring started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container main ready: true, restart count 1
Jan 18 22:53:20.755: INFO: telemeter-client-6bb748465-zttx9 from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (3 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container reload ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container telemeter-client ready: true, restart count 0
Jan 18 22:53:20.755: INFO: thanos-querier-57f44c5498-xjsz4 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (6 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container thanos-query ready: true, restart count 0
Jan 18 22:53:20.755: INFO: multus-additional-cni-plugins-5lmsf from openshift-multus started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 22:53:20.755: INFO: multus-znqj6 from openshift-multus started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 22:53:20.755: INFO: network-metrics-daemon-ffdlk from openshift-multus started at 2023-01-18 21:27:18 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.755: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 22:53:20.755: INFO: network-check-target-xwp8p from openshift-network-diagnostics started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 22:53:20.755: INFO: ovnkube-node-mkzt4 from openshift-ovn-kubernetes started at 2023-01-18 21:27:18 +0000 UTC (5 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.755: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 22:53:20.755: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 22:53:20.755: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 22:53:20.755: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 22:53:20.755: INFO: splunkforwarder-ds-fbjlj from openshift-security started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 22:53:20.755: INFO: deployments-pruner-27901320-vt4j8 from openshift-sre-pruning started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 18 22:53:20.755: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-dwhpp from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.755: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:53:20.755: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-200-13.ec2.internal before test
Jan 18 22:53:20.825: INFO: aws-ebs-csi-driver-node-6nf8l from openshift-cluster-csi-drivers started at 2023-01-18 21:02:39 +0000 UTC (3 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 22:53:20.825: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 22:53:20.825: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 22:53:20.825: INFO: tuned-t6mkr from openshift-cluster-node-tuning-operator started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container tuned ready: true, restart count 1
Jan 18 22:53:20.825: INFO: downloads-6f74f6fcbf-hcggr from openshift-console started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container download-server ready: true, restart count 0
Jan 18 22:53:20.825: INFO: dns-default-vkx5x from openshift-dns started at 2023-01-18 21:03:27 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container dns ready: true, restart count 1
Jan 18 22:53:20.825: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.825: INFO: node-resolver-gxvcl from openshift-dns started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 22:53:20.825: INFO: image-registry-7b8f8dcdc5-4lfps from openshift-image-registry started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container registry ready: true, restart count 0
Jan 18 22:53:20.825: INFO: node-ca-5tw9r from openshift-image-registry started at 2023-01-18 21:02:58 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 22:53:20.825: INFO: ingress-canary-ppjjg from openshift-ingress-canary started at 2023-01-18 21:03:27 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 22:53:20.825: INFO: machine-config-daemon-smj7g from openshift-machine-config-operator started at 2023-01-18 21:02:39 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 22:53:20.825: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 22:53:20.825: INFO: managed-node-metadata-operator-5d4c567575-cbrxh from openshift-managed-node-metadata-operator started at 2023-01-18 21:24:36 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container managed-node-metadata-operator ready: true, restart count 0
Jan 18 22:53:20.825: INFO: configure-alertmanager-operator-7565458cf4-pbmjl from openshift-monitoring started at 2023-01-18 21:24:36 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Jan 18 22:53:20.825: INFO: node-exporter-gzf8b from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.825: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 22:53:20.825: INFO: sre-dns-latency-exporter-s6nk6 from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container main ready: true, restart count 1
Jan 18 22:53:20.825: INFO: token-refresher-6d8f85f497-mhth9 from openshift-monitoring started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container token-refresher ready: true, restart count 0
Jan 18 22:53:20.825: INFO: multus-additional-cni-plugins-nm8v9 from openshift-multus started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 22:53:20.825: INFO: multus-cfrv2 from openshift-multus started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 22:53:20.825: INFO: network-metrics-daemon-m4c6r from openshift-multus started at 2023-01-18 21:02:39 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.825: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 22:53:20.825: INFO: network-check-target-hpzlx from openshift-network-diagnostics started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 22:53:20.825: INFO: obo-prometheus-operator-admission-webhook-5b4dc9bff5-l6qvc from openshift-observability-operator started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 22:53:20.825: INFO: ocm-agent-75d95f8dc7-gtjwg from openshift-ocm-agent-operator started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container ocm-agent ready: true, restart count 0
Jan 18 22:53:20.825: INFO: collect-profiles-27901335-75lm8 from openshift-operator-lifecycle-manager started at 2023-01-18 22:15:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 18 22:53:20.825: INFO: collect-profiles-27901350-z8qrc from openshift-operator-lifecycle-manager started at 2023-01-18 22:30:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 18 22:53:20.825: INFO: collect-profiles-27901365-9rmcw from openshift-operator-lifecycle-manager started at 2023-01-18 22:45:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 18 22:53:20.825: INFO: osd-metrics-exporter-756f85967-8z4cz from openshift-osd-metrics started at 2023-01-18 21:25:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container osd-metrics-exporter ready: true, restart count 0
Jan 18 22:53:20.825: INFO: osd-metrics-exporter-registry-6rnzs from openshift-osd-metrics started at 2023-01-18 21:24:12 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:53:20.825: INFO: ovnkube-node-h62xt from openshift-ovn-kubernetes started at 2023-01-18 21:02:39 +0000 UTC (5 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.825: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 22:53:20.825: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 22:53:20.825: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 22:53:20.825: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 22:53:20.825: INFO: rbac-permissions-operator-7f6bc8977-vsvkh from openshift-rbac-permissions started at 2023-01-18 21:24:29 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Jan 18 22:53:20.825: INFO: blackbox-exporter-dfdd57dd6-tj4kz from openshift-route-monitor-operator started at 2023-01-18 21:24:50 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan 18 22:53:20.825: INFO: route-monitor-operator-controller-manager-cbc597f9d-84p87 from openshift-route-monitor-operator started at 2023-01-18 21:24:37 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:53:20.825: INFO: route-monitor-operator-registry-cgj4n from openshift-route-monitor-operator started at 2023-01-18 21:24:36 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:53:20.825: INFO: splunkforwarder-ds-n7d7t from openshift-security started at 2023-01-18 21:24:58 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container splunk-uf ready: true, restart count 0
Jan 18 22:53:20.825: INFO: splunk-forwarder-operator-6f5bf57497-jwcz5 from openshift-splunk-forwarder-operator started at 2023-01-18 21:24:40 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Jan 18 22:53:20.825: INFO: managed-velero-operator-f9f4c8b45-xml88 from openshift-velero started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container managed-velero-operator ready: true, restart count 0
Jan 18 22:53:20.825: INFO: velero-749f7746d8-ds5cs from openshift-velero started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container velero ready: true, restart count 0
Jan 18 22:53:20.825: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-mdcrg from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.825: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:53:20.825: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:53:20.825: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-211-217.ec2.internal before test
Jan 18 22:53:20.892: INFO: addon-operator-webhooks-569bd4cfc5-mdklc from openshift-addon-operator started at 2023-01-18 21:39:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container webhook ready: true, restart count 0
Jan 18 22:53:20.892: INFO: osd-delete-ownerrefs-serviceaccounts-27901327-dmq56 from openshift-backplane-srep started at 2023-01-18 22:07:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 18 22:53:20.892: INFO: osd-delete-ownerrefs-serviceaccounts-27901357-rd8sm from openshift-backplane-srep started at 2023-01-18 22:37:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 18 22:53:20.892: INFO: osd-delete-backplane-serviceaccounts-27901350-7wt5p from openshift-backplane started at 2023-01-18 22:30:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 18 22:53:20.892: INFO: osd-delete-backplane-serviceaccounts-27901360-7zfbh from openshift-backplane started at 2023-01-18 22:40:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 18 22:53:20.892: INFO: osd-delete-backplane-serviceaccounts-27901370-954ch from openshift-backplane started at 2023-01-18 22:50:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 18 22:53:20.892: INFO: sre-build-test-27901331-ppqbv from openshift-build-test started at 2023-01-18 22:11:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container sre-build-test ready: false, restart count 0
Jan 18 22:53:20.892: INFO: aws-ebs-csi-driver-node-l4ng8 from openshift-cluster-csi-drivers started at 2023-01-18 21:27:42 +0000 UTC (3 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 22:53:20.892: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 22:53:20.892: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 22:53:20.892: INFO: tuned-6pmzm from openshift-cluster-node-tuning-operator started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container tuned ready: true, restart count 1
Jan 18 22:53:20.892: INFO: dns-default-7sqw2 from openshift-dns started at 2023-01-18 22:06:24 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container dns ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:53:20.892: INFO: node-resolver-qsc9k from openshift-dns started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 22:53:20.892: INFO: image-pruner-27901320-rjnq8 from openshift-image-registry started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container image-pruner ready: false, restart count 0
Jan 18 22:53:20.892: INFO: node-ca-9gnsm from openshift-image-registry started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 22:53:20.892: INFO: ingress-canary-k52kt from openshift-ingress-canary started at 2023-01-18 21:28:29 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 22:53:20.892: INFO: router-default-7cff97cd98-lstfd from openshift-ingress started at 2023-01-18 21:39:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container router ready: true, restart count 0
Jan 18 22:53:20.892: INFO: machine-config-daemon-krjvz from openshift-machine-config-operator started at 2023-01-18 21:27:42 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 22:53:20.892: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 22:53:20.892: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-18 21:39:18 +0000 UTC (6 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container alertmanager ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 22:53:20.892: INFO: node-exporter-bpnc5 from openshift-monitoring started at 2023-01-18 21:27:42 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.892: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 22:53:20.892: INFO: osd-rebalance-infra-nodes-27901335-msvmm from openshift-monitoring started at 2023-01-18 22:15:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 18 22:53:20.892: INFO: osd-rebalance-infra-nodes-27901350-fnqm2 from openshift-monitoring started at 2023-01-18 22:30:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 18 22:53:20.892: INFO: osd-rebalance-infra-nodes-27901365-hqkcb from openshift-monitoring started at 2023-01-18 22:45:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 18 22:53:20.892: INFO: prometheus-adapter-cf64f7f46-jw64l from openshift-monitoring started at 2023-01-18 21:39:31 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 18 22:53:20.892: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-18 21:39:17 +0000 UTC (6 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container prometheus ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 18 22:53:20.892: INFO: prometheus-operator-admission-webhook-577cc9c956-lwfrr from openshift-monitoring started at 2023-01-18 21:39:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 22:53:20.892: INFO: sre-dns-latency-exporter-tlqwt from openshift-monitoring started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container main ready: true, restart count 1
Jan 18 22:53:20.892: INFO: thanos-querier-57f44c5498-fzjjb from openshift-monitoring started at 2023-01-18 21:39:18 +0000 UTC (6 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container thanos-query ready: true, restart count 0
Jan 18 22:53:20.892: INFO: multus-additional-cni-plugins-2q4s4 from openshift-multus started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 22:53:20.892: INFO: multus-cz5tw from openshift-multus started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 22:53:20.892: INFO: network-metrics-daemon-jnkxc from openshift-multus started at 2023-01-18 21:27:42 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.892: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 22:53:20.892: INFO: network-check-target-pjt5v from openshift-network-diagnostics started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 22:53:20.892: INFO: ovnkube-node-bf8j7 from openshift-ovn-kubernetes started at 2023-01-18 21:27:42 +0000 UTC (5 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.892: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 22:53:20.892: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 22:53:20.892: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 22:53:20.892: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 22:53:20.892: INFO: splunkforwarder-ds-gr4fb from openshift-security started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 22:53:20.892: INFO: builds-pruner-27901320-2bl9l from openshift-sre-pruning started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 18 22:53:20.892: INFO: sonobuoy-e2e-job-aed0e1be5f434192 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container e2e ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:53:20.892: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-tjlv7 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.892: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:53:20.892: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-219-147.ec2.internal before test
Jan 18 22:53:20.932: INFO: aws-ebs-csi-driver-node-qprtw from openshift-cluster-csi-drivers started at 2023-01-18 21:03:48 +0000 UTC (3 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 22:53:20.932: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 22:53:20.932: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 22:53:20.932: INFO: tuned-246td from openshift-cluster-node-tuning-operator started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container tuned ready: true, restart count 1
Jan 18 22:53:20.932: INFO: dns-default-lm6xx from openshift-dns started at 2023-01-18 22:22:26 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container dns ready: true, restart count 0
Jan 18 22:53:20.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:53:20.932: INFO: node-resolver-svmsb from openshift-dns started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 22:53:20.932: INFO: node-ca-nzbcp from openshift-image-registry started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 22:53:20.932: INFO: ingress-canary-c2z69 from openshift-ingress-canary started at 2023-01-18 22:22:26 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 18 22:53:20.932: INFO: machine-config-daemon-nb5xf from openshift-machine-config-operator started at 2023-01-18 21:03:48 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 22:53:20.932: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 22:53:20.932: INFO: node-exporter-6m8v9 from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.932: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 22:53:20.932: INFO: sre-dns-latency-exporter-rxh5d from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container main ready: true, restart count 1
Jan 18 22:53:20.932: INFO: multus-55cv4 from openshift-multus started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 22:53:20.932: INFO: multus-additional-cni-plugins-6n2cz from openshift-multus started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 22:53:20.932: INFO: network-metrics-daemon-mkvpn from openshift-multus started at 2023-01-18 21:03:48 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.932: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 22:53:20.932: INFO: network-check-target-cqk2p from openshift-network-diagnostics started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 22:53:20.932: INFO: ovnkube-node-7vtb8 from openshift-ovn-kubernetes started at 2023-01-18 21:03:48 +0000 UTC (5 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.932: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 22:53:20.932: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 22:53:20.932: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 22:53:20.932: INFO: 	Container ovnkube-node ready: true, restart count 4
Jan 18 22:53:20.932: INFO: splunkforwarder-ds-b6n2g from openshift-security started at 2023-01-18 21:24:58 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 22:53:20.932: INFO: sonobuoy from sonobuoy started at 2023-01-18 22:14:33 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 22:53:20.932: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-shjrl from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.932: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:53:20.932: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:53:20.932: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-236-5.ec2.internal before test
Jan 18 22:53:20.987: INFO: addon-operator-catalog-67nfj from openshift-addon-operator started at 2023-01-18 21:26:21 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:53:20.987: INFO: cloud-ingress-operator-registry-6f5sh from openshift-cloud-ingress-operator started at 2023-01-18 21:31:22 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:53:20.987: INFO: aws-ebs-csi-driver-node-cv9gd from openshift-cluster-csi-drivers started at 2023-01-18 21:03:23 +0000 UTC (3 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 22:53:20.987: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 22:53:20.987: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 22:53:20.987: INFO: tuned-b2gvw from openshift-cluster-node-tuning-operator started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container tuned ready: true, restart count 1
Jan 18 22:53:20.987: INFO: downloads-6f74f6fcbf-w6ht7 from openshift-console started at 2023-01-18 21:31:17 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container download-server ready: true, restart count 0
Jan 18 22:53:20.987: INFO: custom-domains-operator-7f97f586c8-czz24 from openshift-custom-domains-operator started at 2023-01-18 21:26:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container custom-domains-operator ready: true, restart count 0
Jan 18 22:53:20.987: INFO: custom-domains-operator-registry-sd6db from openshift-custom-domains-operator started at 2023-01-18 21:26:24 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:53:20.987: INFO: dns-default-4cddc from openshift-dns started at 2023-01-18 21:04:10 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container dns ready: true, restart count 1
Jan 18 22:53:20.987: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.987: INFO: node-resolver-8lsl6 from openshift-dns started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 22:53:20.987: INFO: node-ca-r29sw from openshift-image-registry started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 22:53:20.987: INFO: ingress-canary-d6jm7 from openshift-ingress-canary started at 2023-01-18 21:04:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 22:53:20.987: INFO: machine-config-daemon-xv85b from openshift-machine-config-operator started at 2023-01-18 21:03:23 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 22:53:20.987: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 22:53:20.987: INFO: managed-node-metadata-operator-registry-fn84g from openshift-managed-node-metadata-operator started at 2023-01-18 21:31:22 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:53:20.987: INFO: managed-upgrade-operator-catalog-g29pr from openshift-managed-upgrade-operator started at 2023-01-18 21:31:23 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:53:20.987: INFO: configure-alertmanager-operator-registry-vzl8c from openshift-monitoring started at 2023-01-18 21:31:25 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:53:20.987: INFO: node-exporter-6q46p from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.987: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 22:53:20.987: INFO: osd-cluster-ready-kfqk7 from openshift-monitoring started at 2023-01-18 21:26:19 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container osd-cluster-ready ready: false, restart count 21
Jan 18 22:53:20.987: INFO: sre-dns-latency-exporter-q7lp5 from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container main ready: true, restart count 1
Jan 18 22:53:20.987: INFO: sre-ebs-iops-reporter-1-l2chd from openshift-monitoring started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container main ready: true, restart count 0
Jan 18 22:53:20.987: INFO: sre-stuck-ebs-vols-1-mjmmn from openshift-monitoring started at 2023-01-18 21:26:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container main ready: true, restart count 0
Jan 18 22:53:20.987: INFO: multus-additional-cni-plugins-p78ph from openshift-multus started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 22:53:20.987: INFO: multus-wtvrb from openshift-multus started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 22:53:20.987: INFO: network-metrics-daemon-dxnrq from openshift-multus started at 2023-01-18 21:03:23 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.987: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 22:53:20.987: INFO: must-gather-operator-5f47db765d-zf7fj from openshift-must-gather-operator started at 2023-01-18 21:26:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container must-gather-operator ready: true, restart count 0
Jan 18 22:53:20.987: INFO: must-gather-operator-registry-gp8pc from openshift-must-gather-operator started at 2023-01-18 21:26:22 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:53:20.987: INFO: network-check-target-tr8pn from openshift-network-diagnostics started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 22:53:20.987: INFO: obo-prometheus-operator-6cb5cfc7b9-2rkdt from openshift-observability-operator started at 2023-01-18 21:26:20 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 18 22:53:20.987: INFO: obo-prometheus-operator-admission-webhook-5b4dc9bff5-f8pm5 from openshift-observability-operator started at 2023-01-18 21:26:21 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 22:53:20.987: INFO: observability-operator-5b467d8ccb-twlnn from openshift-observability-operator started at 2023-01-18 21:26:20 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container operator ready: true, restart count 0
Jan 18 22:53:20.987: INFO: observability-operator-catalog-khgpx from openshift-observability-operator started at 2023-01-18 21:26:25 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:53:20.987: INFO: ocm-agent-operator-9bd68bf49-z6qxl from openshift-ocm-agent-operator started at 2023-01-18 21:26:19 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container ocm-agent-operator ready: true, restart count 0
Jan 18 22:53:20.987: INFO: ocm-agent-operator-registry-qj69h from openshift-ocm-agent-operator started at 2023-01-18 21:26:24 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:53:20.987: INFO: ovnkube-node-g6c8t from openshift-ovn-kubernetes started at 2023-01-18 21:03:23 +0000 UTC (5 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:20.987: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 22:53:20.987: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 22:53:20.987: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 22:53:20.987: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 22:53:20.987: INFO: rbac-permissions-operator-registry-mqh7k from openshift-rbac-permissions started at 2023-01-18 21:26:26 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:53:20.987: INFO: splunkforwarder-ds-zhvhl from openshift-security started at 2023-01-18 21:25:56 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container splunk-uf ready: true, restart count 0
Jan 18 22:53:20.987: INFO: splunk-forwarder-operator-catalog-twzbw from openshift-splunk-forwarder-operator started at 2023-01-18 21:26:27 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:53:20.987: INFO: managed-velero-operator-registry-9j2kp from openshift-velero started at 2023-01-18 21:31:21 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:53:20.987: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-t5zp4 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:20.987: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:53:20.987: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:53:20.987: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-253-152.ec2.internal before test
Jan 18 22:53:21.039: INFO: cloud-ingress-operator-78d58985cd-9kp9q from openshift-cloud-ingress-operator started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Jan 18 22:53:21.039: INFO: aws-ebs-csi-driver-node-97pk2 from openshift-cluster-csi-drivers started at 2023-01-18 21:03:06 +0000 UTC (3 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 22:53:21.039: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 22:53:21.039: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 22:53:21.039: INFO: tuned-5wpdt from openshift-cluster-node-tuning-operator started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container tuned ready: true, restart count 1
Jan 18 22:53:21.039: INFO: dns-default-g4nw5 from openshift-dns started at 2023-01-18 21:03:53 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container dns ready: true, restart count 1
Jan 18 22:53:21.039: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:21.039: INFO: node-resolver-ffd24 from openshift-dns started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 22:53:21.039: INFO: image-registry-7b8f8dcdc5-2bvm7 from openshift-image-registry started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container registry ready: true, restart count 0
Jan 18 22:53:21.039: INFO: node-ca-bh7jz from openshift-image-registry started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 22:53:21.039: INFO: ingress-canary-vz8dj from openshift-ingress-canary started at 2023-01-18 21:03:53 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 22:53:21.039: INFO: machine-config-daemon-5zsxx from openshift-machine-config-operator started at 2023-01-18 21:03:06 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 22:53:21.039: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 22:53:21.039: INFO: node-exporter-z5k8l from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:21.039: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 22:53:21.039: INFO: sre-dns-latency-exporter-zsmln from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container main ready: true, restart count 1
Jan 18 22:53:21.039: INFO: multus-additional-cni-plugins-hsjtm from openshift-multus started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 22:53:21.039: INFO: multus-xzh2h from openshift-multus started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 22:53:21.039: INFO: network-metrics-daemon-cq5sl from openshift-multus started at 2023-01-18 21:03:06 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:21.039: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 22:53:21.039: INFO: network-check-source-5cb989cf6f-42gl2 from openshift-network-diagnostics started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container check-endpoints ready: true, restart count 0
Jan 18 22:53:21.039: INFO: network-check-target-9fq7l from openshift-network-diagnostics started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 22:53:21.039: INFO: ovnkube-node-mtfln from openshift-ovn-kubernetes started at 2023-01-18 21:03:06 +0000 UTC (5 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:53:21.039: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 22:53:21.039: INFO: 	Container ovn-acl-logging ready: true, restart count 2
Jan 18 22:53:21.039: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 22:53:21.039: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 22:53:21.039: INFO: splunkforwarder-ds-xsjtq from openshift-security started at 2023-01-18 21:24:58 +0000 UTC (1 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 22:53:21.039: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-ktxs4 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 22:53:21.039: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:53:21.039: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a357f06a-f622-4842-8600-9ed98e58d7cf 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-a357f06a-f622-4842-8600-9ed98e58d7cf off the node ip-10-0-219-147.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a357f06a-f622-4842-8600-9ed98e58d7cf
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 18 22:53:29.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3841" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:8.545 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":356,"completed":110,"skipped":1830,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:53:29.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-8aeffd5c-b703-46af-95e8-98a33306b6a2 in namespace container-probe-7648
W0118 22:53:29.236483      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:53:33.253: INFO: Started pod liveness-8aeffd5c-b703-46af-95e8-98a33306b6a2 in namespace container-probe-7648
STEP: checking the pod's current state and verifying that restartCount is present
Jan 18 22:53:33.255: INFO: Initial restart count of pod liveness-8aeffd5c-b703-46af-95e8-98a33306b6a2 is 0
Jan 18 22:53:51.293: INFO: Restart count of pod container-probe-7648/liveness-8aeffd5c-b703-46af-95e8-98a33306b6a2 is now 1 (18.037932956s elapsed)
Jan 18 22:54:11.354: INFO: Restart count of pod container-probe-7648/liveness-8aeffd5c-b703-46af-95e8-98a33306b6a2 is now 2 (38.09919679s elapsed)
Jan 18 22:54:31.384: INFO: Restart count of pod container-probe-7648/liveness-8aeffd5c-b703-46af-95e8-98a33306b6a2 is now 3 (58.129298933s elapsed)
Jan 18 22:54:51.420: INFO: Restart count of pod container-probe-7648/liveness-8aeffd5c-b703-46af-95e8-98a33306b6a2 is now 4 (1m18.165632737s elapsed)
Jan 18 22:55:55.562: INFO: Restart count of pod container-probe-7648/liveness-8aeffd5c-b703-46af-95e8-98a33306b6a2 is now 5 (2m22.307135737s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 18 22:55:55.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7648" for this suite.

• [SLOW TEST:146.421 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":356,"completed":111,"skipped":1853,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:55:55.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 22:55:55.919: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 22:55:57.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 55, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 55, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 55, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 55, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:55:59.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 55, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 55, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 55, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 55, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 22:56:02.943: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 22:56:03.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8961" for this suite.
STEP: Destroying namespace "webhook-8961-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.611 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":356,"completed":112,"skipped":1853,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:56:03.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
W0118 22:56:03.241776      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:56:03.251: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 18 22:56:08.255: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 18 22:56:08.255: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 22:56:08.270: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9492  21c29e25-ebed-4e16-9af4-b9ef28088a83 234154 1 2023-01-18 22:56:08 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2023-01-18 22:56:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c8d0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 18 22:56:08.272: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 18 22:56:08.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9492" for this suite.

• [SLOW TEST:5.093 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":356,"completed":113,"skipped":1856,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:56:08.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 22:56:08.357: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f18f02e8-f28d-4fb5-8226-575a14dc1b88" in namespace "downward-api-4574" to be "Succeeded or Failed"
Jan 18 22:56:08.374: INFO: Pod "downwardapi-volume-f18f02e8-f28d-4fb5-8226-575a14dc1b88": Phase="Pending", Reason="", readiness=false. Elapsed: 17.744476ms
Jan 18 22:56:10.385: INFO: Pod "downwardapi-volume-f18f02e8-f28d-4fb5-8226-575a14dc1b88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028208395s
Jan 18 22:56:12.388: INFO: Pod "downwardapi-volume-f18f02e8-f28d-4fb5-8226-575a14dc1b88": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031536911s
Jan 18 22:56:14.393: INFO: Pod "downwardapi-volume-f18f02e8-f28d-4fb5-8226-575a14dc1b88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036430379s
STEP: Saw pod success
Jan 18 22:56:14.393: INFO: Pod "downwardapi-volume-f18f02e8-f28d-4fb5-8226-575a14dc1b88" satisfied condition "Succeeded or Failed"
Jan 18 22:56:14.397: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-f18f02e8-f28d-4fb5-8226-575a14dc1b88 container client-container: <nil>
STEP: delete the pod
Jan 18 22:56:14.504: INFO: Waiting for pod downwardapi-volume-f18f02e8-f28d-4fb5-8226-575a14dc1b88 to disappear
Jan 18 22:56:14.506: INFO: Pod downwardapi-volume-f18f02e8-f28d-4fb5-8226-575a14dc1b88 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 18 22:56:14.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4574" for this suite.

• [SLOW TEST:6.230 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":114,"skipped":1860,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:56:14.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-7ff8a116-9373-46a1-bc91-cdffbee625e9
STEP: Creating a pod to test consume secrets
W0118 22:56:14.581667      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:56:14.581: INFO: Waiting up to 5m0s for pod "pod-secrets-7b02d2d9-4e2f-467e-b500-a84caa8a1761" in namespace "secrets-8666" to be "Succeeded or Failed"
Jan 18 22:56:14.584: INFO: Pod "pod-secrets-7b02d2d9-4e2f-467e-b500-a84caa8a1761": Phase="Pending", Reason="", readiness=false. Elapsed: 2.228209ms
Jan 18 22:56:16.597: INFO: Pod "pod-secrets-7b02d2d9-4e2f-467e-b500-a84caa8a1761": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015650203s
Jan 18 22:56:18.600: INFO: Pod "pod-secrets-7b02d2d9-4e2f-467e-b500-a84caa8a1761": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018565517s
Jan 18 22:56:20.603: INFO: Pod "pod-secrets-7b02d2d9-4e2f-467e-b500-a84caa8a1761": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022194776s
STEP: Saw pod success
Jan 18 22:56:20.604: INFO: Pod "pod-secrets-7b02d2d9-4e2f-467e-b500-a84caa8a1761" satisfied condition "Succeeded or Failed"
Jan 18 22:56:20.606: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-secrets-7b02d2d9-4e2f-467e-b500-a84caa8a1761 container secret-volume-test: <nil>
STEP: delete the pod
Jan 18 22:56:20.621: INFO: Waiting for pod pod-secrets-7b02d2d9-4e2f-467e-b500-a84caa8a1761 to disappear
Jan 18 22:56:20.624: INFO: Pod pod-secrets-7b02d2d9-4e2f-467e-b500-a84caa8a1761 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 18 22:56:20.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8666" for this suite.

• [SLOW TEST:6.117 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":115,"skipped":1884,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:56:20.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:56:20.668: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-90d08b1b-c48f-4349-9e5e-200410dffbb3
STEP: Creating secret with name s-test-opt-upd-a52d8d32-a99a-4cb0-9f3f-ac66d969aa3c
STEP: Creating the pod
W0118 22:56:20.708663      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 22:56:20.713: INFO: The status of Pod pod-secrets-901a65b4-9646-40e3-a406-64fbb0fb175d is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:56:22.716: INFO: The status of Pod pod-secrets-901a65b4-9646-40e3-a406-64fbb0fb175d is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:56:24.716: INFO: The status of Pod pod-secrets-901a65b4-9646-40e3-a406-64fbb0fb175d is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-90d08b1b-c48f-4349-9e5e-200410dffbb3
STEP: Updating secret s-test-opt-upd-a52d8d32-a99a-4cb0-9f3f-ac66d969aa3c
STEP: Creating secret with name s-test-opt-create-c21856c0-60b3-4504-b328-1679b42b66aa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 18 22:57:51.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9652" for this suite.

• [SLOW TEST:90.410 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":116,"skipped":1951,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:57:51.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 22:57:51.082: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 22:57:51.105: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 22:57:51.111: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-128-7.ec2.internal before test
Jan 18 22:57:51.166: INFO: addon-operator-manager-c8c4859bf-84wp8 from openshift-addon-operator started at 2023-01-18 21:36:25 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container metrics-relay-server ready: true, restart count 0
Jan 18 22:57:51.166: INFO: addon-operator-webhooks-569bd4cfc5-dz6nk from openshift-addon-operator started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container webhook ready: true, restart count 0
Jan 18 22:57:51.166: INFO: osd-delete-ownerrefs-serviceaccounts-27901297-slzq7 from openshift-backplane-srep started at 2023-01-18 21:37:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 18 22:57:51.166: INFO: aws-ebs-csi-driver-node-mq4qf from openshift-cluster-csi-drivers started at 2023-01-18 21:27:18 +0000 UTC (3 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 22:57:51.166: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 22:57:51.166: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 22:57:51.166: INFO: tuned-l9d9s from openshift-cluster-node-tuning-operator started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container tuned ready: true, restart count 1
Jan 18 22:57:51.166: INFO: dns-default-btls8 from openshift-dns started at 2023-01-18 22:06:24 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container dns ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:57:51.166: INFO: node-resolver-vhz2m from openshift-dns started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 22:57:51.166: INFO: node-ca-6cvvj from openshift-image-registry started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 22:57:51.166: INFO: ingress-canary-qw8v9 from openshift-ingress-canary started at 2023-01-18 21:28:03 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 22:57:51.166: INFO: router-default-7cff97cd98-dqgwn from openshift-ingress started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container router ready: true, restart count 0
Jan 18 22:57:51.166: INFO: machine-config-daemon-htf69 from openshift-machine-config-operator started at 2023-01-18 21:27:18 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 22:57:51.166: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 22:57:51.166: INFO: osd-patch-subscription-source-27901320-gz2jm from openshift-marketplace started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 18 22:57:51.166: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (6 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container alertmanager ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 22:57:51.166: INFO: kube-state-metrics-76877575d5-ptnh8 from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (3 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 18 22:57:51.166: INFO: node-exporter-spmbc from openshift-monitoring started at 2023-01-18 21:27:18 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.166: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 22:57:51.166: INFO: openshift-state-metrics-59fc669d8d-v25vz from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (3 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jan 18 22:57:51.166: INFO: prometheus-adapter-cf64f7f46-7bf79 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 18 22:57:51.166: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (6 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container prometheus ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 18 22:57:51.166: INFO: prometheus-operator-76957bb5bd-2pztb from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 18 22:57:51.166: INFO: prometheus-operator-admission-webhook-577cc9c956-d6fkv from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 22:57:51.166: INFO: sre-dns-latency-exporter-8dptc from openshift-monitoring started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container main ready: true, restart count 1
Jan 18 22:57:51.166: INFO: telemeter-client-6bb748465-zttx9 from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (3 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container reload ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container telemeter-client ready: true, restart count 0
Jan 18 22:57:51.166: INFO: thanos-querier-57f44c5498-xjsz4 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (6 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container thanos-query ready: true, restart count 0
Jan 18 22:57:51.166: INFO: multus-additional-cni-plugins-5lmsf from openshift-multus started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 22:57:51.166: INFO: multus-znqj6 from openshift-multus started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 22:57:51.166: INFO: network-metrics-daemon-ffdlk from openshift-multus started at 2023-01-18 21:27:18 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.166: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 22:57:51.166: INFO: network-check-target-xwp8p from openshift-network-diagnostics started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 22:57:51.166: INFO: ovnkube-node-mkzt4 from openshift-ovn-kubernetes started at 2023-01-18 21:27:18 +0000 UTC (5 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.166: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 22:57:51.166: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 22:57:51.166: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 22:57:51.166: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 22:57:51.166: INFO: splunkforwarder-ds-fbjlj from openshift-security started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 22:57:51.166: INFO: deployments-pruner-27901320-vt4j8 from openshift-sre-pruning started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 18 22:57:51.166: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-dwhpp from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.166: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:57:51.166: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-200-13.ec2.internal before test
Jan 18 22:57:51.201: INFO: aws-ebs-csi-driver-node-6nf8l from openshift-cluster-csi-drivers started at 2023-01-18 21:02:39 +0000 UTC (3 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 22:57:51.201: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 22:57:51.201: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 22:57:51.201: INFO: tuned-t6mkr from openshift-cluster-node-tuning-operator started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container tuned ready: true, restart count 1
Jan 18 22:57:51.201: INFO: downloads-6f74f6fcbf-hcggr from openshift-console started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container download-server ready: true, restart count 0
Jan 18 22:57:51.201: INFO: dns-default-vkx5x from openshift-dns started at 2023-01-18 21:03:27 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container dns ready: true, restart count 1
Jan 18 22:57:51.201: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.201: INFO: node-resolver-gxvcl from openshift-dns started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 22:57:51.201: INFO: image-registry-7b8f8dcdc5-4lfps from openshift-image-registry started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container registry ready: true, restart count 0
Jan 18 22:57:51.201: INFO: node-ca-5tw9r from openshift-image-registry started at 2023-01-18 21:02:58 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 22:57:51.201: INFO: ingress-canary-ppjjg from openshift-ingress-canary started at 2023-01-18 21:03:27 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 22:57:51.201: INFO: machine-config-daemon-smj7g from openshift-machine-config-operator started at 2023-01-18 21:02:39 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 22:57:51.201: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 22:57:51.201: INFO: managed-node-metadata-operator-5d4c567575-cbrxh from openshift-managed-node-metadata-operator started at 2023-01-18 21:24:36 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container managed-node-metadata-operator ready: true, restart count 0
Jan 18 22:57:51.201: INFO: configure-alertmanager-operator-7565458cf4-pbmjl from openshift-monitoring started at 2023-01-18 21:24:36 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Jan 18 22:57:51.201: INFO: node-exporter-gzf8b from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.201: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 22:57:51.201: INFO: sre-dns-latency-exporter-s6nk6 from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container main ready: true, restart count 1
Jan 18 22:57:51.201: INFO: token-refresher-6d8f85f497-mhth9 from openshift-monitoring started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container token-refresher ready: true, restart count 0
Jan 18 22:57:51.201: INFO: multus-additional-cni-plugins-nm8v9 from openshift-multus started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 22:57:51.201: INFO: multus-cfrv2 from openshift-multus started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 22:57:51.201: INFO: network-metrics-daemon-m4c6r from openshift-multus started at 2023-01-18 21:02:39 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.201: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 22:57:51.201: INFO: network-check-target-hpzlx from openshift-network-diagnostics started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 22:57:51.201: INFO: obo-prometheus-operator-admission-webhook-5b4dc9bff5-l6qvc from openshift-observability-operator started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 22:57:51.201: INFO: ocm-agent-75d95f8dc7-gtjwg from openshift-ocm-agent-operator started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container ocm-agent ready: true, restart count 0
Jan 18 22:57:51.201: INFO: collect-profiles-27901335-75lm8 from openshift-operator-lifecycle-manager started at 2023-01-18 22:15:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 18 22:57:51.201: INFO: collect-profiles-27901350-z8qrc from openshift-operator-lifecycle-manager started at 2023-01-18 22:30:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 18 22:57:51.201: INFO: collect-profiles-27901365-9rmcw from openshift-operator-lifecycle-manager started at 2023-01-18 22:45:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 18 22:57:51.201: INFO: osd-metrics-exporter-756f85967-8z4cz from openshift-osd-metrics started at 2023-01-18 21:25:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container osd-metrics-exporter ready: true, restart count 0
Jan 18 22:57:51.201: INFO: osd-metrics-exporter-registry-6rnzs from openshift-osd-metrics started at 2023-01-18 21:24:12 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:57:51.201: INFO: ovnkube-node-h62xt from openshift-ovn-kubernetes started at 2023-01-18 21:02:39 +0000 UTC (5 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.201: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 22:57:51.201: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 22:57:51.201: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 22:57:51.201: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 22:57:51.201: INFO: rbac-permissions-operator-7f6bc8977-vsvkh from openshift-rbac-permissions started at 2023-01-18 21:24:29 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Jan 18 22:57:51.201: INFO: blackbox-exporter-dfdd57dd6-tj4kz from openshift-route-monitor-operator started at 2023-01-18 21:24:50 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan 18 22:57:51.201: INFO: route-monitor-operator-controller-manager-cbc597f9d-84p87 from openshift-route-monitor-operator started at 2023-01-18 21:24:37 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:57:51.201: INFO: route-monitor-operator-registry-cgj4n from openshift-route-monitor-operator started at 2023-01-18 21:24:36 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:57:51.201: INFO: splunkforwarder-ds-n7d7t from openshift-security started at 2023-01-18 21:24:58 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container splunk-uf ready: true, restart count 0
Jan 18 22:57:51.201: INFO: splunk-forwarder-operator-6f5bf57497-jwcz5 from openshift-splunk-forwarder-operator started at 2023-01-18 21:24:40 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Jan 18 22:57:51.201: INFO: managed-velero-operator-f9f4c8b45-xml88 from openshift-velero started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container managed-velero-operator ready: true, restart count 0
Jan 18 22:57:51.201: INFO: velero-749f7746d8-ds5cs from openshift-velero started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container velero ready: true, restart count 0
Jan 18 22:57:51.201: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-mdcrg from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.201: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:57:51.201: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:57:51.201: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-211-217.ec2.internal before test
Jan 18 22:57:51.245: INFO: addon-operator-webhooks-569bd4cfc5-mdklc from openshift-addon-operator started at 2023-01-18 21:39:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container webhook ready: true, restart count 0
Jan 18 22:57:51.245: INFO: osd-delete-ownerrefs-serviceaccounts-27901327-dmq56 from openshift-backplane-srep started at 2023-01-18 22:07:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 18 22:57:51.245: INFO: osd-delete-ownerrefs-serviceaccounts-27901357-rd8sm from openshift-backplane-srep started at 2023-01-18 22:37:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 18 22:57:51.245: INFO: osd-delete-backplane-serviceaccounts-27901350-7wt5p from openshift-backplane started at 2023-01-18 22:30:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 18 22:57:51.245: INFO: osd-delete-backplane-serviceaccounts-27901360-7zfbh from openshift-backplane started at 2023-01-18 22:40:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 18 22:57:51.245: INFO: osd-delete-backplane-serviceaccounts-27901370-954ch from openshift-backplane started at 2023-01-18 22:50:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 18 22:57:51.245: INFO: sre-build-test-27901331-ppqbv from openshift-build-test started at 2023-01-18 22:11:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container sre-build-test ready: false, restart count 0
Jan 18 22:57:51.245: INFO: aws-ebs-csi-driver-node-l4ng8 from openshift-cluster-csi-drivers started at 2023-01-18 21:27:42 +0000 UTC (3 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 22:57:51.245: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 22:57:51.245: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 22:57:51.245: INFO: tuned-6pmzm from openshift-cluster-node-tuning-operator started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container tuned ready: true, restart count 1
Jan 18 22:57:51.245: INFO: dns-default-7sqw2 from openshift-dns started at 2023-01-18 22:06:24 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container dns ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:57:51.245: INFO: node-resolver-qsc9k from openshift-dns started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 22:57:51.245: INFO: image-pruner-27901320-rjnq8 from openshift-image-registry started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container image-pruner ready: false, restart count 0
Jan 18 22:57:51.245: INFO: node-ca-9gnsm from openshift-image-registry started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 22:57:51.245: INFO: ingress-canary-k52kt from openshift-ingress-canary started at 2023-01-18 21:28:29 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 22:57:51.245: INFO: router-default-7cff97cd98-lstfd from openshift-ingress started at 2023-01-18 21:39:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container router ready: true, restart count 0
Jan 18 22:57:51.245: INFO: machine-config-daemon-krjvz from openshift-machine-config-operator started at 2023-01-18 21:27:42 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 22:57:51.245: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 22:57:51.245: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-18 21:39:18 +0000 UTC (6 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container alertmanager ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 22:57:51.245: INFO: node-exporter-bpnc5 from openshift-monitoring started at 2023-01-18 21:27:42 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.245: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 22:57:51.245: INFO: osd-rebalance-infra-nodes-27901335-msvmm from openshift-monitoring started at 2023-01-18 22:15:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 18 22:57:51.245: INFO: osd-rebalance-infra-nodes-27901350-fnqm2 from openshift-monitoring started at 2023-01-18 22:30:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 18 22:57:51.245: INFO: osd-rebalance-infra-nodes-27901365-hqkcb from openshift-monitoring started at 2023-01-18 22:45:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 18 22:57:51.245: INFO: prometheus-adapter-cf64f7f46-jw64l from openshift-monitoring started at 2023-01-18 21:39:31 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 18 22:57:51.245: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-18 21:39:17 +0000 UTC (6 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container prometheus ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 18 22:57:51.245: INFO: prometheus-operator-admission-webhook-577cc9c956-lwfrr from openshift-monitoring started at 2023-01-18 21:39:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 22:57:51.245: INFO: sre-dns-latency-exporter-tlqwt from openshift-monitoring started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container main ready: true, restart count 1
Jan 18 22:57:51.245: INFO: thanos-querier-57f44c5498-fzjjb from openshift-monitoring started at 2023-01-18 21:39:18 +0000 UTC (6 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container thanos-query ready: true, restart count 0
Jan 18 22:57:51.245: INFO: multus-additional-cni-plugins-2q4s4 from openshift-multus started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 22:57:51.245: INFO: multus-cz5tw from openshift-multus started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 22:57:51.245: INFO: network-metrics-daemon-jnkxc from openshift-multus started at 2023-01-18 21:27:42 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.245: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 22:57:51.245: INFO: network-check-target-pjt5v from openshift-network-diagnostics started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 22:57:51.245: INFO: ovnkube-node-bf8j7 from openshift-ovn-kubernetes started at 2023-01-18 21:27:42 +0000 UTC (5 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.245: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 22:57:51.245: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 22:57:51.245: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 22:57:51.245: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 22:57:51.245: INFO: splunkforwarder-ds-gr4fb from openshift-security started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 22:57:51.245: INFO: builds-pruner-27901320-2bl9l from openshift-sre-pruning started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 18 22:57:51.245: INFO: sonobuoy-e2e-job-aed0e1be5f434192 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container e2e ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:57:51.245: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-tjlv7 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.245: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:57:51.245: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-219-147.ec2.internal before test
Jan 18 22:57:51.336: INFO: aws-ebs-csi-driver-node-qprtw from openshift-cluster-csi-drivers started at 2023-01-18 21:03:48 +0000 UTC (3 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 22:57:51.336: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 22:57:51.336: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 22:57:51.336: INFO: tuned-246td from openshift-cluster-node-tuning-operator started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container tuned ready: true, restart count 1
Jan 18 22:57:51.336: INFO: dns-default-lm6xx from openshift-dns started at 2023-01-18 22:22:26 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container dns ready: true, restart count 0
Jan 18 22:57:51.336: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 22:57:51.336: INFO: node-resolver-svmsb from openshift-dns started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 22:57:51.336: INFO: node-ca-nzbcp from openshift-image-registry started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 22:57:51.336: INFO: ingress-canary-c2z69 from openshift-ingress-canary started at 2023-01-18 22:22:26 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 18 22:57:51.336: INFO: machine-config-daemon-nb5xf from openshift-machine-config-operator started at 2023-01-18 21:03:48 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 22:57:51.336: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 22:57:51.336: INFO: node-exporter-6m8v9 from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.336: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 22:57:51.336: INFO: sre-dns-latency-exporter-rxh5d from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container main ready: true, restart count 1
Jan 18 22:57:51.336: INFO: multus-55cv4 from openshift-multus started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 22:57:51.336: INFO: multus-additional-cni-plugins-6n2cz from openshift-multus started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 22:57:51.336: INFO: network-metrics-daemon-mkvpn from openshift-multus started at 2023-01-18 21:03:48 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.336: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 22:57:51.336: INFO: network-check-target-cqk2p from openshift-network-diagnostics started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 22:57:51.336: INFO: ovnkube-node-7vtb8 from openshift-ovn-kubernetes started at 2023-01-18 21:03:48 +0000 UTC (5 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.336: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 22:57:51.336: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 22:57:51.336: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 22:57:51.336: INFO: 	Container ovnkube-node ready: true, restart count 4
Jan 18 22:57:51.336: INFO: splunkforwarder-ds-b6n2g from openshift-security started at 2023-01-18 21:24:58 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 22:57:51.336: INFO: pod-secrets-901a65b4-9646-40e3-a406-64fbb0fb175d from secrets-9652 started at 2023-01-18 22:56:20 +0000 UTC (3 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container creates-volume-test ready: true, restart count 0
Jan 18 22:57:51.336: INFO: 	Container dels-volume-test ready: true, restart count 0
Jan 18 22:57:51.336: INFO: 	Container upds-volume-test ready: true, restart count 0
Jan 18 22:57:51.336: INFO: sonobuoy from sonobuoy started at 2023-01-18 22:14:33 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 22:57:51.336: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-shjrl from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.336: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:57:51.336: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:57:51.336: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-236-5.ec2.internal before test
Jan 18 22:57:51.425: INFO: addon-operator-catalog-67nfj from openshift-addon-operator started at 2023-01-18 21:26:21 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:57:51.426: INFO: cloud-ingress-operator-registry-6f5sh from openshift-cloud-ingress-operator started at 2023-01-18 21:31:22 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:57:51.426: INFO: aws-ebs-csi-driver-node-cv9gd from openshift-cluster-csi-drivers started at 2023-01-18 21:03:23 +0000 UTC (3 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 22:57:51.426: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 22:57:51.426: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 22:57:51.426: INFO: tuned-b2gvw from openshift-cluster-node-tuning-operator started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container tuned ready: true, restart count 1
Jan 18 22:57:51.426: INFO: downloads-6f74f6fcbf-w6ht7 from openshift-console started at 2023-01-18 21:31:17 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container download-server ready: true, restart count 0
Jan 18 22:57:51.426: INFO: custom-domains-operator-7f97f586c8-czz24 from openshift-custom-domains-operator started at 2023-01-18 21:26:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container custom-domains-operator ready: true, restart count 0
Jan 18 22:57:51.426: INFO: custom-domains-operator-registry-sd6db from openshift-custom-domains-operator started at 2023-01-18 21:26:24 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:57:51.426: INFO: dns-default-4cddc from openshift-dns started at 2023-01-18 21:04:10 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container dns ready: true, restart count 1
Jan 18 22:57:51.426: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.426: INFO: node-resolver-8lsl6 from openshift-dns started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 22:57:51.426: INFO: node-ca-r29sw from openshift-image-registry started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 22:57:51.426: INFO: ingress-canary-d6jm7 from openshift-ingress-canary started at 2023-01-18 21:04:10 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 22:57:51.426: INFO: machine-config-daemon-xv85b from openshift-machine-config-operator started at 2023-01-18 21:03:23 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 22:57:51.426: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 22:57:51.426: INFO: managed-node-metadata-operator-registry-fn84g from openshift-managed-node-metadata-operator started at 2023-01-18 21:31:22 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:57:51.426: INFO: managed-upgrade-operator-catalog-g29pr from openshift-managed-upgrade-operator started at 2023-01-18 21:31:23 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:57:51.426: INFO: configure-alertmanager-operator-registry-vzl8c from openshift-monitoring started at 2023-01-18 21:31:25 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:57:51.426: INFO: node-exporter-6q46p from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.426: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 22:57:51.426: INFO: osd-cluster-ready-kfqk7 from openshift-monitoring started at 2023-01-18 21:26:19 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container osd-cluster-ready ready: false, restart count 22
Jan 18 22:57:51.426: INFO: sre-dns-latency-exporter-q7lp5 from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container main ready: true, restart count 1
Jan 18 22:57:51.426: INFO: sre-ebs-iops-reporter-1-l2chd from openshift-monitoring started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container main ready: true, restart count 0
Jan 18 22:57:51.426: INFO: sre-stuck-ebs-vols-1-mjmmn from openshift-monitoring started at 2023-01-18 21:26:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container main ready: true, restart count 0
Jan 18 22:57:51.426: INFO: multus-additional-cni-plugins-p78ph from openshift-multus started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 22:57:51.426: INFO: multus-wtvrb from openshift-multus started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 22:57:51.426: INFO: network-metrics-daemon-dxnrq from openshift-multus started at 2023-01-18 21:03:23 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.426: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 22:57:51.426: INFO: must-gather-operator-5f47db765d-zf7fj from openshift-must-gather-operator started at 2023-01-18 21:26:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container must-gather-operator ready: true, restart count 0
Jan 18 22:57:51.426: INFO: must-gather-operator-registry-gp8pc from openshift-must-gather-operator started at 2023-01-18 21:26:22 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:57:51.426: INFO: network-check-target-tr8pn from openshift-network-diagnostics started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 22:57:51.426: INFO: obo-prometheus-operator-6cb5cfc7b9-2rkdt from openshift-observability-operator started at 2023-01-18 21:26:20 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 18 22:57:51.426: INFO: obo-prometheus-operator-admission-webhook-5b4dc9bff5-f8pm5 from openshift-observability-operator started at 2023-01-18 21:26:21 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 22:57:51.426: INFO: observability-operator-5b467d8ccb-twlnn from openshift-observability-operator started at 2023-01-18 21:26:20 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container operator ready: true, restart count 0
Jan 18 22:57:51.426: INFO: observability-operator-catalog-khgpx from openshift-observability-operator started at 2023-01-18 21:26:25 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:57:51.426: INFO: ocm-agent-operator-9bd68bf49-z6qxl from openshift-ocm-agent-operator started at 2023-01-18 21:26:19 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container ocm-agent-operator ready: true, restart count 0
Jan 18 22:57:51.426: INFO: ocm-agent-operator-registry-qj69h from openshift-ocm-agent-operator started at 2023-01-18 21:26:24 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:57:51.426: INFO: ovnkube-node-g6c8t from openshift-ovn-kubernetes started at 2023-01-18 21:03:23 +0000 UTC (5 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.426: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 22:57:51.426: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 22:57:51.426: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 22:57:51.426: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 22:57:51.426: INFO: rbac-permissions-operator-registry-mqh7k from openshift-rbac-permissions started at 2023-01-18 21:26:26 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:57:51.426: INFO: splunkforwarder-ds-zhvhl from openshift-security started at 2023-01-18 21:25:56 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container splunk-uf ready: true, restart count 0
Jan 18 22:57:51.426: INFO: splunk-forwarder-operator-catalog-twzbw from openshift-splunk-forwarder-operator started at 2023-01-18 21:26:27 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:57:51.426: INFO: managed-velero-operator-registry-9j2kp from openshift-velero started at 2023-01-18 21:31:21 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 22:57:51.426: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-t5zp4 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.426: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:57:51.426: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:57:51.426: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-253-152.ec2.internal before test
Jan 18 22:57:51.511: INFO: cloud-ingress-operator-78d58985cd-9kp9q from openshift-cloud-ingress-operator started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Jan 18 22:57:51.511: INFO: aws-ebs-csi-driver-node-97pk2 from openshift-cluster-csi-drivers started at 2023-01-18 21:03:06 +0000 UTC (3 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 22:57:51.511: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 22:57:51.511: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 22:57:51.511: INFO: tuned-5wpdt from openshift-cluster-node-tuning-operator started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container tuned ready: true, restart count 1
Jan 18 22:57:51.511: INFO: dns-default-g4nw5 from openshift-dns started at 2023-01-18 21:03:53 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container dns ready: true, restart count 1
Jan 18 22:57:51.511: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.511: INFO: node-resolver-ffd24 from openshift-dns started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 22:57:51.511: INFO: image-registry-7b8f8dcdc5-2bvm7 from openshift-image-registry started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container registry ready: true, restart count 0
Jan 18 22:57:51.511: INFO: node-ca-bh7jz from openshift-image-registry started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 22:57:51.511: INFO: ingress-canary-vz8dj from openshift-ingress-canary started at 2023-01-18 21:03:53 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 22:57:51.511: INFO: machine-config-daemon-5zsxx from openshift-machine-config-operator started at 2023-01-18 21:03:06 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 22:57:51.511: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 22:57:51.511: INFO: node-exporter-z5k8l from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.511: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 22:57:51.511: INFO: sre-dns-latency-exporter-zsmln from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container main ready: true, restart count 1
Jan 18 22:57:51.511: INFO: multus-additional-cni-plugins-hsjtm from openshift-multus started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 22:57:51.511: INFO: multus-xzh2h from openshift-multus started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 22:57:51.511: INFO: network-metrics-daemon-cq5sl from openshift-multus started at 2023-01-18 21:03:06 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.511: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 22:57:51.511: INFO: network-check-source-5cb989cf6f-42gl2 from openshift-network-diagnostics started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container check-endpoints ready: true, restart count 0
Jan 18 22:57:51.511: INFO: network-check-target-9fq7l from openshift-network-diagnostics started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 22:57:51.511: INFO: ovnkube-node-mtfln from openshift-ovn-kubernetes started at 2023-01-18 21:03:06 +0000 UTC (5 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 22:57:51.511: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 22:57:51.511: INFO: 	Container ovn-acl-logging ready: true, restart count 2
Jan 18 22:57:51.511: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 22:57:51.511: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 22:57:51.511: INFO: splunkforwarder-ds-xsjtq from openshift-security started at 2023-01-18 21:24:58 +0000 UTC (1 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 22:57:51.511: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-ktxs4 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 22:57:51.511: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:57:51.511: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
STEP: verifying the node has the label node ip-10-0-128-7.ec2.internal
STEP: verifying the node has the label node ip-10-0-200-13.ec2.internal
STEP: verifying the node has the label node ip-10-0-211-217.ec2.internal
STEP: verifying the node has the label node ip-10-0-219-147.ec2.internal
STEP: verifying the node has the label node ip-10-0-236-5.ec2.internal
STEP: verifying the node has the label node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.947: INFO: Pod addon-operator-catalog-67nfj requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.947: INFO: Pod addon-operator-manager-c8c4859bf-84wp8 requesting resource cpu=200m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.947: INFO: Pod addon-operator-webhooks-569bd4cfc5-dz6nk requesting resource cpu=100m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.947: INFO: Pod addon-operator-webhooks-569bd4cfc5-mdklc requesting resource cpu=100m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.947: INFO: Pod cloud-ingress-operator-78d58985cd-9kp9q requesting resource cpu=200m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.947: INFO: Pod cloud-ingress-operator-registry-6f5sh requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.947: INFO: Pod aws-ebs-csi-driver-node-6nf8l requesting resource cpu=30m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.947: INFO: Pod aws-ebs-csi-driver-node-97pk2 requesting resource cpu=30m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.947: INFO: Pod aws-ebs-csi-driver-node-cv9gd requesting resource cpu=30m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.947: INFO: Pod aws-ebs-csi-driver-node-l4ng8 requesting resource cpu=30m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod aws-ebs-csi-driver-node-mq4qf requesting resource cpu=30m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod aws-ebs-csi-driver-node-qprtw requesting resource cpu=30m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod tuned-246td requesting resource cpu=10m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod tuned-5wpdt requesting resource cpu=10m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod tuned-6pmzm requesting resource cpu=10m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod tuned-b2gvw requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod tuned-l9d9s requesting resource cpu=10m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod tuned-t6mkr requesting resource cpu=10m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod downloads-6f74f6fcbf-hcggr requesting resource cpu=10m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod downloads-6f74f6fcbf-w6ht7 requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod custom-domains-operator-7f97f586c8-czz24 requesting resource cpu=0m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod custom-domains-operator-registry-sd6db requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod dns-default-4cddc requesting resource cpu=60m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod dns-default-7sqw2 requesting resource cpu=60m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod dns-default-btls8 requesting resource cpu=60m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod dns-default-g4nw5 requesting resource cpu=60m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod dns-default-lm6xx requesting resource cpu=60m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod dns-default-vkx5x requesting resource cpu=60m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-resolver-8lsl6 requesting resource cpu=5m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-resolver-ffd24 requesting resource cpu=5m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-resolver-gxvcl requesting resource cpu=5m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-resolver-qsc9k requesting resource cpu=5m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-resolver-svmsb requesting resource cpu=5m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-resolver-vhz2m requesting resource cpu=5m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod image-registry-7b8f8dcdc5-2bvm7 requesting resource cpu=100m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod image-registry-7b8f8dcdc5-4lfps requesting resource cpu=100m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-ca-5tw9r requesting resource cpu=10m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-ca-6cvvj requesting resource cpu=10m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-ca-9gnsm requesting resource cpu=10m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-ca-bh7jz requesting resource cpu=10m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-ca-nzbcp requesting resource cpu=10m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-ca-r29sw requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod ingress-canary-c2z69 requesting resource cpu=10m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod ingress-canary-d6jm7 requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod ingress-canary-k52kt requesting resource cpu=10m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod ingress-canary-ppjjg requesting resource cpu=10m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod ingress-canary-qw8v9 requesting resource cpu=10m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod ingress-canary-vz8dj requesting resource cpu=10m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod router-default-7cff97cd98-dqgwn requesting resource cpu=100m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod router-default-7cff97cd98-lstfd requesting resource cpu=100m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod machine-config-daemon-5zsxx requesting resource cpu=40m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod machine-config-daemon-htf69 requesting resource cpu=40m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod machine-config-daemon-krjvz requesting resource cpu=40m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod machine-config-daemon-nb5xf requesting resource cpu=40m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod machine-config-daemon-smj7g requesting resource cpu=40m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod machine-config-daemon-xv85b requesting resource cpu=40m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod managed-node-metadata-operator-5d4c567575-cbrxh requesting resource cpu=10m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod managed-node-metadata-operator-registry-fn84g requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod managed-upgrade-operator-catalog-g29pr requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod configure-alertmanager-operator-7565458cf4-pbmjl requesting resource cpu=0m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod configure-alertmanager-operator-registry-vzl8c requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod kube-state-metrics-76877575d5-ptnh8 requesting resource cpu=4m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-exporter-6m8v9 requesting resource cpu=9m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-exporter-6q46p requesting resource cpu=9m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-exporter-bpnc5 requesting resource cpu=9m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-exporter-gzf8b requesting resource cpu=9m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-exporter-spmbc requesting resource cpu=9m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod node-exporter-z5k8l requesting resource cpu=9m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod openshift-state-metrics-59fc669d8d-v25vz requesting resource cpu=3m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod osd-cluster-ready-kfqk7 requesting resource cpu=0m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod prometheus-adapter-cf64f7f46-7bf79 requesting resource cpu=1m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod prometheus-adapter-cf64f7f46-jw64l requesting resource cpu=1m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod prometheus-operator-76957bb5bd-2pztb requesting resource cpu=6m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod prometheus-operator-admission-webhook-577cc9c956-d6fkv requesting resource cpu=5m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod prometheus-operator-admission-webhook-577cc9c956-lwfrr requesting resource cpu=5m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sre-dns-latency-exporter-8dptc requesting resource cpu=0m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sre-dns-latency-exporter-q7lp5 requesting resource cpu=0m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sre-dns-latency-exporter-rxh5d requesting resource cpu=0m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sre-dns-latency-exporter-s6nk6 requesting resource cpu=0m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sre-dns-latency-exporter-tlqwt requesting resource cpu=0m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sre-dns-latency-exporter-zsmln requesting resource cpu=0m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sre-ebs-iops-reporter-1-l2chd requesting resource cpu=0m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sre-stuck-ebs-vols-1-mjmmn requesting resource cpu=0m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod telemeter-client-6bb748465-zttx9 requesting resource cpu=3m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod thanos-querier-57f44c5498-fzjjb requesting resource cpu=15m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod thanos-querier-57f44c5498-xjsz4 requesting resource cpu=15m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod token-refresher-6d8f85f497-mhth9 requesting resource cpu=0m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod multus-55cv4 requesting resource cpu=10m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod multus-additional-cni-plugins-2q4s4 requesting resource cpu=10m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod multus-additional-cni-plugins-5lmsf requesting resource cpu=10m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod multus-additional-cni-plugins-6n2cz requesting resource cpu=10m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod multus-additional-cni-plugins-hsjtm requesting resource cpu=10m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod multus-additional-cni-plugins-nm8v9 requesting resource cpu=10m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod multus-additional-cni-plugins-p78ph requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod multus-cfrv2 requesting resource cpu=10m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod multus-cz5tw requesting resource cpu=10m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod multus-wtvrb requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod multus-xzh2h requesting resource cpu=10m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod multus-znqj6 requesting resource cpu=10m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod network-metrics-daemon-cq5sl requesting resource cpu=20m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod network-metrics-daemon-dxnrq requesting resource cpu=20m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod network-metrics-daemon-ffdlk requesting resource cpu=20m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod network-metrics-daemon-jnkxc requesting resource cpu=20m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod network-metrics-daemon-m4c6r requesting resource cpu=20m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod network-metrics-daemon-mkvpn requesting resource cpu=20m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod must-gather-operator-5f47db765d-zf7fj requesting resource cpu=0m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod must-gather-operator-registry-gp8pc requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod network-check-source-5cb989cf6f-42gl2 requesting resource cpu=10m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod network-check-target-9fq7l requesting resource cpu=10m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod network-check-target-cqk2p requesting resource cpu=10m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod network-check-target-hpzlx requesting resource cpu=10m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod network-check-target-pjt5v requesting resource cpu=10m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod network-check-target-tr8pn requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod network-check-target-xwp8p requesting resource cpu=10m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod obo-prometheus-operator-6cb5cfc7b9-2rkdt requesting resource cpu=5m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod obo-prometheus-operator-admission-webhook-5b4dc9bff5-f8pm5 requesting resource cpu=50m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod obo-prometheus-operator-admission-webhook-5b4dc9bff5-l6qvc requesting resource cpu=50m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod observability-operator-5b467d8ccb-twlnn requesting resource cpu=5m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod observability-operator-catalog-khgpx requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod ocm-agent-75d95f8dc7-gtjwg requesting resource cpu=1m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod ocm-agent-operator-9bd68bf49-z6qxl requesting resource cpu=1m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod ocm-agent-operator-registry-qj69h requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod osd-metrics-exporter-756f85967-8z4cz requesting resource cpu=0m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod osd-metrics-exporter-registry-6rnzs requesting resource cpu=10m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod ovnkube-node-7vtb8 requesting resource cpu=50m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod ovnkube-node-bf8j7 requesting resource cpu=50m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod ovnkube-node-g6c8t requesting resource cpu=50m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod ovnkube-node-h62xt requesting resource cpu=50m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod ovnkube-node-mkzt4 requesting resource cpu=50m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod ovnkube-node-mtfln requesting resource cpu=50m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod rbac-permissions-operator-7f6bc8977-vsvkh requesting resource cpu=0m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod rbac-permissions-operator-registry-mqh7k requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod blackbox-exporter-dfdd57dd6-tj4kz requesting resource cpu=0m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod route-monitor-operator-controller-manager-cbc597f9d-84p87 requesting resource cpu=100m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod route-monitor-operator-registry-cgj4n requesting resource cpu=10m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod splunkforwarder-ds-b6n2g requesting resource cpu=0m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod splunkforwarder-ds-fbjlj requesting resource cpu=0m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod splunkforwarder-ds-gr4fb requesting resource cpu=0m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod splunkforwarder-ds-n7d7t requesting resource cpu=0m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod splunkforwarder-ds-xsjtq requesting resource cpu=0m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod splunkforwarder-ds-zhvhl requesting resource cpu=0m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod splunk-forwarder-operator-6f5bf57497-jwcz5 requesting resource cpu=0m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod splunk-forwarder-operator-catalog-twzbw requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod managed-velero-operator-f9f4c8b45-xml88 requesting resource cpu=0m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod managed-velero-operator-registry-9j2kp requesting resource cpu=10m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod velero-749f7746d8-ds5cs requesting resource cpu=0m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod pod-secrets-901a65b4-9646-40e3-a406-64fbb0fb175d requesting resource cpu=0m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sonobuoy-e2e-job-aed0e1be5f434192 requesting resource cpu=0m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sonobuoy-systemd-logs-daemon-set-034419193cd64c26-dwhpp requesting resource cpu=0m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sonobuoy-systemd-logs-daemon-set-034419193cd64c26-ktxs4 requesting resource cpu=0m on Node ip-10-0-253-152.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sonobuoy-systemd-logs-daemon-set-034419193cd64c26-mdcrg requesting resource cpu=0m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sonobuoy-systemd-logs-daemon-set-034419193cd64c26-shjrl requesting resource cpu=0m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sonobuoy-systemd-logs-daemon-set-034419193cd64c26-t5zp4 requesting resource cpu=0m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:51.948: INFO: Pod sonobuoy-systemd-logs-daemon-set-034419193cd64c26-tjlv7 requesting resource cpu=0m on Node ip-10-0-211-217.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
Jan 18 22:57:51.948: INFO: Creating a pod which consumes cpu=2187m on Node ip-10-0-128-7.ec2.internal
Jan 18 22:57:51.994: INFO: Creating a pod which consumes cpu=2348m on Node ip-10-0-200-13.ec2.internal
Jan 18 22:57:52.009: INFO: Creating a pod which consumes cpu=2338m on Node ip-10-0-211-217.ec2.internal
Jan 18 22:57:52.028: INFO: Creating a pod which consumes cpu=2552m on Node ip-10-0-219-147.ec2.internal
Jan 18 22:57:52.055: INFO: Creating a pod which consumes cpu=2418m on Node ip-10-0-236-5.ec2.internal
Jan 18 22:57:52.082: INFO: Creating a pod which consumes cpu=2335m on Node ip-10-0-253-152.ec2.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-104383ef-0612-4c37-b8a4-26c2fa84e242.173b89521495a8e1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6811/filler-pod-104383ef-0612-4c37-b8a4-26c2fa84e242 to ip-10-0-253-152.ec2.internal by ip-10-0-176-161]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-104383ef-0612-4c37-b8a4-26c2fa84e242.173b8952a7ab1eda], Reason = [AddedInterface], Message = [Add eth0 [10.128.10.36/23] from ovn-kubernetes]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-104383ef-0612-4c37-b8a4-26c2fa84e242.173b8952a8ed7b17], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-104383ef-0612-4c37-b8a4-26c2fa84e242.173b8952b093b68b], Reason = [Created], Message = [Created container filler-pod-104383ef-0612-4c37-b8a4-26c2fa84e242]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-104383ef-0612-4c37-b8a4-26c2fa84e242.173b8952b24413fc], Reason = [Started], Message = [Started container filler-pod-104383ef-0612-4c37-b8a4-26c2fa84e242]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1c657368-4f30-4431-852f-344b4fb34b09.173b895211081eac], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6811/filler-pod-1c657368-4f30-4431-852f-344b4fb34b09 to ip-10-0-219-147.ec2.internal by ip-10-0-176-161]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1c657368-4f30-4431-852f-344b4fb34b09.173b89529e49a0be], Reason = [AddedInterface], Message = [Add eth0 [10.128.12.128/23] from ovn-kubernetes]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1c657368-4f30-4431-852f-344b4fb34b09.173b8952a11d9787], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1c657368-4f30-4431-852f-344b4fb34b09.173b8952a991992b], Reason = [Created], Message = [Created container filler-pod-1c657368-4f30-4431-852f-344b4fb34b09]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1c657368-4f30-4431-852f-344b4fb34b09.173b8952aa8f8aaa], Reason = [Started], Message = [Started container filler-pod-1c657368-4f30-4431-852f-344b4fb34b09]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-38ea1fdd-b1f4-433d-b28d-307f603c3b1d.173b89520d3638fe], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6811/filler-pod-38ea1fdd-b1f4-433d-b28d-307f603c3b1d to ip-10-0-128-7.ec2.internal by ip-10-0-176-161]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-38ea1fdd-b1f4-433d-b28d-307f603c3b1d.173b895298bfbdab], Reason = [AddedInterface], Message = [Add eth0 [10.128.14.74/23] from ovn-kubernetes]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-38ea1fdd-b1f4-433d-b28d-307f603c3b1d.173b89529a5e40ea], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-38ea1fdd-b1f4-433d-b28d-307f603c3b1d.173b8952a2c572b3], Reason = [Created], Message = [Created container filler-pod-38ea1fdd-b1f4-433d-b28d-307f603c3b1d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-38ea1fdd-b1f4-433d-b28d-307f603c3b1d.173b8952a4a15794], Reason = [Started], Message = [Started container filler-pod-38ea1fdd-b1f4-433d-b28d-307f603c3b1d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-84b714e3-a6e6-4506-b4ad-9eb72065cb60.173b89520f33409d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6811/filler-pod-84b714e3-a6e6-4506-b4ad-9eb72065cb60 to ip-10-0-211-217.ec2.internal by ip-10-0-176-161]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-84b714e3-a6e6-4506-b4ad-9eb72065cb60.173b89529ec14f01], Reason = [AddedInterface], Message = [Add eth0 [10.128.16.92/23] from ovn-kubernetes]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-84b714e3-a6e6-4506-b4ad-9eb72065cb60.173b8952a08a90cc], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-84b714e3-a6e6-4506-b4ad-9eb72065cb60.173b8952afec2b65], Reason = [Created], Message = [Created container filler-pod-84b714e3-a6e6-4506-b4ad-9eb72065cb60]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-84b714e3-a6e6-4506-b4ad-9eb72065cb60.173b8952b1529bcc], Reason = [Started], Message = [Started container filler-pod-84b714e3-a6e6-4506-b4ad-9eb72065cb60]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a69db42d-e6fb-4ccb-bdeb-e15ef9295a85.173b895212c37148], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6811/filler-pod-a69db42d-e6fb-4ccb-bdeb-e15ef9295a85 to ip-10-0-236-5.ec2.internal by ip-10-0-176-161]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a69db42d-e6fb-4ccb-bdeb-e15ef9295a85.173b89528f6d3f19], Reason = [AddedInterface], Message = [Add eth0 [10.128.8.61/23] from ovn-kubernetes]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a69db42d-e6fb-4ccb-bdeb-e15ef9295a85.173b895290adc35a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a69db42d-e6fb-4ccb-bdeb-e15ef9295a85.173b895297f7644f], Reason = [Created], Message = [Created container filler-pod-a69db42d-e6fb-4ccb-bdeb-e15ef9295a85]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a69db42d-e6fb-4ccb-bdeb-e15ef9295a85.173b895299b14c94], Reason = [Started], Message = [Started container filler-pod-a69db42d-e6fb-4ccb-bdeb-e15ef9295a85]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a7f7fcfd-c283-49ae-bc64-2b7dd596d491.173b89520e0882b4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6811/filler-pod-a7f7fcfd-c283-49ae-bc64-2b7dd596d491 to ip-10-0-200-13.ec2.internal by ip-10-0-176-161]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a7f7fcfd-c283-49ae-bc64-2b7dd596d491.173b89526d61be5d], Reason = [AddedInterface], Message = [Add eth0 [10.128.6.59/23] from ovn-kubernetes]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a7f7fcfd-c283-49ae-bc64-2b7dd596d491.173b89526f16da37], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a7f7fcfd-c283-49ae-bc64-2b7dd596d491.173b89527e5f1272], Reason = [Created], Message = [Created container filler-pod-a7f7fcfd-c283-49ae-bc64-2b7dd596d491]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a7f7fcfd-c283-49ae-bc64-2b7dd596d491.173b895280435c6d], Reason = [Started], Message = [Started container filler-pod-a7f7fcfd-c283-49ae-bc64-2b7dd596d491]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173b895304a9fc94], Reason = [FailedScheduling], Message = [0/9 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 Insufficient cpu. preemption: 0/9 nodes are available: 3 Preemption is not helpful for scheduling, 6 No preemption victims found for incoming pod.]
STEP: removing the label node off the node ip-10-0-128-7.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-200-13.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-211-217.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-219-147.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-236-5.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-253-152.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 18 22:57:57.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6811" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:6.281 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":356,"completed":117,"skipped":2004,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:57:57.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 18 22:57:57.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-9152 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 18 22:57:57.433: INFO: stderr: ""
Jan 18 22:57:57.433: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jan 18 22:57:57.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-9152 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 18 22:57:59.790: INFO: stderr: ""
Jan 18 22:57:59.790: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 18 22:57:59.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-9152 delete pods e2e-test-httpd-pod'
Jan 18 22:58:02.599: INFO: stderr: ""
Jan 18 22:58:02.599: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 22:58:02.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9152" for this suite.

• [SLOW TEST:5.285 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:927
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":356,"completed":118,"skipped":2013,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:58:02.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 18 22:58:13.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8654" for this suite.

• [SLOW TEST:11.082 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":356,"completed":119,"skipped":2065,"failed":0}
SSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:58:13.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-f135684a-e896-4fd6-ab0a-e5224ad5f0e7 in namespace container-probe-5214
Jan 18 22:58:19.778: INFO: Started pod busybox-f135684a-e896-4fd6-ab0a-e5224ad5f0e7 in namespace container-probe-5214
STEP: checking the pod's current state and verifying that restartCount is present
Jan 18 22:58:19.780: INFO: Initial restart count of pod busybox-f135684a-e896-4fd6-ab0a-e5224ad5f0e7 is 0
Jan 18 22:59:07.875: INFO: Restart count of pod container-probe-5214/busybox-f135684a-e896-4fd6-ab0a-e5224ad5f0e7 is now 1 (48.095310111s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 18 22:59:07.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5214" for this suite.

• [SLOW TEST:54.201 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":120,"skipped":2072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:59:07.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 18 22:59:08.056: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:08.056: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:08.056: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:08.063: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:59:08.063: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:59:09.067: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:09.068: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:09.068: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:09.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:59:09.071: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:59:10.068: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:10.068: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:10.068: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:10.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:59:10.071: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:59:11.068: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:11.068: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:11.068: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:11.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 18 22:59:11.071: INFO: Node ip-10-0-236-5.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:59:12.067: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:12.067: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:12.067: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:12.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 18 22:59:12.070: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 18 22:59:12.084: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:12.084: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:12.084: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:12.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 18 22:59:12.086: INFO: Node ip-10-0-200-13.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:59:13.091: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:13.091: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:13.091: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:13.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 18 22:59:13.094: INFO: Node ip-10-0-200-13.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:59:14.092: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:14.092: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:14.092: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:14.095: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 18 22:59:14.095: INFO: Node ip-10-0-200-13.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:59:15.091: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:15.091: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:15.091: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:15.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 18 22:59:15.094: INFO: Node ip-10-0-200-13.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:59:16.097: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:16.097: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:16.097: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:16.100: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 18 22:59:16.100: INFO: Node ip-10-0-200-13.ec2.internal is running 0 daemon pod, expected 1
Jan 18 22:59:17.091: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:17.092: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:17.092: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:59:17.096: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 18 22:59:17.096: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2848, will wait for the garbage collector to delete the pods
Jan 18 22:59:17.155: INFO: Deleting DaemonSet.extensions daemon-set took: 4.731097ms
Jan 18 22:59:17.256: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.893274ms
Jan 18 22:59:19.360: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:59:19.360: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 22:59:19.362: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"240836"},"items":null}

Jan 18 22:59:19.363: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"240836"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 18 22:59:19.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2848" for this suite.

• [SLOW TEST:11.493 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":356,"completed":121,"skipped":2106,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 22:59:19.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 22:59:19.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Creating first CR 
Jan 18 22:59:22.041: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:59:22Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:59:22Z]] name:name1 resourceVersion:240970 uid:ff84a9cc-a45c-425b-bbfb-7c7a9cc13f45] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jan 18 22:59:32.047: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:59:32Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:59:32Z]] name:name2 resourceVersion:241350 uid:9db5c609-30f6-4dda-8daf-6e8a60fd9abf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jan 18 22:59:42.053: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:59:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:59:42Z]] name:name1 resourceVersion:241629 uid:ff84a9cc-a45c-425b-bbfb-7c7a9cc13f45] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jan 18 22:59:52.067: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:59:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:59:52Z]] name:name2 resourceVersion:241907 uid:9db5c609-30f6-4dda-8daf-6e8a60fd9abf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jan 18 23:00:02.074: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:59:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:59:42Z]] name:name1 resourceVersion:242276 uid:ff84a9cc-a45c-425b-bbfb-7c7a9cc13f45] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jan 18 23:00:12.081: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:59:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:59:52Z]] name:name2 resourceVersion:242674 uid:9db5c609-30f6-4dda-8daf-6e8a60fd9abf] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:00:22.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8138" for this suite.

• [SLOW TEST:63.212 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":356,"completed":122,"skipped":2109,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:00:22.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
STEP: create deployment with httpd image
Jan 18 23:00:22.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8505 create -f -'
Jan 18 23:00:25.168: INFO: stderr: ""
Jan 18 23:00:25.168: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jan 18 23:00:25.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8505 diff -f -'
Jan 18 23:00:27.367: INFO: rc: 1
Jan 18 23:00:27.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8505 delete -f -'
Jan 18 23:00:27.420: INFO: stderr: ""
Jan 18 23:00:27.420: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 23:00:27.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8505" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":356,"completed":123,"skipped":2117,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:00:27.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
W0118 23:00:27.461253      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:00:27.467: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 23:00:32.471: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 18 23:00:32.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1123" for this suite.

• [SLOW TEST:5.096 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":356,"completed":124,"skipped":2123,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests 
  should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:00:32.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename conformance-tests
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
STEP: Getting node addresses
Jan 18 23:00:32.565: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:188
Jan 18 23:00:32.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-9872" for this suite.
•{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","total":356,"completed":125,"skipped":2173,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:00:32.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 23:00:33.643: INFO: Waiting up to 5m0s for pod "downwardapi-volume-199899ec-41ef-4621-a5f8-79ea2b5777d1" in namespace "projected-8151" to be "Succeeded or Failed"
Jan 18 23:00:33.646: INFO: Pod "downwardapi-volume-199899ec-41ef-4621-a5f8-79ea2b5777d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.212913ms
Jan 18 23:00:35.649: INFO: Pod "downwardapi-volume-199899ec-41ef-4621-a5f8-79ea2b5777d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005390988s
Jan 18 23:00:37.654: INFO: Pod "downwardapi-volume-199899ec-41ef-4621-a5f8-79ea2b5777d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010246848s
Jan 18 23:00:39.657: INFO: Pod "downwardapi-volume-199899ec-41ef-4621-a5f8-79ea2b5777d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013845704s
STEP: Saw pod success
Jan 18 23:00:39.657: INFO: Pod "downwardapi-volume-199899ec-41ef-4621-a5f8-79ea2b5777d1" satisfied condition "Succeeded or Failed"
Jan 18 23:00:39.659: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-199899ec-41ef-4621-a5f8-79ea2b5777d1 container client-container: <nil>
STEP: delete the pod
Jan 18 23:00:39.675: INFO: Waiting for pod downwardapi-volume-199899ec-41ef-4621-a5f8-79ea2b5777d1 to disappear
Jan 18 23:00:39.677: INFO: Pod downwardapi-volume-199899ec-41ef-4621-a5f8-79ea2b5777d1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 18 23:00:39.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8151" for this suite.

• [SLOW TEST:7.089 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":356,"completed":126,"skipped":2212,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:00:39.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 18 23:00:39.750: INFO: Waiting up to 5m0s for pod "pod-a56234aa-201f-41ba-ba46-f17b218a4df5" in namespace "emptydir-6869" to be "Succeeded or Failed"
Jan 18 23:00:39.756: INFO: Pod "pod-a56234aa-201f-41ba-ba46-f17b218a4df5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.449639ms
Jan 18 23:00:41.759: INFO: Pod "pod-a56234aa-201f-41ba-ba46-f17b218a4df5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008508322s
Jan 18 23:00:43.762: INFO: Pod "pod-a56234aa-201f-41ba-ba46-f17b218a4df5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011776078s
Jan 18 23:00:45.765: INFO: Pod "pod-a56234aa-201f-41ba-ba46-f17b218a4df5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014563331s
STEP: Saw pod success
Jan 18 23:00:45.765: INFO: Pod "pod-a56234aa-201f-41ba-ba46-f17b218a4df5" satisfied condition "Succeeded or Failed"
Jan 18 23:00:45.767: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-a56234aa-201f-41ba-ba46-f17b218a4df5 container test-container: <nil>
STEP: delete the pod
Jan 18 23:00:45.785: INFO: Waiting for pod pod-a56234aa-201f-41ba-ba46-f17b218a4df5 to disappear
Jan 18 23:00:45.788: INFO: Pod pod-a56234aa-201f-41ba-ba46-f17b218a4df5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 18 23:00:45.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6869" for this suite.

• [SLOW TEST:6.110 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":127,"skipped":2224,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:00:45.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0118 23:00:45.850161      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-readonly-fs7e39cc9f-14bd-48ad-853c-842e4f5b6133" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-readonly-fs7e39cc9f-14bd-48ad-853c-842e4f5b6133" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-readonly-fs7e39cc9f-14bd-48ad-853c-842e4f5b6133" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-readonly-fs7e39cc9f-14bd-48ad-853c-842e4f5b6133" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:00:45.852: INFO: The status of Pod busybox-readonly-fs7e39cc9f-14bd-48ad-853c-842e4f5b6133 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:00:47.856: INFO: The status of Pod busybox-readonly-fs7e39cc9f-14bd-48ad-853c-842e4f5b6133 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:00:49.855: INFO: The status of Pod busybox-readonly-fs7e39cc9f-14bd-48ad-853c-842e4f5b6133 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 18 23:00:49.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5481" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":128,"skipped":2237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:00:49.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5404
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
W0118 23:00:49.911944      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:00:49.919: INFO: Found 0 stateful pods, waiting for 1
Jan 18 23:00:59.923: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Jan 18 23:00:59.937: INFO: Found 1 stateful pods, waiting for 2
Jan 18 23:01:09.944: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:01:09.944: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 23:01:09.959: INFO: Deleting all statefulset in ns statefulset-5404
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 18 23:01:09.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5404" for this suite.

• [SLOW TEST:20.109 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":356,"completed":129,"skipped":2270,"failed":0}
SSSS
------------------------------
[sig-node] PodTemplates 
  should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:01:09.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a pod template
W0118 23:01:10.012000      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Replace a pod template
Jan 18 23:01:10.026: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jan 18 23:01:10.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3925" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","total":356,"completed":130,"skipped":2274,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:01:10.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:01:10.063: INFO: Creating deployment "test-recreate-deployment"
W0118 23:01:10.073178      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:01:10.073: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 18 23:01:10.084: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 18 23:01:12.090: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 18 23:01:12.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 1, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 1, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 1, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 1, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-848969dbcd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:01:14.104: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 18 23:01:14.114: INFO: Updating deployment test-recreate-deployment
Jan 18 23:01:14.114: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 23:01:14.205: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8621  3cd97722-c4e8-4ac0-ba34-b90d3bad958f 245380 2 2023-01-18 23:01:10 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-01-18 23:01:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:01:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bc80c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 23:01:14 +0000 UTC,LastTransitionTime:2023-01-18 23:01:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cd8586fc7" is progressing.,LastUpdateTime:2023-01-18 23:01:14 +0000 UTC,LastTransitionTime:2023-01-18 23:01:10 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 18 23:01:14.211: INFO: New ReplicaSet "test-recreate-deployment-cd8586fc7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cd8586fc7  deployment-8621  eb7f96cc-f596-4985-988e-0d7cdf511308 245378 1 2023-01-18 23:01:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 3cd97722-c4e8-4ac0-ba34-b90d3bad958f 0xc00f5d2d40 0xc00f5d2d41}] []  [{kube-controller-manager Update apps/v1 2023-01-18 23:01:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3cd97722-c4e8-4ac0-ba34-b90d3bad958f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:01:14 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cd8586fc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00f5d2dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:01:14.211: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 18 23:01:14.211: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-848969dbcd  deployment-8621  b67caf28-1bbc-4fb1-9a00-cb17d6f77192 245367 2 2023-01-18 23:01:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:848969dbcd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 3cd97722-c4e8-4ac0-ba34-b90d3bad958f 0xc00f5d2c27 0xc00f5d2c28}] []  [{kube-controller-manager Update apps/v1 2023-01-18 23:01:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3cd97722-c4e8-4ac0-ba34-b90d3bad958f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:01:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 848969dbcd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:848969dbcd] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00f5d2cd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:01:14.214: INFO: Pod "test-recreate-deployment-cd8586fc7-mhv4p" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cd8586fc7-mhv4p test-recreate-deployment-cd8586fc7- deployment-8621  d895842b-262c-405f-8975-87300a591331 245383 0 2023-01-18 23:01:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.16.100/23"],"mac_address":"0a:58:0a:80:10:64","gateway_ips":["10.128.16.1"],"ip_address":"10.128.16.100/23","gateway_ip":"10.128.16.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-cd8586fc7 eb7f96cc-f596-4985-988e-0d7cdf511308 0xc00f5d3257 0xc00f5d3258}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:01:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:01:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb7f96cc-f596-4985-988e-0d7cdf511308\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:01:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-khp7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-khp7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-211-217.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c50,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-94lpb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:01:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:01:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:01:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:01:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.211.217,PodIP:,StartTime:2023-01-18 23:01:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 18 23:01:14.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8621" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":131,"skipped":2358,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:01:14.227: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:01:14.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: creating the pod
STEP: submitting the pod to kubernetes
W0118 23:01:14.294410      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "main" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "main" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "main" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "main" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:01:14.308: INFO: The status of Pod pod-logs-websocket-60b209c3-1d9c-4d2e-a28e-7b720f2de02f is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:01:16.311: INFO: The status of Pod pod-logs-websocket-60b209c3-1d9c-4d2e-a28e-7b720f2de02f is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:01:18.317: INFO: The status of Pod pod-logs-websocket-60b209c3-1d9c-4d2e-a28e-7b720f2de02f is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 18 23:01:18.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3075" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":356,"completed":132,"skipped":2367,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:01:18.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:01:18.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jan 18 23:01:27.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-1577 --namespace=crd-publish-openapi-1577 create -f -'
Jan 18 23:01:29.052: INFO: stderr: ""
Jan 18 23:01:29.052: INFO: stdout: "e2e-test-crd-publish-openapi-651-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 18 23:01:29.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-1577 --namespace=crd-publish-openapi-1577 delete e2e-test-crd-publish-openapi-651-crds test-cr'
Jan 18 23:01:29.141: INFO: stderr: ""
Jan 18 23:01:29.141: INFO: stdout: "e2e-test-crd-publish-openapi-651-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 18 23:01:29.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-1577 --namespace=crd-publish-openapi-1577 apply -f -'
Jan 18 23:01:30.497: INFO: stderr: ""
Jan 18 23:01:30.497: INFO: stdout: "e2e-test-crd-publish-openapi-651-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 18 23:01:30.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-1577 --namespace=crd-publish-openapi-1577 delete e2e-test-crd-publish-openapi-651-crds test-cr'
Jan 18 23:01:30.566: INFO: stderr: ""
Jan 18 23:01:30.566: INFO: stdout: "e2e-test-crd-publish-openapi-651-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jan 18 23:01:30.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-1577 explain e2e-test-crd-publish-openapi-651-crds'
Jan 18 23:01:30.848: INFO: stderr: ""
Jan 18 23:01:30.848: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-651-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:01:39.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1577" for this suite.

• [SLOW TEST:21.684 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":356,"completed":133,"skipped":2381,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity 
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:01:40.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename csistoragecapacity
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/storage.k8s.io
STEP: getting /apis/storage.k8s.io/v1
STEP: creating
STEP: watching
Jan 18 23:01:40.132: INFO: starting watch
STEP: getting
STEP: listing in namespace
STEP: listing across namespaces
STEP: patching
STEP: updating
Jan 18 23:01:40.175: INFO: waiting for watch events with expected annotations in namespace
Jan 18 23:01:40.175: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:188
Jan 18 23:01:40.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-8051" for this suite.
•{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","total":356,"completed":134,"skipped":2403,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:01:40.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 18 23:01:40.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7406" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":135,"skipped":2437,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:01:40.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5304
STEP: creating service affinity-nodeport in namespace services-5304
STEP: creating replication controller affinity-nodeport in namespace services-5304
I0118 23:01:40.440648      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5304, replica count: 3
I0118 23:01:43.492017      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 23:01:46.493134      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 23:01:46.507: INFO: Creating new exec pod
Jan 18 23:01:51.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-5304 exec execpod-affinitysggsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 18 23:01:51.678: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 18 23:01:51.678: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:01:51.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-5304 exec execpod-affinitysggsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.200.129 80'
Jan 18 23:01:51.820: INFO: stderr: "+ + ncecho -v hostName\n -t -w 2 172.30.200.129 80\nConnection to 172.30.200.129 80 port [tcp/http] succeeded!\n"
Jan 18 23:01:51.821: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:01:51.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-5304 exec execpod-affinitysggsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.128.7 30642'
Jan 18 23:01:51.952: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.128.7 30642\nConnection to 10.0.128.7 30642 port [tcp/*] succeeded!\n"
Jan 18 23:01:51.952: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:01:51.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-5304 exec execpod-affinitysggsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.211.217 30642'
Jan 18 23:01:52.070: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.211.217 30642\nConnection to 10.0.211.217 30642 port [tcp/*] succeeded!\n"
Jan 18 23:01:52.070: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:01:52.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-5304 exec execpod-affinitysggsz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.128.7:30642/ ; done'
Jan 18 23:01:52.260: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:30642/\n"
Jan 18 23:01:52.260: INFO: stdout: "\naffinity-nodeport-l88cs\naffinity-nodeport-l88cs\naffinity-nodeport-l88cs\naffinity-nodeport-l88cs\naffinity-nodeport-l88cs\naffinity-nodeport-l88cs\naffinity-nodeport-l88cs\naffinity-nodeport-l88cs\naffinity-nodeport-l88cs\naffinity-nodeport-l88cs\naffinity-nodeport-l88cs\naffinity-nodeport-l88cs\naffinity-nodeport-l88cs\naffinity-nodeport-l88cs\naffinity-nodeport-l88cs\naffinity-nodeport-l88cs"
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Received response from host: affinity-nodeport-l88cs
Jan 18 23:01:52.260: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5304, will wait for the garbage collector to delete the pods
Jan 18 23:01:52.392: INFO: Deleting ReplicationController affinity-nodeport took: 4.969284ms
Jan 18 23:01:52.493: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.429227ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 18 23:01:55.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5304" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:14.930 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":136,"skipped":2445,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:01:55.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 23:01:55.827: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 23:01:57.837: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 1, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 1, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 1, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 1, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 23:02:00.853: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jan 18 23:02:04.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=webhook-460 attach --namespace=webhook-460 to-be-attached-pod -i -c=container1'
Jan 18 23:02:04.967: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:02:04.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-460" for this suite.
STEP: Destroying namespace "webhook-460-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:9.796 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":356,"completed":137,"skipped":2466,"failed":0}
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:02:05.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jan 18 23:02:11.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-193" for this suite.
STEP: Destroying namespace "nsdeletetest-5210" for this suite.
Jan 18 23:02:11.493: INFO: Namespace nsdeletetest-5210 was already deleted
STEP: Destroying namespace "nsdeletetest-3234" for this suite.

• [SLOW TEST:6.458 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":356,"completed":138,"skipped":2466,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:02:11.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
Jan 18 23:02:11.563: INFO: created test-event-1
Jan 18 23:02:11.569: INFO: created test-event-2
Jan 18 23:02:11.574: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jan 18 23:02:11.581: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jan 18 23:02:11.674: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Jan 18 23:02:11.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8100" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":356,"completed":139,"skipped":2492,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:02:11.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:02:11.752: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-9a2902ec-a8bb-4275-9b7c-41f34d8702de
STEP: Creating configMap with name cm-test-opt-upd-de51b261-927b-41c3-bc19-e19a81d68bb4
STEP: Creating the pod
Jan 18 23:02:11.823: INFO: The status of Pod pod-configmaps-021eefe8-f6a2-4d36-abca-90d3a586eb61 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:02:13.829: INFO: The status of Pod pod-configmaps-021eefe8-f6a2-4d36-abca-90d3a586eb61 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:02:15.826: INFO: The status of Pod pod-configmaps-021eefe8-f6a2-4d36-abca-90d3a586eb61 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-9a2902ec-a8bb-4275-9b7c-41f34d8702de
STEP: Updating configmap cm-test-opt-upd-de51b261-927b-41c3-bc19-e19a81d68bb4
STEP: Creating configMap with name cm-test-opt-create-d234dbf1-c9a6-4d3e-811c-d4fa9c4fef08
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 18 23:02:17.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7430" for this suite.

• [SLOW TEST:6.169 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":140,"skipped":2513,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:02:17.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3912.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3912.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3912.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3912.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3912.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3912.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3912.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3912.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3912.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3912.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3912.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3912.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3912.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3912.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3912.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3912.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 18 23:02:25.980: INFO: DNS probes using dns-3912/dns-test-51db5602-f466-4563-b6dc-8a4cab737901 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 18 23:02:26.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3912" for this suite.

• [SLOW TEST:8.123 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":356,"completed":141,"skipped":2519,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:02:26.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jan 18 23:02:26.066: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jan 18 23:02:26.102: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 18 23:02:26.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9501" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":356,"completed":142,"skipped":2547,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:02:26.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-5884/configmap-test-09213846-4fd8-4a09-af7d-1d65bd89be6c
STEP: Creating a pod to test consume configMaps
Jan 18 23:02:26.321: INFO: Waiting up to 5m0s for pod "pod-configmaps-b89c019c-8d89-4413-adb8-81939e841426" in namespace "configmap-5884" to be "Succeeded or Failed"
Jan 18 23:02:26.326: INFO: Pod "pod-configmaps-b89c019c-8d89-4413-adb8-81939e841426": Phase="Pending", Reason="", readiness=false. Elapsed: 4.627384ms
Jan 18 23:02:28.329: INFO: Pod "pod-configmaps-b89c019c-8d89-4413-adb8-81939e841426": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008202008s
Jan 18 23:02:30.332: INFO: Pod "pod-configmaps-b89c019c-8d89-4413-adb8-81939e841426": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011100074s
Jan 18 23:02:32.336: INFO: Pod "pod-configmaps-b89c019c-8d89-4413-adb8-81939e841426": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015103018s
STEP: Saw pod success
Jan 18 23:02:32.336: INFO: Pod "pod-configmaps-b89c019c-8d89-4413-adb8-81939e841426" satisfied condition "Succeeded or Failed"
Jan 18 23:02:32.338: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-configmaps-b89c019c-8d89-4413-adb8-81939e841426 container env-test: <nil>
STEP: delete the pod
Jan 18 23:02:32.351: INFO: Waiting for pod pod-configmaps-b89c019c-8d89-4413-adb8-81939e841426 to disappear
Jan 18 23:02:32.354: INFO: Pod pod-configmaps-b89c019c-8d89-4413-adb8-81939e841426 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 18 23:02:32.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5884" for this suite.

• [SLOW TEST:6.192 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":143,"skipped":2551,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:02:32.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
W0118 23:02:32.429729      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:02:32.432: INFO: The status of Pod labelsupdate4694616f-0491-44de-951f-e1f287327932 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:02:34.436: INFO: The status of Pod labelsupdate4694616f-0491-44de-951f-e1f287327932 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:02:36.436: INFO: The status of Pod labelsupdate4694616f-0491-44de-951f-e1f287327932 is Running (Ready = true)
Jan 18 23:02:36.959: INFO: Successfully updated pod "labelsupdate4694616f-0491-44de-951f-e1f287327932"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 18 23:02:38.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9715" for this suite.

• [SLOW TEST:6.616 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":144,"skipped":2552,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:02:38.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:02:39.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:02:39.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-944" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":356,"completed":145,"skipped":2577,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:02:39.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 18 23:02:39.641: INFO: The status of Pod annotationupdatefff2b775-da3a-4ded-b947-512c8f31cedd is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:02:41.647: INFO: The status of Pod annotationupdatefff2b775-da3a-4ded-b947-512c8f31cedd is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:02:43.645: INFO: The status of Pod annotationupdatefff2b775-da3a-4ded-b947-512c8f31cedd is Running (Ready = true)
Jan 18 23:02:44.181: INFO: Successfully updated pod "annotationupdatefff2b775-da3a-4ded-b947-512c8f31cedd"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 18 23:02:46.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6733" for this suite.

• [SLOW TEST:6.640 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":146,"skipped":2591,"failed":0}
SS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:02:46.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Service
STEP: watching for the Service to be added
Jan 18 23:02:46.285: INFO: Found Service test-service-xlgbn in namespace services-8319 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 18 23:02:46.285: INFO: Service test-service-xlgbn created
STEP: Getting /status
Jan 18 23:02:46.299: INFO: Service test-service-xlgbn has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jan 18 23:02:46.308: INFO: observed Service test-service-xlgbn in namespace services-8319 with annotations: map[] & LoadBalancer: {[]}
Jan 18 23:02:46.308: INFO: Found Service test-service-xlgbn in namespace services-8319 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 18 23:02:46.308: INFO: Service test-service-xlgbn has service status patched
STEP: updating the ServiceStatus
Jan 18 23:02:46.324: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jan 18 23:02:46.325: INFO: Observed Service test-service-xlgbn in namespace services-8319 with annotations: map[] & Conditions: {[]}
Jan 18 23:02:46.325: INFO: Observed event: &Service{ObjectMeta:{test-service-xlgbn  services-8319  7d9f3e0f-f494-46f1-958b-a6c506ae318b 249635 0 2023-01-18 23:02:46 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2023-01-18 23:02:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-18 23:02:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.30.85.195,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.30.85.195],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 18 23:02:46.326: INFO: Found Service test-service-xlgbn in namespace services-8319 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 23:02:46.326: INFO: Service test-service-xlgbn has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jan 18 23:02:46.404: INFO: observed Service test-service-xlgbn in namespace services-8319 with labels: map[test-service-static:true]
Jan 18 23:02:46.407: INFO: observed Service test-service-xlgbn in namespace services-8319 with labels: map[test-service-static:true]
Jan 18 23:02:46.407: INFO: observed Service test-service-xlgbn in namespace services-8319 with labels: map[test-service-static:true]
Jan 18 23:02:46.407: INFO: Found Service test-service-xlgbn in namespace services-8319 with labels: map[test-service:patched test-service-static:true]
Jan 18 23:02:46.407: INFO: Service test-service-xlgbn patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jan 18 23:02:46.438: INFO: Observed event: ADDED
Jan 18 23:02:46.438: INFO: Observed event: MODIFIED
Jan 18 23:02:46.438: INFO: Observed event: MODIFIED
Jan 18 23:02:46.438: INFO: Observed event: MODIFIED
Jan 18 23:02:46.438: INFO: Found Service test-service-xlgbn in namespace services-8319 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 18 23:02:46.438: INFO: Service test-service-xlgbn deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 18 23:02:46.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8319" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":356,"completed":147,"skipped":2593,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:02:46.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:02:46.514: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-4c5acd75-7318-43d0-9d9e-35d3cd269cf0
STEP: Creating the pod
Jan 18 23:02:46.552: INFO: The status of Pod pod-projected-configmaps-4450b758-c0ac-4819-bf16-06ad00fe965f is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:02:48.555: INFO: The status of Pod pod-projected-configmaps-4450b758-c0ac-4819-bf16-06ad00fe965f is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:02:50.554: INFO: The status of Pod pod-projected-configmaps-4450b758-c0ac-4819-bf16-06ad00fe965f is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-4c5acd75-7318-43d0-9d9e-35d3cd269cf0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 18 23:04:04.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8618" for this suite.

• [SLOW TEST:78.317 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":148,"skipped":2614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:04:04.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1540
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 18 23:04:04.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8007 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Jan 18 23:04:04.899: INFO: stderr: ""
Jan 18 23:04:04.899: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1544
Jan 18 23:04:04.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8007 delete pods e2e-test-httpd-pod'
Jan 18 23:04:09.460: INFO: stderr: ""
Jan 18 23:04:09.460: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 23:04:09.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8007" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":356,"completed":149,"skipped":2671,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:04:09.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 23:04:09.541: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80c17150-52df-4332-a728-e201ddc09183" in namespace "downward-api-2092" to be "Succeeded or Failed"
Jan 18 23:04:09.544: INFO: Pod "downwardapi-volume-80c17150-52df-4332-a728-e201ddc09183": Phase="Pending", Reason="", readiness=false. Elapsed: 3.814685ms
Jan 18 23:04:11.550: INFO: Pod "downwardapi-volume-80c17150-52df-4332-a728-e201ddc09183": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009660522s
Jan 18 23:04:13.553: INFO: Pod "downwardapi-volume-80c17150-52df-4332-a728-e201ddc09183": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012383998s
Jan 18 23:04:15.556: INFO: Pod "downwardapi-volume-80c17150-52df-4332-a728-e201ddc09183": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015172289s
STEP: Saw pod success
Jan 18 23:04:15.556: INFO: Pod "downwardapi-volume-80c17150-52df-4332-a728-e201ddc09183" satisfied condition "Succeeded or Failed"
Jan 18 23:04:15.558: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-80c17150-52df-4332-a728-e201ddc09183 container client-container: <nil>
STEP: delete the pod
Jan 18 23:04:15.575: INFO: Waiting for pod downwardapi-volume-80c17150-52df-4332-a728-e201ddc09183 to disappear
Jan 18 23:04:15.577: INFO: Pod downwardapi-volume-80c17150-52df-4332-a728-e201ddc09183 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 18 23:04:15.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2092" for this suite.

• [SLOW TEST:6.115 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":150,"skipped":2682,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:04:15.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 23:04:15.656: INFO: Waiting up to 5m0s for pod "downwardapi-volume-916a24ea-c46b-463f-955c-25e3886b3296" in namespace "projected-495" to be "Succeeded or Failed"
Jan 18 23:04:15.661: INFO: Pod "downwardapi-volume-916a24ea-c46b-463f-955c-25e3886b3296": Phase="Pending", Reason="", readiness=false. Elapsed: 4.538293ms
Jan 18 23:04:17.664: INFO: Pod "downwardapi-volume-916a24ea-c46b-463f-955c-25e3886b3296": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007712688s
Jan 18 23:04:19.668: INFO: Pod "downwardapi-volume-916a24ea-c46b-463f-955c-25e3886b3296": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012236966s
Jan 18 23:04:21.672: INFO: Pod "downwardapi-volume-916a24ea-c46b-463f-955c-25e3886b3296": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015618639s
STEP: Saw pod success
Jan 18 23:04:21.672: INFO: Pod "downwardapi-volume-916a24ea-c46b-463f-955c-25e3886b3296" satisfied condition "Succeeded or Failed"
Jan 18 23:04:21.674: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-916a24ea-c46b-463f-955c-25e3886b3296 container client-container: <nil>
STEP: delete the pod
Jan 18 23:04:21.694: INFO: Waiting for pod downwardapi-volume-916a24ea-c46b-463f-955c-25e3886b3296 to disappear
Jan 18 23:04:21.696: INFO: Pod downwardapi-volume-916a24ea-c46b-463f-955c-25e3886b3296 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 18 23:04:21.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-495" for this suite.

• [SLOW TEST:6.122 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":151,"skipped":2696,"failed":0}
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:04:21.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan 18 23:04:21.751: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 23:05:21.998: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:05:22.005: INFO: Starting informer...
STEP: Starting pod...
Jan 18 23:05:22.221: INFO: Pod is running on ip-10-0-219-147.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jan 18 23:05:22.241: INFO: Pod wasn't evicted. Proceeding
Jan 18 23:05:22.241: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jan 18 23:06:37.276: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:188
Jan 18 23:06:37.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9535" for this suite.

• [SLOW TEST:135.581 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":356,"completed":152,"skipped":2696,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:06:37.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 18 23:07:05.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8116" for this suite.

• [SLOW TEST:28.087 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":356,"completed":153,"skipped":2700,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:07:05.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7660
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-7660
I0118 23:07:05.451591      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7660, replica count: 2
I0118 23:07:08.502851      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 23:07:11.503982      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 23:07:11.504: INFO: Creating new exec pod
Jan 18 23:07:16.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-7660 exec execpodvml9n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 18 23:07:16.682: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 23:07:16.682: INFO: stdout: "externalname-service-wkqp4"
Jan 18 23:07:16.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-7660 exec execpodvml9n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.193.211 80'
Jan 18 23:07:16.824: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.193.211 80\nConnection to 172.30.193.211 80 port [tcp/http] succeeded!\n"
Jan 18 23:07:16.824: INFO: stdout: "externalname-service-qfr7v"
Jan 18 23:07:16.824: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 18 23:07:16.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7660" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:11.491 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":356,"completed":154,"skipped":2711,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:07:16.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in volume subpath
Jan 18 23:07:16.956: INFO: Waiting up to 5m0s for pod "var-expansion-126ed865-3b1a-47b3-b778-c624f98ce0f0" in namespace "var-expansion-9882" to be "Succeeded or Failed"
Jan 18 23:07:16.963: INFO: Pod "var-expansion-126ed865-3b1a-47b3-b778-c624f98ce0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.926222ms
Jan 18 23:07:18.966: INFO: Pod "var-expansion-126ed865-3b1a-47b3-b778-c624f98ce0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009818161s
Jan 18 23:07:20.969: INFO: Pod "var-expansion-126ed865-3b1a-47b3-b778-c624f98ce0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013040525s
Jan 18 23:07:22.972: INFO: Pod "var-expansion-126ed865-3b1a-47b3-b778-c624f98ce0f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015804455s
STEP: Saw pod success
Jan 18 23:07:22.972: INFO: Pod "var-expansion-126ed865-3b1a-47b3-b778-c624f98ce0f0" satisfied condition "Succeeded or Failed"
Jan 18 23:07:22.974: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod var-expansion-126ed865-3b1a-47b3-b778-c624f98ce0f0 container dapi-container: <nil>
STEP: delete the pod
Jan 18 23:07:23.065: INFO: Waiting for pod var-expansion-126ed865-3b1a-47b3-b778-c624f98ce0f0 to disappear
Jan 18 23:07:23.082: INFO: Pod var-expansion-126ed865-3b1a-47b3-b778-c624f98ce0f0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 18 23:07:23.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9882" for this suite.

• [SLOW TEST:6.223 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":356,"completed":155,"skipped":2730,"failed":0}
SS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:07:23.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
W0118 23:07:23.138871      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jan 18 23:07:23.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2278" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":356,"completed":156,"skipped":2732,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:07:23.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  test/e2e/framework/framework.go:652
STEP: Request ServerVersion
STEP: Confirm major version
Jan 18 23:07:23.291: INFO: Major version: 1
STEP: Confirm minor version
Jan 18 23:07:23.291: INFO: cleanMinorVersion: 24
Jan 18 23:07:23.291: INFO: Minor version: 24
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:188
Jan 18 23:07:23.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8186" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":356,"completed":157,"skipped":2737,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:07:23.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 18 23:07:23.404: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7404  eb392d8f-2928-4406-8b17-1b08322b0441 253343 0 2023-01-18 23:07:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-18 23:07:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:07:23.404: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7404  eb392d8f-2928-4406-8b17-1b08322b0441 253343 0 2023-01-18 23:07:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-18 23:07:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 18 23:07:23.413: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7404  eb392d8f-2928-4406-8b17-1b08322b0441 253344 0 2023-01-18 23:07:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-18 23:07:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:07:23.413: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7404  eb392d8f-2928-4406-8b17-1b08322b0441 253344 0 2023-01-18 23:07:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-18 23:07:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 18 23:07:23.439: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7404  eb392d8f-2928-4406-8b17-1b08322b0441 253351 0 2023-01-18 23:07:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-18 23:07:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:07:23.439: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7404  eb392d8f-2928-4406-8b17-1b08322b0441 253351 0 2023-01-18 23:07:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-18 23:07:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 18 23:07:23.450: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7404  eb392d8f-2928-4406-8b17-1b08322b0441 253354 0 2023-01-18 23:07:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-18 23:07:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:07:23.450: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7404  eb392d8f-2928-4406-8b17-1b08322b0441 253354 0 2023-01-18 23:07:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-18 23:07:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 18 23:07:23.462: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7404  aa09896a-28d4-4be0-9652-427a5262248f 253357 0 2023-01-18 23:07:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-18 23:07:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:07:23.462: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7404  aa09896a-28d4-4be0-9652-427a5262248f 253357 0 2023-01-18 23:07:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-18 23:07:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 18 23:07:33.469: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7404  aa09896a-28d4-4be0-9652-427a5262248f 253591 0 2023-01-18 23:07:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-18 23:07:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:07:33.470: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7404  aa09896a-28d4-4be0-9652-427a5262248f 253591 0 2023-01-18 23:07:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-18 23:07:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 18 23:07:43.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7404" for this suite.

• [SLOW TEST:20.159 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":356,"completed":158,"skipped":2738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:07:43.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service nodeport-test with type=NodePort in namespace services-6369
STEP: creating replication controller nodeport-test in namespace services-6369
I0118 23:07:43.562789      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-6369, replica count: 2
I0118 23:07:46.615628      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 23:07:49.616: INFO: Creating new exec pod
I0118 23:07:49.616792      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 23:07:54.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-6369 exec execpodsgkln -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 18 23:07:54.773: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 18 23:07:54.773: INFO: stdout: "nodeport-test-qqxxx"
Jan 18 23:07:54.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-6369 exec execpodsgkln -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.161.183 80'
Jan 18 23:07:54.884: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.161.183 80\nConnection to 172.30.161.183 80 port [tcp/http] succeeded!\n"
Jan 18 23:07:54.884: INFO: stdout: "nodeport-test-mzw6x"
Jan 18 23:07:54.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-6369 exec execpodsgkln -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.211.217 31588'
Jan 18 23:07:55.023: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.211.217 31588\nConnection to 10.0.211.217 31588 port [tcp/*] succeeded!\n"
Jan 18 23:07:55.023: INFO: stdout: "nodeport-test-qqxxx"
Jan 18 23:07:55.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-6369 exec execpodsgkln -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.253.152 31588'
Jan 18 23:07:55.134: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.253.152 31588\nConnection to 10.0.253.152 31588 port [tcp/*] succeeded!\n"
Jan 18 23:07:55.134: INFO: stdout: ""
Jan 18 23:07:56.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-6369 exec execpodsgkln -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.253.152 31588'
Jan 18 23:07:56.275: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.253.152 31588\nConnection to 10.0.253.152 31588 port [tcp/*] succeeded!\n"
Jan 18 23:07:56.275: INFO: stdout: "nodeport-test-mzw6x"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 18 23:07:56.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6369" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:12.801 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":356,"completed":159,"skipped":2769,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:07:56.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 23:07:56.356: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 23:08:56.604: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:08:56.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jan 18 23:09:00.701: INFO: found a healthy node: ip-10-0-219-147.ec2.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:09:14.787: INFO: pods created so far: [1 1 1]
Jan 18 23:09:14.787: INFO: length of pods created so far: 3
Jan 18 23:09:16.803: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:188
Jan 18 23:09:23.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7131" for this suite.
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 18 23:09:23.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7027" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:87.676 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":356,"completed":160,"skipped":2774,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:09:23.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pod templates
W0118 23:09:23.999982      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:09:24.000: INFO: created test-podtemplate-1
W0118 23:09:24.008880      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:09:24.008: INFO: created test-podtemplate-2
Jan 18 23:09:24.014: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jan 18 23:09:24.017: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jan 18 23:09:24.042: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jan 18 23:09:24.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1900" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":356,"completed":161,"skipped":2805,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:09:24.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 18 23:09:24.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9428" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":356,"completed":162,"skipped":3000,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:09:24.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Jan 18 23:09:24.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:10:10.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7232" for this suite.

• [SLOW TEST:46.787 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":356,"completed":163,"skipped":3096,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:10:10.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
W0118 23:10:11.010646      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0118 23:10:12.037651      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0118 23:10:12.037672      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 23:10:12.037: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 18 23:10:12.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4465" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":356,"completed":164,"skipped":3105,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:10:12.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:10:12.119: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 23:10:17.123: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jan 18 23:10:17.136: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jan 18 23:10:17.146: INFO: observed ReplicaSet test-rs in namespace replicaset-3071 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 23:10:17.192: INFO: observed ReplicaSet test-rs in namespace replicaset-3071 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 23:10:17.237: INFO: observed ReplicaSet test-rs in namespace replicaset-3071 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 23:10:17.259: INFO: observed ReplicaSet test-rs in namespace replicaset-3071 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 23:10:19.556: INFO: observed ReplicaSet test-rs in namespace replicaset-3071 with ReadyReplicas 2, AvailableReplicas 2
Jan 18 23:10:20.295: INFO: observed Replicaset test-rs in namespace replicaset-3071 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 18 23:10:20.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3071" for this suite.

• [SLOW TEST:8.259 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":356,"completed":165,"skipped":3137,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:10:20.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:10:22.372: INFO: Deleting pod "var-expansion-257ee3a8-573f-4844-ab9e-fada80f56e24" in namespace "var-expansion-7667"
Jan 18 23:10:22.378: INFO: Wait up to 5m0s for pod "var-expansion-257ee3a8-573f-4844-ab9e-fada80f56e24" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 18 23:10:26.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7667" for this suite.

• [SLOW TEST:6.094 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":356,"completed":166,"skipped":3167,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:10:26.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-07c1f64a-16e2-4b37-9266-1f83f96e02df
STEP: Creating a pod to test consume secrets
Jan 18 23:10:26.475: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2c42bbba-2000-4809-899d-46c6a2add15f" in namespace "projected-7265" to be "Succeeded or Failed"
Jan 18 23:10:26.479: INFO: Pod "pod-projected-secrets-2c42bbba-2000-4809-899d-46c6a2add15f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.161453ms
Jan 18 23:10:28.482: INFO: Pod "pod-projected-secrets-2c42bbba-2000-4809-899d-46c6a2add15f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006750174s
Jan 18 23:10:30.485: INFO: Pod "pod-projected-secrets-2c42bbba-2000-4809-899d-46c6a2add15f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010172132s
Jan 18 23:10:32.488: INFO: Pod "pod-projected-secrets-2c42bbba-2000-4809-899d-46c6a2add15f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013422338s
STEP: Saw pod success
Jan 18 23:10:32.488: INFO: Pod "pod-projected-secrets-2c42bbba-2000-4809-899d-46c6a2add15f" satisfied condition "Succeeded or Failed"
Jan 18 23:10:32.491: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-projected-secrets-2c42bbba-2000-4809-899d-46c6a2add15f container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 18 23:10:32.510: INFO: Waiting for pod pod-projected-secrets-2c42bbba-2000-4809-899d-46c6a2add15f to disappear
Jan 18 23:10:32.512: INFO: Pod pod-projected-secrets-2c42bbba-2000-4809-899d-46c6a2add15f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 18 23:10:32.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7265" for this suite.

• [SLOW TEST:6.121 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":167,"skipped":3183,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:10:32.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7967
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
W0118 23:10:32.568424      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:10:32.571: INFO: Found 0 stateful pods, waiting for 3
Jan 18 23:10:42.575: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:10:42.575: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:10:42.575: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:10:42.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-7967 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 23:10:42.698: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 23:10:42.698: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 23:10:42.698: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jan 18 23:10:52.726: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 18 23:11:02.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-7967 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 23:11:02.875: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 23:11:02.875: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 23:11:02.875: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Jan 18 23:11:22.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-7967 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 23:11:23.024: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 23:11:23.024: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 23:11:23.024: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 23:11:33.053: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 18 23:11:43.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=statefulset-7967 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 23:11:43.204: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 23:11:43.204: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 23:11:43.204: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 23:11:53.218: INFO: Waiting for StatefulSet statefulset-7967/ss2 to complete update
Jan 18 23:11:53.218: INFO: Waiting for Pod statefulset-7967/ss2-0 to have revision ss2-57bbdd95cb update revision ss2-5f8764d585
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 23:12:03.225: INFO: Deleting all statefulset in ns statefulset-7967
Jan 18 23:12:03.227: INFO: Scaling statefulset ss2 to 0
Jan 18 23:12:13.241: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 23:12:13.243: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 18 23:12:13.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7967" for this suite.

• [SLOW TEST:100.746 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":356,"completed":168,"skipped":3205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:12:13.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jan 18 23:12:13.351: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:12:15.353: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:12:17.353: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 18 23:12:18.368: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 18 23:12:19.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7746" for this suite.

• [SLOW TEST:6.154 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":356,"completed":169,"skipped":3240,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:12:19.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 23:12:19.879: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 23:12:21.890: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 12, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 12, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 12, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 12, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 23:12:24.902: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:12:24.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7436-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:12:27.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6073" for this suite.
STEP: Destroying namespace "webhook-6073-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.627 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":356,"completed":170,"skipped":3243,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:12:28.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 23:12:28.191: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e8809ce-f80a-497e-808c-bfdf6d6c124b" in namespace "downward-api-4948" to be "Succeeded or Failed"
Jan 18 23:12:28.205: INFO: Pod "downwardapi-volume-6e8809ce-f80a-497e-808c-bfdf6d6c124b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.448528ms
Jan 18 23:12:30.208: INFO: Pod "downwardapi-volume-6e8809ce-f80a-497e-808c-bfdf6d6c124b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017396818s
Jan 18 23:12:32.212: INFO: Pod "downwardapi-volume-6e8809ce-f80a-497e-808c-bfdf6d6c124b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0208182s
Jan 18 23:12:34.215: INFO: Pod "downwardapi-volume-6e8809ce-f80a-497e-808c-bfdf6d6c124b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024428029s
STEP: Saw pod success
Jan 18 23:12:34.215: INFO: Pod "downwardapi-volume-6e8809ce-f80a-497e-808c-bfdf6d6c124b" satisfied condition "Succeeded or Failed"
Jan 18 23:12:34.217: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-6e8809ce-f80a-497e-808c-bfdf6d6c124b container client-container: <nil>
STEP: delete the pod
Jan 18 23:12:34.244: INFO: Waiting for pod downwardapi-volume-6e8809ce-f80a-497e-808c-bfdf6d6c124b to disappear
Jan 18 23:12:34.247: INFO: Pod downwardapi-volume-6e8809ce-f80a-497e-808c-bfdf6d6c124b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 18 23:12:34.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4948" for this suite.

• [SLOW TEST:6.216 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":356,"completed":171,"skipped":3244,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:12:34.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 18 23:12:34.380: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:12:36.383: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:12:38.382: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 18 23:12:38.396: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:12:40.400: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:12:42.401: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 18 23:12:42.415: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 23:12:42.417: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 18 23:12:44.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 23:12:44.421: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 18 23:12:46.418: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 23:12:46.421: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 18 23:12:46.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5958" for this suite.

• [SLOW TEST:12.167 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":356,"completed":172,"skipped":3252,"failed":0}
SSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:12:46.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:12:46.528: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-984fb062-5245-47ee-9c08-018a6c638c50" in namespace "security-context-test-931" to be "Succeeded or Failed"
Jan 18 23:12:46.534: INFO: Pod "alpine-nnp-false-984fb062-5245-47ee-9c08-018a6c638c50": Phase="Pending", Reason="", readiness=false. Elapsed: 6.454755ms
Jan 18 23:12:48.538: INFO: Pod "alpine-nnp-false-984fb062-5245-47ee-9c08-018a6c638c50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009692333s
Jan 18 23:12:50.541: INFO: Pod "alpine-nnp-false-984fb062-5245-47ee-9c08-018a6c638c50": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012650571s
Jan 18 23:12:52.544: INFO: Pod "alpine-nnp-false-984fb062-5245-47ee-9c08-018a6c638c50": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015665292s
Jan 18 23:12:54.548: INFO: Pod "alpine-nnp-false-984fb062-5245-47ee-9c08-018a6c638c50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019875038s
Jan 18 23:12:54.548: INFO: Pod "alpine-nnp-false-984fb062-5245-47ee-9c08-018a6c638c50" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 18 23:12:54.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-931" for this suite.

• [SLOW TEST:8.137 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:298
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":173,"skipped":3256,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:12:54.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3547.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3547.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3547.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3547.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3547.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3547.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3547.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3547.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 181.176.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.176.181_udp@PTR;check="$$(dig +tcp +noall +answer +search 181.176.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.176.181_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3547.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3547.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3547.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3547.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3547.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3547.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3547.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3547.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 181.176.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.176.181_udp@PTR;check="$$(dig +tcp +noall +answer +search 181.176.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.176.181_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 18 23:12:58.677: INFO: Unable to read wheezy_udp@dns-test-service.dns-3547.svc.cluster.local from pod dns-3547/dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9: the server could not find the requested resource (get pods dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9)
Jan 18 23:12:58.679: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3547.svc.cluster.local from pod dns-3547/dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9: the server could not find the requested resource (get pods dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9)
Jan 18 23:12:58.682: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3547.svc.cluster.local from pod dns-3547/dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9: the server could not find the requested resource (get pods dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9)
Jan 18 23:12:58.684: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3547.svc.cluster.local from pod dns-3547/dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9: the server could not find the requested resource (get pods dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9)
Jan 18 23:12:58.697: INFO: Unable to read jessie_udp@dns-test-service.dns-3547.svc.cluster.local from pod dns-3547/dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9: the server could not find the requested resource (get pods dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9)
Jan 18 23:12:58.699: INFO: Unable to read jessie_tcp@dns-test-service.dns-3547.svc.cluster.local from pod dns-3547/dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9: the server could not find the requested resource (get pods dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9)
Jan 18 23:12:58.701: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3547.svc.cluster.local from pod dns-3547/dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9: the server could not find the requested resource (get pods dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9)
Jan 18 23:12:58.704: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3547.svc.cluster.local from pod dns-3547/dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9: the server could not find the requested resource (get pods dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9)
Jan 18 23:12:58.713: INFO: Lookups using dns-3547/dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9 failed for: [wheezy_udp@dns-test-service.dns-3547.svc.cluster.local wheezy_tcp@dns-test-service.dns-3547.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3547.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3547.svc.cluster.local jessie_udp@dns-test-service.dns-3547.svc.cluster.local jessie_tcp@dns-test-service.dns-3547.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3547.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3547.svc.cluster.local]

Jan 18 23:13:03.753: INFO: DNS probes using dns-3547/dns-test-03a026c3-a477-47d5-99c0-27582a4ffcc9 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 18 23:13:03.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3547" for this suite.

• [SLOW TEST:9.244 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":356,"completed":174,"skipped":3290,"failed":0}
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:13:03.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:13:03.857: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
W0118 23:13:03.878762      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:13:03.882: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 23:13:08.886: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 18 23:13:08.886: INFO: Creating deployment "test-rolling-update-deployment"
Jan 18 23:13:08.891: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 18 23:13:08.896: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 18 23:13:10.901: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 18 23:13:10.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 13, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 13, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 13, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 13, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-5579c56cf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:13:12.906: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 23:13:12.912: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6592  864bb8ad-f76d-49ed-acdc-275a71a7fea8 259525 1 2023-01-18 23:13:08 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2023-01-18 23:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:13:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038fb1d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 23:13:08 +0000 UTC,LastTransitionTime:2023-01-18 23:13:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-5579c56cf8" has successfully progressed.,LastUpdateTime:2023-01-18 23:13:11 +0000 UTC,LastTransitionTime:2023-01-18 23:13:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 23:13:12.914: INFO: New ReplicaSet "test-rolling-update-deployment-5579c56cf8" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-5579c56cf8  deployment-6592  b6271846-a36b-49e9-aa73-bb1a7d913239 259514 1 2023-01-18 23:13:08 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:5579c56cf8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 864bb8ad-f76d-49ed-acdc-275a71a7fea8 0xc0038fb6c7 0xc0038fb6c8}] []  [{kube-controller-manager Update apps/v1 2023-01-18 23:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"864bb8ad-f76d-49ed-acdc-275a71a7fea8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:13:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 5579c56cf8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:5579c56cf8] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038fb778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:13:12.914: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 18 23:13:12.914: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6592  66c964bc-9ab7-418c-908d-01bac72ea4dd 259524 2 2023-01-18 23:13:03 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 864bb8ad-f76d-49ed-acdc-275a71a7fea8 0xc0038fb597 0xc0038fb598}] []  [{e2e.test Update apps/v1 2023-01-18 23:13:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:13:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"864bb8ad-f76d-49ed-acdc-275a71a7fea8\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:13:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0038fb658 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:13:12.916: INFO: Pod "test-rolling-update-deployment-5579c56cf8-g6h4t" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-5579c56cf8-g6h4t test-rolling-update-deployment-5579c56cf8- deployment-6592  64482fd2-705d-4132-9342-33833804ae74 259513 0 2023-01-18 23:13:08 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:5579c56cf8] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.16.116/23"],"mac_address":"0a:58:0a:80:10:74","gateway_ips":["10.128.16.1"],"ip_address":"10.128.16.116/23","gateway_ip":"10.128.16.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.16.116"
    ],
    "mac": "0a:58:0a:80:10:74",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.16.116"
    ],
    "mac": "0a:58:0a:80:10:74",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-5579c56cf8 b6271846-a36b-49e9-aa73-bb1a7d913239 0xc0038fbbd7 0xc0038fbbd8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6271846-a36b-49e9-aa73-bb1a7d913239\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:13:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.16.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-18 23:13:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jjht4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjht4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-211-217.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-bj28l,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:13:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:13:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:13:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:13:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.211.217,PodIP:10.128.16.116,StartTime:2023-01-18 23:13:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:13:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403,ContainerID:cri-o://e69bc57cdcfcf23f847cd20ff1e747cdbb45c630557a2dc1bf42e3d19cdbe269,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.16.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 18 23:13:12.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6592" for this suite.

• [SLOW TEST:9.112 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":175,"skipped":3290,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:13:12.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 23:13:13.004: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc4eb8f2-b72d-422c-bee7-bba1d7a128e1" in namespace "downward-api-9416" to be "Succeeded or Failed"
Jan 18 23:13:13.009: INFO: Pod "downwardapi-volume-bc4eb8f2-b72d-422c-bee7-bba1d7a128e1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.085763ms
Jan 18 23:13:15.013: INFO: Pod "downwardapi-volume-bc4eb8f2-b72d-422c-bee7-bba1d7a128e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008475798s
Jan 18 23:13:17.015: INFO: Pod "downwardapi-volume-bc4eb8f2-b72d-422c-bee7-bba1d7a128e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011237144s
Jan 18 23:13:19.019: INFO: Pod "downwardapi-volume-bc4eb8f2-b72d-422c-bee7-bba1d7a128e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015094222s
STEP: Saw pod success
Jan 18 23:13:19.019: INFO: Pod "downwardapi-volume-bc4eb8f2-b72d-422c-bee7-bba1d7a128e1" satisfied condition "Succeeded or Failed"
Jan 18 23:13:19.023: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-bc4eb8f2-b72d-422c-bee7-bba1d7a128e1 container client-container: <nil>
STEP: delete the pod
Jan 18 23:13:19.035: INFO: Waiting for pod downwardapi-volume-bc4eb8f2-b72d-422c-bee7-bba1d7a128e1 to disappear
Jan 18 23:13:19.037: INFO: Pod downwardapi-volume-bc4eb8f2-b72d-422c-bee7-bba1d7a128e1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 18 23:13:19.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9416" for this suite.

• [SLOW TEST:6.120 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":176,"skipped":3309,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:13:19.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jan 18 23:13:35.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2586" for this suite.
STEP: Destroying namespace "nsdeletetest-6061" for this suite.
Jan 18 23:13:35.278: INFO: Namespace nsdeletetest-6061 was already deleted
STEP: Destroying namespace "nsdeletetest-123" for this suite.

• [SLOW TEST:16.245 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":356,"completed":177,"skipped":3328,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:13:35.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jan 18 23:13:35.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4918" for this suite.
STEP: Destroying namespace "nspatchtest-d206ff3f-ec32-4811-80d9-b5c9d3ba13e4-3107" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":356,"completed":178,"skipped":3381,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:13:35.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-cc35b7ca-3b80-47e4-a0e3-e3eb8327ed69
STEP: Creating a pod to test consume configMaps
Jan 18 23:13:35.532: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8518668d-bcba-4005-9f1a-87930316c78e" in namespace "projected-3738" to be "Succeeded or Failed"
Jan 18 23:13:35.538: INFO: Pod "pod-projected-configmaps-8518668d-bcba-4005-9f1a-87930316c78e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.265152ms
Jan 18 23:13:37.542: INFO: Pod "pod-projected-configmaps-8518668d-bcba-4005-9f1a-87930316c78e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009587232s
Jan 18 23:13:39.545: INFO: Pod "pod-projected-configmaps-8518668d-bcba-4005-9f1a-87930316c78e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013424451s
Jan 18 23:13:41.548: INFO: Pod "pod-projected-configmaps-8518668d-bcba-4005-9f1a-87930316c78e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015586937s
STEP: Saw pod success
Jan 18 23:13:41.548: INFO: Pod "pod-projected-configmaps-8518668d-bcba-4005-9f1a-87930316c78e" satisfied condition "Succeeded or Failed"
Jan 18 23:13:41.549: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-projected-configmaps-8518668d-bcba-4005-9f1a-87930316c78e container agnhost-container: <nil>
STEP: delete the pod
Jan 18 23:13:41.565: INFO: Waiting for pod pod-projected-configmaps-8518668d-bcba-4005-9f1a-87930316c78e to disappear
Jan 18 23:13:41.567: INFO: Pod pod-projected-configmaps-8518668d-bcba-4005-9f1a-87930316c78e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 18 23:13:41.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3738" for this suite.

• [SLOW TEST:6.152 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":179,"skipped":3382,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:13:41.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:79
Jan 18 23:13:41.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the sample API server.
Jan 18 23:13:42.117: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jan 18 23:13:44.162: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:13:46.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:13:48.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:13:50.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 13, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:13:52.485: INFO: Waited 315.838189ms for the sample-apiserver to be ready to handle requests.
I0118 23:13:53.526333      22 request.go:601] Waited for 1.021607743s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/performance.openshift.io/v2
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jan 18 23:13:55.135: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:69
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:188
Jan 18 23:13:55.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3020" for this suite.

• [SLOW TEST:14.455 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":356,"completed":180,"skipped":3424,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:13:56.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:188
Jan 18 23:13:56.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8098" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":356,"completed":181,"skipped":3434,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:13:56.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 23:13:56.658: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 23:13:58.666: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 13, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 13, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 13, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 13, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 23:14:01.679: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:14:13.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3714" for this suite.
STEP: Destroying namespace "webhook-3714-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:17.717 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":356,"completed":182,"skipped":3465,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:14:13.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating api versions
Jan 18 23:14:13.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3900 api-versions'
Jan 18 23:14:13.967: INFO: stderr: ""
Jan 18 23:14:13.967: INFO: stdout: "addons.managed.openshift.io/v1alpha1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloud.network.openshift.io/v1\ncloudcredential.openshift.io/v1\ncloudingress.managed.openshift.io/v1alpha1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nk8s.ovn.org/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmanaged.openshift.io/v1alpha1\nmanaged.openshift.io/v1alpha2\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nmonitoring.openshift.io/v1alpha1\nmonitoring.rhobs/v1\nmonitoring.rhobs/v1alpha1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\nocmagent.managed.openshift.io/v1alpha1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nsplunkforwarder.managed.openshift.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nupgrade.managed.openshift.io/v1alpha1\nuser.openshift.io/v1\nv1\nvelero.io/v1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 23:14:13.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3900" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":356,"completed":183,"skipped":3474,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:14:13.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 18 23:14:14.099: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9375  c82dc17f-dd74-4467-a1e5-360ad8067434 261208 0 2023-01-18 23:14:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-18 23:14:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:14:14.099: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9375  c82dc17f-dd74-4467-a1e5-360ad8067434 261212 0 2023-01-18 23:14:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-18 23:14:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:14:14.099: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9375  c82dc17f-dd74-4467-a1e5-360ad8067434 261221 0 2023-01-18 23:14:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-18 23:14:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 18 23:14:24.154: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9375  c82dc17f-dd74-4467-a1e5-360ad8067434 261444 0 2023-01-18 23:14:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-18 23:14:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:14:24.154: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9375  c82dc17f-dd74-4467-a1e5-360ad8067434 261445 0 2023-01-18 23:14:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-18 23:14:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:14:24.155: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9375  c82dc17f-dd74-4467-a1e5-360ad8067434 261446 0 2023-01-18 23:14:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-18 23:14:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 18 23:14:24.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9375" for this suite.

• [SLOW TEST:10.170 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":356,"completed":184,"skipped":3570,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:14:24.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Jan 18 23:14:24.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 create -f -'
Jan 18 23:14:26.446: INFO: stderr: ""
Jan 18 23:14:26.446: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 18 23:14:26.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:14:26.517: INFO: stderr: ""
Jan 18 23:14:26.517: INFO: stdout: "update-demo-nautilus-5kp8f update-demo-nautilus-6nx9s "
Jan 18 23:14:26.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods update-demo-nautilus-5kp8f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:14:26.591: INFO: stderr: ""
Jan 18 23:14:26.591: INFO: stdout: ""
Jan 18 23:14:26.591: INFO: update-demo-nautilus-5kp8f is created but not running
Jan 18 23:14:31.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:14:31.891: INFO: stderr: ""
Jan 18 23:14:31.891: INFO: stdout: "update-demo-nautilus-5kp8f update-demo-nautilus-6nx9s "
Jan 18 23:14:31.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods update-demo-nautilus-5kp8f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:14:31.967: INFO: stderr: ""
Jan 18 23:14:31.967: INFO: stdout: "true"
Jan 18 23:14:31.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods update-demo-nautilus-5kp8f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:14:32.046: INFO: stderr: ""
Jan 18 23:14:32.046: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 18 23:14:32.046: INFO: validating pod update-demo-nautilus-5kp8f
Jan 18 23:14:32.052: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:14:32.052: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:14:32.052: INFO: update-demo-nautilus-5kp8f is verified up and running
Jan 18 23:14:32.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods update-demo-nautilus-6nx9s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:14:32.121: INFO: stderr: ""
Jan 18 23:14:32.121: INFO: stdout: ""
Jan 18 23:14:32.121: INFO: update-demo-nautilus-6nx9s is created but not running
Jan 18 23:14:37.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:14:37.203: INFO: stderr: ""
Jan 18 23:14:37.203: INFO: stdout: "update-demo-nautilus-5kp8f update-demo-nautilus-6nx9s "
Jan 18 23:14:37.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods update-demo-nautilus-5kp8f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:14:37.285: INFO: stderr: ""
Jan 18 23:14:37.285: INFO: stdout: "true"
Jan 18 23:14:37.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods update-demo-nautilus-5kp8f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:14:37.368: INFO: stderr: ""
Jan 18 23:14:37.368: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 18 23:14:37.368: INFO: validating pod update-demo-nautilus-5kp8f
Jan 18 23:14:37.371: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:14:37.371: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:14:37.371: INFO: update-demo-nautilus-5kp8f is verified up and running
Jan 18 23:14:37.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods update-demo-nautilus-6nx9s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:14:37.434: INFO: stderr: ""
Jan 18 23:14:37.434: INFO: stdout: "true"
Jan 18 23:14:37.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods update-demo-nautilus-6nx9s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:14:37.488: INFO: stderr: ""
Jan 18 23:14:37.488: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 18 23:14:37.488: INFO: validating pod update-demo-nautilus-6nx9s
Jan 18 23:14:37.493: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:14:37.493: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:14:37.493: INFO: update-demo-nautilus-6nx9s is verified up and running
STEP: scaling down the replication controller
Jan 18 23:14:37.497: INFO: scanned /root for discovery docs: <nil>
Jan 18 23:14:37.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 18 23:14:38.580: INFO: stderr: ""
Jan 18 23:14:38.580: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 18 23:14:38.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:14:38.636: INFO: stderr: ""
Jan 18 23:14:38.636: INFO: stdout: "update-demo-nautilus-5kp8f update-demo-nautilus-6nx9s "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 18 23:14:43.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:14:43.699: INFO: stderr: ""
Jan 18 23:14:43.699: INFO: stdout: "update-demo-nautilus-5kp8f "
Jan 18 23:14:43.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods update-demo-nautilus-5kp8f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:14:43.754: INFO: stderr: ""
Jan 18 23:14:43.754: INFO: stdout: "true"
Jan 18 23:14:43.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods update-demo-nautilus-5kp8f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:14:43.810: INFO: stderr: ""
Jan 18 23:14:43.810: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 18 23:14:43.810: INFO: validating pod update-demo-nautilus-5kp8f
Jan 18 23:14:43.813: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:14:43.813: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:14:43.813: INFO: update-demo-nautilus-5kp8f is verified up and running
STEP: scaling up the replication controller
Jan 18 23:14:43.816: INFO: scanned /root for discovery docs: <nil>
Jan 18 23:14:43.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 18 23:14:44.890: INFO: stderr: ""
Jan 18 23:14:44.890: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 18 23:14:44.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:14:44.947: INFO: stderr: ""
Jan 18 23:14:44.947: INFO: stdout: "update-demo-nautilus-2fnqs update-demo-nautilus-5kp8f "
Jan 18 23:14:44.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods update-demo-nautilus-2fnqs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:14:45.001: INFO: stderr: ""
Jan 18 23:14:45.001: INFO: stdout: ""
Jan 18 23:14:45.001: INFO: update-demo-nautilus-2fnqs is created but not running
Jan 18 23:14:50.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:14:50.061: INFO: stderr: ""
Jan 18 23:14:50.061: INFO: stdout: "update-demo-nautilus-2fnqs update-demo-nautilus-5kp8f "
Jan 18 23:14:50.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods update-demo-nautilus-2fnqs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:14:50.125: INFO: stderr: ""
Jan 18 23:14:50.125: INFO: stdout: "true"
Jan 18 23:14:50.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods update-demo-nautilus-2fnqs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:14:50.180: INFO: stderr: ""
Jan 18 23:14:50.180: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 18 23:14:50.180: INFO: validating pod update-demo-nautilus-2fnqs
Jan 18 23:14:50.185: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:14:50.185: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:14:50.185: INFO: update-demo-nautilus-2fnqs is verified up and running
Jan 18 23:14:50.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods update-demo-nautilus-5kp8f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:14:50.240: INFO: stderr: ""
Jan 18 23:14:50.240: INFO: stdout: "true"
Jan 18 23:14:50.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods update-demo-nautilus-5kp8f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:14:50.296: INFO: stderr: ""
Jan 18 23:14:50.296: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 18 23:14:50.296: INFO: validating pod update-demo-nautilus-5kp8f
Jan 18 23:14:50.299: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:14:50.299: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:14:50.299: INFO: update-demo-nautilus-5kp8f is verified up and running
STEP: using delete to clean up resources
Jan 18 23:14:50.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 delete --grace-period=0 --force -f -'
Jan 18 23:14:50.353: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 23:14:50.353: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 18 23:14:50.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get rc,svc -l name=update-demo --no-headers'
Jan 18 23:14:50.430: INFO: stderr: "No resources found in kubectl-8845 namespace.\n"
Jan 18 23:14:50.430: INFO: stdout: ""
Jan 18 23:14:50.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-8845 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 23:14:50.487: INFO: stderr: ""
Jan 18 23:14:50.487: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 23:14:50.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8845" for this suite.

• [SLOW TEST:26.334 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should scale a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":356,"completed":185,"skipped":3570,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:14:50.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-0a3a16c3-339a-4d04-9191-830331c7829b
STEP: Creating a pod to test consume configMaps
Jan 18 23:14:50.567: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2dd2b024-3698-4fdf-94ae-8d229cc24b61" in namespace "projected-6651" to be "Succeeded or Failed"
Jan 18 23:14:50.570: INFO: Pod "pod-projected-configmaps-2dd2b024-3698-4fdf-94ae-8d229cc24b61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.460058ms
Jan 18 23:14:52.573: INFO: Pod "pod-projected-configmaps-2dd2b024-3698-4fdf-94ae-8d229cc24b61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005363035s
Jan 18 23:14:54.576: INFO: Pod "pod-projected-configmaps-2dd2b024-3698-4fdf-94ae-8d229cc24b61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008501848s
Jan 18 23:14:56.579: INFO: Pod "pod-projected-configmaps-2dd2b024-3698-4fdf-94ae-8d229cc24b61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011365579s
STEP: Saw pod success
Jan 18 23:14:56.579: INFO: Pod "pod-projected-configmaps-2dd2b024-3698-4fdf-94ae-8d229cc24b61" satisfied condition "Succeeded or Failed"
Jan 18 23:14:56.581: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-projected-configmaps-2dd2b024-3698-4fdf-94ae-8d229cc24b61 container agnhost-container: <nil>
STEP: delete the pod
Jan 18 23:14:56.594: INFO: Waiting for pod pod-projected-configmaps-2dd2b024-3698-4fdf-94ae-8d229cc24b61 to disappear
Jan 18 23:14:56.596: INFO: Pod pod-projected-configmaps-2dd2b024-3698-4fdf-94ae-8d229cc24b61 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 18 23:14:56.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6651" for this suite.

• [SLOW TEST:6.113 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":186,"skipped":3577,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:14:56.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption is created
Jan 18 23:14:56.677: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:14:58.680: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:15:00.680: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 18 23:15:00.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8777" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":356,"completed":187,"skipped":3578,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:15:00.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-8798
Jan 18 23:15:00.824: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:15:02.831: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 18 23:15:02.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 18 23:15:03.002: INFO: rc: 7
Jan 18 23:15:03.010: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 18 23:15:03.013: INFO: Pod kube-proxy-mode-detector no longer exists
Jan 18 23:15:03.013: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-8798
STEP: creating replication controller affinity-nodeport-timeout in namespace services-8798
I0118 23:15:03.036620      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-8798, replica count: 3
I0118 23:15:06.087628      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 23:15:09.089980      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 23:15:09.098: INFO: Creating new exec pod
Jan 18 23:15:14.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec execpod-affinityhjms6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 18 23:15:14.252: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 18 23:15:14.252: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:15:14.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec execpod-affinityhjms6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.194.199 80'
Jan 18 23:15:14.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.194.199 80\nConnection to 172.30.194.199 80 port [tcp/http] succeeded!\n"
Jan 18 23:15:14.366: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:15:14.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec execpod-affinityhjms6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.236.5 32057'
Jan 18 23:15:14.490: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.236.5 32057\nConnection to 10.0.236.5 32057 port [tcp/*] succeeded!\n"
Jan 18 23:15:14.490: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:15:14.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec execpod-affinityhjms6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.253.152 32057'
Jan 18 23:15:14.606: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.253.152 32057\nConnection to 10.0.253.152 32057 port [tcp/*] succeeded!\n"
Jan 18 23:15:14.606: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:15:14.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec execpod-affinityhjms6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.128.7:32057/ ; done'
Jan 18 23:15:14.787: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n"
Jan 18 23:15:14.787: INFO: stdout: "\naffinity-nodeport-timeout-97zv4\naffinity-nodeport-timeout-97zv4\naffinity-nodeport-timeout-97zv4\naffinity-nodeport-timeout-97zv4\naffinity-nodeport-timeout-97zv4\naffinity-nodeport-timeout-97zv4\naffinity-nodeport-timeout-97zv4\naffinity-nodeport-timeout-97zv4\naffinity-nodeport-timeout-97zv4\naffinity-nodeport-timeout-97zv4\naffinity-nodeport-timeout-97zv4\naffinity-nodeport-timeout-97zv4\naffinity-nodeport-timeout-97zv4\naffinity-nodeport-timeout-97zv4\naffinity-nodeport-timeout-97zv4\naffinity-nodeport-timeout-97zv4"
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Received response from host: affinity-nodeport-timeout-97zv4
Jan 18 23:15:14.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec execpod-affinityhjms6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.128.7:32057/'
Jan 18 23:15:14.900: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n"
Jan 18 23:15:14.900: INFO: stdout: "affinity-nodeport-timeout-97zv4"
Jan 18 23:15:34.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec execpod-affinityhjms6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.128.7:32057/'
Jan 18 23:15:35.047: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n"
Jan 18 23:15:35.047: INFO: stdout: "affinity-nodeport-timeout-97zv4"
Jan 18 23:15:55.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec execpod-affinityhjms6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.128.7:32057/'
Jan 18 23:15:55.190: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n"
Jan 18 23:15:55.190: INFO: stdout: "affinity-nodeport-timeout-97zv4"
Jan 18 23:16:15.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec execpod-affinityhjms6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.128.7:32057/'
Jan 18 23:16:15.339: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n"
Jan 18 23:16:15.339: INFO: stdout: "affinity-nodeport-timeout-97zv4"
Jan 18 23:16:35.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec execpod-affinityhjms6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.128.7:32057/'
Jan 18 23:16:35.485: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n"
Jan 18 23:16:35.485: INFO: stdout: "affinity-nodeport-timeout-97zv4"
Jan 18 23:16:55.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec execpod-affinityhjms6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.128.7:32057/'
Jan 18 23:16:55.630: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n"
Jan 18 23:16:55.630: INFO: stdout: "affinity-nodeport-timeout-97zv4"
Jan 18 23:17:15.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec execpod-affinityhjms6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.128.7:32057/'
Jan 18 23:17:15.756: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n"
Jan 18 23:17:15.756: INFO: stdout: "affinity-nodeport-timeout-97zv4"
Jan 18 23:17:35.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec execpod-affinityhjms6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.128.7:32057/'
Jan 18 23:17:35.877: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n"
Jan 18 23:17:35.877: INFO: stdout: "affinity-nodeport-timeout-97zv4"
Jan 18 23:17:55.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec execpod-affinityhjms6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.128.7:32057/'
Jan 18 23:17:56.014: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n"
Jan 18 23:17:56.014: INFO: stdout: "affinity-nodeport-timeout-97zv4"
Jan 18 23:18:16.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-8798 exec execpod-affinityhjms6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.128.7:32057/'
Jan 18 23:18:16.158: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.128.7:32057/\n"
Jan 18 23:18:16.158: INFO: stdout: "affinity-nodeport-timeout-97zv4"
Jan 18 23:18:36.159: FAIL: Session is sticky after reaching the timeout

Full Stack Trace
k8s.io/kubernetes/test/e2e/network.execAffinityTestForSessionAffinityTimeout(0xc000c1bc80, {0x7a6bf58, 0xc00381e300}, 0xc000d79400)
	test/e2e/network/service.go:3367 +0xb2e
k8s.io/kubernetes/test/e2e/network.glob..func25.26()
	test/e2e/network/service.go:1877 +0x8b
k8s.io/kubernetes/test/e2e.RunE2ETests(0x257e5b7?)
	test/e2e/e2e.go:130 +0x686
k8s.io/kubernetes/test/e2e.TestE2E(0x24efd19?)
	test/e2e/e2e_test.go:136 +0x19
testing.tRunner(0xc00072f6c0, 0x73fa160)
	/usr/local/go/src/testing/testing.go:1439 +0x102
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:1486 +0x35f
Jan 18 23:18:36.160: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-8798, will wait for the garbage collector to delete the pods
Jan 18 23:18:36.228: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 4.921841ms
Jan 18 23:18:36.329: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.431009ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
STEP: Collecting events from namespace "services-8798".
STEP: Found 32 events.
Jan 18 23:18:38.958: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for affinity-nodeport-timeout-5p474: { } Scheduled: Successfully assigned services-8798/affinity-nodeport-timeout-5p474 to ip-10-0-128-7.ec2.internal by ip-10-0-176-161
Jan 18 23:18:38.958: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for affinity-nodeport-timeout-6tzsz: { } Scheduled: Successfully assigned services-8798/affinity-nodeport-timeout-6tzsz to ip-10-0-219-147.ec2.internal by ip-10-0-176-161
Jan 18 23:18:38.958: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for affinity-nodeport-timeout-97zv4: { } Scheduled: Successfully assigned services-8798/affinity-nodeport-timeout-97zv4 to ip-10-0-211-217.ec2.internal by ip-10-0-176-161
Jan 18 23:18:38.958: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for execpod-affinityhjms6: { } Scheduled: Successfully assigned services-8798/execpod-affinityhjms6 to ip-10-0-219-147.ec2.internal by ip-10-0-176-161
Jan 18 23:18:38.958: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for kube-proxy-mode-detector: { } Scheduled: Successfully assigned services-8798/kube-proxy-mode-detector to ip-10-0-219-147.ec2.internal by ip-10-0-176-161
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:01 +0000 UTC - event for kube-proxy-mode-detector: {kubelet ip-10-0-219-147.ec2.internal} Started: Started container agnhost-container
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:01 +0000 UTC - event for kube-proxy-mode-detector: {kubelet ip-10-0-219-147.ec2.internal} Created: Created container agnhost-container
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:01 +0000 UTC - event for kube-proxy-mode-detector: {kubelet ip-10-0-219-147.ec2.internal} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.36" already present on machine
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:03 +0000 UTC - event for affinity-nodeport-timeout: {replication-controller } SuccessfulCreate: Created pod: affinity-nodeport-timeout-5p474
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:03 +0000 UTC - event for affinity-nodeport-timeout: {replication-controller } SuccessfulCreate: Created pod: affinity-nodeport-timeout-97zv4
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:03 +0000 UTC - event for affinity-nodeport-timeout: {replication-controller } SuccessfulCreate: Created pod: affinity-nodeport-timeout-6tzsz
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:04 +0000 UTC - event for kube-proxy-mode-detector: {kubelet ip-10-0-219-147.ec2.internal} Killing: Stopping container agnhost-container
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:05 +0000 UTC - event for affinity-nodeport-timeout-5p474: {kubelet ip-10-0-128-7.ec2.internal} Created: Created container affinity-nodeport-timeout
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:05 +0000 UTC - event for affinity-nodeport-timeout-5p474: {kubelet ip-10-0-128-7.ec2.internal} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.36" already present on machine
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:05 +0000 UTC - event for affinity-nodeport-timeout-5p474: {multus } AddedInterface: Add eth0 [10.128.14.85/23] from ovn-kubernetes
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:05 +0000 UTC - event for affinity-nodeport-timeout-5p474: {kubelet ip-10-0-128-7.ec2.internal} Started: Started container affinity-nodeport-timeout
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:05 +0000 UTC - event for affinity-nodeport-timeout-6tzsz: {kubelet ip-10-0-219-147.ec2.internal} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.36" already present on machine
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:05 +0000 UTC - event for affinity-nodeport-timeout-6tzsz: {multus } AddedInterface: Add eth0 [10.128.12.186/23] from ovn-kubernetes
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:05 +0000 UTC - event for affinity-nodeport-timeout-6tzsz: {kubelet ip-10-0-219-147.ec2.internal} Started: Started container affinity-nodeport-timeout
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:05 +0000 UTC - event for affinity-nodeport-timeout-6tzsz: {kubelet ip-10-0-219-147.ec2.internal} Created: Created container affinity-nodeport-timeout
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:05 +0000 UTC - event for affinity-nodeport-timeout-97zv4: {multus } AddedInterface: Add eth0 [10.128.16.120/23] from ovn-kubernetes
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:05 +0000 UTC - event for affinity-nodeport-timeout-97zv4: {kubelet ip-10-0-211-217.ec2.internal} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.36" already present on machine
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:05 +0000 UTC - event for affinity-nodeport-timeout-97zv4: {kubelet ip-10-0-211-217.ec2.internal} Created: Created container affinity-nodeport-timeout
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:05 +0000 UTC - event for affinity-nodeport-timeout-97zv4: {kubelet ip-10-0-211-217.ec2.internal} Started: Started container affinity-nodeport-timeout
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:11 +0000 UTC - event for execpod-affinityhjms6: {multus } AddedInterface: Add eth0 [10.128.12.187/23] from ovn-kubernetes
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:11 +0000 UTC - event for execpod-affinityhjms6: {kubelet ip-10-0-219-147.ec2.internal} Started: Started container agnhost-container
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:11 +0000 UTC - event for execpod-affinityhjms6: {kubelet ip-10-0-219-147.ec2.internal} Created: Created container agnhost-container
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:15:11 +0000 UTC - event for execpod-affinityhjms6: {kubelet ip-10-0-219-147.ec2.internal} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.36" already present on machine
Jan 18 23:18:38.958: INFO: At 2023-01-18 23:18:36 +0000 UTC - event for affinity-nodeport-timeout-5p474: {kubelet ip-10-0-128-7.ec2.internal} Killing: Stopping container affinity-nodeport-timeout
Jan 18 23:18:38.959: INFO: At 2023-01-18 23:18:36 +0000 UTC - event for affinity-nodeport-timeout-6tzsz: {kubelet ip-10-0-219-147.ec2.internal} Killing: Stopping container affinity-nodeport-timeout
Jan 18 23:18:38.959: INFO: At 2023-01-18 23:18:36 +0000 UTC - event for affinity-nodeport-timeout-97zv4: {kubelet ip-10-0-211-217.ec2.internal} Killing: Stopping container affinity-nodeport-timeout
Jan 18 23:18:38.959: INFO: At 2023-01-18 23:18:36 +0000 UTC - event for execpod-affinityhjms6: {kubelet ip-10-0-219-147.ec2.internal} Killing: Stopping container agnhost-container
Jan 18 23:18:38.964: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
Jan 18 23:18:38.964: INFO: 
Jan 18 23:18:38.969: INFO: 
Logging node info for node ip-10-0-128-7.ec2.internal
Jan 18 23:18:38.974: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-128-7.ec2.internal    0e2fac99-ae24-4675-b8b4-0c59f1661da4 263269 0 2023-01-18 21:27:09 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:r5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-128-7.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io:infra node-role.kubernetes.io/infra: node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:r5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-010e853cf354cb872","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0abfe3af21c04dd6c"} k8s.ovn.org/host-addresses:["10.0.128.7"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-128-7.ec2.internal","mac-address":"02:a3:9c:91:ae:55","ip-addresses":["10.0.128.7/17"],"ip-address":"10.0.128.7/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:52724080-26ac-4555-b2d7-40245661c309 k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.9/16"} k8s.ovn.org/node-mgmt-port-mac-address:ae:4c:ac:9d:28:d3 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.128.7/17"} k8s.ovn.org/node-subnets:{"default":"10.128.14.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-infra-us-east-1a-f5bmn machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done managed.openshift.com/customlabels:node-role.kubernetes.io,node-role.kubernetes.io/infra volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{ancient-changes Update v1 2023-01-18 21:27:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {nodelink-controller Update v1 2023-01-18 21:27:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}},"f:labels":{"f:node-role.kubernetes.io":{},"f:node-role.kubernetes.io/infra":{}}}} } {cloud-network-config-controller Update v1 2023-01-18 21:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{}}}} } {machine-config-daemon Update v1 2023-01-18 21:27:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {ip-10-0-128-7 Update v1 2023-01-18 21:27:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {machine-config-controller Update v1 2023-01-18 21:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {manager Update v1 2023-01-18 21:36:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:managed.openshift.com/customlabels":{}}}} } {kube-controller-manager Update v1 2023-01-18 21:36:17 +0000 UTC FieldsV1 {"f:status":{"f:volumesAttached":{}}} status} {kubelet Update v1 2023-01-18 22:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}},"f:volumesInUse":{}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-0abfe3af21c04dd6c,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321574121472 0} {<nil>} 314037228Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{33222410240 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3920 -3} {<nil>} 3920m DecimalSI},ephemeral-storage: {{289416708846 0} {<nil>} 289416708846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{29423880765440 -3} {<nil>} 29423880765440m DecimalSI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-18 23:16:43 +0000 UTC,LastTransitionTime:2023-01-18 21:36:02 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-18 23:16:43 +0000 UTC,LastTransitionTime:2023-01-18 21:36:02 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-18 23:16:43 +0000 UTC,LastTransitionTime:2023-01-18 21:36:02 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-18 23:16:43 +0000 UTC,LastTransitionTime:2023-01-18 21:36:02 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.128.7,},NodeAddress{Type:Hostname,Address:ip-10-0-128-7.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-128-7.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec21c658414c8d2d06336e7d6a9b3742,SystemUUID:ec21c658-414c-8d2d-0633-6e7d6a9b3742,BootID:c7eefbc8-7dc5-4eb8-9fc3-8763853c85cf,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[image-registry.openshift-image-registry.svc:5000/openshift/cli@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45 image-registry.openshift-image-registry.svc:5000/openshift/cli:latest],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c7df53b796e81ba8301ba74d02317226329bd5752fd31c1b44d028e4832f21c3],SizeBytes:442026998,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f1134e2e28f44375c3bd9a6ee34d7f9972fdbd3305a3ee2b95e6ed3cede02140],SizeBytes:409661973,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:36fc214537c763b3a3f0a9dc7a1bd4378a80428c31b2629df8786a9b09155e6d],SizeBytes:398604719,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab0d2f0b6d01a4a3b9952710cc49e787ebb0e8649a288438774c1ffe0fc3fae0],SizeBytes:392510137,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5813d3728229c2a09a05bc454753e0128ac2bbd203e7000a4d954daab12fbb79],SizeBytes:377171632,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c956edcee9b9ba5462572b65b6a92983b20ace63dae50e3237bfdbd6d8c0b972],SizeBytes:375087838,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5e1c69d005727e3245604cfca7a63e4f9bc6e15128c7489e41d5e967305089e],SizeBytes:371248553,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:77c5db690d9438ac077736cad8f28c04de476c04c3a97f39910ed86b6c395b85],SizeBytes:368328246,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f3ebf99182733dff2666e6a5e7e217085e06029362036ad79c91278e69dcbf7],SizeBytes:366270626,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2269a2b10b3ba2a6783dde2a97451f57f5716a20ebef4a82bea20d25df8761fa],SizeBytes:365147114,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:32fa95a3d4ecfebe96152242926addf27789555b12413203927f255a712c8a0c],SizeBytes:358569445,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:91308d35c1e56463f55c1aaa519ff4de7335d43b254c21abdb845fc8c72821a1],SizeBytes:347460509,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:255a95e8b514dcdc4fa12436789115739355c13fae93eccbba660298746d9aa1],SizeBytes:346372629,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a4fdcff8b0038d858d46740c9622d5dc428074dcf7e662deacd9e6c0aff18286],SizeBytes:344400730,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:209e20410ec2d3d7a502f568d2b7fe1cd1beadcb36fff2d1e6f59d77be3200e3],SizeBytes:339844669,},ContainerImage{Names:[quay.io/openshift/origin-kube-rbac-proxy@sha256:baedb268ac66456018fb30af395bb3d69af5fff3252ff5d549f0231b1ebb6901 quay.io/openshift/origin-kube-rbac-proxy:4.10.0],SizeBytes:337627888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:140f8947593d92e1517e50a201e83bdef8eb965b552a21d3caf346a250d0cf6e],SizeBytes:335139477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01de581f7f0c7e43f809e9346ced7e60c0ecdd7c0078e97a8f64b24d4d3fa0b7],SizeBytes:332949901,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ed634acd9b29e16b578c74b38d5425156cd39bb72c0884a9564331f95f80911],SizeBytes:330558591,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs@sha256:83da6a30accf7d97453051973ba8c2ae1f78fa0472d2aeb0f8fcfdd8ba04e11a docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:324891489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:923999744ad1557510fc5c3a95cf978b27c453988489ada899c574ab1d71a218],SizeBytes:312722401,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-base@sha256:8da4f871cc18d3ac288a2fbb5c37ea27eabea4314dabca68f3f8469c7d7a4a21 quay.io/app-sre/managed-prometheus-exporter-base:latest],SizeBytes:303857745,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2550b2cbdf864515b1edacf43c25eb6b6f179713c1df34e51f6e9bba48d6430a],SizeBytes:303075904,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9260bbcc285a9fd4c1c7df865280261723eb7679050905da4acbfe728f96ee1b],SizeBytes:298386295,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder@sha256:afc413cd2504b586b6f28c16355db243c8bfdef536cc80e44f61e03c01e12235],SizeBytes:229564388,},ContainerImage{Names:[quay.io/app-sre/addon-operator-manager@sha256:3a5f9c6073f3b3542e53be155f1364bcba10233ed69011eaadbc73318776b775 quay.io/app-sre/addon-operator-manager:a45b2b8],SizeBytes:176376786,},ContainerImage{Names:[quay.io/app-sre/addon-operator-webhook@sha256:16a258633afc63916d84a447a6dd8a234e1803a4dfa22a15887ff1dc71781e75 quay.io/app-sre/addon-operator-webhook:a45b2b8],SizeBytes:173194705,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:20f25f275d46aa728f7615a1ccc19c78b2ed89435bf943a44b339f70f45508e6 k8s.gcr.io/e2e-test-images/httpd@sha256:5d28f127fae41261c56abccd96df481f8f4fba2a3305fece212e11aafe646943 k8s.gcr.io/e2e-test-images/httpd:2.4.39-2],SizeBytes:132295599,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 k8s.gcr.io/e2e-test-images/httpd@sha256:6589a0d3a4b40f996ab554f0d58e8c567c2850c41bc99d05498378a50b7e1cb4 k8s.gcr.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:128894651,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403 k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8 k8s.gcr.io/e2e-test-images/agnhost:2.36],SizeBytes:126252387,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:7f3ae784f32d750e63495dacd39c3bc57a9a0fc3e7ffc4b6f65e36be03cb368c docker.io/sonobuoy/sonobuoy@sha256:eaa42dd0660ece6c18d06199b78a41bd532a4851fc32a5a99acfafc03556852e docker.io/sonobuoy/sonobuoy:v0.56.14],SizeBytes:49637374,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nginx@sha256:13616070e3f29de4417eee434a8ef472221c9e51b3d037b5a6b46cef08eb7443 k8s.gcr.io/e2e-test-images/nginx@sha256:eee1822ee5bafc780db34f9ab68456c5fdcc0f994e1c1e01d5cde593cb3897a1 k8s.gcr.io/e2e-test-images/nginx:1.14-2],SizeBytes:17245363,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c k8s.gcr.io/pause@sha256:f81611a21cf91214c1ea751c5b525931a0e2ebabe62b3937b6158039ff6f922d k8s.gcr.io/pause:3.7],SizeBytes:717997,},},VolumesInUse:[kubernetes.io/csi/ebs.csi.aws.com^vol-03be0e2c9156fe2c9 kubernetes.io/csi/ebs.csi.aws.com^vol-0d9cb1f50abc74bfa],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/csi/ebs.csi.aws.com^vol-03be0e2c9156fe2c9,DevicePath:,},AttachedVolume{Name:kubernetes.io/csi/ebs.csi.aws.com^vol-0d9cb1f50abc74bfa,DevicePath:,},},Config:nil,},}
Jan 18 23:18:38.974: INFO: 
Logging kubelet events for node ip-10-0-128-7.ec2.internal
Jan 18 23:18:38.981: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-128-7.ec2.internal
Jan 18 23:18:39.024: INFO: splunkforwarder-ds-fbjlj started at 2023-01-18 21:27:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 23:18:39.024: INFO: node-exporter-spmbc started at 2023-01-18 21:27:18 +0000 UTC (1+2 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:18:39.024: INFO: tuned-l9d9s started at 2023-01-18 21:27:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:18:39.024: INFO: ingress-canary-qw8v9 started at 2023-01-18 21:28:03 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 23:18:39.024: INFO: addon-operator-manager-c8c4859bf-84wp8 started at 2023-01-18 21:36:25 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container manager ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container metrics-relay-server ready: true, restart count 0
Jan 18 23:18:39.024: INFO: network-check-target-xwp8p started at 2023-01-18 21:27:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:18:39.024: INFO: multus-znqj6 started at 2023-01-18 21:27:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:18:39.024: INFO: node-resolver-vhz2m started at 2023-01-18 21:27:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:18:39.024: INFO: telemeter-client-6bb748465-zttx9 started at 2023-01-18 21:36:25 +0000 UTC (0+3 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container reload ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container telemeter-client ready: true, restart count 0
Jan 18 23:18:39.024: INFO: network-metrics-daemon-ffdlk started at 2023-01-18 21:27:18 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:18:39.024: INFO: machine-config-daemon-htf69 started at 2023-01-18 21:27:18 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:18:39.024: INFO: kube-state-metrics-76877575d5-ptnh8 started at 2023-01-18 21:36:25 +0000 UTC (0+3 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 18 23:18:39.024: INFO: deployments-pruner-27901380-b6sl8 started at 2023-01-18 23:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 18 23:18:39.024: INFO: sre-dns-latency-exporter-8dptc started at 2023-01-18 21:27:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container main ready: true, restart count 1
Jan 18 23:18:39.024: INFO: aws-ebs-csi-driver-node-mq4qf started at 2023-01-18 21:27:18 +0000 UTC (0+3 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:18:39.024: INFO: prometheus-k8s-1 started at 2023-01-18 21:36:15 +0000 UTC (1+6 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Init container init-config-reloader ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container prometheus ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 18 23:18:39.024: INFO: addon-operator-webhooks-569bd4cfc5-dz6nk started at 2023-01-18 21:36:15 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container webhook ready: true, restart count 0
Jan 18 23:18:39.024: INFO: prometheus-operator-76957bb5bd-2pztb started at 2023-01-18 21:36:25 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 18 23:18:39.024: INFO: openshift-state-metrics-59fc669d8d-v25vz started at 2023-01-18 21:36:25 +0000 UTC (0+3 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jan 18 23:18:39.024: INFO: deployments-pruner-27901320-vt4j8 started at 2023-01-18 22:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 18 23:18:39.024: INFO: node-ca-6cvvj started at 2023-01-18 21:27:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:18:39.024: INFO: thanos-querier-57f44c5498-xjsz4 started at 2023-01-18 21:36:15 +0000 UTC (0+6 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container thanos-query ready: true, restart count 0
Jan 18 23:18:39.024: INFO: prometheus-operator-admission-webhook-577cc9c956-d6fkv started at 2023-01-18 21:36:15 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 23:18:39.024: INFO: alertmanager-main-1 started at 2023-01-18 21:36:15 +0000 UTC (0+6 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container alertmanager ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 23:18:39.024: INFO: osd-patch-subscription-source-27901320-gz2jm started at 2023-01-18 22:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 18 23:18:39.024: INFO: dns-default-btls8 started at 2023-01-18 22:06:24 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container dns ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.024: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-dwhpp started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:18:39.024: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:18:39.024: INFO: router-default-7cff97cd98-dqgwn started at 2023-01-18 21:36:15 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container router ready: true, restart count 0
Jan 18 23:18:39.024: INFO: prometheus-adapter-cf64f7f46-7bf79 started at 2023-01-18 21:36:15 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 18 23:18:39.024: INFO: multus-additional-cni-plugins-5lmsf started at 2023-01-18 21:27:18 +0000 UTC (6+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:18:39.024: INFO: ovnkube-node-mkzt4 started at 2023-01-18 21:27:18 +0000 UTC (0+5 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:18:39.024: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:18:39.024: INFO: builds-pruner-27901380-cpft8 started at 2023-01-18 23:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.024: INFO: 	Container builds-pruner ready: false, restart count 0
W0118 23:18:39.036906      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 18 23:18:39.070: INFO: 
Latency metrics for node ip-10-0-128-7.ec2.internal
Jan 18 23:18:39.070: INFO: 
Logging node info for node ip-10-0-164-47.ec2.internal
Jan 18 23:18:39.073: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-164-47.ec2.internal    e59b7a45-572f-4e61-9a5b-f3f783808f02 264275 0 2023-01-18 20:50:38 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.2xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-164-47.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io/master: node.kubernetes.io/instance-type:m5.2xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-0b3b3d8daf4bd7665","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-001d375a61a9acedb"} k8s.ovn.org/host-addresses:["10.0.164.47"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-164-47.ec2.internal","mac-address":"02:eb:d1:75:d1:d3","ip-addresses":["10.0.164.47/17"],"ip-address":"10.0.164.47/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:8ae32da4-7cb3-4d76-8e05-755f3c313edb k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.3/16"} k8s.ovn.org/node-mgmt-port-mac-address:22:25:b3:75:3a:91 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.164.47/17"} k8s.ovn.org/node-subnets:{"default":"10.128.2.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-master-1 machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/desiredConfig:rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubelet Update v1 2023-01-18 20:50:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/master":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {ip-10-0-164-47 Update v1 2023-01-18 20:56:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {cloud-network-config-controller Update v1 2023-01-18 20:58:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{}}}} } {nodelink-controller Update v1 2023-01-18 20:58:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}} } {machine-config-daemon Update v1 2023-01-18 20:59:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {machine-config-controller Update v1 2023-01-18 21:24:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {kube-controller-manager Update v1 2023-01-18 21:28:43 +0000 UTC FieldsV1 {"f:spec":{"f:taints":{}}} } {kubelet Update v1 2023-01-18 21:30:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:capacity":{"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-001d375a61a9acedb,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{8 0} {<nil>} 8 DecimalSI},ephemeral-storage: {{375261212672 0} {<nil>} 366466028Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{33221505024 0} {<nil>} 32442876Ki BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{7910 -3} {<nil>} 7910m DecimalSI},ephemeral-storage: {{337735090846 0} {<nil>} 337735090846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{29422975549440 -3} {<nil>} 29422975549440m DecimalSI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-18 23:18:37 +0000 UTC,LastTransitionTime:2023-01-18 21:30:27 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-18 23:18:37 +0000 UTC,LastTransitionTime:2023-01-18 21:30:27 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-18 23:18:37 +0000 UTC,LastTransitionTime:2023-01-18 21:30:27 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-18 23:18:37 +0000 UTC,LastTransitionTime:2023-01-18 21:30:27 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.164.47,},NodeAddress{Type:Hostname,Address:ip-10-0-164-47.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-164-47.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec28c4af2b916b7eacf9cea932ffe6d0,SystemUUID:ec28c4af-2b91-6b7e-acf9-cea932ffe6d0,BootID:5e0ae34f-f696-4bf5-9841-70e984334861,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[registry.redhat.io/redhat/redhat-operator-index@sha256:19e8a9a4d95111fd4db559838dd26ffa823b26dcac8004cd553a2ce39388b31f registry.redhat.io/redhat/redhat-operator-index@sha256:efdc74d42cfb372d6df37bb383af9585db327f2b47366818118addbe2e5825c1 registry.redhat.io/redhat/redhat-operator-index:v4.11],SizeBytes:1393216950,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[registry.redhat.io/redhat/community-operator-index@sha256:9d33175d7dcd7fd04bb53aea6e96e4bd902b866a09eef7fc7b5982c340929929 registry.redhat.io/redhat/community-operator-index@sha256:b36e3032da5f096768c703d5112a06325083783602ae8186b923def349a15578 registry.redhat.io/redhat/community-operator-index:v4.11],SizeBytes:960075608,},ContainerImage{Names:[registry.redhat.io/redhat/certified-operator-index@sha256:08d51898f4a8b59ec52f51c835f7f46871905001248d2f91a8c50c5c6ae04a43 registry.redhat.io/redhat/certified-operator-index@sha256:e3617454b09c589d685c05e3f16dcc9fb73c02688cf720599f3fe453f718da74 registry.redhat.io/redhat/certified-operator-index:v4.11],SizeBytes:932364392,},ContainerImage{Names:[registry.redhat.io/redhat/redhat-marketplace-index@sha256:8bba3a2e3edd687602c0702153f5a2c2f5055c4fb51093152fca24252fca87c4 registry.redhat.io/redhat/redhat-marketplace-index@sha256:ab8a83a2a96ad8efdd2b923b615867953cf746d779320facab9d533e0ea00c9a registry.redhat.io/redhat/redhat-marketplace-index:v4.11],SizeBytes:826959445,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c32772c6199a054c863126285a8b30cdadd6b1296cff1e4f22d6d3e3f9c84a7c],SizeBytes:823606942,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08ab62da9265ef67f4eb4b526b38346a3ae3d35e2d5bfbdd58a0858b76b21f77],SizeBytes:681959546,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:43612fe6e941c7292ed3ab0933e60703ce9115357a44aa20be25ed07b7cc7959],SizeBytes:612295182,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:515d4b9ff441191257d7b9ff99666737682d3b5e5a1879ba7abdd1e019f1dff2],SizeBytes:480023620,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c305cab192dd40b909dbb56981ad14c8460a6eac9bed9cbeebcd569a5a2eb4f0],SizeBytes:466400791,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:865e66be494fc077d12492a7dc204616fc8a13bf4c906f4081ab3555ce2cc970],SizeBytes:454669803,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72befefa8585db143140a682046ee43f4a62d6b0b5fc16a20b48186dc2b1dcb3],SizeBytes:450775130,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fead5da1351abee9e77f5c46247c7947d6f9cf76b5bf7a8b4905222fe2564665],SizeBytes:434647633,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cdf42132f29ddf587b7da228d83d184b152d81ed7cdff83e5b4ab20eb79d42b4],SizeBytes:434254012,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:26686a92917f2c24599df67391f6774cbf6637ccff23aa47a9ed0761d951291e],SizeBytes:427979869,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:856a2097e01dbaad614b0f57570a5616d99e5348ea6f35ad4cea2fdd6d18126b],SizeBytes:415053325,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:252813800adf8140c408e6aa0956ccac8f9a3ddd4cf743f140fcaef61b23762f],SizeBytes:414242392,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e5a9884f8bb84ee5256d0e0701c1f45d2a4e39b3a12d5befbbf2c83daf4d174],SizeBytes:412745337,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6da5f65dc70f3308d7dc9d3f64e7462458e2778268f291c3b55601b5f6f13e9d],SizeBytes:412482144,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:09d0eae43aa689add7aee2704b547a10c1423f4d065b777ac40260578ebdfcdd],SizeBytes:412124822,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f3ab7facf3d8b744025ac716a731ca8ec3c3fb6aaf7f4030fffe4b27fb9b3167],SizeBytes:411670500,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:06c9b59044c11816be5e21b6a98b941ff5b4dbac525b552f260eaf29b58d506c],SizeBytes:411066934,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1e17af5c01fa12b7703ff1bf4f568c3c9871a595d93712cb0bc0644de261c8b2],SizeBytes:410568245,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8307057acd3af8dadf87323bfe9e2d4d9e74a5910cfe17373f9b30a96c7a796f],SizeBytes:409308238,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e591be349fc890140870a9203b7ad17732ba3793986184696dd6573c02f7512a],SizeBytes:408835766,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d35bbe6e090f32f26efabbe515cf115709859021559568593e0cf683c448f9a2],SizeBytes:408565287,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fafa9771e41b970fb0ebf8cdbfe159b8dd8b48bfbc1acd225bb3d72838be3d07],SizeBytes:408260136,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:676243be57bc8a7bb88ced454753032c2c0709a3d04c4378c625576e2956c274],SizeBytes:407648939,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:064c7c0257d1de0cad1d8e883b505853bed53fd835686d4105eca3c4b734e0b5],SizeBytes:407165482,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:506acacd57b6cf51722a41278c0278a28df68ac86d131c59ff8b687d6254aca3],SizeBytes:404880817,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2912cae3175adb7b70e41632ac83ede3d4af241a937d84bfce4cd446697616c1],SizeBytes:403284531,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9abc51cf4faf2d8f2a85c70ca6e48e28ff04d94714dab5f332a51284dd18868b],SizeBytes:401406468,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-release@sha256:300bce8246cf880e792e106607925de0a404484637627edf5f517375517d54a4],SizeBytes:395682633,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c2a9eff33dd53c4e0dcf9fdd5ef471263f621424652368b819c4e4180b4bc197],SizeBytes:394107296,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:77a20c01792b26383b3365b8cf3cbe4d62bb4044be8372247d82a33bb8db680e],SizeBytes:389399190,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1cb2a173c991cb3521bbed849cc2d72d94f57700f68726516d10b674eeb89fef],SizeBytes:387320849,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:56d1f8f14d2c423be54b468489b901f0986334daa52d933f6d8c393f0f3294c1],SizeBytes:385729777,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e538dd11b15508ec363b8855e5496b51f65365ecfa62419899d3cdb21252f44],SizeBytes:385715621,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jan 18 23:18:39.078: INFO: 
Logging kubelet events for node ip-10-0-164-47.ec2.internal
Jan 18 23:18:39.081: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-164-47.ec2.internal
Jan 18 23:18:39.125: INFO: authentication-operator-7449fd4467-x9fzx started at 2023-01-18 21:30:57 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.125: INFO: 	Container authentication-operator ready: true, restart count 0
Jan 18 23:18:39.125: INFO: cloud-network-config-controller-c6495b94b-49k5d started at 2023-01-18 21:30:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.125: INFO: 	Container controller ready: true, restart count 0
Jan 18 23:18:39.125: INFO: cloud-credential-operator-5ff5b4df7c-67ctf started at 2023-01-18 21:30:59 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.125: INFO: 	Container cloud-credential-operator ready: true, restart count 0
Jan 18 23:18:39.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.125: INFO: olm-operator-84c588b5d4-d2bgw started at 2023-01-18 21:31:06 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.125: INFO: 	Container olm-operator ready: true, restart count 0
Jan 18 23:18:39.125: INFO: dns-default-dqbtt started at 2023-01-18 20:58:10 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.125: INFO: 	Container dns ready: true, restart count 1
Jan 18 23:18:39.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.125: INFO: splunkforwarder-ds-c6bl8 started at 2023-01-18 21:24:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.125: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 23:18:39.125: INFO: openshift-config-operator-6c4d478fcc-v4lzl started at 2023-01-18 21:30:57 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container openshift-config-operator ready: true, restart count 0
Jan 18 23:18:39.126: INFO: cluster-image-registry-operator-8654985b9-tlbrt started at 2023-01-18 21:31:02 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jan 18 23:18:39.126: INFO: controller-manager-wsvkm started at 2023-01-18 21:26:39 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container controller-manager ready: true, restart count 1
Jan 18 23:18:39.126: INFO: revision-pruner-9-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:26:56 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container pruner ready: false, restart count 0
Jan 18 23:18:39.126: INFO: openshift-apiserver-operator-749c855c4b-p78ll started at 2023-01-18 21:30:57 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container openshift-apiserver-operator ready: true, restart count 0
Jan 18 23:18:39.126: INFO: csi-snapshot-webhook-f7487cb9f-26nwq started at 2023-01-18 21:30:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container webhook ready: true, restart count 0
Jan 18 23:18:39.126: INFO: ingress-operator-8646cf484d-wfqwx started at 2023-01-18 21:31:02 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container ingress-operator ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.126: INFO: kube-controller-manager-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:01:39 +0000 UTC (0+4 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container cluster-policy-controller ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container kube-controller-manager-cert-syncer ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container kube-controller-manager-recovery-controller ready: true, restart count 0
Jan 18 23:18:39.126: INFO: network-metrics-daemon-vcckr started at 2023-01-18 20:55:15 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:18:39.126: INFO: aws-ebs-csi-driver-node-zfr78 started at 2023-01-18 20:57:36 +0000 UTC (0+3 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:18:39.126: INFO: tuned-hp4hs started at 2023-01-18 20:58:22 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:18:39.126: INFO: installer-6-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:30:28 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container installer ready: false, restart count 0
Jan 18 23:18:39.126: INFO: kube-apiserver-guard-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:30:50 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container guard ready: true, restart count 0
Jan 18 23:18:39.126: INFO: console-67d8cf9f45-dxkwr started at 2023-01-18 21:30:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container console ready: true, restart count 0
Jan 18 23:18:39.126: INFO: revision-pruner-7-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:40:22 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container pruner ready: false, restart count 0
Jan 18 23:18:39.126: INFO: ovnkube-node-9z5m7 started at 2023-01-18 20:55:24 +0000 UTC (0+5 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:18:39.126: INFO: machine-config-server-52vnt started at 2023-01-18 20:57:49 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container machine-config-server ready: true, restart count 1
Jan 18 23:18:39.126: INFO: etcd-operator-5d7bb49b6f-mr69f started at 2023-01-18 21:30:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container etcd-operator ready: true, restart count 0
Jan 18 23:18:39.126: INFO: machine-config-daemon-9c9lc started at 2023-01-18 20:57:06 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:18:39.126: INFO: node-ca-jb8rs started at 2023-01-18 21:02:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:18:39.126: INFO: apiserver-58dcc4c5df-xjj54 started at 2023-01-18 21:30:42 +0000 UTC (1+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container oauth-apiserver ready: true, restart count 0
Jan 18 23:18:39.126: INFO: kube-controller-manager-guard-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:30:49 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container guard ready: true, restart count 0
Jan 18 23:18:39.126: INFO: machine-approver-5c7fdbf98f-p8nf9 started at 2023-01-18 21:30:58 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container machine-approver-controller ready: true, restart count 0
Jan 18 23:18:39.126: INFO: packageserver-749dbf8d56-f68km started at 2023-01-18 21:31:06 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container packageserver ready: true, restart count 0
Jan 18 23:18:39.126: INFO: etcd-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:00:48 +0000 UTC (3+5 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Init container setup ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Init container etcd-ensure-env-vars ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Init container etcd-resources-copy ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container etcd ready: true, restart count 3
Jan 18 23:18:39.126: INFO: 	Container etcd-health-monitor ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container etcd-metrics ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container etcd-readyz ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container etcdctl ready: true, restart count 1
Jan 18 23:18:39.126: INFO: installer-11-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:34:21 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container installer ready: false, restart count 0
Jan 18 23:18:39.126: INFO: revision-pruner-10-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:30:28 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container pruner ready: false, restart count 0
Jan 18 23:18:39.126: INFO: aws-ebs-csi-driver-controller-6f56cff656-hvllp started at 2023-01-18 21:30:58 +0000 UTC (0+11 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container attacher-kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container csi-driver ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container driver-kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container provisioner-kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container resizer-kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container snapshotter-kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.126: INFO: kube-controller-manager-operator-7dddcdbdcf-z7kth started at 2023-01-18 21:31:07 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container kube-controller-manager-operator ready: true, restart count 0
Jan 18 23:18:39.126: INFO: installer-7-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:39:16 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container installer ready: false, restart count 0
Jan 18 23:18:39.126: INFO: openshift-kube-scheduler-ip-10-0-164-47.ec2.internal started at 2023-01-18 20:59:04 +0000 UTC (1+3 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Init container wait-for-host-port ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container kube-scheduler-cert-syncer ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container kube-scheduler-recovery-controller ready: true, restart count 0
Jan 18 23:18:39.126: INFO: revision-pruner-9-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:26:24 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container pruner ready: false, restart count 0
Jan 18 23:18:39.126: INFO: network-check-target-7sxbt started at 2023-01-18 20:55:26 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:18:39.126: INFO: oauth-openshift-7cd8db5c84-tnrnn started at 2023-01-18 21:30:42 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container oauth-openshift ready: true, restart count 0
Jan 18 23:18:39.126: INFO: openshift-kube-scheduler-guard-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:30:44 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container guard ready: true, restart count 0
Jan 18 23:18:39.126: INFO: cluster-baremetal-operator-5444b4778c-h7zf6 started at 2023-01-18 21:31:04 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container baremetal-kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container cluster-baremetal-operator ready: true, restart count 0
Jan 18 23:18:39.126: INFO: revision-pruner-11-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:34:12 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container pruner ready: false, restart count 0
Jan 18 23:18:39.126: INFO: multus-additional-cni-plugins-mdbtj started at 2023-01-18 20:55:14 +0000 UTC (6+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:18:39.126: INFO: node-resolver-wmqhb started at 2023-01-18 20:58:11 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:18:39.126: INFO: cluster-node-tuning-operator-759d66c58c-84fll started at 2023-01-18 21:30:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jan 18 23:18:39.126: INFO: marketplace-operator-587887dd98-cbnd7 started at 2023-01-18 21:31:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container marketplace-operator ready: true, restart count 0
Jan 18 23:18:39.126: INFO: kube-storage-version-migrator-operator-6868b77b99-jdp6z started at 2023-01-18 21:31:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 0
Jan 18 23:18:39.126: INFO: openshift-kube-scheduler-operator-f5fdbcc7f-mh7l5 started at 2023-01-18 21:31:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container kube-scheduler-operator-container ready: true, restart count 0
Jan 18 23:18:39.126: INFO: node-exporter-jpjhd started at 2023-01-18 21:08:32 +0000 UTC (1+2 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:18:39.126: INFO: cluster-monitoring-operator-54dd78cc74-5kf77 started at 2023-01-18 21:31:01 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.126: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-hgd9k started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:18:39.126: INFO: multus-qhptl started at 2023-01-18 20:55:14 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:18:39.126: INFO: multus-admission-controller-jdrvf started at 2023-01-18 20:57:06 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container multus-admission-controller ready: true, restart count 1
Jan 18 23:18:39.126: INFO: validation-webhook-ptpcg started at 2023-01-18 21:18:05 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container webhooks ready: true, restart count 1
Jan 18 23:18:39.126: INFO: installer-6-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:26:42 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container installer ready: false, restart count 0
Jan 18 23:18:39.126: INFO: apiserver-66fc6ccdbf-mdrrn started at 2023-01-18 21:30:42 +0000 UTC (1+2 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container openshift-apiserver ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container openshift-apiserver-check-endpoints ready: true, restart count 0
Jan 18 23:18:39.126: INFO: installer-10-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:31:55 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container installer ready: false, restart count 0
Jan 18 23:18:39.126: INFO: kube-apiserver-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:11:38 +0000 UTC (1+5 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Init container setup ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container kube-apiserver-cert-regeneration-controller ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container kube-apiserver-cert-syncer ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container kube-apiserver-check-endpoints ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container kube-apiserver-insecure-readyz ready: true, restart count 0
Jan 18 23:18:39.126: INFO: ovnkube-master-87l2b started at 2023-01-18 20:55:24 +0000 UTC (0+6 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container nbdb ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container northd ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container ovn-dbchecker ready: true, restart count 1
Jan 18 23:18:39.126: INFO: 	Container ovnkube-master ready: true, restart count 2
Jan 18 23:18:39.126: INFO: 	Container sbdb ready: true, restart count 1
Jan 18 23:18:39.126: INFO: sre-dns-latency-exporter-skzfn started at 2023-01-18 21:18:07 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container main ready: true, restart count 1
Jan 18 23:18:39.126: INFO: audit-exporter-swhwq started at 2023-01-18 21:19:38 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container audit-exporter ready: true, restart count 1
Jan 18 23:18:39.126: INFO: dns-operator-69db46cc47-prtl4 started at 2023-01-18 21:30:58 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container dns-operator ready: true, restart count 0
Jan 18 23:18:39.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.126: INFO: openshift-controller-manager-operator-598f9758bb-vjtl2 started at 2023-01-18 21:30:59 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container openshift-controller-manager-operator ready: true, restart count 0
Jan 18 23:18:39.126: INFO: catalog-operator-d8bc95486-bckt7 started at 2023-01-18 21:31:05 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container catalog-operator ready: true, restart count 1
Jan 18 23:18:39.126: INFO: insights-operator-ddf4f58bc-6tvtt started at 2023-01-18 21:31:07 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container insights-operator ready: true, restart count 0
Jan 18 23:18:39.126: INFO: etcd-guard-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:30:45 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container guard ready: true, restart count 0
Jan 18 23:18:39.126: INFO: cluster-storage-operator-58f544dddc-8w72f started at 2023-01-18 21:30:57 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Jan 18 23:18:39.126: INFO: installer-7-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:40:31 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container installer ready: false, restart count 0
Jan 18 23:18:39.126: INFO: csi-snapshot-controller-64fc8cb68f-5xx4c started at 2023-01-18 21:30:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 18 23:18:39.126: INFO: kube-apiserver-operator-87c785995-rdmfm started at 2023-01-18 21:31:03 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.126: INFO: 	Container kube-apiserver-operator ready: true, restart count 0
W0118 23:18:39.128277      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 18 23:18:39.155: INFO: 
Latency metrics for node ip-10-0-164-47.ec2.internal
Jan 18 23:18:39.155: INFO: 
Logging node info for node ip-10-0-176-161.ec2.internal
Jan 18 23:18:39.159: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-176-161.ec2.internal    b5315323-0061-47b2-b15b-c33b1d1b7b4d 260584 0 2023-01-18 20:55:08 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.2xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-176-161.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io/master: node.kubernetes.io/instance-type:m5.2xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-014476fb4887fbb3c","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0fa2697d329ce1dd7"} k8s.ovn.org/host-addresses:["10.0.176.161"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-176-161.ec2.internal","mac-address":"02:4b:a7:84:a8:1b","ip-addresses":["10.0.176.161/17"],"ip-address":"10.0.176.161/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:da8863b7-8575-4776-881c-d426297d05fd k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.4/16"} k8s.ovn.org/node-mgmt-port-mac-address:82:f3:04:f6:5f:98 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.176.161/17"} k8s.ovn.org/node-subnets:{"default":"10.128.4.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-master-0 machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/desiredConfig:rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubelet Update v1 2023-01-18 20:55:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/master":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {ip-10-0-164-47 Update v1 2023-01-18 20:56:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {ip-10-0-176-161 Update v1 2023-01-18 20:56:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {machine-config-daemon Update v1 2023-01-18 20:58:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {cloud-network-config-controller Update v1 2023-01-18 20:58:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{}}}} } {nodelink-controller Update v1 2023-01-18 20:58:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}} } {kube-controller-manager Update v1 2023-01-18 21:30:55 +0000 UTC FieldsV1 {"f:spec":{"f:taints":{}}} } {machine-config-controller Update v1 2023-01-18 21:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {kubelet Update v1 2023-01-18 21:36:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-0fa2697d329ce1dd7,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{8 0} {<nil>} 8 DecimalSI},ephemeral-storage: {{375261212672 0} {<nil>} 366466028Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{33221505024 0} {<nil>} 32442876Ki BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{7910 -3} {<nil>} 7910m DecimalSI},ephemeral-storage: {{337735090846 0} {<nil>} 337735090846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{29422975549440 -3} {<nil>} 29422975549440m DecimalSI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-18 23:13:47 +0000 UTC,LastTransitionTime:2023-01-18 21:36:48 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-18 23:13:47 +0000 UTC,LastTransitionTime:2023-01-18 21:36:48 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-18 23:13:47 +0000 UTC,LastTransitionTime:2023-01-18 21:36:48 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-18 23:13:47 +0000 UTC,LastTransitionTime:2023-01-18 21:36:48 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.176.161,},NodeAddress{Type:Hostname,Address:ip-10-0-176-161.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-176-161.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec28a0ad2d0b01eb828efbed3def1995,SystemUUID:ec28a0ad-2d0b-01eb-828e-fbed3def1995,BootID:c3c8cea8-86ac-4752-8b78-3b121525af7d,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c32772c6199a054c863126285a8b30cdadd6b1296cff1e4f22d6d3e3f9c84a7c],SizeBytes:823606942,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08ab62da9265ef67f4eb4b526b38346a3ae3d35e2d5bfbdd58a0858b76b21f77],SizeBytes:681959546,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:43612fe6e941c7292ed3ab0933e60703ce9115357a44aa20be25ed07b7cc7959],SizeBytes:612295182,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:515d4b9ff441191257d7b9ff99666737682d3b5e5a1879ba7abdd1e019f1dff2],SizeBytes:480023620,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c305cab192dd40b909dbb56981ad14c8460a6eac9bed9cbeebcd569a5a2eb4f0],SizeBytes:466400791,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:865e66be494fc077d12492a7dc204616fc8a13bf4c906f4081ab3555ce2cc970],SizeBytes:454669803,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fead5da1351abee9e77f5c46247c7947d6f9cf76b5bf7a8b4905222fe2564665],SizeBytes:434647633,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:26686a92917f2c24599df67391f6774cbf6637ccff23aa47a9ed0761d951291e],SizeBytes:427979869,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:856a2097e01dbaad614b0f57570a5616d99e5348ea6f35ad4cea2fdd6d18126b],SizeBytes:415053325,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:252813800adf8140c408e6aa0956ccac8f9a3ddd4cf743f140fcaef61b23762f],SizeBytes:414242392,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e5a9884f8bb84ee5256d0e0701c1f45d2a4e39b3a12d5befbbf2c83daf4d174],SizeBytes:412745337,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6da5f65dc70f3308d7dc9d3f64e7462458e2778268f291c3b55601b5f6f13e9d],SizeBytes:412482144,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:09d0eae43aa689add7aee2704b547a10c1423f4d065b777ac40260578ebdfcdd],SizeBytes:412124822,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f3ab7facf3d8b744025ac716a731ca8ec3c3fb6aaf7f4030fffe4b27fb9b3167],SizeBytes:411670500,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1e17af5c01fa12b7703ff1bf4f568c3c9871a595d93712cb0bc0644de261c8b2],SizeBytes:410568245,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8307057acd3af8dadf87323bfe9e2d4d9e74a5910cfe17373f9b30a96c7a796f],SizeBytes:409308238,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e591be349fc890140870a9203b7ad17732ba3793986184696dd6573c02f7512a],SizeBytes:408835766,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3634a2d516f4455d78a54952446bcf7cdebb913c5fcf736de59e48d067479b1e],SizeBytes:408726214,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e9013f41f06bd4c609f94b4ff8d4087961e0af67e4317f2cb03f45fec08892f6],SizeBytes:408639000,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d35bbe6e090f32f26efabbe515cf115709859021559568593e0cf683c448f9a2],SizeBytes:408565287,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:676243be57bc8a7bb88ced454753032c2c0709a3d04c4378c625576e2956c274],SizeBytes:407648939,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:064c7c0257d1de0cad1d8e883b505853bed53fd835686d4105eca3c4b734e0b5],SizeBytes:407165482,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:506acacd57b6cf51722a41278c0278a28df68ac86d131c59ff8b687d6254aca3],SizeBytes:404880817,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2912cae3175adb7b70e41632ac83ede3d4af241a937d84bfce4cd446697616c1],SizeBytes:403284531,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9abc51cf4faf2d8f2a85c70ca6e48e28ff04d94714dab5f332a51284dd18868b],SizeBytes:401406468,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c2a9eff33dd53c4e0dcf9fdd5ef471263f621424652368b819c4e4180b4bc197],SizeBytes:394107296,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:77a20c01792b26383b3365b8cf3cbe4d62bb4044be8372247d82a33bb8db680e],SizeBytes:389399190,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1cb2a173c991cb3521bbed849cc2d72d94f57700f68726516d10b674eeb89fef],SizeBytes:387320849,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:56d1f8f14d2c423be54b468489b901f0986334daa52d933f6d8c393f0f3294c1],SizeBytes:385729777,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e538dd11b15508ec363b8855e5496b51f65365ecfa62419899d3cdb21252f44],SizeBytes:385715621,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e463f54f6a78c0ff0f995dc9b6557fd3af350cd20f7116f56160cdfa8ddc6dc],SizeBytes:382829546,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:90616b372816c6b9e06c670b613dd6292b0b078d78c835f1c5da8374f908f39a],SizeBytes:382625292,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8aae812f23337d5143f2be66cc79c512d422e5e3b3b11307a81db299da31764c],SizeBytes:382218627,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cca1847c9a763bfa7e79fcbcc4d7e82918461e7ade26ec61b681937e9c92ff23],SizeBytes:380734570,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0cc5bd656b0355619fdac6c8920a2388edabbe14cdc7ab93069bacdc1e2db1d1],SizeBytes:380104717,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f7e1af0b22f2e5943636d18fba82b080cf70828a5e62866e4891a97f8f7113b5],SizeBytes:377151511,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b5be929f4ccae9b9f3b1c5f19281c9a24f1adb54bf9c358691af988556dce27],SizeBytes:376385161,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jan 18 23:18:39.159: INFO: 
Logging kubelet events for node ip-10-0-176-161.ec2.internal
Jan 18 23:18:39.162: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-176-161.ec2.internal
Jan 18 23:18:39.191: INFO: machine-config-daemon-kjrpw started at 2023-01-18 20:57:06 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.191: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:18:39.191: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:18:39.191: INFO: revision-pruner-9-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:36:48 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.191: INFO: 	Container pruner ready: false, restart count 0
Jan 18 23:18:39.191: INFO: openshift-kube-scheduler-guard-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:37:11 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.191: INFO: 	Container guard ready: true, restart count 0
Jan 18 23:18:39.191: INFO: kube-controller-manager-guard-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:37:11 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.191: INFO: 	Container guard ready: true, restart count 0
Jan 18 23:18:39.191: INFO: etcd-guard-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:37:11 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.191: INFO: 	Container guard ready: true, restart count 0
Jan 18 23:18:39.191: INFO: validation-webhook-fnxnh started at 2023-01-18 21:18:05 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.191: INFO: 	Container webhooks ready: true, restart count 1
Jan 18 23:18:39.192: INFO: apiserver-58dcc4c5df-jftb5 started at 2023-01-18 21:37:10 +0000 UTC (1+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Jan 18 23:18:39.192: INFO: 	Container oauth-apiserver ready: true, restart count 0
Jan 18 23:18:39.192: INFO: installer-11-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:38:15 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container installer ready: false, restart count 0
Jan 18 23:18:39.192: INFO: multus-ljzjp started at 2023-01-18 20:55:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:18:39.192: INFO: ovnkube-node-6f4f4 started at 2023-01-18 20:55:24 +0000 UTC (0+5 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:18:39.192: INFO: sre-dns-latency-exporter-pqxpd started at 2023-01-18 21:18:07 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container main ready: true, restart count 1
Jan 18 23:18:39.192: INFO: controller-manager-6chhz started at 2023-01-18 21:26:39 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container controller-manager ready: true, restart count 1
Jan 18 23:18:39.192: INFO: kube-apiserver-guard-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:37:13 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container guard ready: true, restart count 0
Jan 18 23:18:39.192: INFO: etcd-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:02:31 +0000 UTC (3+5 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Init container setup ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Init container etcd-ensure-env-vars ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Init container etcd-resources-copy ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container etcd ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container etcd-health-monitor ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container etcd-metrics ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container etcd-readyz ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container etcdctl ready: true, restart count 1
Jan 18 23:18:39.192: INFO: installer-7-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:36:48 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container installer ready: false, restart count 0
Jan 18 23:18:39.192: INFO: oauth-openshift-7cd8db5c84-n8xww started at 2023-01-18 21:37:10 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container oauth-openshift ready: true, restart count 0
Jan 18 23:18:39.192: INFO: network-check-target-6lqkp started at 2023-01-18 20:55:26 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:18:39.192: INFO: multus-admission-controller-bcql5 started at 2023-01-18 20:56:59 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container multus-admission-controller ready: true, restart count 1
Jan 18 23:18:39.192: INFO: dns-default-kmk24 started at 2023-01-18 20:58:10 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container dns ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.192: INFO: splunkforwarder-ds-rb44w started at 2023-01-18 21:24:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 23:18:39.192: INFO: multus-additional-cni-plugins-qrxcd started at 2023-01-18 20:55:18 +0000 UTC (6+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:18:39.192: INFO: machine-config-server-x47zb started at 2023-01-18 20:57:49 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container machine-config-server ready: true, restart count 1
Jan 18 23:18:39.192: INFO: node-resolver-2hwdq started at 2023-01-18 20:58:11 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:18:39.192: INFO: node-ca-w59x4 started at 2023-01-18 21:02:57 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:18:39.192: INFO: openshift-kube-scheduler-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:00:43 +0000 UTC (1+3 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Init container wait-for-host-port ready: true, restart count 0
Jan 18 23:18:39.192: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan 18 23:18:39.192: INFO: 	Container kube-scheduler-cert-syncer ready: true, restart count 0
Jan 18 23:18:39.192: INFO: 	Container kube-scheduler-recovery-controller ready: true, restart count 0
Jan 18 23:18:39.192: INFO: revision-pruner-7-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:40:25 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container pruner ready: false, restart count 0
Jan 18 23:18:39.192: INFO: installer-7-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:36:48 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container installer ready: false, restart count 0
Jan 18 23:18:39.192: INFO: kube-controller-manager-ip-10-0-176-161.ec2.internal started at 2023-01-18 20:59:27 +0000 UTC (0+4 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container cluster-policy-controller ready: true, restart count 0
Jan 18 23:18:39.192: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan 18 23:18:39.192: INFO: 	Container kube-controller-manager-cert-syncer ready: true, restart count 0
Jan 18 23:18:39.192: INFO: 	Container kube-controller-manager-recovery-controller ready: true, restart count 0
Jan 18 23:18:39.192: INFO: network-metrics-daemon-75l4s started at 2023-01-18 20:55:18 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:18:39.192: INFO: ovnkube-master-lcjzg started at 2023-01-18 20:55:24 +0000 UTC (0+6 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container nbdb ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container northd ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container ovn-dbchecker ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container ovnkube-master ready: true, restart count 4
Jan 18 23:18:39.192: INFO: 	Container sbdb ready: true, restart count 1
Jan 18 23:18:39.192: INFO: aws-ebs-csi-driver-node-8n9wm started at 2023-01-18 20:57:42 +0000 UTC (0+3 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:18:39.192: INFO: tuned-r9x6w started at 2023-01-18 20:58:22 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:18:39.192: INFO: node-exporter-xxq2s started at 2023-01-18 21:08:32 +0000 UTC (1+2 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.192: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:18:39.192: INFO: audit-exporter-cn4nn started at 2023-01-18 21:19:38 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container audit-exporter ready: true, restart count 1
Jan 18 23:18:39.192: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-6psbg started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:18:39.192: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:18:39.192: INFO: revision-pruner-11-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:36:48 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Container pruner ready: false, restart count 0
Jan 18 23:18:39.192: INFO: apiserver-66fc6ccdbf-n4lfr started at 2023-01-18 21:37:10 +0000 UTC (1+2 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Jan 18 23:18:39.192: INFO: 	Container openshift-apiserver ready: true, restart count 0
Jan 18 23:18:39.192: INFO: 	Container openshift-apiserver-check-endpoints ready: true, restart count 0
Jan 18 23:18:39.192: INFO: kube-apiserver-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:13:11 +0000 UTC (1+5 container statuses recorded)
Jan 18 23:18:39.192: INFO: 	Init container setup ready: true, restart count 0
Jan 18 23:18:39.192: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 18 23:18:39.192: INFO: 	Container kube-apiserver-cert-regeneration-controller ready: true, restart count 0
Jan 18 23:18:39.192: INFO: 	Container kube-apiserver-cert-syncer ready: true, restart count 0
Jan 18 23:18:39.192: INFO: 	Container kube-apiserver-check-endpoints ready: true, restart count 0
Jan 18 23:18:39.192: INFO: 	Container kube-apiserver-insecure-readyz ready: true, restart count 0
W0118 23:18:39.194627      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 18 23:18:39.227: INFO: 
Latency metrics for node ip-10-0-176-161.ec2.internal
Jan 18 23:18:39.227: INFO: 
Logging node info for node ip-10-0-176-170.ec2.internal
Jan 18 23:18:39.229: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-176-170.ec2.internal    a7fcbe04-baee-4285-bb14-33b17935a43d 260523 0 2023-01-18 20:50:23 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.2xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-176-170.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io/master: node.kubernetes.io/instance-type:m5.2xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-01989b7189c1e0adb","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0020e87e2124183e6"} k8s.ovn.org/host-addresses:["10.0.176.170"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-176-170.ec2.internal","mac-address":"02:cb:07:d5:72:d5","ip-addresses":["10.0.176.170/17"],"ip-address":"10.0.176.170/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:0ce8e931-99cf-445b-b666-7a3e580e351d k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.2/16"} k8s.ovn.org/node-mgmt-port-mac-address:a6:4c:04:da:60:d8 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.176.170/17"} k8s.ovn.org/node-subnets:{"default":"10.128.0.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-master-2 machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/desiredConfig:rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubelet Update v1 2023-01-18 20:50:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/master":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {ip-10-0-164-47 Update v1 2023-01-18 20:56:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {ip-10-0-176-170 Update v1 2023-01-18 20:56:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {cloud-network-config-controller Update v1 2023-01-18 20:58:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{}}}} } {machine-config-daemon Update v1 2023-01-18 20:58:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {nodelink-controller Update v1 2023-01-18 20:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}} } {kube-controller-manager Update v1 2023-01-18 21:18:19 +0000 UTC FieldsV1 {"f:spec":{"f:taints":{}}} } {machine-config-controller Update v1 2023-01-18 21:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {kubelet Update v1 2023-01-18 21:23:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-0020e87e2124183e6,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{8 0} {<nil>} 8 DecimalSI},ephemeral-storage: {{375261212672 0} {<nil>} 366466028Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{33221505024 0} {<nil>} 32442876Ki BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{7910 -3} {<nil>} 7910m DecimalSI},ephemeral-storage: {{337735090846 0} {<nil>} 337735090846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{29422975549440 -3} {<nil>} 29422975549440m DecimalSI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-18 23:13:44 +0000 UTC,LastTransitionTime:2023-01-18 20:50:23 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-18 23:13:44 +0000 UTC,LastTransitionTime:2023-01-18 20:50:23 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-18 23:13:44 +0000 UTC,LastTransitionTime:2023-01-18 20:50:23 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-18 23:13:44 +0000 UTC,LastTransitionTime:2023-01-18 20:57:01 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.176.170,},NodeAddress{Type:Hostname,Address:ip-10-0-176-170.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-176-170.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec28eb65055ec31071286c18c22a5e4c,SystemUUID:ec28eb65-055e-c310-7128-6c18c22a5e4c,BootID:46a64a14-b2eb-454a-8d74-74eda176c814,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[registry.redhat.io/redhat/redhat-operator-index@sha256:19e8a9a4d95111fd4db559838dd26ffa823b26dcac8004cd553a2ce39388b31f registry.redhat.io/redhat/redhat-operator-index@sha256:efdc74d42cfb372d6df37bb383af9585db327f2b47366818118addbe2e5825c1 registry.redhat.io/redhat/redhat-operator-index:v4.11],SizeBytes:1393216950,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[registry.redhat.io/redhat/community-operator-index@sha256:9d33175d7dcd7fd04bb53aea6e96e4bd902b866a09eef7fc7b5982c340929929 registry.redhat.io/redhat/community-operator-index@sha256:b36e3032da5f096768c703d5112a06325083783602ae8186b923def349a15578 registry.redhat.io/redhat/community-operator-index:v4.11],SizeBytes:960075608,},ContainerImage{Names:[registry.redhat.io/redhat/certified-operator-index@sha256:08d51898f4a8b59ec52f51c835f7f46871905001248d2f91a8c50c5c6ae04a43 registry.redhat.io/redhat/certified-operator-index@sha256:e3617454b09c589d685c05e3f16dcc9fb73c02688cf720599f3fe453f718da74 registry.redhat.io/redhat/certified-operator-index:v4.11],SizeBytes:932364392,},ContainerImage{Names:[registry.redhat.io/redhat/redhat-marketplace-index@sha256:8bba3a2e3edd687602c0702153f5a2c2f5055c4fb51093152fca24252fca87c4 registry.redhat.io/redhat/redhat-marketplace-index@sha256:ab8a83a2a96ad8efdd2b923b615867953cf746d779320facab9d533e0ea00c9a registry.redhat.io/redhat/redhat-marketplace-index:v4.11],SizeBytes:826959445,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c32772c6199a054c863126285a8b30cdadd6b1296cff1e4f22d6d3e3f9c84a7c],SizeBytes:823606942,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08ab62da9265ef67f4eb4b526b38346a3ae3d35e2d5bfbdd58a0858b76b21f77],SizeBytes:681959546,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:43612fe6e941c7292ed3ab0933e60703ce9115357a44aa20be25ed07b7cc7959],SizeBytes:612295182,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c305cab192dd40b909dbb56981ad14c8460a6eac9bed9cbeebcd569a5a2eb4f0],SizeBytes:466400791,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:865e66be494fc077d12492a7dc204616fc8a13bf4c906f4081ab3555ce2cc970],SizeBytes:454669803,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72befefa8585db143140a682046ee43f4a62d6b0b5fc16a20b48186dc2b1dcb3],SizeBytes:450775130,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fead5da1351abee9e77f5c46247c7947d6f9cf76b5bf7a8b4905222fe2564665],SizeBytes:434647633,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cdf42132f29ddf587b7da228d83d184b152d81ed7cdff83e5b4ab20eb79d42b4],SizeBytes:434254012,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:856a2097e01dbaad614b0f57570a5616d99e5348ea6f35ad4cea2fdd6d18126b],SizeBytes:415053325,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:252813800adf8140c408e6aa0956ccac8f9a3ddd4cf743f140fcaef61b23762f],SizeBytes:414242392,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:09d0eae43aa689add7aee2704b547a10c1423f4d065b777ac40260578ebdfcdd],SizeBytes:412124822,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f3ab7facf3d8b744025ac716a731ca8ec3c3fb6aaf7f4030fffe4b27fb9b3167],SizeBytes:411670500,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:06c9b59044c11816be5e21b6a98b941ff5b4dbac525b552f260eaf29b58d506c],SizeBytes:411066934,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8307057acd3af8dadf87323bfe9e2d4d9e74a5910cfe17373f9b30a96c7a796f],SizeBytes:409308238,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3634a2d516f4455d78a54952446bcf7cdebb913c5fcf736de59e48d067479b1e],SizeBytes:408726214,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e9013f41f06bd4c609f94b4ff8d4087961e0af67e4317f2cb03f45fec08892f6],SizeBytes:408639000,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fafa9771e41b970fb0ebf8cdbfe159b8dd8b48bfbc1acd225bb3d72838be3d07],SizeBytes:408260136,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:064c7c0257d1de0cad1d8e883b505853bed53fd835686d4105eca3c4b734e0b5],SizeBytes:407165482,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:506acacd57b6cf51722a41278c0278a28df68ac86d131c59ff8b687d6254aca3],SizeBytes:404880817,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-release@sha256:300bce8246cf880e792e106607925de0a404484637627edf5f517375517d54a4],SizeBytes:395682633,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c2a9eff33dd53c4e0dcf9fdd5ef471263f621424652368b819c4e4180b4bc197],SizeBytes:394107296,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e538dd11b15508ec363b8855e5496b51f65365ecfa62419899d3cdb21252f44],SizeBytes:385715621,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e463f54f6a78c0ff0f995dc9b6557fd3af350cd20f7116f56160cdfa8ddc6dc],SizeBytes:382829546,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:90616b372816c6b9e06c670b613dd6292b0b078d78c835f1c5da8374f908f39a],SizeBytes:382625292,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8aae812f23337d5143f2be66cc79c512d422e5e3b3b11307a81db299da31764c],SizeBytes:382218627,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0cc5bd656b0355619fdac6c8920a2388edabbe14cdc7ab93069bacdc1e2db1d1],SizeBytes:380104717,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5813d3728229c2a09a05bc454753e0128ac2bbd203e7000a4d954daab12fbb79],SizeBytes:377171632,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b5be929f4ccae9b9f3b1c5f19281c9a24f1adb54bf9c358691af988556dce27],SizeBytes:376385161,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e403f3e14cbbf2e91ce0d5ed7ad90d88f95e617a446e907b973c8c7f4916cbe1],SizeBytes:372710918,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:abcba83a0a4224a97f080480404808da1e5f42de24b6ded0062aad7a598dfc87],SizeBytes:371310125,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5e1c69d005727e3245604cfca7a63e4f9bc6e15128c7489e41d5e967305089e],SizeBytes:371248553,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:212f5bba67dd176f7843b808bc04d7680709f5c8a177fe39a204412aeacf3c5a],SizeBytes:368625226,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f3ebf99182733dff2666e6a5e7e217085e06029362036ad79c91278e69dcbf7],SizeBytes:366270626,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f5228e2d779565e500aed3a80e7f8f8fafc45a4af65acbd9e47a7e21239ba709],SizeBytes:364674190,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jan 18 23:18:39.230: INFO: 
Logging kubelet events for node ip-10-0-176-170.ec2.internal
Jan 18 23:18:39.232: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-176-170.ec2.internal
Jan 18 23:18:39.282: INFO: node-resolver-f7pfq started at 2023-01-18 20:58:11 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:18:39.282: INFO: validation-webhook-8pdq4 started at 2023-01-18 21:18:05 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container webhooks ready: true, restart count 1
Jan 18 23:18:39.282: INFO: network-operator-84894578b9-l8pbp started at 2023-01-18 21:24:15 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container network-operator ready: true, restart count 0
Jan 18 23:18:39.282: INFO: kube-apiserver-guard-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:27:01 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container guard ready: true, restart count 0
Jan 18 23:18:39.282: INFO: revision-pruner-10-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:27:19 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container pruner ready: false, restart count 0
Jan 18 23:18:39.282: INFO: multus-admission-controller-mvgmc started at 2023-01-18 20:57:01 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.282: INFO: 	Container multus-admission-controller ready: true, restart count 1
Jan 18 23:18:39.282: INFO: machine-config-daemon-9wdbz started at 2023-01-18 20:57:06 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:18:39.282: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:18:39.282: INFO: installer-10-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:27:48 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container installer ready: false, restart count 0
Jan 18 23:18:39.282: INFO: revision-pruner-7-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:40:28 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container pruner ready: false, restart count 0
Jan 18 23:18:39.282: INFO: network-check-target-58b7z started at 2023-01-18 20:55:26 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:18:39.282: INFO: installer-7-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:38:46 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container installer ready: false, restart count 0
Jan 18 23:18:39.282: INFO: etcd-guard-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:26:27 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container guard ready: true, restart count 0
Jan 18 23:18:39.282: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-k8rgl started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:18:39.282: INFO: ovnkube-node-w2dnv started at 2023-01-18 20:55:24 +0000 UTC (0+5 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.282: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:18:39.282: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:18:39.282: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:18:39.282: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:18:39.282: INFO: machine-config-controller-fdb8b6c8d-jvpq4 started at 2023-01-18 20:57:44 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container machine-config-controller ready: true, restart count 2
Jan 18 23:18:39.282: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:18:39.282: INFO: cluster-version-operator-796d5bc86b-m6s67 started at 2023-01-18 21:24:16 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container cluster-version-operator ready: true, restart count 0
Jan 18 23:18:39.282: INFO: certified-operators-kz76t started at 2023-01-18 21:24:17 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.282: INFO: kube-apiserver-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:00:53 +0000 UTC (1+5 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Init container setup ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container kube-apiserver-cert-regeneration-controller ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container kube-apiserver-cert-syncer ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container kube-apiserver-check-endpoints ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container kube-apiserver-insecure-readyz ready: true, restart count 0
Jan 18 23:18:39.282: INFO: ovnkube-master-ljq6z started at 2023-01-18 20:55:24 +0000 UTC (0+6 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.282: INFO: 	Container nbdb ready: true, restart count 1
Jan 18 23:18:39.282: INFO: 	Container northd ready: true, restart count 1
Jan 18 23:18:39.282: INFO: 	Container ovn-dbchecker ready: true, restart count 1
Jan 18 23:18:39.282: INFO: 	Container ovnkube-master ready: true, restart count 5
Jan 18 23:18:39.282: INFO: 	Container sbdb ready: true, restart count 1
Jan 18 23:18:39.282: INFO: machine-config-server-7skcj started at 2023-01-18 20:57:49 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container machine-config-server ready: true, restart count 1
Jan 18 23:18:39.282: INFO: revision-pruner-9-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:23:46 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container pruner ready: false, restart count 0
Jan 18 23:18:39.282: INFO: aws-ebs-csi-driver-controller-6f56cff656-vxff5 started at 2023-01-18 21:24:17 +0000 UTC (0+11 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container attacher-kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container csi-driver ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container driver-kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container provisioner-kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container resizer-kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container snapshotter-kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.282: INFO: console-67d8cf9f45-k9j6z started at 2023-01-18 21:27:16 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container console ready: true, restart count 0
Jan 18 23:18:39.282: INFO: kube-controller-manager-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:00:32 +0000 UTC (0+4 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container cluster-policy-controller ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container kube-controller-manager-cert-syncer ready: true, restart count 0
Jan 18 23:18:39.282: INFO: 	Container kube-controller-manager-recovery-controller ready: true, restart count 0
Jan 18 23:18:39.282: INFO: node-ca-dlqnz started at 2023-01-18 21:02:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:18:39.282: INFO: redhat-marketplace-ks27p started at 2023-01-18 21:24:22 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.282: INFO: apiserver-66fc6ccdbf-7psdj started at 2023-01-18 21:32:17 +0000 UTC (1+2 container statuses recorded)
Jan 18 23:18:39.282: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container openshift-apiserver ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container openshift-apiserver-check-endpoints ready: true, restart count 0
Jan 18 23:18:39.283: INFO: network-metrics-daemon-qqj8q started at 2023-01-18 20:55:15 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:18:39.283: INFO: revision-pruner-11-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:34:08 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container pruner ready: false, restart count 0
Jan 18 23:18:39.283: INFO: multus-additional-cni-plugins-rm96m started at 2023-01-18 20:55:14 +0000 UTC (6+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:18:39.283: INFO: csi-snapshot-webhook-f7487cb9f-shngp started at 2023-01-18 21:24:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container webhook ready: true, restart count 0
Jan 18 23:18:39.283: INFO: redhat-operators-4nktc started at 2023-01-18 21:24:20 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.283: INFO: openshift-kube-scheduler-guard-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:26:22 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container guard ready: true, restart count 0
Jan 18 23:18:39.283: INFO: machine-api-controllers-65b8bc9f-ffb75 started at 2023-01-18 21:24:12 +0000 UTC (0+7 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container kube-rbac-proxy-machine-mtrc ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container kube-rbac-proxy-machineset-mtrc ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container kube-rbac-proxy-mhc-mtrc ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container machine-controller ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container machine-healthcheck-controller ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container machineset-controller ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container nodelink-controller ready: true, restart count 0
Jan 18 23:18:39.283: INFO: aws-ebs-csi-driver-node-dvvjd started at 2023-01-18 20:57:36 +0000 UTC (0+3 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:18:39.283: INFO: sre-dns-latency-exporter-pgd7n started at 2023-01-18 21:18:07 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container main ready: true, restart count 1
Jan 18 23:18:39.283: INFO: managed-upgrade-operator-6748688f8b-qcjzq started at 2023-01-18 21:24:23 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container managed-upgrade-operator ready: true, restart count 0
Jan 18 23:18:39.283: INFO: node-exporter-xsg8v started at 2023-01-18 21:08:32 +0000 UTC (1+2 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:18:39.283: INFO: apiserver-58dcc4c5df-tlmzz started at 2023-01-18 21:24:08 +0000 UTC (1+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container oauth-apiserver ready: true, restart count 0
Jan 18 23:18:39.283: INFO: cluster-samples-operator-665bf757cb-jntc9 started at 2023-01-18 21:24:12 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jan 18 23:18:39.283: INFO: packageserver-749dbf8d56-cntts started at 2023-01-18 21:24:13 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container packageserver ready: true, restart count 0
Jan 18 23:18:39.283: INFO: installer-6-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:32:47 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container installer ready: false, restart count 0
Jan 18 23:18:39.283: INFO: tuned-tdrgh started at 2023-01-18 20:58:22 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:18:39.283: INFO: dns-default-hz2tr started at 2023-01-18 20:58:10 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container dns ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.283: INFO: migrator-59f7fcfc8f-7shcf started at 2023-01-18 21:24:13 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container migrator ready: true, restart count 0
Jan 18 23:18:39.283: INFO: openshift-kube-scheduler-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:02:13 +0000 UTC (1+3 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Init container wait-for-host-port ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container kube-scheduler-cert-syncer ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container kube-scheduler-recovery-controller ready: true, restart count 0
Jan 18 23:18:39.283: INFO: etcd-ip-10-0-176-170.ec2.internal started at 2023-01-18 20:59:00 +0000 UTC (3+5 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Init container setup ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Init container etcd-ensure-env-vars ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Init container etcd-resources-copy ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Container etcd ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Container etcd-health-monitor ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Container etcd-metrics ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Container etcd-readyz ready: true, restart count 1
Jan 18 23:18:39.283: INFO: 	Container etcdctl ready: true, restart count 1
Jan 18 23:18:39.283: INFO: pod-identity-webhook-679b695d64-9v6dr started at 2023-01-18 21:24:14 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container pod-identity-webhook ready: true, restart count 0
Jan 18 23:18:39.283: INFO: community-operators-km2p8 started at 2023-01-18 21:24:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.283: INFO: splunkforwarder-ds-bbtz4 started at 2023-01-18 21:24:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container splunk-uf ready: true, restart count 0
Jan 18 23:18:39.283: INFO: service-ca-d5df49fff-lhrcw started at 2023-01-18 21:30:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container service-ca-controller ready: false, restart count 0
Jan 18 23:18:39.283: INFO: oauth-openshift-7cd8db5c84-f5qw5 started at 2023-01-18 21:37:42 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container oauth-openshift ready: true, restart count 0
Jan 18 23:18:39.283: INFO: audit-exporter-zgvd7 started at 2023-01-18 21:19:38 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container audit-exporter ready: true, restart count 0
Jan 18 23:18:39.283: INFO: csi-snapshot-controller-64fc8cb68f-zlghz started at 2023-01-18 21:24:17 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 18 23:18:39.283: INFO: aws-ebs-csi-driver-operator-cf879859c-txgsp started at 2023-01-18 21:24:17 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container aws-ebs-csi-driver-operator ready: true, restart count 0
Jan 18 23:18:39.283: INFO: controller-manager-cd92f started at 2023-01-18 21:26:39 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container controller-manager ready: true, restart count 0
Jan 18 23:18:39.283: INFO: installer-11-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:42:12 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container installer ready: false, restart count 0
Jan 18 23:18:39.283: INFO: csi-snapshot-controller-operator-6c49b58f57-99q68 started at 2023-01-18 21:31:06 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Jan 18 23:18:39.283: INFO: multus-c6bxn started at 2023-01-18 20:55:14 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:18:39.283: INFO: cluster-cloud-controller-manager-operator-5bbd997669-hdb8j started at 2023-01-18 21:24:12 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container cluster-cloud-controller-manager ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container config-sync-controllers ready: true, restart count 0
Jan 18 23:18:39.283: INFO: package-server-manager-7bd9967fd7-hftlg started at 2023-01-18 21:31:01 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container package-server-manager ready: true, restart count 0
Jan 18 23:18:39.283: INFO: cluster-autoscaler-operator-6f86dcbd8d-r826n started at 2023-01-18 21:31:03 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container cluster-autoscaler-operator ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.283: INFO: machine-api-operator-84bc76c576-ldbg4 started at 2023-01-18 21:31:05 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.283: INFO: 	Container machine-api-operator ready: true, restart count 0
Jan 18 23:18:39.283: INFO: service-ca-operator-688c645b8c-gzbtx started at 2023-01-18 21:31:06 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container service-ca-operator ready: true, restart count 0
Jan 18 23:18:39.283: INFO: installer-7-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:38:09 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container installer ready: false, restart count 0
Jan 18 23:18:39.283: INFO: revision-pruner-9-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:26:21 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container pruner ready: false, restart count 0
Jan 18 23:18:39.283: INFO: kube-controller-manager-guard-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:26:39 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container guard ready: true, restart count 0
Jan 18 23:18:39.283: INFO: machine-config-operator-59b6ff77f9-8k25m started at 2023-01-18 21:31:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container machine-config-operator ready: true, restart count 0
Jan 18 23:18:39.283: INFO: console-operator-7498759ddc-jnrvj started at 2023-01-18 21:24:12 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.283: INFO: 	Container console-operator ready: true, restart count 0
W0118 23:18:39.285585      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 18 23:18:39.312: INFO: 
Latency metrics for node ip-10-0-176-170.ec2.internal
Jan 18 23:18:39.312: INFO: 
Logging node info for node ip-10-0-200-13.ec2.internal
Jan 18 23:18:39.316: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-200-13.ec2.internal    1e41ad69-39ff-411d-ae35-02a8ab8f162a 263361 0 2023-01-18 21:02:39 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-200-13.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-08a1adf43d92b1771","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0494590192945872e"} k8s.ovn.org/host-addresses:["10.0.200.13"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-200-13.ec2.internal","mac-address":"02:54:53:77:0c:0f","ip-addresses":["10.0.200.13/17"],"ip-address":"10.0.200.13/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:5761956d-9599-4f6a-a0bd-9faad9420cd7 k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.5/16"} k8s.ovn.org/node-mgmt-port-mac-address:82:8d:90:60:54:7c k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.200.13/17"} k8s.ovn.org/node-subnets:{"default":"10.128.6.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-worker-us-east-1a-q8ss7 machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done managed.openshift.com/customlabels: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{cloud-network-config-controller Update v1 2023-01-18 21:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{}}}} } {ip-10-0-164-47 Update v1 2023-01-18 21:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {kubelet Update v1 2023-01-18 21:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {nodelink-controller Update v1 2023-01-18 21:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}} } {ip-10-0-200-13 Update v1 2023-01-18 21:03:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {machine-config-daemon Update v1 2023-01-18 21:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {machine-config-controller Update v1 2023-01-18 21:18:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {manager Update v1 2023-01-18 21:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:managed.openshift.com/customlabels":{}}}} } {kubelet Update v1 2023-01-18 22:51:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-0494590192945872e,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321574121472 0} {<nil>} 314037228Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16487145472 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3920 -3} {<nil>} 3920m DecimalSI},ephemeral-storage: {{289416708846 0} {<nil>} 289416708846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{13697933312 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-18 23:16:55 +0000 UTC,LastTransitionTime:2023-01-18 21:02:39 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-18 23:16:55 +0000 UTC,LastTransitionTime:2023-01-18 21:02:39 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-18 23:16:55 +0000 UTC,LastTransitionTime:2023-01-18 21:02:39 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-18 23:16:55 +0000 UTC,LastTransitionTime:2023-01-18 21:03:20 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.200.13,},NodeAddress{Type:Hostname,Address:ip-10-0-200-13.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-200-13.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec2e839fdd9360a6cb050f8bfd6a64a2,SystemUUID:ec2e839f-dd93-60a6-cb05-0f8bfd6a64a2,BootID:37f61e92-0a53-4e5c-8366-d0abf103fe13,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e682077b6cbbe34d4c5e45ed10ffa9bc37e113e17e92a1d5330a7a9fffeab8a3],SizeBytes:1294522975,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08ab62da9265ef67f4eb4b526b38346a3ae3d35e2d5bfbdd58a0858b76b21f77],SizeBytes:681959546,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c7df53b796e81ba8301ba74d02317226329bd5752fd31c1b44d028e4832f21c3],SizeBytes:442026998,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f1134e2e28f44375c3bd9a6ee34d7f9972fdbd3305a3ee2b95e6ed3cede02140],SizeBytes:409661973,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:36fc214537c763b3a3f0a9dc7a1bd4378a80428c31b2629df8786a9b09155e6d],SizeBytes:398604719,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab0d2f0b6d01a4a3b9952710cc49e787ebb0e8649a288438774c1ffe0fc3fae0],SizeBytes:392510137,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5e1c69d005727e3245604cfca7a63e4f9bc6e15128c7489e41d5e967305089e],SizeBytes:371248553,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:77c5db690d9438ac077736cad8f28c04de476c04c3a97f39910ed86b6c395b85],SizeBytes:368328246,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f3ebf99182733dff2666e6a5e7e217085e06029362036ad79c91278e69dcbf7],SizeBytes:366270626,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2269a2b10b3ba2a6783dde2a97451f57f5716a20ebef4a82bea20d25df8761fa],SizeBytes:365147114,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:32fa95a3d4ecfebe96152242926addf27789555b12413203927f255a712c8a0c],SizeBytes:358569445,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:91308d35c1e56463f55c1aaa519ff4de7335d43b254c21abdb845fc8c72821a1],SizeBytes:347460509,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:255a95e8b514dcdc4fa12436789115739355c13fae93eccbba660298746d9aa1],SizeBytes:346372629,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a4fdcff8b0038d858d46740c9622d5dc428074dcf7e662deacd9e6c0aff18286],SizeBytes:344400730,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:209e20410ec2d3d7a502f568d2b7fe1cd1beadcb36fff2d1e6f59d77be3200e3],SizeBytes:339844669,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:140f8947593d92e1517e50a201e83bdef8eb965b552a21d3caf346a250d0cf6e],SizeBytes:335139477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01de581f7f0c7e43f809e9346ced7e60c0ecdd7c0078e97a8f64b24d4d3fa0b7],SizeBytes:332949901,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ed634acd9b29e16b578c74b38d5425156cd39bb72c0884a9564331f95f80911],SizeBytes:330558591,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs@sha256:83da6a30accf7d97453051973ba8c2ae1f78fa0472d2aeb0f8fcfdd8ba04e11a docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:324891489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:923999744ad1557510fc5c3a95cf978b27c453988489ada899c574ab1d71a218],SizeBytes:312722401,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-base@sha256:8da4f871cc18d3ac288a2fbb5c37ea27eabea4314dabca68f3f8469c7d7a4a21 quay.io/app-sre/managed-prometheus-exporter-base:latest],SizeBytes:303857745,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2550b2cbdf864515b1edacf43c25eb6b6f179713c1df34e51f6e9bba48d6430a],SizeBytes:303075904,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9260bbcc285a9fd4c1c7df865280261723eb7679050905da4acbfe728f96ee1b],SizeBytes:298386295,},ContainerImage{Names:[quay.io/konveyor/velero@sha256:6abd52244096680eeb3c80289999a00f642069edcd3d2e6c6948317b4bdd9bcd],SizeBytes:230325612,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder@sha256:afc413cd2504b586b6f28c16355db243c8bfdef536cc80e44f61e03c01e12235],SizeBytes:229564388,},ContainerImage{Names:[quay.io/app-sre/managed-velero-operator@sha256:587741a4e6772774f01efd91ed9b93ed5d0a21fcf9ff1f3bebaf14eb98466132],SizeBytes:155001069,},ContainerImage{Names:[quay.io/konveyor/velero-plugin-for-aws@sha256:7c22d5ae59862a66bac77e3fb48e6cd9c1556e4c9d7277aad4f093a198cb4373],SizeBytes:149334789,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder-operator@sha256:6c775fd9bfbbdabc114aaf28fa765b8290775030bbc8ddf36ea9a7522e73e804],SizeBytes:144945179,},ContainerImage{Names:[quay.io/app-sre/configure-alertmanager-operator@sha256:53691c2d7c4ff4314ceecdd01386a1260c85d4a4012d7434af7b61ead4d27eda],SizeBytes:144363279,},ContainerImage{Names:[quay.io/app-sre/osd-metrics-exporter@sha256:d413e14c899d4160c9be3d7854f2f5d8899ac649f6214f5405bd37cb98b3fc3d],SizeBytes:144088719,},ContainerImage{Names:[quay.io/app-sre/managed-node-metadata-operator@sha256:81cb5d1fcc8261858727876036c86fcf45b5b3cf9f5129450019c6f107cd11da],SizeBytes:143306922,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 k8s.gcr.io/e2e-test-images/httpd@sha256:6589a0d3a4b40f996ab554f0d58e8c567c2850c41bc99d05498378a50b7e1cb4 k8s.gcr.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:128894651,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403 k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8 k8s.gcr.io/e2e-test-images/agnhost:2.36],SizeBytes:126252387,},ContainerImage{Names:[quay.io/app-sre/custom-domains-operator-registry@sha256:365524ae13f1b5e5b252190c828e7ce9a5c1916a6aba32566326240d676059dc],SizeBytes:118428594,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator-registry@sha256:22c7691bceb31faeb7413d4e0fa56a52afb9d1bb64c11890874100d15fce0e9c],SizeBytes:117297595,},ContainerImage{Names:[quay.io/app-sre/route-monitor-operator-registry@sha256:bedb73a971e8a4094ea296e5df7279587edb6b293fa4b9227072792773cab1dc],SizeBytes:111841208,},ContainerImage{Names:[quay.io/app-sre/osd-metrics-exporter-registry@sha256:c9fa231e704f53d0e7b69e4bf1f746e53f916998f451da26fdfbb3e74dc9b197],SizeBytes:108399030,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jan 18 23:18:39.316: INFO: 
Logging kubelet events for node ip-10-0-200-13.ec2.internal
Jan 18 23:18:39.319: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-200-13.ec2.internal
Jan 18 23:18:39.342: INFO: machine-config-daemon-smj7g started at 2023-01-18 21:02:39 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.342: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:18:39.342: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:18:39.342: INFO: network-metrics-daemon-m4c6r started at 2023-01-18 21:02:39 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.342: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.342: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:18:39.342: INFO: node-resolver-gxvcl started at 2023-01-18 21:02:39 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.342: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:18:39.342: INFO: downloads-6f74f6fcbf-hcggr started at 2023-01-18 21:24:10 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.342: INFO: 	Container download-server ready: true, restart count 0
Jan 18 23:18:39.342: INFO: rbac-permissions-operator-7f6bc8977-vsvkh started at 2023-01-18 21:24:29 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.342: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Jan 18 23:18:39.342: INFO: ovnkube-node-h62xt started at 2023-01-18 21:02:39 +0000 UTC (0+5 container statuses recorded)
Jan 18 23:18:39.342: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.342: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:18:39.343: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:18:39.343: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:18:39.343: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:18:39.343: INFO: collect-profiles-27901365-9rmcw started at 2023-01-18 22:45:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 18 23:18:39.343: INFO: ingress-canary-ppjjg started at 2023-01-18 21:03:27 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 23:18:39.343: INFO: ocm-agent-75d95f8dc7-gtjwg started at 2023-01-18 21:24:10 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container ocm-agent ready: true, restart count 0
Jan 18 23:18:39.343: INFO: multus-cfrv2 started at 2023-01-18 21:02:39 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:18:39.343: INFO: managed-velero-operator-f9f4c8b45-xml88 started at 2023-01-18 21:24:10 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container managed-velero-operator ready: true, restart count 0
Jan 18 23:18:39.343: INFO: velero-749f7746d8-ds5cs started at 2023-01-18 21:24:10 +0000 UTC (1+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Init container konveyor-velero-plugin-for-aws ready: true, restart count 0
Jan 18 23:18:39.343: INFO: 	Container velero ready: true, restart count 0
Jan 18 23:18:39.343: INFO: aws-ebs-csi-driver-node-6nf8l started at 2023-01-18 21:02:39 +0000 UTC (0+3 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:18:39.343: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:18:39.343: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:18:39.343: INFO: splunk-forwarder-operator-6f5bf57497-jwcz5 started at 2023-01-18 21:24:40 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Jan 18 23:18:39.343: INFO: network-check-target-hpzlx started at 2023-01-18 21:02:39 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:18:39.343: INFO: image-registry-7b8f8dcdc5-4lfps started at 2023-01-18 21:24:10 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container registry ready: true, restart count 0
Jan 18 23:18:39.343: INFO: osd-metrics-exporter-registry-6rnzs started at 2023-01-18 21:24:12 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.343: INFO: managed-node-metadata-operator-5d4c567575-cbrxh started at 2023-01-18 21:24:36 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container managed-node-metadata-operator ready: true, restart count 0
Jan 18 23:18:39.343: INFO: collect-profiles-27901395-s69sn started at 2023-01-18 23:15:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 18 23:18:39.343: INFO: node-exporter-gzf8b started at 2023-01-18 21:08:32 +0000 UTC (1+2 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 18 23:18:39.343: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.343: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:18:39.343: INFO: dns-default-vkx5x started at 2023-01-18 21:03:27 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container dns ready: true, restart count 1
Jan 18 23:18:39.343: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.343: INFO: sre-dns-latency-exporter-s6nk6 started at 2023-01-18 21:18:07 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container main ready: true, restart count 1
Jan 18 23:18:39.343: INFO: splunkforwarder-ds-n7d7t started at 2023-01-18 21:24:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container splunk-uf ready: true, restart count 0
Jan 18 23:18:39.343: INFO: collect-profiles-27901380-ttthd started at 2023-01-18 23:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 18 23:18:39.343: INFO: tuned-t6mkr started at 2023-01-18 21:02:39 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:18:39.343: INFO: token-refresher-6d8f85f497-mhth9 started at 2023-01-18 21:24:10 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container token-refresher ready: true, restart count 0
Jan 18 23:18:39.343: INFO: obo-prometheus-operator-admission-webhook-5b4dc9bff5-l6qvc started at 2023-01-18 21:24:10 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 23:18:39.343: INFO: configure-alertmanager-operator-7565458cf4-pbmjl started at 2023-01-18 21:24:36 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Jan 18 23:18:39.343: INFO: route-monitor-operator-registry-cgj4n started at 2023-01-18 21:24:36 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.343: INFO: node-ca-5tw9r started at 2023-01-18 21:02:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:18:39.343: INFO: blackbox-exporter-dfdd57dd6-tj4kz started at 2023-01-18 21:24:50 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan 18 23:18:39.343: INFO: route-monitor-operator-controller-manager-cbc597f9d-84p87 started at 2023-01-18 21:24:37 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container manager ready: true, restart count 0
Jan 18 23:18:39.343: INFO: osd-metrics-exporter-756f85967-8z4cz started at 2023-01-18 21:25:10 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container osd-metrics-exporter ready: true, restart count 0
Jan 18 23:18:39.343: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-mdcrg started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:18:39.343: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:18:39.343: INFO: multus-additional-cni-plugins-nm8v9 started at 2023-01-18 21:02:39 +0000 UTC (6+1 container statuses recorded)
Jan 18 23:18:39.343: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 18 23:18:39.343: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 18 23:18:39.343: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 18 23:18:39.343: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 18 23:18:39.343: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 18 23:18:39.343: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 18 23:18:39.343: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
W0118 23:18:39.345474      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 18 23:18:39.368: INFO: 
Latency metrics for node ip-10-0-200-13.ec2.internal
Jan 18 23:18:39.368: INFO: 
Logging node info for node ip-10-0-211-217.ec2.internal
Jan 18 23:18:39.370: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-211-217.ec2.internal    8dc19be1-68f9-4ba1-8988-e3dd116688ce 261711 0 2023-01-18 21:27:17 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:r5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-211-217.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io:infra node-role.kubernetes.io/infra: node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:r5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-04da74abf7ad6ed49","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-05b843346e3d96659"} k8s.ovn.org/host-addresses:["10.0.211.217"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-211-217.ec2.internal","mac-address":"02:33:6b:05:74:ad","ip-addresses":["10.0.211.217/17"],"ip-address":"10.0.211.217/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:68435c44-ccc8-48aa-9dca-17a66621f9dd k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.10/16"} k8s.ovn.org/node-mgmt-port-mac-address:b6:30:0a:a5:94:93 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.211.217/17"} k8s.ovn.org/node-subnets:{"default":"10.128.16.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-infra-us-east-1a-htq2c machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done managed.openshift.com/customlabels:node-role.kubernetes.io,node-role.kubernetes.io/infra volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{ancient-changes Update v1 2023-01-18 21:27:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{},"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {kubelet Update v1 2023-01-18 21:27:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {nodelink-controller Update v1 2023-01-18 21:27:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}},"f:labels":{"f:node-role.kubernetes.io":{},"f:node-role.kubernetes.io/infra":{}}}} } {ip-10-0-211-217 Update v1 2023-01-18 21:28:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {machine-config-daemon Update v1 2023-01-18 21:28:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {machine-config-controller Update v1 2023-01-18 21:36:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {manager Update v1 2023-01-18 21:39:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:managed.openshift.com/customlabels":{}}}} } {kube-controller-manager Update v1 2023-01-18 21:39:20 +0000 UTC FieldsV1 {"f:status":{"f:volumesAttached":{}}} status} {kubelet Update v1 2023-01-18 22:51:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}},"f:volumesInUse":{}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-05b843346e3d96659,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321574121472 0} {<nil>} 314037228Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{33222410240 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3920 -3} {<nil>} 3920m DecimalSI},ephemeral-storage: {{289416708846 0} {<nil>} 289416708846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{29423880765440 -3} {<nil>} 29423880765440m DecimalSI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-18 23:14:39 +0000 UTC,LastTransitionTime:2023-01-18 21:39:04 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-18 23:14:39 +0000 UTC,LastTransitionTime:2023-01-18 21:39:04 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-18 23:14:39 +0000 UTC,LastTransitionTime:2023-01-18 21:39:04 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-18 23:14:39 +0000 UTC,LastTransitionTime:2023-01-18 21:39:04 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.211.217,},NodeAddress{Type:Hostname,Address:ip-10-0-211-217.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-211-217.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec25a10687a71ed601256f15b22bf2ae,SystemUUID:ec25a106-87a7-1ed6-0125-6f15b22bf2ae,BootID:f5f35c2c-ec0c-49ff-a53f-fc5203750137,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[image-registry.openshift-image-registry.svc:5000/openshift/cli@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45 image-registry.openshift-image-registry.svc:5000/openshift/cli:latest],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c7df53b796e81ba8301ba74d02317226329bd5752fd31c1b44d028e4832f21c3],SizeBytes:442026998,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f1134e2e28f44375c3bd9a6ee34d7f9972fdbd3305a3ee2b95e6ed3cede02140],SizeBytes:409661973,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:36fc214537c763b3a3f0a9dc7a1bd4378a80428c31b2629df8786a9b09155e6d],SizeBytes:398604719,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab0d2f0b6d01a4a3b9952710cc49e787ebb0e8649a288438774c1ffe0fc3fae0],SizeBytes:392510137,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5813d3728229c2a09a05bc454753e0128ac2bbd203e7000a4d954daab12fbb79],SizeBytes:377171632,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c956edcee9b9ba5462572b65b6a92983b20ace63dae50e3237bfdbd6d8c0b972],SizeBytes:375087838,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5e1c69d005727e3245604cfca7a63e4f9bc6e15128c7489e41d5e967305089e],SizeBytes:371248553,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:77c5db690d9438ac077736cad8f28c04de476c04c3a97f39910ed86b6c395b85],SizeBytes:368328246,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f3ebf99182733dff2666e6a5e7e217085e06029362036ad79c91278e69dcbf7],SizeBytes:366270626,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2269a2b10b3ba2a6783dde2a97451f57f5716a20ebef4a82bea20d25df8761fa],SizeBytes:365147114,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:32fa95a3d4ecfebe96152242926addf27789555b12413203927f255a712c8a0c],SizeBytes:358569445,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:91308d35c1e56463f55c1aaa519ff4de7335d43b254c21abdb845fc8c72821a1],SizeBytes:347460509,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:255a95e8b514dcdc4fa12436789115739355c13fae93eccbba660298746d9aa1],SizeBytes:346372629,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a4fdcff8b0038d858d46740c9622d5dc428074dcf7e662deacd9e6c0aff18286],SizeBytes:344400730,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:209e20410ec2d3d7a502f568d2b7fe1cd1beadcb36fff2d1e6f59d77be3200e3],SizeBytes:339844669,},ContainerImage{Names:[quay.io/openshift/origin-kube-rbac-proxy@sha256:baedb268ac66456018fb30af395bb3d69af5fff3252ff5d549f0231b1ebb6901 quay.io/openshift/origin-kube-rbac-proxy:4.10.0],SizeBytes:337627888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:140f8947593d92e1517e50a201e83bdef8eb965b552a21d3caf346a250d0cf6e],SizeBytes:335139477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01de581f7f0c7e43f809e9346ced7e60c0ecdd7c0078e97a8f64b24d4d3fa0b7],SizeBytes:332949901,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ed634acd9b29e16b578c74b38d5425156cd39bb72c0884a9564331f95f80911],SizeBytes:330558591,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs@sha256:83da6a30accf7d97453051973ba8c2ae1f78fa0472d2aeb0f8fcfdd8ba04e11a docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:324891489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:923999744ad1557510fc5c3a95cf978b27c453988489ada899c574ab1d71a218],SizeBytes:312722401,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-base@sha256:8da4f871cc18d3ac288a2fbb5c37ea27eabea4314dabca68f3f8469c7d7a4a21 quay.io/app-sre/managed-prometheus-exporter-base:latest],SizeBytes:303857745,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2550b2cbdf864515b1edacf43c25eb6b6f179713c1df34e51f6e9bba48d6430a],SizeBytes:303075904,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9260bbcc285a9fd4c1c7df865280261723eb7679050905da4acbfe728f96ee1b],SizeBytes:298386295,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/jessie-dnsutils@sha256:11e6a66017ba4e4b938c1612b7a54a3befcefd354796c04e1dba76873a13518e k8s.gcr.io/e2e-test-images/jessie-dnsutils@sha256:a11a6fb43a910882e547b12b511b70b912834b9d3fc1a1d00b374c173b156d87 k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.5],SizeBytes:260976825,},ContainerImage{Names:[registry.k8s.io/conformance@sha256:77097c50d228096fc7af733bb98aff4a66e7846fb6ab62e3d46f84299cf8c235 registry.k8s.io/conformance@sha256:b9f91081bf4b080ad3b29977b15594ba68f0380242e35f31850616c12cedd323 registry.k8s.io/conformance:v1.24.0],SizeBytes:252262821,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder@sha256:afc413cd2504b586b6f28c16355db243c8bfdef536cc80e44f61e03c01e12235],SizeBytes:229564388,},ContainerImage{Names:[quay.io/app-sre/addon-operator-manager@sha256:3a5f9c6073f3b3542e53be155f1364bcba10233ed69011eaadbc73318776b775 quay.io/app-sre/addon-operator-manager:a45b2b8],SizeBytes:176376786,},ContainerImage{Names:[quay.io/app-sre/addon-operator-webhook@sha256:16a258633afc63916d84a447a6dd8a234e1803a4dfa22a15887ff1dc71781e75 quay.io/app-sre/addon-operator-webhook:a45b2b8],SizeBytes:173194705,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:20f25f275d46aa728f7615a1ccc19c78b2ed89435bf943a44b339f70f45508e6 k8s.gcr.io/e2e-test-images/httpd@sha256:5d28f127fae41261c56abccd96df481f8f4fba2a3305fece212e11aafe646943 k8s.gcr.io/e2e-test-images/httpd:2.4.39-2],SizeBytes:132295599,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 k8s.gcr.io/e2e-test-images/httpd@sha256:6589a0d3a4b40f996ab554f0d58e8c567c2850c41bc99d05498378a50b7e1cb4 k8s.gcr.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:128894651,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403 k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8 k8s.gcr.io/e2e-test-images/agnhost:2.36],SizeBytes:126252387,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nautilus@sha256:99c0d6f1ad24a1aa1905d9c6534d193f268f7b23f9add2ae6bb41f31094bdd5c k8s.gcr.io/e2e-test-images/nautilus@sha256:d3190bb7fb53d99b11da40aab7ca81b5f40379345deb8c09349d080e243c9e0c k8s.gcr.io/e2e-test-images/nautilus:1.5],SizeBytes:125565966,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:7f3ae784f32d750e63495dacd39c3bc57a9a0fc3e7ffc4b6f65e36be03cb368c docker.io/sonobuoy/sonobuoy@sha256:eaa42dd0660ece6c18d06199b78a41bd532a4851fc32a5a99acfafc03556852e docker.io/sonobuoy/sonobuoy:v0.56.14],SizeBytes:49637374,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nginx@sha256:13616070e3f29de4417eee434a8ef472221c9e51b3d037b5a6b46cef08eb7443 k8s.gcr.io/e2e-test-images/nginx@sha256:eee1822ee5bafc780db34f9ab68456c5fdcc0f994e1c1e01d5cde593cb3897a1 k8s.gcr.io/e2e-test-images/nginx:1.14-2],SizeBytes:17245363,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf k8s.gcr.io/e2e-test-images/busybox@sha256:f1fa97a4e1dab565da2790e6c76f8aecf87c44c9ac7ba53a9c65af96ec472c4a k8s.gcr.io/e2e-test-images/busybox:1.29-2],SizeBytes:1374588,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c k8s.gcr.io/pause@sha256:f81611a21cf91214c1ea751c5b525931a0e2ebabe62b3937b6158039ff6f922d k8s.gcr.io/pause:3.7],SizeBytes:717997,},},VolumesInUse:[kubernetes.io/csi/ebs.csi.aws.com^vol-00a181b5d3d011a60 kubernetes.io/csi/ebs.csi.aws.com^vol-04efe798559a2c542],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/csi/ebs.csi.aws.com^vol-04efe798559a2c542,DevicePath:,},AttachedVolume{Name:kubernetes.io/csi/ebs.csi.aws.com^vol-00a181b5d3d011a60,DevicePath:,},},Config:nil,},}
Jan 18 23:18:39.370: INFO: 
Logging kubelet events for node ip-10-0-211-217.ec2.internal
Jan 18 23:18:39.374: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-211-217.ec2.internal
Jan 18 23:18:39.410: INFO: node-exporter-bpnc5 started at 2023-01-18 21:27:42 +0000 UTC (1+2 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 18 23:18:39.410: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.410: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:18:39.410: INFO: addon-operator-webhooks-569bd4cfc5-mdklc started at 2023-01-18 21:39:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container webhook ready: true, restart count 0
Jan 18 23:18:39.410: INFO: osd-rebalance-infra-nodes-27901395-h44mq started at 2023-01-18 23:15:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 18 23:18:39.410: INFO: network-metrics-daemon-jnkxc started at 2023-01-18 21:27:42 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.410: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:18:39.410: INFO: ovnkube-node-bf8j7 started at 2023-01-18 21:27:42 +0000 UTC (0+5 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.410: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:18:39.410: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:18:39.410: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:18:39.410: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:18:39.410: INFO: ingress-canary-k52kt started at 2023-01-18 21:28:29 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 23:18:39.410: INFO: router-default-7cff97cd98-lstfd started at 2023-01-18 21:39:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container router ready: true, restart count 0
Jan 18 23:18:39.410: INFO: osd-delete-backplane-serviceaccounts-27901380-z2gm4 started at 2023-01-18 23:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 18 23:18:39.410: INFO: multus-cz5tw started at 2023-01-18 21:27:42 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:18:39.410: INFO: tuned-6pmzm started at 2023-01-18 21:27:42 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:18:39.410: INFO: prometheus-k8s-0 started at 2023-01-18 21:39:17 +0000 UTC (1+6 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Init container init-config-reloader ready: true, restart count 0
Jan 18 23:18:39.410: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 23:18:39.410: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.410: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 18 23:18:39.410: INFO: 	Container prometheus ready: true, restart count 0
Jan 18 23:18:39.410: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 18 23:18:39.410: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 18 23:18:39.410: INFO: osd-delete-ownerrefs-serviceaccounts-27901387-lzh55 started at 2023-01-18 23:07:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 18 23:18:39.410: INFO: node-ca-9gnsm started at 2023-01-18 21:27:42 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:18:39.410: INFO: multus-additional-cni-plugins-2q4s4 started at 2023-01-18 21:27:42 +0000 UTC (6+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 18 23:18:39.410: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 18 23:18:39.410: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 18 23:18:39.410: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 18 23:18:39.410: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 18 23:18:39.410: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 18 23:18:39.410: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:18:39.410: INFO: image-pruner-27901380-mbpm4 started at 2023-01-18 23:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container image-pruner ready: false, restart count 0
Jan 18 23:18:39.410: INFO: network-check-target-pjt5v started at 2023-01-18 21:27:42 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:18:39.410: INFO: aws-ebs-csi-driver-node-l4ng8 started at 2023-01-18 21:27:42 +0000 UTC (0+3 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:18:39.410: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:18:39.410: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:18:39.410: INFO: prometheus-operator-admission-webhook-577cc9c956-lwfrr started at 2023-01-18 21:39:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 23:18:39.410: INFO: osd-delete-ownerrefs-serviceaccounts-27901327-dmq56 started at 2023-01-18 22:07:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 18 23:18:39.410: INFO: prometheus-adapter-cf64f7f46-jw64l started at 2023-01-18 21:39:31 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 18 23:18:39.410: INFO: alertmanager-main-0 started at 2023-01-18 21:39:18 +0000 UTC (0+6 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container alertmanager ready: true, restart count 0
Jan 18 23:18:39.410: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 18 23:18:39.410: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 23:18:39.410: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.410: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 18 23:18:39.410: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 23:18:39.410: INFO: dns-default-7sqw2 started at 2023-01-18 22:06:24 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container dns ready: true, restart count 0
Jan 18 23:18:39.410: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.410: INFO: osd-delete-ownerrefs-serviceaccounts-27901357-rd8sm started at 2023-01-18 22:37:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 18 23:18:39.410: INFO: osd-rebalance-infra-nodes-27901365-hqkcb started at 2023-01-18 22:45:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 18 23:18:39.410: INFO: osd-patch-subscription-source-27901380-gmtz6 started at 2023-01-18 23:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 18 23:18:39.410: INFO: image-pruner-27901320-rjnq8 started at 2023-01-18 22:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container image-pruner ready: false, restart count 0
Jan 18 23:18:39.410: INFO: splunkforwarder-ds-gr4fb started at 2023-01-18 21:27:42 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 23:18:39.410: INFO: builds-pruner-27901320-2bl9l started at 2023-01-18 22:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 18 23:18:39.410: INFO: osd-delete-backplane-serviceaccounts-27901370-954ch started at 2023-01-18 22:50:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 18 23:18:39.410: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-tjlv7 started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.410: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:18:39.411: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:18:39.411: INFO: machine-config-daemon-krjvz started at 2023-01-18 21:27:42 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.411: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:18:39.411: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:18:39.411: INFO: thanos-querier-57f44c5498-fzjjb started at 2023-01-18 21:39:18 +0000 UTC (0+6 container statuses recorded)
Jan 18 23:18:39.411: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.411: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 18 23:18:39.411: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 18 23:18:39.411: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 18 23:18:39.411: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 23:18:39.411: INFO: 	Container thanos-query ready: true, restart count 0
Jan 18 23:18:39.411: INFO: osd-rebalance-infra-nodes-27901380-59lzw started at 2023-01-18 23:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.411: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 18 23:18:39.411: INFO: sre-build-test-27901391-sgdf9 started at 2023-01-18 23:11:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.411: INFO: 	Container sre-build-test ready: false, restart count 0
Jan 18 23:18:39.411: INFO: node-resolver-qsc9k started at 2023-01-18 21:27:42 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.411: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:18:39.411: INFO: sre-dns-latency-exporter-tlqwt started at 2023-01-18 21:27:42 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.411: INFO: 	Container main ready: true, restart count 1
Jan 18 23:18:39.411: INFO: sonobuoy-e2e-job-aed0e1be5f434192 started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.411: INFO: 	Container e2e ready: true, restart count 0
Jan 18 23:18:39.411: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:18:39.411: INFO: osd-delete-backplane-serviceaccounts-27901390-w4rdw started at 2023-01-18 23:10:00 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.411: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
W0118 23:18:39.413006      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 18 23:18:39.437: INFO: 
Latency metrics for node ip-10-0-211-217.ec2.internal
Jan 18 23:18:39.437: INFO: 
Logging node info for node ip-10-0-219-147.ec2.internal
Jan 18 23:18:39.439: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-219-147.ec2.internal    7f02bac5-3479-43f2-9907-d04fbea8675c 261876 0 2023-01-18 21:03:48 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-219-147.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-0dd82b6977e2626fa","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0910f24871101dbf4"} k8s.ovn.org/host-addresses:["10.0.219.147"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-219-147.ec2.internal","mac-address":"02:8a:81:6a:3c:b1","ip-addresses":["10.0.219.147/17"],"ip-address":"10.0.219.147/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:0ddbb2d7-7bcd-4adb-91e4-f6546b32776d k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.8/16"} k8s.ovn.org/node-mgmt-port-mac-address:f6:73:8e:97:41:a3 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.219.147/17"} k8s.ovn.org/node-subnets:{"default":"10.128.12.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-worker-us-east-1a-5kt8g machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done managed.openshift.com/customlabels: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{ancient-changes Update v1 2023-01-18 21:03:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{},"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {kubelet Update v1 2023-01-18 21:03:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {nodelink-controller Update v1 2023-01-18 21:03:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}} } {machine-config-daemon Update v1 2023-01-18 21:07:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {ip-10-0-219-147 Update v1 2023-01-18 21:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {manager Update v1 2023-01-18 21:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:managed.openshift.com/customlabels":{}}}} } {machine-config-controller Update v1 2023-01-18 21:31:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {e2e.test Update v1 2023-01-18 23:09:00 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:example.com/fakecpu":{}}}} status} {kubelet Update v1 2023-01-18 23:09:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:example.com/fakecpu":{},"f:memory":{}},"f:capacity":{"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-0910f24871101dbf4,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321574121472 0} {<nil>} 314037228Ki BinarySI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16487145472 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3920 -3} {<nil>} 3920m DecimalSI},ephemeral-storage: {{289416708846 0} {<nil>} 289416708846 DecimalSI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{13697933312 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-18 23:14:50 +0000 UTC,LastTransitionTime:2023-01-18 21:33:15 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-18 23:14:50 +0000 UTC,LastTransitionTime:2023-01-18 21:33:15 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-18 23:14:50 +0000 UTC,LastTransitionTime:2023-01-18 21:33:15 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-18 23:14:50 +0000 UTC,LastTransitionTime:2023-01-18 21:33:15 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.219.147,},NodeAddress{Type:Hostname,Address:ip-10-0-219-147.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-219-147.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec22e883e23ff437129dfb17e11a2173,SystemUUID:ec22e883-e23f-f437-129d-fb17e11a2173,BootID:e31e18d4-9c53-4050-87d7-b931ed6c39ae,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e682077b6cbbe34d4c5e45ed10ffa9bc37e113e17e92a1d5330a7a9fffeab8a3],SizeBytes:1294522975,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ca84d407cbe329b9efad6656408b6c6cade8899633305d6fe45d9aaf079cc45c],SizeBytes:574313810,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c7df53b796e81ba8301ba74d02317226329bd5752fd31c1b44d028e4832f21c3],SizeBytes:442026998,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:36fc214537c763b3a3f0a9dc7a1bd4378a80428c31b2629df8786a9b09155e6d],SizeBytes:398604719,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab0d2f0b6d01a4a3b9952710cc49e787ebb0e8649a288438774c1ffe0fc3fae0],SizeBytes:392510137,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5e1c69d005727e3245604cfca7a63e4f9bc6e15128c7489e41d5e967305089e],SizeBytes:371248553,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f3ebf99182733dff2666e6a5e7e217085e06029362036ad79c91278e69dcbf7],SizeBytes:366270626,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:91308d35c1e56463f55c1aaa519ff4de7335d43b254c21abdb845fc8c72821a1],SizeBytes:347460509,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:255a95e8b514dcdc4fa12436789115739355c13fae93eccbba660298746d9aa1],SizeBytes:346372629,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a4fdcff8b0038d858d46740c9622d5dc428074dcf7e662deacd9e6c0aff18286],SizeBytes:344400730,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:209e20410ec2d3d7a502f568d2b7fe1cd1beadcb36fff2d1e6f59d77be3200e3],SizeBytes:339844669,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:140f8947593d92e1517e50a201e83bdef8eb965b552a21d3caf346a250d0cf6e],SizeBytes:335139477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01de581f7f0c7e43f809e9346ced7e60c0ecdd7c0078e97a8f64b24d4d3fa0b7],SizeBytes:332949901,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ed634acd9b29e16b578c74b38d5425156cd39bb72c0884a9564331f95f80911],SizeBytes:330558591,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs@sha256:83da6a30accf7d97453051973ba8c2ae1f78fa0472d2aeb0f8fcfdd8ba04e11a docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:324891489,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-initcontainer@sha256:f859874cf8ef92e8e806ff615f33472992917545ec94d461caa8e6e13b8a1983 quay.io/app-sre/managed-prometheus-exporter-initcontainer:latest],SizeBytes:315381355,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:923999744ad1557510fc5c3a95cf978b27c453988489ada899c574ab1d71a218],SizeBytes:312722401,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-base@sha256:8da4f871cc18d3ac288a2fbb5c37ea27eabea4314dabca68f3f8469c7d7a4a21 quay.io/app-sre/managed-prometheus-exporter-base:latest],SizeBytes:303857745,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2550b2cbdf864515b1edacf43c25eb6b6f179713c1df34e51f6e9bba48d6430a],SizeBytes:303075904,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:13f53ed1d91e2e11aac476ee9a0269fdda6cc4874eba903efd40daf50c55eee5 k8s.gcr.io/etcd@sha256:678382ed340f6996ad40cdba4a4745a2ada41ed9c322c026a2a695338a93dcbe k8s.gcr.io/etcd:3.5.3-0],SizeBytes:300857875,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9260bbcc285a9fd4c1c7df865280261723eb7679050905da4acbfe728f96ee1b],SizeBytes:298386295,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/jessie-dnsutils@sha256:11e6a66017ba4e4b938c1612b7a54a3befcefd354796c04e1dba76873a13518e k8s.gcr.io/e2e-test-images/jessie-dnsutils@sha256:a11a6fb43a910882e547b12b511b70b912834b9d3fc1a1d00b374c173b156d87 k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.5],SizeBytes:260976825,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder@sha256:afc413cd2504b586b6f28c16355db243c8bfdef536cc80e44f61e03c01e12235],SizeBytes:229564388,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:20f25f275d46aa728f7615a1ccc19c78b2ed89435bf943a44b339f70f45508e6 k8s.gcr.io/e2e-test-images/httpd@sha256:5d28f127fae41261c56abccd96df481f8f4fba2a3305fece212e11aafe646943 k8s.gcr.io/e2e-test-images/httpd:2.4.39-2],SizeBytes:132295599,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 k8s.gcr.io/e2e-test-images/httpd@sha256:6589a0d3a4b40f996ab554f0d58e8c567c2850c41bc99d05498378a50b7e1cb4 k8s.gcr.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:128894651,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403 k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8 k8s.gcr.io/e2e-test-images/agnhost:2.36],SizeBytes:126252387,},ContainerImage{Names:[quay.io/app-sre/managed-upgrade-operator-registry@sha256:1584ad8c5f28533cad5a72bcf7fc69d03fc7eac87a580897553bdcc01c89f1c6],SizeBytes:126047670,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nautilus@sha256:99c0d6f1ad24a1aa1905d9c6534d193f268f7b23f9add2ae6bb41f31094bdd5c k8s.gcr.io/e2e-test-images/nautilus@sha256:d3190bb7fb53d99b11da40aab7ca81b5f40379345deb8c09349d080e243c9e0c k8s.gcr.io/e2e-test-images/nautilus:1.5],SizeBytes:125565966,},ContainerImage{Names:[quay.io/app-sre/managed-velero-operator-registry@sha256:70df9cc14d07e0212d2f16b2237483a1fe12bb4b0f3aa3fc4240db0d18387aee],SizeBytes:118411702,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator-registry@sha256:22c7691bceb31faeb7413d4e0fa56a52afb9d1bb64c11890874100d15fce0e9c],SizeBytes:117297595,},ContainerImage{Names:[quay.io/app-sre/managed-node-metadata-operator-registry@sha256:bfa8c6e594b330941714269ce0e0dbbc88e65642ce1141496fd88329255f2e04],SizeBytes:117254580,},ContainerImage{Names:[quay.io/app-sre/configure-alertmanager-operator-registry@sha256:3aa6059809b09b01557a31227e624ead2e52867a7e0249f06819a1e554dbc1ae],SizeBytes:111741880,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator@sha256:cfb005c1b5144447dad26c5e7ccbf486bfa82d52673488356fc9522dc8420f69],SizeBytes:111630880,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/sample-apiserver@sha256:48c8ebf147fc7255093e4e8527e99f9a0c74a6d3ea5afd1c1c10b2a8fda9e6f7 k8s.gcr.io/e2e-test-images/sample-apiserver@sha256:f9c93b92b6ff750b41a93c4e4fe0bfe384597aeb841e2539d5444815c55b2d8f k8s.gcr.io/e2e-test-images/sample-apiserver:1.17.5],SizeBytes:56769717,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:7f3ae784f32d750e63495dacd39c3bc57a9a0fc3e7ffc4b6f65e36be03cb368c docker.io/sonobuoy/sonobuoy@sha256:eaa42dd0660ece6c18d06199b78a41bd532a4851fc32a5a99acfafc03556852e docker.io/sonobuoy/sonobuoy:v0.56.14],SizeBytes:49637374,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nginx@sha256:13616070e3f29de4417eee434a8ef472221c9e51b3d037b5a6b46cef08eb7443 k8s.gcr.io/e2e-test-images/nginx@sha256:eee1822ee5bafc780db34f9ab68456c5fdcc0f994e1c1e01d5cde593cb3897a1 k8s.gcr.io/e2e-test-images/nginx:1.14-2],SizeBytes:17245363,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nonewprivs@sha256:8ac1264691820febacf3aea5d152cbde6d10685731ec14966a9401c6f47a68ac k8s.gcr.io/e2e-test-images/nonewprivs@sha256:f6b1c4aef11b116c2a065ea60ed071a8f205444f1897bed9aa2e98a5d78cbdae k8s.gcr.io/e2e-test-images/nonewprivs:1.3],SizeBytes:7373984,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jan 18 23:18:39.440: INFO: 
Logging kubelet events for node ip-10-0-219-147.ec2.internal
Jan 18 23:18:39.442: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-219-147.ec2.internal
Jan 18 23:18:39.460: INFO: tuned-246td started at 2023-01-18 21:03:48 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:18:39.460: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-shjrl started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:18:39.460: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:18:39.460: INFO: network-check-target-cqk2p started at 2023-01-18 21:03:48 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:18:39.460: INFO: node-ca-nzbcp started at 2023-01-18 21:03:48 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:18:39.460: INFO: ovnkube-node-7vtb8 started at 2023-01-18 21:03:48 +0000 UTC (0+5 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Container ovnkube-node ready: true, restart count 4
Jan 18 23:18:39.460: INFO: ingress-canary-fvl4z started at 2023-01-18 23:05:23 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 18 23:18:39.460: INFO: dns-default-hhrdx started at 2023-01-18 23:05:43 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Container dns ready: true, restart count 0
Jan 18 23:18:39.460: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:18:39.460: INFO: node-resolver-svmsb started at 2023-01-18 21:03:48 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:18:39.460: INFO: aws-ebs-csi-driver-node-qprtw started at 2023-01-18 21:03:48 +0000 UTC (0+3 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:18:39.460: INFO: node-exporter-6m8v9 started at 2023-01-18 21:08:32 +0000 UTC (1+2 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:18:39.460: INFO: multus-additional-cni-plugins-6n2cz started at 2023-01-18 21:03:48 +0000 UTC (6+1 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:18:39.460: INFO: machine-config-daemon-nb5xf started at 2023-01-18 21:03:48 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:18:39.460: INFO: sre-dns-latency-exporter-rxh5d started at 2023-01-18 21:18:07 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Container main ready: true, restart count 1
Jan 18 23:18:39.460: INFO: splunkforwarder-ds-b6n2g started at 2023-01-18 21:24:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 23:18:39.460: INFO: sonobuoy started at 2023-01-18 22:14:33 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 23:18:39.460: INFO: network-metrics-daemon-mkvpn started at 2023-01-18 21:03:48 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.460: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:18:39.460: INFO: multus-55cv4 started at 2023-01-18 21:03:48 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.460: INFO: 	Container kube-multus ready: true, restart count 1
W0118 23:18:39.462264      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 18 23:18:39.487: INFO: 
Latency metrics for node ip-10-0-219-147.ec2.internal
Jan 18 23:18:39.487: INFO: 
Logging node info for node ip-10-0-236-5.ec2.internal
Jan 18 23:18:39.489: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-236-5.ec2.internal    3857b2bb-1ebc-43ac-88d7-3457c64fe28c 264174 0 2023-01-18 21:02:48 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-236-5.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-003fca968413295a7","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0d3be9ff8902e98c7"} k8s.ovn.org/host-addresses:["10.0.236.5"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-236-5.ec2.internal","mac-address":"02:6c:41:55:cb:7d","ip-addresses":["10.0.236.5/17"],"ip-address":"10.0.236.5/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:dd4a5019-6dcb-4e50-a381-1ed2a7495c37 k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.6/16"} k8s.ovn.org/node-mgmt-port-mac-address:8a:d5:1e:e7:7c:71 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.236.5/17"} k8s.ovn.org/node-subnets:{"default":"10.128.8.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-worker-us-east-1a-2tqnm machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done managed.openshift.com/customlabels: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{ancient-changes Update v1 2023-01-18 21:02:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:machine.openshift.io/machine":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {cloud-network-config-controller Update v1 2023-01-18 21:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{}}}} } {ip-10-0-164-47 Update v1 2023-01-18 21:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {ip-10-0-236-5 Update v1 2023-01-18 21:03:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {machine-config-daemon Update v1 2023-01-18 21:03:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {machine-config-controller Update v1 2023-01-18 21:24:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {manager Update v1 2023-01-18 21:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:managed.openshift.com/customlabels":{}}}} } {kubelet Update v1 2023-01-18 22:51:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-0d3be9ff8902e98c7,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321574121472 0} {<nil>} 314037228Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16487145472 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3920 -3} {<nil>} 3920m DecimalSI},ephemeral-storage: {{289416708846 0} {<nil>} 289416708846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{13697933312 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-18 23:18:28 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-18 23:18:28 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-18 23:18:28 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-18 23:18:28 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.236.5,},NodeAddress{Type:Hostname,Address:ip-10-0-236-5.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-236-5.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec2af1d21417952ad207fe06f4d3ae06,SystemUUID:ec2af1d2-1417-952a-d207-fe06f4d3ae06,BootID:176a37bd-3afd-4445-ab83-1d05946c1425,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e682077b6cbbe34d4c5e45ed10ffa9bc37e113e17e92a1d5330a7a9fffeab8a3],SizeBytes:1294522975,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cad43cf9f6fbf861e09237e05c5765d15af9a8bd7ee82e557443d8ecba381f56],SizeBytes:454652760,},ContainerImage{Names:[image-registry.openshift-image-registry.svc:5000/openshift/cli@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45 image-registry.openshift-image-registry.svc:5000/openshift/cli:latest],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c7df53b796e81ba8301ba74d02317226329bd5752fd31c1b44d028e4832f21c3],SizeBytes:442026998,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f1134e2e28f44375c3bd9a6ee34d7f9972fdbd3305a3ee2b95e6ed3cede02140],SizeBytes:409661973,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:36fc214537c763b3a3f0a9dc7a1bd4378a80428c31b2629df8786a9b09155e6d],SizeBytes:398604719,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab0d2f0b6d01a4a3b9952710cc49e787ebb0e8649a288438774c1ffe0fc3fae0],SizeBytes:392510137,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c956edcee9b9ba5462572b65b6a92983b20ace63dae50e3237bfdbd6d8c0b972],SizeBytes:375087838,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5e1c69d005727e3245604cfca7a63e4f9bc6e15128c7489e41d5e967305089e],SizeBytes:371248553,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f3ebf99182733dff2666e6a5e7e217085e06029362036ad79c91278e69dcbf7],SizeBytes:366270626,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2269a2b10b3ba2a6783dde2a97451f57f5716a20ebef4a82bea20d25df8761fa],SizeBytes:365147114,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:32fa95a3d4ecfebe96152242926addf27789555b12413203927f255a712c8a0c],SizeBytes:358569445,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:91308d35c1e56463f55c1aaa519ff4de7335d43b254c21abdb845fc8c72821a1],SizeBytes:347460509,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:255a95e8b514dcdc4fa12436789115739355c13fae93eccbba660298746d9aa1],SizeBytes:346372629,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a4fdcff8b0038d858d46740c9622d5dc428074dcf7e662deacd9e6c0aff18286],SizeBytes:344400730,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:209e20410ec2d3d7a502f568d2b7fe1cd1beadcb36fff2d1e6f59d77be3200e3],SizeBytes:339844669,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:140f8947593d92e1517e50a201e83bdef8eb965b552a21d3caf346a250d0cf6e],SizeBytes:335139477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01de581f7f0c7e43f809e9346ced7e60c0ecdd7c0078e97a8f64b24d4d3fa0b7],SizeBytes:332949901,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ed634acd9b29e16b578c74b38d5425156cd39bb72c0884a9564331f95f80911],SizeBytes:330558591,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs@sha256:83da6a30accf7d97453051973ba8c2ae1f78fa0472d2aeb0f8fcfdd8ba04e11a docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:324891489,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-initcontainer@sha256:f859874cf8ef92e8e806ff615f33472992917545ec94d461caa8e6e13b8a1983 quay.io/app-sre/managed-prometheus-exporter-initcontainer:latest],SizeBytes:315381355,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:923999744ad1557510fc5c3a95cf978b27c453988489ada899c574ab1d71a218],SizeBytes:312722401,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-base@sha256:8da4f871cc18d3ac288a2fbb5c37ea27eabea4314dabca68f3f8469c7d7a4a21 quay.io/app-sre/managed-prometheus-exporter-base:latest],SizeBytes:303857745,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2550b2cbdf864515b1edacf43c25eb6b6f179713c1df34e51f6e9bba48d6430a],SizeBytes:303075904,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9260bbcc285a9fd4c1c7df865280261723eb7679050905da4acbfe728f96ee1b],SizeBytes:298386295,},ContainerImage{Names:[quay.io/konveyor/velero@sha256:6abd52244096680eeb3c80289999a00f642069edcd3d2e6c6948317b4bdd9bcd],SizeBytes:230325612,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder@sha256:afc413cd2504b586b6f28c16355db243c8bfdef536cc80e44f61e03c01e12235],SizeBytes:229564388,},ContainerImage{Names:[quay.io/app-sre/must-gather-operator@sha256:2e9c61f5bb3d7f95805130843b0368853d38934756e892954cdf3e251941ac43],SizeBytes:187166704,},ContainerImage{Names:[quay.io/app-sre/managed-velero-operator@sha256:587741a4e6772774f01efd91ed9b93ed5d0a21fcf9ff1f3bebaf14eb98466132],SizeBytes:155001069,},ContainerImage{Names:[quay.io/konveyor/velero-plugin-for-aws@sha256:7c22d5ae59862a66bac77e3fb48e6cd9c1556e4c9d7277aad4f093a198cb4373],SizeBytes:149334789,},ContainerImage{Names:[quay.io/app-sre/custom-domains-operator@sha256:59158acb6a90927f89d85ffaf448e4831b8e8b352d4539b597b97e5f5ff92f6b],SizeBytes:147473321,},ContainerImage{Names:[quay.io/app-sre/ocm-agent-operator@sha256:d697996b5051ee1fdf174a7d23fdd0f0fe612876e8b0d8e94a0c92fee72ea011],SizeBytes:143355987,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 k8s.gcr.io/e2e-test-images/httpd@sha256:6589a0d3a4b40f996ab554f0d58e8c567c2850c41bc99d05498378a50b7e1cb4 k8s.gcr.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:128894651,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403 k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8 k8s.gcr.io/e2e-test-images/agnhost:2.36],SizeBytes:126252387,},ContainerImage{Names:[quay.io/app-sre/managed-upgrade-operator-registry@sha256:1584ad8c5f28533cad5a72bcf7fc69d03fc7eac87a580897553bdcc01c89f1c6],SizeBytes:126047670,},ContainerImage{Names:[quay.io/app-sre/ocm-agent-operator-registry@sha256:1b80b1e0493e7803f6a498ab5617b30009e9b2bcccca93e82057317eadf1a2cb],SizeBytes:118500277,},ContainerImage{Names:[quay.io/app-sre/custom-domains-operator-registry@sha256:365524ae13f1b5e5b252190c828e7ce9a5c1916a6aba32566326240d676059dc],SizeBytes:118428594,},ContainerImage{Names:[quay.io/app-sre/managed-velero-operator-registry@sha256:70df9cc14d07e0212d2f16b2237483a1fe12bb4b0f3aa3fc4240db0d18387aee],SizeBytes:118411702,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jan 18 23:18:39.490: INFO: 
Logging kubelet events for node ip-10-0-236-5.ec2.internal
Jan 18 23:18:39.493: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-236-5.ec2.internal
Jan 18 23:18:39.517: INFO: sre-ebs-iops-reporter-1-l2chd started at 2023-01-18 21:31:18 +0000 UTC (1+1 container statuses recorded)
Jan 18 23:18:39.517: INFO: 	Init container setupcreds ready: true, restart count 0
Jan 18 23:18:39.517: INFO: 	Container main ready: true, restart count 0
Jan 18 23:18:39.517: INFO: managed-upgrade-operator-catalog-g29pr started at 2023-01-18 21:31:23 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.517: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.517: INFO: dns-default-4cddc started at 2023-01-18 21:04:10 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.517: INFO: 	Container dns ready: true, restart count 1
Jan 18 23:18:39.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.517: INFO: splunkforwarder-ds-zhvhl started at 2023-01-18 21:25:56 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.517: INFO: 	Container splunk-uf ready: true, restart count 0
Jan 18 23:18:39.517: INFO: obo-prometheus-operator-6cb5cfc7b9-2rkdt started at 2023-01-18 21:26:20 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.517: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 18 23:18:39.517: INFO: addon-operator-catalog-67nfj started at 2023-01-18 21:26:21 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.517: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.517: INFO: ocm-agent-operator-9bd68bf49-z6qxl started at 2023-01-18 21:26:19 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.517: INFO: 	Container ocm-agent-operator ready: true, restart count 0
Jan 18 23:18:39.517: INFO: machine-config-daemon-xv85b started at 2023-01-18 21:03:23 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.517: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:18:39.517: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:18:39.517: INFO: observability-operator-5b467d8ccb-twlnn started at 2023-01-18 21:26:20 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.517: INFO: 	Container operator ready: true, restart count 0
Jan 18 23:18:39.518: INFO: rbac-permissions-operator-registry-mqh7k started at 2023-01-18 21:26:26 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.518: INFO: multus-wtvrb started at 2023-01-18 21:03:23 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:18:39.518: INFO: node-ca-r29sw started at 2023-01-18 21:03:23 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:18:39.518: INFO: obo-prometheus-operator-admission-webhook-5b4dc9bff5-f8pm5 started at 2023-01-18 21:26:21 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 23:18:39.518: INFO: ocm-agent-operator-registry-qj69h started at 2023-01-18 21:26:24 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.518: INFO: network-check-target-tr8pn started at 2023-01-18 21:03:23 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:18:39.518: INFO: sre-stuck-ebs-vols-1-mjmmn started at 2023-01-18 21:26:18 +0000 UTC (1+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Init container setupcreds ready: true, restart count 0
Jan 18 23:18:39.518: INFO: 	Container main ready: true, restart count 0
Jan 18 23:18:39.518: INFO: osd-cluster-ready-kfqk7 started at 2023-01-18 21:26:19 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container osd-cluster-ready ready: false, restart count 26
Jan 18 23:18:39.518: INFO: aws-ebs-csi-driver-node-cv9gd started at 2023-01-18 21:03:23 +0000 UTC (0+3 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:18:39.518: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:18:39.518: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:18:39.518: INFO: ovnkube-node-g6c8t started at 2023-01-18 21:03:23 +0000 UTC (0+5 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.518: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:18:39.518: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:18:39.518: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:18:39.518: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:18:39.518: INFO: sre-dns-latency-exporter-q7lp5 started at 2023-01-18 21:18:07 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container main ready: true, restart count 1
Jan 18 23:18:39.518: INFO: node-resolver-8lsl6 started at 2023-01-18 21:03:23 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:18:39.518: INFO: configure-alertmanager-operator-registry-vzl8c started at 2023-01-18 21:31:25 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.518: INFO: downloads-6f74f6fcbf-w6ht7 started at 2023-01-18 21:31:17 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container download-server ready: true, restart count 0
Jan 18 23:18:39.518: INFO: managed-node-metadata-operator-registry-fn84g started at 2023-01-18 21:31:22 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.518: INFO: network-metrics-daemon-dxnrq started at 2023-01-18 21:03:23 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.518: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:18:39.518: INFO: multus-additional-cni-plugins-p78ph started at 2023-01-18 21:03:23 +0000 UTC (6+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 18 23:18:39.518: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 18 23:18:39.518: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 18 23:18:39.518: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 18 23:18:39.518: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 18 23:18:39.518: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 18 23:18:39.518: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:18:39.518: INFO: splunk-forwarder-operator-catalog-twzbw started at 2023-01-18 21:26:27 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.518: INFO: tuned-b2gvw started at 2023-01-18 21:03:23 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:18:39.518: INFO: custom-domains-operator-7f97f586c8-czz24 started at 2023-01-18 21:26:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container custom-domains-operator ready: true, restart count 0
Jan 18 23:18:39.518: INFO: observability-operator-catalog-khgpx started at 2023-01-18 21:26:25 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.518: INFO: must-gather-operator-registry-gp8pc started at 2023-01-18 21:26:22 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.518: INFO: custom-domains-operator-registry-sd6db started at 2023-01-18 21:26:24 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.518: INFO: managed-velero-operator-registry-9j2kp started at 2023-01-18 21:31:21 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.518: INFO: node-exporter-6q46p started at 2023-01-18 21:08:32 +0000 UTC (1+2 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 18 23:18:39.518: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.518: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:18:39.518: INFO: cloud-ingress-operator-registry-6f5sh started at 2023-01-18 21:31:22 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:18:39.518: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-t5zp4 started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:18:39.518: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:18:39.518: INFO: ingress-canary-d6jm7 started at 2023-01-18 21:04:10 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 23:18:39.518: INFO: must-gather-operator-5f47db765d-zf7fj started at 2023-01-18 21:26:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.518: INFO: 	Container must-gather-operator ready: true, restart count 0
W0118 23:18:39.520172      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 18 23:18:39.542: INFO: 
Latency metrics for node ip-10-0-236-5.ec2.internal
Jan 18 23:18:39.542: INFO: 
Logging node info for node ip-10-0-253-152.ec2.internal
Jan 18 23:18:39.544: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-253-152.ec2.internal    97b1bb86-6430-4e5f-9977-2fd85f6253db 263394 0 2023-01-18 21:03:02 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-253-152.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-0a8bc1206b380c762","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0e74592248ae3d3ce"} k8s.ovn.org/host-addresses:["10.0.253.152"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-253-152.ec2.internal","mac-address":"02:e2:0c:32:ec:b1","ip-addresses":["10.0.253.152/17"],"ip-address":"10.0.253.152/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:8e77c1cf-2007-41db-be37-b3782a8818cd k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.7/16"} k8s.ovn.org/node-mgmt-port-mac-address:3a:95:1e:aa:38:c3 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.253.152/17"} k8s.ovn.org/node-subnets:{"default":"10.128.10.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-worker-us-east-1a-wrmcr machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done managed.openshift.com/customlabels: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{ancient-changes Update v1 2023-01-18 21:03:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{},"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {kubelet Update v1 2023-01-18 21:03:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {nodelink-controller Update v1 2023-01-18 21:03:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}} } {ip-10-0-253-152 Update v1 2023-01-18 21:03:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {machine-config-daemon Update v1 2023-01-18 21:03:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {manager Update v1 2023-01-18 21:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:managed.openshift.com/customlabels":{}}}} } {machine-config-controller Update v1 2023-01-18 21:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {kubelet Update v1 2023-01-18 22:51:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-0e74592248ae3d3ce,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321574121472 0} {<nil>} 314037228Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16487137280 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3920 -3} {<nil>} 3920m DecimalSI},ephemeral-storage: {{289416708846 0} {<nil>} 289416708846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{13697925120 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-18 23:16:59 +0000 UTC,LastTransitionTime:2023-01-18 21:30:40 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-18 23:16:59 +0000 UTC,LastTransitionTime:2023-01-18 21:30:40 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-18 23:16:59 +0000 UTC,LastTransitionTime:2023-01-18 21:30:40 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-18 23:16:59 +0000 UTC,LastTransitionTime:2023-01-18 21:30:40 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.253.152,},NodeAddress{Type:Hostname,Address:ip-10-0-253-152.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-253-152.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec24fa9bda8e441ce667ebd3ad1fbec6,SystemUUID:ec24fa9b-da8e-441c-e667-ebd3ad1fbec6,BootID:0478c97d-2400-438e-8e0d-403cc784e17a,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b741193c0e8f5dd0b254c4185b936a9dd2907f53296a82d2db26bb290a5040a2],SizeBytes:733158564,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08ab62da9265ef67f4eb4b526b38346a3ae3d35e2d5bfbdd58a0858b76b21f77],SizeBytes:681959546,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cad43cf9f6fbf861e09237e05c5765d15af9a8bd7ee82e557443d8ecba381f56],SizeBytes:454652760,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f1134e2e28f44375c3bd9a6ee34d7f9972fdbd3305a3ee2b95e6ed3cede02140],SizeBytes:409661973,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:36fc214537c763b3a3f0a9dc7a1bd4378a80428c31b2629df8786a9b09155e6d],SizeBytes:398604719,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab0d2f0b6d01a4a3b9952710cc49e787ebb0e8649a288438774c1ffe0fc3fae0],SizeBytes:392510137,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c956edcee9b9ba5462572b65b6a92983b20ace63dae50e3237bfdbd6d8c0b972],SizeBytes:375087838,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5e1c69d005727e3245604cfca7a63e4f9bc6e15128c7489e41d5e967305089e],SizeBytes:371248553,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:77c5db690d9438ac077736cad8f28c04de476c04c3a97f39910ed86b6c395b85],SizeBytes:368328246,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f3ebf99182733dff2666e6a5e7e217085e06029362036ad79c91278e69dcbf7],SizeBytes:366270626,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:91308d35c1e56463f55c1aaa519ff4de7335d43b254c21abdb845fc8c72821a1],SizeBytes:347460509,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:255a95e8b514dcdc4fa12436789115739355c13fae93eccbba660298746d9aa1],SizeBytes:346372629,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a4fdcff8b0038d858d46740c9622d5dc428074dcf7e662deacd9e6c0aff18286],SizeBytes:344400730,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:209e20410ec2d3d7a502f568d2b7fe1cd1beadcb36fff2d1e6f59d77be3200e3],SizeBytes:339844669,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:140f8947593d92e1517e50a201e83bdef8eb965b552a21d3caf346a250d0cf6e],SizeBytes:335139477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01de581f7f0c7e43f809e9346ced7e60c0ecdd7c0078e97a8f64b24d4d3fa0b7],SizeBytes:332949901,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ed634acd9b29e16b578c74b38d5425156cd39bb72c0884a9564331f95f80911],SizeBytes:330558591,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs@sha256:83da6a30accf7d97453051973ba8c2ae1f78fa0472d2aeb0f8fcfdd8ba04e11a docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:324891489,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-initcontainer@sha256:f859874cf8ef92e8e806ff615f33472992917545ec94d461caa8e6e13b8a1983 quay.io/app-sre/managed-prometheus-exporter-initcontainer:latest],SizeBytes:315381355,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:923999744ad1557510fc5c3a95cf978b27c453988489ada899c574ab1d71a218],SizeBytes:312722401,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-base@sha256:8da4f871cc18d3ac288a2fbb5c37ea27eabea4314dabca68f3f8469c7d7a4a21 quay.io/app-sre/managed-prometheus-exporter-base:latest],SizeBytes:303857745,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2550b2cbdf864515b1edacf43c25eb6b6f179713c1df34e51f6e9bba48d6430a],SizeBytes:303075904,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9260bbcc285a9fd4c1c7df865280261723eb7679050905da4acbfe728f96ee1b],SizeBytes:298386295,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder@sha256:afc413cd2504b586b6f28c16355db243c8bfdef536cc80e44f61e03c01e12235],SizeBytes:229564388,},ContainerImage{Names:[quay.io/app-sre/must-gather-operator@sha256:2e9c61f5bb3d7f95805130843b0368853d38934756e892954cdf3e251941ac43],SizeBytes:187166704,},ContainerImage{Names:[quay.io/app-sre/custom-domains-operator@sha256:59158acb6a90927f89d85ffaf448e4831b8e8b352d4539b597b97e5f5ff92f6b],SizeBytes:147473321,},ContainerImage{Names:[quay.io/app-sre/ocm-agent-operator@sha256:d697996b5051ee1fdf174a7d23fdd0f0fe612876e8b0d8e94a0c92fee72ea011],SizeBytes:143355987,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 k8s.gcr.io/e2e-test-images/httpd@sha256:6589a0d3a4b40f996ab554f0d58e8c567c2850c41bc99d05498378a50b7e1cb4 k8s.gcr.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:128894651,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403 k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8 k8s.gcr.io/e2e-test-images/agnhost:2.36],SizeBytes:126252387,},ContainerImage{Names:[quay.io/app-sre/ocm-agent-operator-registry@sha256:1b80b1e0493e7803f6a498ab5617b30009e9b2bcccca93e82057317eadf1a2cb],SizeBytes:118500277,},ContainerImage{Names:[quay.io/app-sre/custom-domains-operator-registry@sha256:365524ae13f1b5e5b252190c828e7ce9a5c1916a6aba32566326240d676059dc],SizeBytes:118428594,},ContainerImage{Names:[quay.io/app-sre/must-gather-operator-registry@sha256:b8833c045365b8f0eaa0ca353672d3bf117bf856fcfc66369c438cfb1fc39649],SizeBytes:117960118,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator@sha256:cfb005c1b5144447dad26c5e7ccbf486bfa82d52673488356fc9522dc8420f69],SizeBytes:111630880,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder-operator-registry@sha256:883305a3920f97afac14ab10d39b74c4a34d30c342e35f8fa225a1d9f01c3730],SizeBytes:110390713,},ContainerImage{Names:[quay.io/app-sre/rbac-permissions-operator-registry@sha256:75a26e1a178730f3e8d81d3b41aae78a5b05b8812569d4252b42a1fbe2b1452d],SizeBytes:108948408,},ContainerImage{Names:[quay.io/app-sre/osd-cluster-ready@sha256:5a346ccfce791546a717d8b1a156707a3a1a9b0efd2f833d60ca4d32f0ff751c],SizeBytes:97521241,},ContainerImage{Names:[quay.io/rhobs/observability-operator-catalog@sha256:f1caec528bfdf530da8d99456b293185715712989484ffa3df9c21f4e8d77030],SizeBytes:78895244,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jan 18 23:18:39.545: INFO: 
Logging kubelet events for node ip-10-0-253-152.ec2.internal
Jan 18 23:18:39.554: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-253-152.ec2.internal
Jan 18 23:18:39.573: INFO: network-metrics-daemon-cq5sl started at 2023-01-18 21:03:06 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:18:39.573: INFO: network-check-target-9fq7l started at 2023-01-18 21:03:06 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:18:39.573: INFO: image-registry-7b8f8dcdc5-2bvm7 started at 2023-01-18 21:31:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container registry ready: true, restart count 0
Jan 18 23:18:39.573: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-ktxs4 started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:18:39.573: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:18:39.573: INFO: tuned-5wpdt started at 2023-01-18 21:03:06 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:18:39.573: INFO: multus-xzh2h started at 2023-01-18 21:03:06 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:18:39.573: INFO: aws-ebs-csi-driver-node-97pk2 started at 2023-01-18 21:03:06 +0000 UTC (0+3 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:18:39.573: INFO: dns-default-g4nw5 started at 2023-01-18 21:03:53 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container dns ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.573: INFO: node-exporter-z5k8l started at 2023-01-18 21:08:32 +0000 UTC (1+2 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:18:39.573: INFO: sre-dns-latency-exporter-zsmln started at 2023-01-18 21:18:07 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container main ready: true, restart count 1
Jan 18 23:18:39.573: INFO: multus-additional-cni-plugins-hsjtm started at 2023-01-18 21:03:06 +0000 UTC (6+1 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:18:39.573: INFO: node-resolver-ffd24 started at 2023-01-18 21:03:06 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:18:39.573: INFO: splunkforwarder-ds-xsjtq started at 2023-01-18 21:24:58 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 23:18:39.573: INFO: cloud-ingress-operator-78d58985cd-9kp9q started at 2023-01-18 21:31:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Jan 18 23:18:39.573: INFO: ovnkube-node-mtfln started at 2023-01-18 21:03:06 +0000 UTC (0+5 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Container ovn-acl-logging ready: true, restart count 2
Jan 18 23:18:39.573: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:18:39.573: INFO: ingress-canary-vz8dj started at 2023-01-18 21:03:53 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 23:18:39.573: INFO: network-check-source-5cb989cf6f-42gl2 started at 2023-01-18 21:31:18 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container check-endpoints ready: true, restart count 0
Jan 18 23:18:39.573: INFO: machine-config-daemon-5zsxx started at 2023-01-18 21:03:06 +0000 UTC (0+2 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:18:39.573: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:18:39.573: INFO: node-ca-bh7jz started at 2023-01-18 21:03:06 +0000 UTC (0+1 container statuses recorded)
Jan 18 23:18:39.573: INFO: 	Container node-ca ready: true, restart count 1
W0118 23:18:39.576060      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 18 23:18:39.597: INFO: 
Latency metrics for node ip-10-0-253-152.ec2.internal
Jan 18 23:18:39.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8798" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• Failure [218.886 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance] [It]
  test/e2e/framework/framework.go:652

  Jan 18 23:18:36.159: Session is sticky after reaching the timeout

  test/e2e/network/service.go:3367
------------------------------
{"msg":"FAILED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":187,"skipped":3579,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:18:39.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:18:39.638: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-cc74ec1a-6953-4e71-9378-9ba87d2a9a65
STEP: Creating configMap with name cm-test-opt-upd-74ca0260-e2ba-4d76-bf3f-772d5ce25eba
STEP: Creating the pod
Jan 18 23:18:39.710: INFO: The status of Pod pod-projected-configmaps-4de2ebff-aa22-4985-9efc-02f27405e160 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:18:41.713: INFO: The status of Pod pod-projected-configmaps-4de2ebff-aa22-4985-9efc-02f27405e160 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:18:43.714: INFO: The status of Pod pod-projected-configmaps-4de2ebff-aa22-4985-9efc-02f27405e160 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-cc74ec1a-6953-4e71-9378-9ba87d2a9a65
STEP: Updating configmap cm-test-opt-upd-74ca0260-e2ba-4d76-bf3f-772d5ce25eba
STEP: Creating configMap with name cm-test-opt-create-156cea90-35cb-43fe-8c4c-f3bc10c12ea5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 18 23:20:02.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-415" for this suite.

• [SLOW TEST:82.434 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":188,"skipped":3593,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:20:02.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 18 23:20:18.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8003" for this suite.

• [SLOW TEST:16.138 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":356,"completed":189,"skipped":3606,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:20:18.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 18 23:20:22.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3560" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":356,"completed":190,"skipped":3640,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:20:22.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's args
Jan 18 23:20:22.399: INFO: Waiting up to 5m0s for pod "var-expansion-dba4273d-a212-4f47-b94a-7c16dbfbc97c" in namespace "var-expansion-1865" to be "Succeeded or Failed"
Jan 18 23:20:22.422: INFO: Pod "var-expansion-dba4273d-a212-4f47-b94a-7c16dbfbc97c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.980554ms
Jan 18 23:20:24.429: INFO: Pod "var-expansion-dba4273d-a212-4f47-b94a-7c16dbfbc97c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030240834s
Jan 18 23:20:26.432: INFO: Pod "var-expansion-dba4273d-a212-4f47-b94a-7c16dbfbc97c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033045044s
Jan 18 23:20:28.435: INFO: Pod "var-expansion-dba4273d-a212-4f47-b94a-7c16dbfbc97c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0359094s
STEP: Saw pod success
Jan 18 23:20:28.435: INFO: Pod "var-expansion-dba4273d-a212-4f47-b94a-7c16dbfbc97c" satisfied condition "Succeeded or Failed"
Jan 18 23:20:28.437: INFO: Trying to get logs from node ip-10-0-211-217.ec2.internal pod var-expansion-dba4273d-a212-4f47-b94a-7c16dbfbc97c container dapi-container: <nil>
STEP: delete the pod
Jan 18 23:20:28.454: INFO: Waiting for pod var-expansion-dba4273d-a212-4f47-b94a-7c16dbfbc97c to disappear
Jan 18 23:20:28.455: INFO: Pod var-expansion-dba4273d-a212-4f47-b94a-7c16dbfbc97c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 18 23:20:28.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1865" for this suite.

• [SLOW TEST:6.214 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":356,"completed":191,"skipped":3679,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:20:28.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 18 23:20:28.543: INFO: Waiting up to 5m0s for pod "pod-28984ae1-3a89-40b5-9d34-ebb521ddc092" in namespace "emptydir-1397" to be "Succeeded or Failed"
Jan 18 23:20:28.552: INFO: Pod "pod-28984ae1-3a89-40b5-9d34-ebb521ddc092": Phase="Pending", Reason="", readiness=false. Elapsed: 8.760429ms
Jan 18 23:20:30.555: INFO: Pod "pod-28984ae1-3a89-40b5-9d34-ebb521ddc092": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012122301s
Jan 18 23:20:32.558: INFO: Pod "pod-28984ae1-3a89-40b5-9d34-ebb521ddc092": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015175134s
Jan 18 23:20:34.561: INFO: Pod "pod-28984ae1-3a89-40b5-9d34-ebb521ddc092": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018034965s
STEP: Saw pod success
Jan 18 23:20:34.561: INFO: Pod "pod-28984ae1-3a89-40b5-9d34-ebb521ddc092" satisfied condition "Succeeded or Failed"
Jan 18 23:20:34.563: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-28984ae1-3a89-40b5-9d34-ebb521ddc092 container test-container: <nil>
STEP: delete the pod
Jan 18 23:20:34.580: INFO: Waiting for pod pod-28984ae1-3a89-40b5-9d34-ebb521ddc092 to disappear
Jan 18 23:20:34.582: INFO: Pod pod-28984ae1-3a89-40b5-9d34-ebb521ddc092 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 18 23:20:34.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1397" for this suite.

• [SLOW TEST:6.128 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":192,"skipped":3689,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:20:34.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
W0118 23:20:34.690535      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "container1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "container1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "container1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "container1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jan 18 23:20:54.884: INFO: EndpointSlice for Service endpointslice-4074/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 18 23:21:04.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4074" for this suite.

• [SLOW TEST:30.313 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":356,"completed":193,"skipped":3704,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:21:04.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 23:21:05.468: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 23:21:07.477: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 21, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 21, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 21, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 21, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 23:21:10.489: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:21:10.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4251-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:21:13.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2101" for this suite.
STEP: Destroying namespace "webhook-2101-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.713 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":356,"completed":194,"skipped":3706,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:21:13.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 18 23:21:29.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3698" for this suite.

• [SLOW TEST:16.217 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":356,"completed":195,"skipped":3712,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:21:29.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 18 23:21:29.907: INFO: Waiting up to 5m0s for pod "downward-api-a9cd5943-a54e-4ef8-a277-fff04f088b07" in namespace "downward-api-8275" to be "Succeeded or Failed"
Jan 18 23:21:29.921: INFO: Pod "downward-api-a9cd5943-a54e-4ef8-a277-fff04f088b07": Phase="Pending", Reason="", readiness=false. Elapsed: 13.381284ms
Jan 18 23:21:31.924: INFO: Pod "downward-api-a9cd5943-a54e-4ef8-a277-fff04f088b07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017211835s
Jan 18 23:21:33.928: INFO: Pod "downward-api-a9cd5943-a54e-4ef8-a277-fff04f088b07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020655082s
Jan 18 23:21:35.932: INFO: Pod "downward-api-a9cd5943-a54e-4ef8-a277-fff04f088b07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025265097s
STEP: Saw pod success
Jan 18 23:21:35.932: INFO: Pod "downward-api-a9cd5943-a54e-4ef8-a277-fff04f088b07" satisfied condition "Succeeded or Failed"
Jan 18 23:21:35.935: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downward-api-a9cd5943-a54e-4ef8-a277-fff04f088b07 container dapi-container: <nil>
STEP: delete the pod
Jan 18 23:21:35.948: INFO: Waiting for pod downward-api-a9cd5943-a54e-4ef8-a277-fff04f088b07 to disappear
Jan 18 23:21:35.952: INFO: Pod downward-api-a9cd5943-a54e-4ef8-a277-fff04f088b07 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 18 23:21:35.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8275" for this suite.

• [SLOW TEST:6.126 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":356,"completed":196,"skipped":3739,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:21:35.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating pod
W0118 23:21:36.048829      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:21:36.080: INFO: The status of Pod pod-hostip-2488574d-a254-4a6b-91cb-84484adf690d is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:21:38.083: INFO: The status of Pod pod-hostip-2488574d-a254-4a6b-91cb-84484adf690d is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:21:40.082: INFO: The status of Pod pod-hostip-2488574d-a254-4a6b-91cb-84484adf690d is Running (Ready = true)
Jan 18 23:21:40.086: INFO: Pod pod-hostip-2488574d-a254-4a6b-91cb-84484adf690d has hostIP: 10.0.219.147
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 18 23:21:40.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8766" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":356,"completed":197,"skipped":3741,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:21:40.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ReplaceConcurrent cronjob
W0118 23:21:40.146553      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 18 23:23:00.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7799" for this suite.

• [SLOW TEST:80.086 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":356,"completed":198,"skipped":3766,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:23:00.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 18 23:23:00.266: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 18 23:23:00.296: INFO: starting watch
STEP: patching
STEP: updating
Jan 18 23:23:00.314: INFO: waiting for watch events with expected annotations
Jan 18 23:23:00.314: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 18 23:23:00.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-511" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":356,"completed":199,"skipped":3768,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:23:00.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5258
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5258
STEP: Waiting until pod test-pod will start running in namespace statefulset-5258
STEP: Creating statefulset with conflicting port in namespace statefulset-5258
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5258
Jan 18 23:23:04.514: INFO: Observed stateful pod in namespace: statefulset-5258, name: ss-0, uid: b5fbe4b7-8330-4f96-bc82-d1e78c95dac0, status phase: Pending. Waiting for statefulset controller to delete.
Jan 18 23:23:04.529: INFO: Observed stateful pod in namespace: statefulset-5258, name: ss-0, uid: b5fbe4b7-8330-4f96-bc82-d1e78c95dac0, status phase: Pending. Waiting for statefulset controller to delete.
Jan 18 23:23:04.537: INFO: Observed stateful pod in namespace: statefulset-5258, name: ss-0, uid: b5fbe4b7-8330-4f96-bc82-d1e78c95dac0, status phase: Failed. Waiting for statefulset controller to delete.
Jan 18 23:23:04.544: INFO: Observed stateful pod in namespace: statefulset-5258, name: ss-0, uid: b5fbe4b7-8330-4f96-bc82-d1e78c95dac0, status phase: Failed. Waiting for statefulset controller to delete.
Jan 18 23:23:04.549: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5258
STEP: Removing pod with conflicting port in namespace statefulset-5258
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5258 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 23:23:08.573: INFO: Deleting all statefulset in ns statefulset-5258
Jan 18 23:23:08.575: INFO: Scaling statefulset ss to 0
Jan 18 23:23:18.589: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 23:23:18.592: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 18 23:23:18.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5258" for this suite.

• [SLOW TEST:18.231 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":356,"completed":200,"skipped":3809,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:23:18.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-4v86
STEP: Creating a pod to test atomic-volume-subpath
Jan 18 23:23:18.691: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4v86" in namespace "subpath-7995" to be "Succeeded or Failed"
Jan 18 23:23:18.695: INFO: Pod "pod-subpath-test-configmap-4v86": Phase="Pending", Reason="", readiness=false. Elapsed: 4.780393ms
Jan 18 23:23:20.698: INFO: Pod "pod-subpath-test-configmap-4v86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007140825s
Jan 18 23:23:22.700: INFO: Pod "pod-subpath-test-configmap-4v86": Phase="Running", Reason="", readiness=true. Elapsed: 4.009925815s
Jan 18 23:23:24.705: INFO: Pod "pod-subpath-test-configmap-4v86": Phase="Running", Reason="", readiness=true. Elapsed: 6.014819736s
Jan 18 23:23:26.709: INFO: Pod "pod-subpath-test-configmap-4v86": Phase="Running", Reason="", readiness=true. Elapsed: 8.018205872s
Jan 18 23:23:28.711: INFO: Pod "pod-subpath-test-configmap-4v86": Phase="Running", Reason="", readiness=true. Elapsed: 10.02062857s
Jan 18 23:23:30.714: INFO: Pod "pod-subpath-test-configmap-4v86": Phase="Running", Reason="", readiness=true. Elapsed: 12.023416717s
Jan 18 23:23:32.717: INFO: Pod "pod-subpath-test-configmap-4v86": Phase="Running", Reason="", readiness=true. Elapsed: 14.026260104s
Jan 18 23:23:34.720: INFO: Pod "pod-subpath-test-configmap-4v86": Phase="Running", Reason="", readiness=true. Elapsed: 16.02987053s
Jan 18 23:23:36.724: INFO: Pod "pod-subpath-test-configmap-4v86": Phase="Running", Reason="", readiness=true. Elapsed: 18.033174075s
Jan 18 23:23:38.727: INFO: Pod "pod-subpath-test-configmap-4v86": Phase="Running", Reason="", readiness=true. Elapsed: 20.036109438s
Jan 18 23:23:40.748: INFO: Pod "pod-subpath-test-configmap-4v86": Phase="Running", Reason="", readiness=true. Elapsed: 22.057938816s
Jan 18 23:23:42.751: INFO: Pod "pod-subpath-test-configmap-4v86": Phase="Running", Reason="", readiness=false. Elapsed: 24.060316366s
Jan 18 23:23:44.756: INFO: Pod "pod-subpath-test-configmap-4v86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.065799701s
STEP: Saw pod success
Jan 18 23:23:44.756: INFO: Pod "pod-subpath-test-configmap-4v86" satisfied condition "Succeeded or Failed"
Jan 18 23:23:44.758: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-subpath-test-configmap-4v86 container test-container-subpath-configmap-4v86: <nil>
STEP: delete the pod
Jan 18 23:23:44.777: INFO: Waiting for pod pod-subpath-test-configmap-4v86 to disappear
Jan 18 23:23:44.779: INFO: Pod pod-subpath-test-configmap-4v86 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4v86
Jan 18 23:23:44.779: INFO: Deleting pod "pod-subpath-test-configmap-4v86" in namespace "subpath-7995"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 18 23:23:44.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7995" for this suite.

• [SLOW TEST:26.183 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","total":356,"completed":201,"skipped":3861,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:23:44.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Jan 18 23:23:44.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3235 create -f -'
Jan 18 23:23:47.156: INFO: stderr: ""
Jan 18 23:23:47.157: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 18 23:23:47.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:23:47.228: INFO: stderr: ""
Jan 18 23:23:47.228: INFO: stdout: "update-demo-nautilus-kzzkm update-demo-nautilus-wftt2 "
Jan 18 23:23:47.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3235 get pods update-demo-nautilus-kzzkm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:23:47.297: INFO: stderr: ""
Jan 18 23:23:47.297: INFO: stdout: ""
Jan 18 23:23:47.297: INFO: update-demo-nautilus-kzzkm is created but not running
Jan 18 23:23:52.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:23:52.354: INFO: stderr: ""
Jan 18 23:23:52.354: INFO: stdout: "update-demo-nautilus-kzzkm update-demo-nautilus-wftt2 "
Jan 18 23:23:52.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3235 get pods update-demo-nautilus-kzzkm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:23:52.408: INFO: stderr: ""
Jan 18 23:23:52.408: INFO: stdout: "true"
Jan 18 23:23:52.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3235 get pods update-demo-nautilus-kzzkm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:23:52.463: INFO: stderr: ""
Jan 18 23:23:52.463: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 18 23:23:52.463: INFO: validating pod update-demo-nautilus-kzzkm
Jan 18 23:23:52.468: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:23:52.469: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:23:52.469: INFO: update-demo-nautilus-kzzkm is verified up and running
Jan 18 23:23:52.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3235 get pods update-demo-nautilus-wftt2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:23:52.523: INFO: stderr: ""
Jan 18 23:23:52.523: INFO: stdout: "true"
Jan 18 23:23:52.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3235 get pods update-demo-nautilus-wftt2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:23:52.582: INFO: stderr: ""
Jan 18 23:23:52.582: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 18 23:23:52.582: INFO: validating pod update-demo-nautilus-wftt2
Jan 18 23:23:52.587: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:23:52.587: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:23:52.587: INFO: update-demo-nautilus-wftt2 is verified up and running
STEP: using delete to clean up resources
Jan 18 23:23:52.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3235 delete --grace-period=0 --force -f -'
Jan 18 23:23:52.641: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 23:23:52.641: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 18 23:23:52.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3235 get rc,svc -l name=update-demo --no-headers'
Jan 18 23:23:52.715: INFO: stderr: "No resources found in kubectl-3235 namespace.\n"
Jan 18 23:23:52.715: INFO: stdout: ""
Jan 18 23:23:52.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-3235 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 23:23:52.775: INFO: stderr: ""
Jan 18 23:23:52.775: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 23:23:52.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3235" for this suite.

• [SLOW TEST:7.995 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should create and stop a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":356,"completed":202,"skipped":3867,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:23:52.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-20cda70e-c930-4e2f-a813-ae923826d856 in namespace container-probe-5115
Jan 18 23:23:56.861: INFO: Started pod busybox-20cda70e-c930-4e2f-a813-ae923826d856 in namespace container-probe-5115
STEP: checking the pod's current state and verifying that restartCount is present
Jan 18 23:23:56.863: INFO: Initial restart count of pod busybox-20cda70e-c930-4e2f-a813-ae923826d856 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 18 23:27:57.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5115" for this suite.

• [SLOW TEST:244.574 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":203,"skipped":3871,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:27:57.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:27:57.397: INFO: Creating deployment "webserver-deployment"
W0118 23:27:57.417679      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:27:57.417: INFO: Waiting for observed generation 1
Jan 18 23:27:59.437: INFO: Waiting for all required pods to come up
Jan 18 23:27:59.439: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 18 23:28:01.449: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 18 23:28:01.453: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 18 23:28:01.474: INFO: Updating deployment webserver-deployment
Jan 18 23:28:01.474: INFO: Waiting for observed generation 2
Jan 18 23:28:03.482: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 18 23:28:03.503: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 18 23:28:03.516: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 18 23:28:03.524: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 18 23:28:03.524: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 18 23:28:03.526: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 18 23:28:03.530: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 18 23:28:03.530: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 18 23:28:03.539: INFO: Updating deployment webserver-deployment
Jan 18 23:28:03.539: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 18 23:28:03.544: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 18 23:28:05.549: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 23:28:05.554: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6866  785e7e2e-4e3a-48a5-931c-0d5db9c29674 271662 3 2023-01-18 23:27:57 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00adf19c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 23:28:03 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-57ccb67bb8" is progressing.,LastUpdateTime:2023-01-18 23:28:03 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 18 23:28:05.559: INFO: New ReplicaSet "webserver-deployment-57ccb67bb8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-57ccb67bb8  deployment-6866  853e84e2-af89-48d4-9aa3-94f40a7e9de4 271652 3 2023-01-18 23:28:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 785e7e2e-4e3a-48a5-931c-0d5db9c29674 0xc00372e2f7 0xc00372e2f8}] []  [{kube-controller-manager Update apps/v1 2023-01-18 23:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"785e7e2e-4e3a-48a5-931c-0d5db9c29674\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:28:01 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 57ccb67bb8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00372e398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:28:05.559: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 18 23:28:05.559: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-55df494869  deployment-6866  52235760-7348-499f-a01d-c4ef5c28ea23 271657 3 2023-01-18 23:27:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 785e7e2e-4e3a-48a5-931c-0d5db9c29674 0xc00372e207 0xc00372e208}] []  [{kube-controller-manager Update apps/v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"785e7e2e-4e3a-48a5-931c-0d5db9c29674\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:27:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00372e298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:28:05.564: INFO: Pod "webserver-deployment-55df494869-45mq2" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-45mq2 webserver-deployment-55df494869- deployment-6866  f2f427c4-32d1-4c16-8f1e-ce757857a1f1 271650 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.16.133/23"],"mac_address":"0a:58:0a:80:10:85","gateway_ips":["10.128.16.1"],"ip_address":"10.128.16.133/23","gateway_ip":"10.128.16.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc00372e877 0xc00372e878}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pprbb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pprbb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-211-217.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.211.217,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.564: INFO: Pod "webserver-deployment-55df494869-4p9ks" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-4p9ks webserver-deployment-55df494869- deployment-6866  f5370c36-8116-4255-a489-31b5b81d549a 271604 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.16.130/23"],"mac_address":"0a:58:0a:80:10:82","gateway_ips":["10.128.16.1"],"ip_address":"10.128.16.130/23","gateway_ip":"10.128.16.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc00372ea97 0xc00372ea98}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-shhx7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-shhx7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-211-217.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.211.217,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.564: INFO: Pod "webserver-deployment-55df494869-4xkg5" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-4xkg5 webserver-deployment-55df494869- deployment-6866  9d4b8137-a0bc-42a7-88b6-bc97e56ab5a6 271336 0 2023-01-18 23:27:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.63/23"],"mac_address":"0a:58:0a:80:08:3f","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.63/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.63"
    ],
    "mac": "0a:58:0a:80:08:3f",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.63"
    ],
    "mac": "0a:58:0a:80:08:3f",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc00372ecb7 0xc00372ecb8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:27:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-18 23:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b87x7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b87x7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-236-5.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.236.5,PodIP:10.128.8.63,StartTime:2023-01-18 23:27:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:27:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://9c2bc408081714127af816abb18bf864736aa8727f696c8098e3cf16be8778b2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.565: INFO: Pod "webserver-deployment-55df494869-5gd77" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-5gd77 webserver-deployment-55df494869- deployment-6866  546e8ed3-7045-4d18-89f7-5792b1e8f6c1 271749 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.6.68/23"],"mac_address":"0a:58:0a:80:06:44","gateway_ips":["10.128.6.1"],"ip_address":"10.128.6.68/23","gateway_ip":"10.128.6.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.6.68"
    ],
    "mac": "0a:58:0a:80:06:44",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.6.68"
    ],
    "mac": "0a:58:0a:80:06:44",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc00372ef17 0xc00372ef18}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-01-18 23:28:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfd86,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfd86,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-200-13.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.200.13,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.565: INFO: Pod "webserver-deployment-55df494869-8gv7p" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-8gv7p webserver-deployment-55df494869- deployment-6866  2bd839e8-432b-4d5c-94d9-2b62da38f0b3 271654 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.204/23"],"mac_address":"0a:58:0a:80:0c:cc","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.204/23","gateway_ip":"10.128.12.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc00372f157 0xc00372f158}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cxdjj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cxdjj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-219-147.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.219.147,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.565: INFO: Pod "webserver-deployment-55df494869-8vxns" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-8vxns webserver-deployment-55df494869- deployment-6866  d0f11e44-9244-49ec-b582-442beff6e9cb 271635 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.14.92/23"],"mac_address":"0a:58:0a:80:0e:5c","gateway_ips":["10.128.14.1"],"ip_address":"10.128.14.92/23","gateway_ip":"10.128.14.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc00372f377 0xc00372f378}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mrv7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mrv7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-7.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.7,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.565: INFO: Pod "webserver-deployment-55df494869-c66bz" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-c66bz webserver-deployment-55df494869- deployment-6866  0fa2afa6-569f-4643-afd2-a4011461c658 271771 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.6.66/23"],"mac_address":"0a:58:0a:80:06:42","gateway_ips":["10.128.6.1"],"ip_address":"10.128.6.66/23","gateway_ip":"10.128.6.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.6.66"
    ],
    "mac": "0a:58:0a:80:06:42",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.6.66"
    ],
    "mac": "0a:58:0a:80:06:42",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc00372f597 0xc00372f598}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-01-18 23:28:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k6mq5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k6mq5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-200-13.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.200.13,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.565: INFO: Pod "webserver-deployment-55df494869-c7lqv" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-c7lqv webserver-deployment-55df494869- deployment-6866  63ecb751-b1d0-46a5-956d-f033f8667a3c 271390 0 2023-01-18 23:27:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.199/23"],"mac_address":"0a:58:0a:80:0c:c7","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.199/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.199"
    ],
    "mac": "0a:58:0a:80:0c:c7",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.199"
    ],
    "mac": "0a:58:0a:80:0c:c7",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc00372f7d7 0xc00372f7d8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-18 23:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-18 23:28:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.12.199\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rrfxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rrfxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-219-147.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.219.147,PodIP:10.128.12.199,StartTime:2023-01-18 23:27:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:27:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://88760d3a6496ee031d9f592af4997eae75bd83ba752ef0ea05b80327ac64ce5b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.12.199,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.565: INFO: Pod "webserver-deployment-55df494869-hz8dw" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-hz8dw webserver-deployment-55df494869- deployment-6866  ee17dc51-6660-4fe1-a0be-ab863efd5eb1 271384 0 2023-01-18 23:27:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.6.64/23"],"mac_address":"0a:58:0a:80:06:40","gateway_ips":["10.128.6.1"],"ip_address":"10.128.6.64/23","gateway_ip":"10.128.6.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.6.64"
    ],
    "mac": "0a:58:0a:80:06:40",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.6.64"
    ],
    "mac": "0a:58:0a:80:06:40",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc00372fa37 0xc00372fa38}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-18 23:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-18 23:28:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.6.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lc685,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lc685,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-200-13.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.200.13,PodIP:10.128.6.64,StartTime:2023-01-18 23:27:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:28:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://508cc80b362edc9f730d7f61760d07baec06aa35dff78245460388550a793693,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.6.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.565: INFO: Pod "webserver-deployment-55df494869-k6jzq" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-k6jzq webserver-deployment-55df494869- deployment-6866  1c9785b9-2c86-4265-bef9-c386a4c67163 271656 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.14.94/23"],"mac_address":"0a:58:0a:80:0e:5e","gateway_ips":["10.128.14.1"],"ip_address":"10.128.14.94/23","gateway_ip":"10.128.14.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc00372fc97 0xc00372fc98}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wbsbf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wbsbf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-7.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.7,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.566: INFO: Pod "webserver-deployment-55df494869-k7nth" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-k7nth webserver-deployment-55df494869- deployment-6866  9873fdec-a3eb-4a77-a92a-ec89f406898c 271661 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.10.40/23"],"mac_address":"0a:58:0a:80:0a:28","gateway_ips":["10.128.10.1"],"ip_address":"10.128.10.40/23","gateway_ip":"10.128.10.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc00372feb7 0xc00372feb8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p4psv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p4psv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-253-152.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.253.152,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.566: INFO: Pod "webserver-deployment-55df494869-kdnfw" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-kdnfw webserver-deployment-55df494869- deployment-6866  4d1d851d-510a-4014-bb32-855a2a7b1872 271670 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.16.134/23"],"mac_address":"0a:58:0a:80:10:86","gateway_ips":["10.128.16.1"],"ip_address":"10.128.16.134/23","gateway_ip":"10.128.16.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc0089300d7 0xc0089300d8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6sb7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6sb7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-211-217.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.211.217,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.566: INFO: Pod "webserver-deployment-55df494869-lb9cr" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-lb9cr webserver-deployment-55df494869- deployment-6866  3d46bed7-457a-4576-9aa1-c02b063afaae 271371 0 2023-01-18 23:27:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.14.89/23"],"mac_address":"0a:58:0a:80:0e:59","gateway_ips":["10.128.14.1"],"ip_address":"10.128.14.89/23","gateway_ip":"10.128.14.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.14.89"
    ],
    "mac": "0a:58:0a:80:0e:59",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.14.89"
    ],
    "mac": "0a:58:0a:80:0e:59",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc0089302f7 0xc0089302f8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-18 23:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-18 23:28:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.14.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rk4tw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rk4tw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-7.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.7,PodIP:10.128.14.89,StartTime:2023-01-18 23:27:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:28:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://183d868b5685cb53ac36bdafbf35f9829b6c01dbfa28bb08b5e8c062f6a1895f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.14.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.566: INFO: Pod "webserver-deployment-55df494869-lqjg9" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-lqjg9 webserver-deployment-55df494869- deployment-6866  50e5c4c0-17e8-4ca3-9cd3-40584782f582 271402 0 2023-01-18 23:27:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.10.38/23"],"mac_address":"0a:58:0a:80:0a:26","gateway_ips":["10.128.10.1"],"ip_address":"10.128.10.38/23","gateway_ip":"10.128.10.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.10.38"
    ],
    "mac": "0a:58:0a:80:0a:26",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.10.38"
    ],
    "mac": "0a:58:0a:80:0a:26",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc008930557 0xc008930558}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-18 23:28:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-18 23:28:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.10.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lkkzb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lkkzb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-253-152.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.253.152,PodIP:10.128.10.38,StartTime:2023-01-18 23:27:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:28:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://41b03e816685aad2fa814d1bb7341d55a17a61fb8821e2e047bfb6f5341d269a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.10.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.566: INFO: Pod "webserver-deployment-55df494869-mtv6m" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-mtv6m webserver-deployment-55df494869- deployment-6866  032e3b73-946e-48fe-8025-ecfaf5c74afd 271388 0 2023-01-18 23:27:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.200/23"],"mac_address":"0a:58:0a:80:0c:c8","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.200/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.200"
    ],
    "mac": "0a:58:0a:80:0c:c8",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.200"
    ],
    "mac": "0a:58:0a:80:0c:c8",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc0089309a7 0xc0089309a8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-18 23:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-18 23:28:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.12.200\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4h6nb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4h6nb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-219-147.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.219.147,PodIP:10.128.12.200,StartTime:2023-01-18 23:27:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:27:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://f04ede57a54f278454a6ed43f13706325f1cb5965592f322cd16535b339012c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.12.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.566: INFO: Pod "webserver-deployment-55df494869-p78v6" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-p78v6 webserver-deployment-55df494869- deployment-6866  d2082391-8569-4efc-b918-7566863f0f10 271668 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.67/23"],"mac_address":"0a:58:0a:80:08:43","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.67/23","gateway_ip":"10.128.8.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc008930c87 0xc008930c88}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f56kl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f56kl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-236-5.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.236.5,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.567: INFO: Pod "webserver-deployment-55df494869-qnfhx" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-qnfhx webserver-deployment-55df494869- deployment-6866  e8a7d78c-2d25-405e-8fa8-42c47ab07a63 271636 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.203/23"],"mac_address":"0a:58:0a:80:0c:cb","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.203/23","gateway_ip":"10.128.12.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc008930ea7 0xc008930ea8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wk5xp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wk5xp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-219-147.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.219.147,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.567: INFO: Pod "webserver-deployment-55df494869-r7n7m" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-r7n7m webserver-deployment-55df494869- deployment-6866  4fe6d0c3-fe8d-4341-bcdf-bda6fbc39642 271594 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.16.129/23"],"mac_address":"0a:58:0a:80:10:81","gateway_ips":["10.128.16.1"],"ip_address":"10.128.16.129/23","gateway_ip":"10.128.16.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc0089310c7 0xc0089310c8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b6skg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b6skg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-211-217.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.211.217,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.567: INFO: Pod "webserver-deployment-55df494869-tlss2" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-tlss2 webserver-deployment-55df494869- deployment-6866  b27de7cc-76d5-4469-a307-fedefb9fda9b 271338 0 2023-01-18 23:27:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.64/23"],"mac_address":"0a:58:0a:80:08:40","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.64/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.64"
    ],
    "mac": "0a:58:0a:80:08:40",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.64"
    ],
    "mac": "0a:58:0a:80:08:40",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc0089312e7 0xc0089312e8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:27:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-18 23:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nfszz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nfszz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-236-5.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.236.5,PodIP:10.128.8.64,StartTime:2023-01-18 23:27:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:27:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://6c365e2d059ce1e2d43a2cc7a40b499d92d5a7d012e7148e2976c26aab92923a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.567: INFO: Pod "webserver-deployment-55df494869-wrvrk" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-wrvrk webserver-deployment-55df494869- deployment-6866  3c7ca302-1439-4613-b5f5-090984b40b71 271373 0 2023-01-18 23:27:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.14.90/23"],"mac_address":"0a:58:0a:80:0e:5a","gateway_ips":["10.128.14.1"],"ip_address":"10.128.14.90/23","gateway_ip":"10.128.14.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.14.90"
    ],
    "mac": "0a:58:0a:80:0e:5a",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.14.90"
    ],
    "mac": "0a:58:0a:80:0e:5a",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52235760-7348-499f-a01d-c4ef5c28ea23 0xc008931547 0xc008931548}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52235760-7348-499f-a01d-c4ef5c28ea23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-18 23:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-18 23:28:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.14.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhqs5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhqs5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-7.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:27:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.7,PodIP:10.128.14.90,StartTime:2023-01-18 23:27:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:28:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://024425a46b1e3178205cdba9f902702e97292c0d0e058f82216ad6a7c6079310,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.14.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.567: INFO: Pod "webserver-deployment-57ccb67bb8-5sznv" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-5sznv webserver-deployment-57ccb67bb8- deployment-6866  5c6c4ebf-b739-40b7-b27b-6d856eae369c 271768 0 2023-01-18 23:28:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.6.65/23"],"mac_address":"0a:58:0a:80:06:41","gateway_ips":["10.128.6.1"],"ip_address":"10.128.6.65/23","gateway_ip":"10.128.6.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.6.65"
    ],
    "mac": "0a:58:0a:80:06:41",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.6.65"
    ],
    "mac": "0a:58:0a:80:06:41",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 853e84e2-af89-48d4-9aa3-94f40a7e9de4 0xc0089317a7 0xc0089317a8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853e84e2-af89-48d4-9aa3-94f40a7e9de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-18 23:28:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.6.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zjjzs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zjjzs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-200-13.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.200.13,PodIP:10.128.6.65,StartTime:2023-01-18 23:28:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.6.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.567: INFO: Pod "webserver-deployment-57ccb67bb8-6gjxb" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-6gjxb webserver-deployment-57ccb67bb8- deployment-6866  aa588ca3-8ecf-4db6-94d8-045e35f08c1e 271751 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.10.39/23"],"mac_address":"0a:58:0a:80:0a:27","gateway_ips":["10.128.10.1"],"ip_address":"10.128.10.39/23","gateway_ip":"10.128.10.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.10.39"
    ],
    "mac": "0a:58:0a:80:0a:27",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.10.39"
    ],
    "mac": "0a:58:0a:80:0a:27",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 853e84e2-af89-48d4-9aa3-94f40a7e9de4 0xc008931a57 0xc008931a58}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853e84e2-af89-48d4-9aa3-94f40a7e9de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-01-18 23:28:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r2rdp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r2rdp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-253-152.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.253.152,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.568: INFO: Pod "webserver-deployment-57ccb67bb8-9ksw5" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-9ksw5 webserver-deployment-57ccb67bb8- deployment-6866  455228b4-a96c-4509-8115-1ecf963124e3 271644 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.14.93/23"],"mac_address":"0a:58:0a:80:0e:5d","gateway_ips":["10.128.14.1"],"ip_address":"10.128.14.93/23","gateway_ip":"10.128.14.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 853e84e2-af89-48d4-9aa3-94f40a7e9de4 0xc008931cb7 0xc008931cb8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853e84e2-af89-48d4-9aa3-94f40a7e9de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rmrqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rmrqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-7.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.7,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.568: INFO: Pod "webserver-deployment-57ccb67bb8-cr5kw" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-cr5kw webserver-deployment-57ccb67bb8- deployment-6866  0cc52091-d99a-4ddf-85fd-775b95660c34 271745 0 2023-01-18 23:28:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.14.91/23"],"mac_address":"0a:58:0a:80:0e:5b","gateway_ips":["10.128.14.1"],"ip_address":"10.128.14.91/23","gateway_ip":"10.128.14.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.14.91"
    ],
    "mac": "0a:58:0a:80:0e:5b",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.14.91"
    ],
    "mac": "0a:58:0a:80:0e:5b",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 853e84e2-af89-48d4-9aa3-94f40a7e9de4 0xc008931ef7 0xc008931ef8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853e84e2-af89-48d4-9aa3-94f40a7e9de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-18 23:28:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-18 23:28:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.14.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-spm96,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-spm96,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-128-7.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.7,PodIP:10.128.14.91,StartTime:2023-01-18 23:28:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.14.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.568: INFO: Pod "webserver-deployment-57ccb67bb8-f7f9s" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-f7f9s webserver-deployment-57ccb67bb8- deployment-6866  223cc384-43d2-4877-8041-b542766866e0 271600 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.202/23"],"mac_address":"0a:58:0a:80:0c:ca","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.202/23","gateway_ip":"10.128.12.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 853e84e2-af89-48d4-9aa3-94f40a7e9de4 0xc002ecb197 0xc002ecb198}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853e84e2-af89-48d4-9aa3-94f40a7e9de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4ctx4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4ctx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-219-147.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.219.147,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.568: INFO: Pod "webserver-deployment-57ccb67bb8-gs5b6" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-gs5b6 webserver-deployment-57ccb67bb8- deployment-6866  62e77c76-ac0f-4ba7-92d4-8fac9c2d116f 271762 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.66/23"],"mac_address":"0a:58:0a:80:08:42","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.66/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.66"
    ],
    "mac": "0a:58:0a:80:08:42",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.66"
    ],
    "mac": "0a:58:0a:80:08:42",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 853e84e2-af89-48d4-9aa3-94f40a7e9de4 0xc002ecb9c7 0xc002ecb9c8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853e84e2-af89-48d4-9aa3-94f40a7e9de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-01-18 23:28:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nz4wp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nz4wp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-236-5.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.236.5,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.568: INFO: Pod "webserver-deployment-57ccb67bb8-k88gj" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-k88gj webserver-deployment-57ccb67bb8- deployment-6866  772c4f38-0860-4c2c-8f1c-117ff6ae306a 271603 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.16.131/23"],"mac_address":"0a:58:0a:80:10:83","gateway_ips":["10.128.16.1"],"ip_address":"10.128.16.131/23","gateway_ip":"10.128.16.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 853e84e2-af89-48d4-9aa3-94f40a7e9de4 0xc00be6c037 0xc00be6c038}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853e84e2-af89-48d4-9aa3-94f40a7e9de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7r2zr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7r2zr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-211-217.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.211.217,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.568: INFO: Pod "webserver-deployment-57ccb67bb8-kznvh" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-kznvh webserver-deployment-57ccb67bb8- deployment-6866  abdec0f3-3aff-42ea-bf35-f5bbc13c2a8f 271756 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.205/23"],"mac_address":"0a:58:0a:80:0c:cd","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.205/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.205"
    ],
    "mac": "0a:58:0a:80:0c:cd",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.205"
    ],
    "mac": "0a:58:0a:80:0c:cd",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 853e84e2-af89-48d4-9aa3-94f40a7e9de4 0xc00be6c277 0xc00be6c278}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853e84e2-af89-48d4-9aa3-94f40a7e9de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-01-18 23:28:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zjlxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zjlxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-219-147.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.219.147,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.568: INFO: Pod "webserver-deployment-57ccb67bb8-mbvff" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-mbvff webserver-deployment-57ccb67bb8- deployment-6866  bb362083-aca2-4fba-8e74-0ad417b1aafc 271646 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.6.67/23"],"mac_address":"0a:58:0a:80:06:43","gateway_ips":["10.128.6.1"],"ip_address":"10.128.6.67/23","gateway_ip":"10.128.6.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 853e84e2-af89-48d4-9aa3-94f40a7e9de4 0xc00be6c4d7 0xc00be6c4d8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853e84e2-af89-48d4-9aa3-94f40a7e9de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6q4k8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6q4k8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-200-13.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.200.13,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.569: INFO: Pod "webserver-deployment-57ccb67bb8-p27xh" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-p27xh webserver-deployment-57ccb67bb8- deployment-6866  18934c41-10a3-4c30-aa6e-cf99a4cec3f5 271666 0 2023-01-18 23:28:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.16.132/23"],"mac_address":"0a:58:0a:80:10:84","gateway_ips":["10.128.16.1"],"ip_address":"10.128.16.132/23","gateway_ip":"10.128.16.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 853e84e2-af89-48d4-9aa3-94f40a7e9de4 0xc00be6c717 0xc00be6c718}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853e84e2-af89-48d4-9aa3-94f40a7e9de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzdzd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzdzd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-211-217.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.211.217,PodIP:,StartTime:2023-01-18 23:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.569: INFO: Pod "webserver-deployment-57ccb67bb8-pfq64" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-pfq64 webserver-deployment-57ccb67bb8- deployment-6866  42f75d12-a5e8-4536-8320-06750ca2a225 271735 0 2023-01-18 23:28:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.65/23"],"mac_address":"0a:58:0a:80:08:41","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.65/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.65"
    ],
    "mac": "0a:58:0a:80:08:41",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.65"
    ],
    "mac": "0a:58:0a:80:08:41",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 853e84e2-af89-48d4-9aa3-94f40a7e9de4 0xc00be6c977 0xc00be6c978}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853e84e2-af89-48d4-9aa3-94f40a7e9de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lb47l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lb47l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-236-5.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.236.5,PodIP:10.128.8.65,StartTime:2023-01-18 23:28:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.569: INFO: Pod "webserver-deployment-57ccb67bb8-rszgh" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-rszgh webserver-deployment-57ccb67bb8- deployment-6866  e73e6db2-c47b-4c5a-852b-2ffce38d684e 271758 0 2023-01-18 23:28:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.16.128/23"],"mac_address":"0a:58:0a:80:10:80","gateway_ips":["10.128.16.1"],"ip_address":"10.128.16.128/23","gateway_ip":"10.128.16.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.16.128"
    ],
    "mac": "0a:58:0a:80:10:80",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.16.128"
    ],
    "mac": "0a:58:0a:80:10:80",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 853e84e2-af89-48d4-9aa3-94f40a7e9de4 0xc00be6cc07 0xc00be6cc08}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853e84e2-af89-48d4-9aa3-94f40a7e9de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-18 23:28:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.16.128\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zk8px,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zk8px,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-211-217.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.211.217,PodIP:10.128.16.128,StartTime:2023-01-18 23:28:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.16.128,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:28:05.569: INFO: Pod "webserver-deployment-57ccb67bb8-xvddq" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-xvddq webserver-deployment-57ccb67bb8- deployment-6866  b8532fa9-89ce-44eb-81d4-a9ddf6d0090e 271725 0 2023-01-18 23:28:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.201/23"],"mac_address":"0a:58:0a:80:0c:c9","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.201/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.201"
    ],
    "mac": "0a:58:0a:80:0c:c9",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.201"
    ],
    "mac": "0a:58:0a:80:0c:c9",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 853e84e2-af89-48d4-9aa3-94f40a7e9de4 0xc00be6cea7 0xc00be6cea8}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853e84e2-af89-48d4-9aa3-94f40a7e9de4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-18 23:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-18 23:28:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.12.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-btv7r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-btv7r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-219-147.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gn5nn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:28:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.219.147,PodIP:10.128.12.201,StartTime:2023-01-18 23:28:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.12.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 18 23:28:05.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6866" for this suite.

• [SLOW TEST:8.217 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":356,"completed":204,"skipped":3873,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:28:05.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test env composition
Jan 18 23:28:05.728: INFO: Waiting up to 5m0s for pod "var-expansion-8b53964a-736b-4ac9-a54e-c76c45448e06" in namespace "var-expansion-8252" to be "Succeeded or Failed"
Jan 18 23:28:05.731: INFO: Pod "var-expansion-8b53964a-736b-4ac9-a54e-c76c45448e06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.32102ms
Jan 18 23:28:07.734: INFO: Pod "var-expansion-8b53964a-736b-4ac9-a54e-c76c45448e06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005466965s
Jan 18 23:28:09.736: INFO: Pod "var-expansion-8b53964a-736b-4ac9-a54e-c76c45448e06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007979162s
Jan 18 23:28:11.754: INFO: Pod "var-expansion-8b53964a-736b-4ac9-a54e-c76c45448e06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025235639s
STEP: Saw pod success
Jan 18 23:28:11.754: INFO: Pod "var-expansion-8b53964a-736b-4ac9-a54e-c76c45448e06" satisfied condition "Succeeded or Failed"
Jan 18 23:28:11.757: INFO: Trying to get logs from node ip-10-0-211-217.ec2.internal pod var-expansion-8b53964a-736b-4ac9-a54e-c76c45448e06 container dapi-container: <nil>
STEP: delete the pod
Jan 18 23:28:11.804: INFO: Waiting for pod var-expansion-8b53964a-736b-4ac9-a54e-c76c45448e06 to disappear
Jan 18 23:28:11.808: INFO: Pod var-expansion-8b53964a-736b-4ac9-a54e-c76c45448e06 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 18 23:28:11.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8252" for this suite.

• [SLOW TEST:6.238 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":356,"completed":205,"skipped":3892,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:28:11.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:28:11.897: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-cfa6f192-49f9-4e14-82fb-b8c3dcbfa705" in namespace "security-context-test-4437" to be "Succeeded or Failed"
Jan 18 23:28:11.903: INFO: Pod "busybox-readonly-false-cfa6f192-49f9-4e14-82fb-b8c3dcbfa705": Phase="Pending", Reason="", readiness=false. Elapsed: 6.536835ms
Jan 18 23:28:13.907: INFO: Pod "busybox-readonly-false-cfa6f192-49f9-4e14-82fb-b8c3dcbfa705": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009604672s
Jan 18 23:28:15.910: INFO: Pod "busybox-readonly-false-cfa6f192-49f9-4e14-82fb-b8c3dcbfa705": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012740571s
Jan 18 23:28:17.913: INFO: Pod "busybox-readonly-false-cfa6f192-49f9-4e14-82fb-b8c3dcbfa705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016234105s
Jan 18 23:28:17.913: INFO: Pod "busybox-readonly-false-cfa6f192-49f9-4e14-82fb-b8c3dcbfa705" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 18 23:28:17.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4437" for this suite.

• [SLOW TEST:6.114 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:173
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":356,"completed":206,"skipped":3913,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:28:17.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 23:28:18.008: INFO: Waiting up to 5m0s for pod "downwardapi-volume-467dca73-aecc-4561-a22b-9c28550053bb" in namespace "downward-api-7280" to be "Succeeded or Failed"
Jan 18 23:28:18.016: INFO: Pod "downwardapi-volume-467dca73-aecc-4561-a22b-9c28550053bb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010825ms
Jan 18 23:28:20.019: INFO: Pod "downwardapi-volume-467dca73-aecc-4561-a22b-9c28550053bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011028742s
Jan 18 23:28:22.022: INFO: Pod "downwardapi-volume-467dca73-aecc-4561-a22b-9c28550053bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014225615s
Jan 18 23:28:24.025: INFO: Pod "downwardapi-volume-467dca73-aecc-4561-a22b-9c28550053bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017586781s
STEP: Saw pod success
Jan 18 23:28:24.025: INFO: Pod "downwardapi-volume-467dca73-aecc-4561-a22b-9c28550053bb" satisfied condition "Succeeded or Failed"
Jan 18 23:28:24.027: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-467dca73-aecc-4561-a22b-9c28550053bb container client-container: <nil>
STEP: delete the pod
Jan 18 23:28:24.045: INFO: Waiting for pod downwardapi-volume-467dca73-aecc-4561-a22b-9c28550053bb to disappear
Jan 18 23:28:24.047: INFO: Pod downwardapi-volume-467dca73-aecc-4561-a22b-9c28550053bb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 18 23:28:24.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7280" for this suite.

• [SLOW TEST:6.123 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":207,"skipped":3962,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:28:24.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-9219
STEP: creating service affinity-nodeport-transition in namespace services-9219
STEP: creating replication controller affinity-nodeport-transition in namespace services-9219
I0118 23:28:24.167463      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9219, replica count: 3
I0118 23:28:27.219773      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 23:28:30.220790      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 23:28:30.230: INFO: Creating new exec pod
Jan 18 23:28:35.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-9219 exec execpod-affinitywsdgb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 18 23:28:35.415: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 18 23:28:35.416: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:28:35.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-9219 exec execpod-affinitywsdgb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.154.30 80'
Jan 18 23:28:35.568: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.154.30 80\nConnection to 172.30.154.30 80 port [tcp/http] succeeded!\n"
Jan 18 23:28:35.568: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:28:35.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-9219 exec execpod-affinitywsdgb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.253.152 31037'
Jan 18 23:28:35.682: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.253.152 31037\nConnection to 10.0.253.152 31037 port [tcp/*] succeeded!\n"
Jan 18 23:28:35.682: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:28:35.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-9219 exec execpod-affinitywsdgb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.236.5 31037'
Jan 18 23:28:35.800: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.236.5 31037\nConnection to 10.0.236.5 31037 port [tcp/*] succeeded!\n"
Jan 18 23:28:35.800: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:28:35.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-9219 exec execpod-affinitywsdgb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.128.7:31037/ ; done'
Jan 18 23:28:36.003: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n"
Jan 18 23:28:36.003: INFO: stdout: "\naffinity-nodeport-transition-jprq7\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-jprq7\naffinity-nodeport-transition-vvhfl\naffinity-nodeport-transition-vvhfl\naffinity-nodeport-transition-jprq7\naffinity-nodeport-transition-vvhfl\naffinity-nodeport-transition-vvhfl\naffinity-nodeport-transition-jprq7\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-jprq7\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-vvhfl\naffinity-nodeport-transition-vvhfl\naffinity-nodeport-transition-jprq7\naffinity-nodeport-transition-kpkph"
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-jprq7
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-jprq7
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-vvhfl
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-vvhfl
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-jprq7
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-vvhfl
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-vvhfl
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-jprq7
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-jprq7
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-vvhfl
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-vvhfl
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-jprq7
Jan 18 23:28:36.003: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-9219 exec execpod-affinitywsdgb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.128.7:31037/ ; done'
Jan 18 23:28:36.198: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.128.7:31037/\n"
Jan 18 23:28:36.198: INFO: stdout: "\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-kpkph\naffinity-nodeport-transition-kpkph"
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Received response from host: affinity-nodeport-transition-kpkph
Jan 18 23:28:36.198: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9219, will wait for the garbage collector to delete the pods
Jan 18 23:28:36.266: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.982695ms
Jan 18 23:28:36.367: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.082679ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 18 23:28:38.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9219" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:14.846 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":208,"skipped":3964,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:28:38.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 18 23:28:39.072: INFO: The status of Pod pod-update-activedeadlineseconds-5ba61202-6832-4123-abd1-da7848c9655b is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:28:41.076: INFO: The status of Pod pod-update-activedeadlineseconds-5ba61202-6832-4123-abd1-da7848c9655b is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:28:43.075: INFO: The status of Pod pod-update-activedeadlineseconds-5ba61202-6832-4123-abd1-da7848c9655b is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 18 23:28:43.592: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5ba61202-6832-4123-abd1-da7848c9655b"
Jan 18 23:28:43.592: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5ba61202-6832-4123-abd1-da7848c9655b" in namespace "pods-8097" to be "terminated due to deadline exceeded"
Jan 18 23:28:43.594: INFO: Pod "pod-update-activedeadlineseconds-5ba61202-6832-4123-abd1-da7848c9655b": Phase="Running", Reason="", readiness=true. Elapsed: 2.18478ms
Jan 18 23:28:45.597: INFO: Pod "pod-update-activedeadlineseconds-5ba61202-6832-4123-abd1-da7848c9655b": Phase="Running", Reason="", readiness=false. Elapsed: 2.005430413s
Jan 18 23:28:47.601: INFO: Pod "pod-update-activedeadlineseconds-5ba61202-6832-4123-abd1-da7848c9655b": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.00897642s
Jan 18 23:28:47.601: INFO: Pod "pod-update-activedeadlineseconds-5ba61202-6832-4123-abd1-da7848c9655b" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 18 23:28:47.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8097" for this suite.

• [SLOW TEST:8.708 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":356,"completed":209,"skipped":3999,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:28:47.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan 18 23:28:47.654: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 18 23:28:53.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5419" for this suite.

• [SLOW TEST:6.279 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":356,"completed":210,"skipped":4008,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:28:53.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-9c2c4f15-8c97-497f-ad70-b0fc9b52d5b8
STEP: Creating a pod to test consume secrets
Jan 18 23:28:54.076: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-92cca93b-a358-4d1c-a7b3-072a75ec93d6" in namespace "projected-8996" to be "Succeeded or Failed"
Jan 18 23:28:54.091: INFO: Pod "pod-projected-secrets-92cca93b-a358-4d1c-a7b3-072a75ec93d6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.415271ms
Jan 18 23:28:56.095: INFO: Pod "pod-projected-secrets-92cca93b-a358-4d1c-a7b3-072a75ec93d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018712891s
Jan 18 23:28:58.098: INFO: Pod "pod-projected-secrets-92cca93b-a358-4d1c-a7b3-072a75ec93d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021659317s
Jan 18 23:29:00.102: INFO: Pod "pod-projected-secrets-92cca93b-a358-4d1c-a7b3-072a75ec93d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025840231s
Jan 18 23:29:02.105: INFO: Pod "pod-projected-secrets-92cca93b-a358-4d1c-a7b3-072a75ec93d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02956883s
STEP: Saw pod success
Jan 18 23:29:02.106: INFO: Pod "pod-projected-secrets-92cca93b-a358-4d1c-a7b3-072a75ec93d6" satisfied condition "Succeeded or Failed"
Jan 18 23:29:02.108: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-projected-secrets-92cca93b-a358-4d1c-a7b3-072a75ec93d6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 18 23:29:02.123: INFO: Waiting for pod pod-projected-secrets-92cca93b-a358-4d1c-a7b3-072a75ec93d6 to disappear
Jan 18 23:29:02.125: INFO: Pod pod-projected-secrets-92cca93b-a358-4d1c-a7b3-072a75ec93d6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 18 23:29:02.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8996" for this suite.

• [SLOW TEST:8.262 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":211,"skipped":4014,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:29:02.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:29:02.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:29:05.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3590" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":356,"completed":212,"skipped":4023,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:29:05.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:188
Jan 18 23:29:05.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-9058" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":356,"completed":213,"skipped":4041,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:29:05.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-c43bdc84-849c-4a90-b267-c45fc4aab38b in namespace container-probe-5749
W0118 23:29:05.705281      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:29:09.722: INFO: Started pod liveness-c43bdc84-849c-4a90-b267-c45fc4aab38b in namespace container-probe-5749
STEP: checking the pod's current state and verifying that restartCount is present
Jan 18 23:29:09.724: INFO: Initial restart count of pod liveness-c43bdc84-849c-4a90-b267-c45fc4aab38b is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 18 23:33:10.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5749" for this suite.

• [SLOW TEST:244.601 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":356,"completed":214,"skipped":4050,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:33:10.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1334
STEP: creating the pod
Jan 18 23:33:10.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-6879 create -f -'
Jan 18 23:33:12.434: INFO: stderr: ""
Jan 18 23:33:12.434: INFO: stdout: "pod/pause created\n"
Jan 18 23:33:12.434: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 18 23:33:12.434: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6879" to be "running and ready"
Jan 18 23:33:12.437: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.17258ms
Jan 18 23:33:14.441: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.00625329s
Jan 18 23:33:14.441: INFO: Pod "pause" satisfied condition "running and ready"
Jan 18 23:33:14.441: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 18 23:33:14.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-6879 label pods pause testing-label=testing-label-value'
Jan 18 23:33:14.510: INFO: stderr: ""
Jan 18 23:33:14.510: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 18 23:33:14.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-6879 get pod pause -L testing-label'
Jan 18 23:33:14.560: INFO: stderr: ""
Jan 18 23:33:14.560: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 18 23:33:14.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-6879 label pods pause testing-label-'
Jan 18 23:33:14.619: INFO: stderr: ""
Jan 18 23:33:14.619: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 18 23:33:14.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-6879 get pod pause -L testing-label'
Jan 18 23:33:14.667: INFO: stderr: ""
Jan 18 23:33:14.667: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Jan 18 23:33:14.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-6879 delete --grace-period=0 --force -f -'
Jan 18 23:33:14.719: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 23:33:14.719: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 18 23:33:14.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-6879 get rc,svc -l name=pause --no-headers'
Jan 18 23:33:14.781: INFO: stderr: "No resources found in kubectl-6879 namespace.\n"
Jan 18 23:33:14.781: INFO: stdout: ""
Jan 18 23:33:14.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-6879 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 23:33:14.828: INFO: stderr: ""
Jan 18 23:33:14.828: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 23:33:14.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6879" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":356,"completed":215,"skipped":4062,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:33:14.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 18 23:33:14.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8406" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":216,"skipped":4063,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:33:14.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replication controller my-hostname-basic-e2f4fc4e-c900-453c-9dd9-69882787705b
W0118 23:33:14.943680      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-e2f4fc4e-c900-453c-9dd9-69882787705b" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-e2f4fc4e-c900-453c-9dd9-69882787705b" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-e2f4fc4e-c900-453c-9dd9-69882787705b" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-e2f4fc4e-c900-453c-9dd9-69882787705b" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:33:14.949: INFO: Pod name my-hostname-basic-e2f4fc4e-c900-453c-9dd9-69882787705b: Found 0 pods out of 1
Jan 18 23:33:19.953: INFO: Pod name my-hostname-basic-e2f4fc4e-c900-453c-9dd9-69882787705b: Found 1 pods out of 1
Jan 18 23:33:19.953: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e2f4fc4e-c900-453c-9dd9-69882787705b" are running
Jan 18 23:33:19.955: INFO: Pod "my-hostname-basic-e2f4fc4e-c900-453c-9dd9-69882787705b-6tr4p" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:33:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:33:17 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:33:17 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:33:14 +0000 UTC Reason: Message:}])
Jan 18 23:33:19.955: INFO: Trying to dial the pod
Jan 18 23:33:24.965: INFO: Controller my-hostname-basic-e2f4fc4e-c900-453c-9dd9-69882787705b: Got expected result from replica 1 [my-hostname-basic-e2f4fc4e-c900-453c-9dd9-69882787705b-6tr4p]: "my-hostname-basic-e2f4fc4e-c900-453c-9dd9-69882787705b-6tr4p", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 18 23:33:24.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9187" for this suite.

• [SLOW TEST:10.074 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":217,"skipped":4086,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:33:24.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-98ce3f7f-fe09-4f6d-a9ef-01dd02ec429f
STEP: Creating a pod to test consume secrets
Jan 18 23:33:25.056: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3bc01607-2d25-4201-aae5-98eb862f7c05" in namespace "projected-2001" to be "Succeeded or Failed"
Jan 18 23:33:25.059: INFO: Pod "pod-projected-secrets-3bc01607-2d25-4201-aae5-98eb862f7c05": Phase="Pending", Reason="", readiness=false. Elapsed: 3.476511ms
Jan 18 23:33:27.062: INFO: Pod "pod-projected-secrets-3bc01607-2d25-4201-aae5-98eb862f7c05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006150634s
Jan 18 23:33:29.064: INFO: Pod "pod-projected-secrets-3bc01607-2d25-4201-aae5-98eb862f7c05": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008529905s
Jan 18 23:33:31.067: INFO: Pod "pod-projected-secrets-3bc01607-2d25-4201-aae5-98eb862f7c05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011191412s
STEP: Saw pod success
Jan 18 23:33:31.067: INFO: Pod "pod-projected-secrets-3bc01607-2d25-4201-aae5-98eb862f7c05" satisfied condition "Succeeded or Failed"
Jan 18 23:33:31.069: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-projected-secrets-3bc01607-2d25-4201-aae5-98eb862f7c05 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 18 23:33:31.088: INFO: Waiting for pod pod-projected-secrets-3bc01607-2d25-4201-aae5-98eb862f7c05 to disappear
Jan 18 23:33:31.090: INFO: Pod pod-projected-secrets-3bc01607-2d25-4201-aae5-98eb862f7c05 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 18 23:33:31.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2001" for this suite.

• [SLOW TEST:6.124 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":218,"skipped":4089,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:33:31.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-6126a791-20aa-459f-88af-40514cd49f25
STEP: Creating a pod to test consume configMaps
Jan 18 23:33:31.189: INFO: Waiting up to 5m0s for pod "pod-configmaps-01cc7681-2aa1-4b76-bcba-a736bfa0f02b" in namespace "configmap-178" to be "Succeeded or Failed"
Jan 18 23:33:31.194: INFO: Pod "pod-configmaps-01cc7681-2aa1-4b76-bcba-a736bfa0f02b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.475204ms
Jan 18 23:33:33.198: INFO: Pod "pod-configmaps-01cc7681-2aa1-4b76-bcba-a736bfa0f02b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009099492s
Jan 18 23:33:35.201: INFO: Pod "pod-configmaps-01cc7681-2aa1-4b76-bcba-a736bfa0f02b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012576243s
Jan 18 23:33:37.205: INFO: Pod "pod-configmaps-01cc7681-2aa1-4b76-bcba-a736bfa0f02b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016531282s
STEP: Saw pod success
Jan 18 23:33:37.205: INFO: Pod "pod-configmaps-01cc7681-2aa1-4b76-bcba-a736bfa0f02b" satisfied condition "Succeeded or Failed"
Jan 18 23:33:37.207: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-configmaps-01cc7681-2aa1-4b76-bcba-a736bfa0f02b container agnhost-container: <nil>
STEP: delete the pod
Jan 18 23:33:37.219: INFO: Waiting for pod pod-configmaps-01cc7681-2aa1-4b76-bcba-a736bfa0f02b to disappear
Jan 18 23:33:37.221: INFO: Pod pod-configmaps-01cc7681-2aa1-4b76-bcba-a736bfa0f02b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 18 23:33:37.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-178" for this suite.

• [SLOW TEST:6.130 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":219,"skipped":4110,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:33:37.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc1
W0118 23:33:37.316571      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0118 23:33:47.927813      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0118 23:33:47.927833      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 23:33:47.927: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 18 23:33:47.927: INFO: Deleting pod "simpletest-rc-to-be-deleted-29phk" in namespace "gc-5362"
Jan 18 23:33:47.937: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gwp2" in namespace "gc-5362"
Jan 18 23:33:47.952: INFO: Deleting pod "simpletest-rc-to-be-deleted-2n989" in namespace "gc-5362"
Jan 18 23:33:47.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vrh4" in namespace "gc-5362"
Jan 18 23:33:47.969: INFO: Deleting pod "simpletest-rc-to-be-deleted-46c27" in namespace "gc-5362"
Jan 18 23:33:47.984: INFO: Deleting pod "simpletest-rc-to-be-deleted-46j6k" in namespace "gc-5362"
Jan 18 23:33:47.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-59cm6" in namespace "gc-5362"
Jan 18 23:33:48.005: INFO: Deleting pod "simpletest-rc-to-be-deleted-5gtwz" in namespace "gc-5362"
Jan 18 23:33:48.016: INFO: Deleting pod "simpletest-rc-to-be-deleted-5k4wc" in namespace "gc-5362"
Jan 18 23:33:48.026: INFO: Deleting pod "simpletest-rc-to-be-deleted-5scv8" in namespace "gc-5362"
Jan 18 23:33:48.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-5szkw" in namespace "gc-5362"
Jan 18 23:33:48.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zdzq" in namespace "gc-5362"
Jan 18 23:33:48.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-695f9" in namespace "gc-5362"
Jan 18 23:33:48.076: INFO: Deleting pod "simpletest-rc-to-be-deleted-6km46" in namespace "gc-5362"
Jan 18 23:33:48.086: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qpnm" in namespace "gc-5362"
Jan 18 23:33:48.098: INFO: Deleting pod "simpletest-rc-to-be-deleted-7klp9" in namespace "gc-5362"
Jan 18 23:33:48.107: INFO: Deleting pod "simpletest-rc-to-be-deleted-7zv8g" in namespace "gc-5362"
Jan 18 23:33:48.117: INFO: Deleting pod "simpletest-rc-to-be-deleted-862lw" in namespace "gc-5362"
Jan 18 23:33:48.126: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fq7z" in namespace "gc-5362"
Jan 18 23:33:48.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ghk7" in namespace "gc-5362"
Jan 18 23:33:48.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gv68" in namespace "gc-5362"
Jan 18 23:33:48.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-8x58q" in namespace "gc-5362"
Jan 18 23:33:48.170: INFO: Deleting pod "simpletest-rc-to-be-deleted-99nrp" in namespace "gc-5362"
Jan 18 23:33:48.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rhcv" in namespace "gc-5362"
Jan 18 23:33:48.190: INFO: Deleting pod "simpletest-rc-to-be-deleted-b4bbb" in namespace "gc-5362"
Jan 18 23:33:48.207: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgzn7" in namespace "gc-5362"
Jan 18 23:33:48.216: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvpbr" in namespace "gc-5362"
Jan 18 23:33:48.229: INFO: Deleting pod "simpletest-rc-to-be-deleted-cm866" in namespace "gc-5362"
Jan 18 23:33:48.240: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfp94" in namespace "gc-5362"
Jan 18 23:33:48.250: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkhcf" in namespace "gc-5362"
Jan 18 23:33:48.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnxbv" in namespace "gc-5362"
Jan 18 23:33:48.279: INFO: Deleting pod "simpletest-rc-to-be-deleted-ds5f9" in namespace "gc-5362"
Jan 18 23:33:48.292: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgrxd" in namespace "gc-5362"
Jan 18 23:33:48.304: INFO: Deleting pod "simpletest-rc-to-be-deleted-flxr7" in namespace "gc-5362"
Jan 18 23:33:48.316: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmttk" in namespace "gc-5362"
Jan 18 23:33:48.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzrzk" in namespace "gc-5362"
Jan 18 23:33:48.339: INFO: Deleting pod "simpletest-rc-to-be-deleted-g25bx" in namespace "gc-5362"
Jan 18 23:33:48.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjj94" in namespace "gc-5362"
Jan 18 23:33:48.383: INFO: Deleting pod "simpletest-rc-to-be-deleted-gldzq" in namespace "gc-5362"
Jan 18 23:33:48.394: INFO: Deleting pod "simpletest-rc-to-be-deleted-gpptx" in namespace "gc-5362"
Jan 18 23:33:48.411: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqvz9" in namespace "gc-5362"
Jan 18 23:33:48.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqrbw" in namespace "gc-5362"
Jan 18 23:33:48.434: INFO: Deleting pod "simpletest-rc-to-be-deleted-j4dgf" in namespace "gc-5362"
Jan 18 23:33:48.445: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7ffs" in namespace "gc-5362"
Jan 18 23:33:48.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-jf4sc" in namespace "gc-5362"
Jan 18 23:33:48.471: INFO: Deleting pod "simpletest-rc-to-be-deleted-jlq87" in namespace "gc-5362"
Jan 18 23:33:48.480: INFO: Deleting pod "simpletest-rc-to-be-deleted-jr7h4" in namespace "gc-5362"
Jan 18 23:33:48.494: INFO: Deleting pod "simpletest-rc-to-be-deleted-jwc89" in namespace "gc-5362"
Jan 18 23:33:48.530: INFO: Deleting pod "simpletest-rc-to-be-deleted-jwclk" in namespace "gc-5362"
Jan 18 23:33:48.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-kcvcn" in namespace "gc-5362"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 18 23:33:48.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5362" for this suite.

• [SLOW TEST:11.344 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":356,"completed":220,"skipped":4124,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:33:48.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating secret secrets-989/secret-test-6927e61c-9f00-4bd5-8b7e-bdd0bac69f92
STEP: Creating a pod to test consume secrets
Jan 18 23:33:48.686: INFO: Waiting up to 5m0s for pod "pod-configmaps-0aa874fe-4ddf-441e-a512-e5345b326489" in namespace "secrets-989" to be "Succeeded or Failed"
Jan 18 23:33:48.690: INFO: Pod "pod-configmaps-0aa874fe-4ddf-441e-a512-e5345b326489": Phase="Pending", Reason="", readiness=false. Elapsed: 3.980678ms
Jan 18 23:33:50.694: INFO: Pod "pod-configmaps-0aa874fe-4ddf-441e-a512-e5345b326489": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007118629s
Jan 18 23:33:52.697: INFO: Pod "pod-configmaps-0aa874fe-4ddf-441e-a512-e5345b326489": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010932611s
Jan 18 23:33:54.701: INFO: Pod "pod-configmaps-0aa874fe-4ddf-441e-a512-e5345b326489": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01454733s
Jan 18 23:33:56.704: INFO: Pod "pod-configmaps-0aa874fe-4ddf-441e-a512-e5345b326489": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.017609266s
STEP: Saw pod success
Jan 18 23:33:56.704: INFO: Pod "pod-configmaps-0aa874fe-4ddf-441e-a512-e5345b326489" satisfied condition "Succeeded or Failed"
Jan 18 23:33:56.706: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-configmaps-0aa874fe-4ddf-441e-a512-e5345b326489 container env-test: <nil>
STEP: delete the pod
Jan 18 23:33:56.718: INFO: Waiting for pod pod-configmaps-0aa874fe-4ddf-441e-a512-e5345b326489 to disappear
Jan 18 23:33:56.720: INFO: Pod pod-configmaps-0aa874fe-4ddf-441e-a512-e5345b326489 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 18 23:33:56.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-989" for this suite.

• [SLOW TEST:8.153 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":221,"skipped":4138,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
[sig-node] Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:33:56.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override all
Jan 18 23:33:56.826: INFO: Waiting up to 5m0s for pod "client-containers-2896d945-1762-4096-9002-53050d91e9a9" in namespace "containers-2472" to be "Succeeded or Failed"
Jan 18 23:33:56.829: INFO: Pod "client-containers-2896d945-1762-4096-9002-53050d91e9a9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.639217ms
Jan 18 23:33:58.832: INFO: Pod "client-containers-2896d945-1762-4096-9002-53050d91e9a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00599599s
Jan 18 23:34:00.835: INFO: Pod "client-containers-2896d945-1762-4096-9002-53050d91e9a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009169228s
Jan 18 23:34:02.838: INFO: Pod "client-containers-2896d945-1762-4096-9002-53050d91e9a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012798284s
STEP: Saw pod success
Jan 18 23:34:02.838: INFO: Pod "client-containers-2896d945-1762-4096-9002-53050d91e9a9" satisfied condition "Succeeded or Failed"
Jan 18 23:34:02.841: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod client-containers-2896d945-1762-4096-9002-53050d91e9a9 container agnhost-container: <nil>
STEP: delete the pod
Jan 18 23:34:02.857: INFO: Waiting for pod client-containers-2896d945-1762-4096-9002-53050d91e9a9 to disappear
Jan 18 23:34:02.859: INFO: Pod client-containers-2896d945-1762-4096-9002-53050d91e9a9 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 18 23:34:02.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2472" for this suite.

• [SLOW TEST:6.138 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":356,"completed":222,"skipped":4138,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:34:02.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 23:34:03.207: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 23:34:05.214: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 34, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 34, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 34, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 34, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 23:34:08.229: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:34:08.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1994" for this suite.
STEP: Destroying namespace "webhook-1994-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.505 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":356,"completed":223,"skipped":4152,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:34:08.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-42d45b65-5f87-4d32-8689-1a53dd7d12c8
STEP: Creating a pod to test consume configMaps
Jan 18 23:34:08.516: INFO: Waiting up to 5m0s for pod "pod-configmaps-2293d664-e0cd-40ae-8e11-de6341fbe7d8" in namespace "configmap-4463" to be "Succeeded or Failed"
Jan 18 23:34:08.526: INFO: Pod "pod-configmaps-2293d664-e0cd-40ae-8e11-de6341fbe7d8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.462385ms
Jan 18 23:34:10.529: INFO: Pod "pod-configmaps-2293d664-e0cd-40ae-8e11-de6341fbe7d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012123792s
Jan 18 23:34:12.532: INFO: Pod "pod-configmaps-2293d664-e0cd-40ae-8e11-de6341fbe7d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015310315s
Jan 18 23:34:14.536: INFO: Pod "pod-configmaps-2293d664-e0cd-40ae-8e11-de6341fbe7d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019173056s
STEP: Saw pod success
Jan 18 23:34:14.536: INFO: Pod "pod-configmaps-2293d664-e0cd-40ae-8e11-de6341fbe7d8" satisfied condition "Succeeded or Failed"
Jan 18 23:34:14.538: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-configmaps-2293d664-e0cd-40ae-8e11-de6341fbe7d8 container agnhost-container: <nil>
STEP: delete the pod
Jan 18 23:34:14.552: INFO: Waiting for pod pod-configmaps-2293d664-e0cd-40ae-8e11-de6341fbe7d8 to disappear
Jan 18 23:34:14.554: INFO: Pod pod-configmaps-2293d664-e0cd-40ae-8e11-de6341fbe7d8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 18 23:34:14.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4463" for this suite.

• [SLOW TEST:6.191 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":224,"skipped":4159,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:34:14.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0118 23:34:14.608679      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:34:14.616: INFO: The status of Pod test-webserver-ece4f277-ad77-472d-b3ca-e0955fefbfbc is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:34:16.619: INFO: The status of Pod test-webserver-ece4f277-ad77-472d-b3ca-e0955fefbfbc is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:34:18.620: INFO: The status of Pod test-webserver-ece4f277-ad77-472d-b3ca-e0955fefbfbc is Running (Ready = false)
Jan 18 23:34:20.619: INFO: The status of Pod test-webserver-ece4f277-ad77-472d-b3ca-e0955fefbfbc is Running (Ready = false)
Jan 18 23:34:22.618: INFO: The status of Pod test-webserver-ece4f277-ad77-472d-b3ca-e0955fefbfbc is Running (Ready = false)
Jan 18 23:34:24.619: INFO: The status of Pod test-webserver-ece4f277-ad77-472d-b3ca-e0955fefbfbc is Running (Ready = false)
Jan 18 23:34:26.619: INFO: The status of Pod test-webserver-ece4f277-ad77-472d-b3ca-e0955fefbfbc is Running (Ready = false)
Jan 18 23:34:28.620: INFO: The status of Pod test-webserver-ece4f277-ad77-472d-b3ca-e0955fefbfbc is Running (Ready = false)
Jan 18 23:34:30.622: INFO: The status of Pod test-webserver-ece4f277-ad77-472d-b3ca-e0955fefbfbc is Running (Ready = false)
Jan 18 23:34:32.623: INFO: The status of Pod test-webserver-ece4f277-ad77-472d-b3ca-e0955fefbfbc is Running (Ready = false)
Jan 18 23:34:34.620: INFO: The status of Pod test-webserver-ece4f277-ad77-472d-b3ca-e0955fefbfbc is Running (Ready = false)
Jan 18 23:34:36.620: INFO: The status of Pod test-webserver-ece4f277-ad77-472d-b3ca-e0955fefbfbc is Running (Ready = true)
Jan 18 23:34:36.622: INFO: Container started at 2023-01-18 23:34:16 +0000 UTC, pod became ready at 2023-01-18 23:34:34 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 18 23:34:36.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-828" for this suite.

• [SLOW TEST:22.070 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":356,"completed":225,"skipped":4191,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:34:36.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 23:34:37.111: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 23:34:39.117: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 34, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 34, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 34, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 34, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 23:34:42.132: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:34:42.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:34:45.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6789" for this suite.
STEP: Destroying namespace "webhook-6789-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.654 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":356,"completed":226,"skipped":4215,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:34:45.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:34:45.433: INFO: The status of Pod pod-secrets-4a0604af-415a-4383-9722-8687799f87d5 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:34:47.436: INFO: The status of Pod pod-secrets-4a0604af-415a-4383-9722-8687799f87d5 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:34:49.438: INFO: The status of Pod pod-secrets-4a0604af-415a-4383-9722-8687799f87d5 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Jan 18 23:34:49.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6933" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":356,"completed":227,"skipped":4220,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:34:49.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap that has name configmap-test-emptyKey-b0388669-ba98-4349-98e1-3f0321b625f4
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 18 23:34:49.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7914" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":356,"completed":228,"skipped":4235,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:34:49.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:34:49.614: INFO: Creating pod...
Jan 18 23:34:53.668: INFO: Creating service...
Jan 18 23:34:53.678: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-4002/pods/agnhost/proxy/some/path/with/DELETE
Jan 18 23:34:53.683: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 23:34:53.683: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-4002/pods/agnhost/proxy/some/path/with/GET
Jan 18 23:34:53.686: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 18 23:34:53.686: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-4002/pods/agnhost/proxy/some/path/with/HEAD
Jan 18 23:34:53.689: INFO: http.Client request:HEAD | StatusCode:200
Jan 18 23:34:53.689: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-4002/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 18 23:34:53.701: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 23:34:53.701: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-4002/pods/agnhost/proxy/some/path/with/PATCH
Jan 18 23:34:53.704: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 23:34:53.704: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-4002/pods/agnhost/proxy/some/path/with/POST
Jan 18 23:34:53.706: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 23:34:53.706: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-4002/pods/agnhost/proxy/some/path/with/PUT
Jan 18 23:34:53.709: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 23:34:53.709: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-4002/services/test-service/proxy/some/path/with/DELETE
Jan 18 23:34:53.713: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 23:34:53.713: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-4002/services/test-service/proxy/some/path/with/GET
Jan 18 23:34:53.716: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 18 23:34:53.716: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-4002/services/test-service/proxy/some/path/with/HEAD
Jan 18 23:34:53.722: INFO: http.Client request:HEAD | StatusCode:200
Jan 18 23:34:53.722: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-4002/services/test-service/proxy/some/path/with/OPTIONS
Jan 18 23:34:53.731: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 23:34:53.731: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-4002/services/test-service/proxy/some/path/with/PATCH
Jan 18 23:34:53.736: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 23:34:53.736: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-4002/services/test-service/proxy/some/path/with/POST
Jan 18 23:34:53.739: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 23:34:53.739: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-4002/services/test-service/proxy/some/path/with/PUT
Jan 18 23:34:53.745: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jan 18 23:34:53.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4002" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":356,"completed":229,"skipped":4236,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:34:53.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service endpoint-test2 in namespace services-6299
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6299 to expose endpoints map[]
Jan 18 23:34:53.839: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 18 23:34:54.845: INFO: successfully validated that service endpoint-test2 in namespace services-6299 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6299
Jan 18 23:34:54.863: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:34:56.865: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:34:58.867: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6299 to expose endpoints map[pod1:[80]]
Jan 18 23:34:58.876: INFO: successfully validated that service endpoint-test2 in namespace services-6299 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Jan 18 23:34:58.876: INFO: Creating new exec pod
Jan 18 23:35:03.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-6299 exec execpodg5zgc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 18 23:35:04.035: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 23:35:04.035: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:35:04.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-6299 exec execpodg5zgc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.223.204 80'
Jan 18 23:35:04.146: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.223.204 80\nConnection to 172.30.223.204 80 port [tcp/http] succeeded!\n"
Jan 18 23:35:04.146: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-6299
Jan 18 23:35:04.161: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:35:06.163: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:35:08.165: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6299 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 18 23:35:08.177: INFO: successfully validated that service endpoint-test2 in namespace services-6299 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Jan 18 23:35:09.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-6299 exec execpodg5zgc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 18 23:35:09.293: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 23:35:09.293: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:35:09.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-6299 exec execpodg5zgc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.223.204 80'
Jan 18 23:35:09.402: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.223.204 80\nConnection to 172.30.223.204 80 port [tcp/http] succeeded!\n"
Jan 18 23:35:09.402: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-6299
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6299 to expose endpoints map[pod2:[80]]
Jan 18 23:35:09.423: INFO: successfully validated that service endpoint-test2 in namespace services-6299 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Jan 18 23:35:10.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-6299 exec execpodg5zgc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 18 23:35:10.561: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 23:35:10.561: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:35:10.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-6299 exec execpodg5zgc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.223.204 80'
Jan 18 23:35:10.708: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.223.204 80\nConnection to 172.30.223.204 80 port [tcp/http] succeeded!\n"
Jan 18 23:35:10.708: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-6299
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6299 to expose endpoints map[]
Jan 18 23:35:11.731: INFO: successfully validated that service endpoint-test2 in namespace services-6299 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 18 23:35:11.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6299" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:18.007 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":356,"completed":230,"skipped":4276,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:35:11.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a collection of services
Jan 18 23:35:11.810: INFO: Creating e2e-svc-a-7gt4r
Jan 18 23:35:11.933: INFO: Creating e2e-svc-b-z8jd7
Jan 18 23:35:11.946: INFO: Creating e2e-svc-c-klxmw
STEP: deleting service collection
Jan 18 23:35:12.047: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 18 23:35:12.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4473" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":356,"completed":231,"skipped":4285,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:35:12.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jan 18 23:35:14.118: INFO: running pods: 0 < 1
Jan 18 23:35:16.122: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 18 23:35:18.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3826" for this suite.

• [SLOW TEST:6.108 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":356,"completed":232,"skipped":4328,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:35:18.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
W0118 23:35:18.230780      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:35:18.235: INFO: The status of Pod labelsupdatea1cf346e-18a0-430d-9a5b-87242ad4ff6e is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:35:20.237: INFO: The status of Pod labelsupdatea1cf346e-18a0-430d-9a5b-87242ad4ff6e is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:35:22.238: INFO: The status of Pod labelsupdatea1cf346e-18a0-430d-9a5b-87242ad4ff6e is Running (Ready = true)
Jan 18 23:35:22.759: INFO: Successfully updated pod "labelsupdatea1cf346e-18a0-430d-9a5b-87242ad4ff6e"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 18 23:35:24.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7377" for this suite.

• [SLOW TEST:6.611 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":233,"skipped":4338,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:35:24.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 18 23:37:00.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2897" for this suite.

• [SLOW TEST:96.082 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":356,"completed":234,"skipped":4338,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:37:00.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name secret-emptykey-test-b224784f-0528-4b50-bcc8-fbc43d86dc2d
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 18 23:37:00.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3678" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":356,"completed":235,"skipped":4339,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:37:00.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-9407, will wait for the garbage collector to delete the pods
Jan 18 23:37:05.098: INFO: Deleting Job.batch foo took: 3.951486ms
Jan 18 23:37:05.199: INFO: Terminating Job.batch foo pods took: 100.555566ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 18 23:37:38.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9407" for this suite.

• [SLOW TEST:37.187 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":356,"completed":236,"skipped":4381,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:37:38.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-1781
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 18 23:37:38.150: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 23:37:38.426: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:37:40.430: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:37:42.430: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:37:44.430: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:37:46.429: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:37:48.429: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:37:50.430: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:37:52.430: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:37:54.431: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:37:56.428: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:37:58.430: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:38:00.430: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 18 23:38:00.434: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 18 23:38:00.438: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jan 18 23:38:00.442: INFO: The status of Pod netserver-3 is Running (Ready = true)
Jan 18 23:38:00.446: INFO: The status of Pod netserver-4 is Running (Ready = true)
Jan 18 23:38:00.450: INFO: The status of Pod netserver-5 is Running (Ready = true)
STEP: Creating test pods
Jan 18 23:38:04.470: INFO: Setting MaxTries for pod polling to 66 for networking test based on endpoint count 6
Jan 18 23:38:04.470: INFO: Breadth first check of 10.128.14.115 on host 10.0.128.7...
Jan 18 23:38:04.472: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.12.251:9080/dial?request=hostname&protocol=http&host=10.128.14.115&port=8083&tries=1'] Namespace:pod-network-test-1781 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:04.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:04.473: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:04.473: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1781/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.12.251%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.14.115%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:38:04.563: INFO: Waiting for responses: map[]
Jan 18 23:38:04.563: INFO: reached 10.128.14.115 after 0/1 tries
Jan 18 23:38:04.563: INFO: Breadth first check of 10.128.6.85 on host 10.0.200.13...
Jan 18 23:38:04.566: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.12.251:9080/dial?request=hostname&protocol=http&host=10.128.6.85&port=8083&tries=1'] Namespace:pod-network-test-1781 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:04.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:04.566: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:04.566: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1781/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.12.251%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.6.85%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:38:04.628: INFO: Waiting for responses: map[]
Jan 18 23:38:04.628: INFO: reached 10.128.6.85 after 0/1 tries
Jan 18 23:38:04.628: INFO: Breadth first check of 10.128.16.163 on host 10.0.211.217...
Jan 18 23:38:04.630: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.12.251:9080/dial?request=hostname&protocol=http&host=10.128.16.163&port=8083&tries=1'] Namespace:pod-network-test-1781 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:04.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:04.630: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:04.630: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1781/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.12.251%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.16.163%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:38:04.693: INFO: Waiting for responses: map[]
Jan 18 23:38:04.693: INFO: reached 10.128.16.163 after 0/1 tries
Jan 18 23:38:04.693: INFO: Breadth first check of 10.128.12.250 on host 10.0.219.147...
Jan 18 23:38:04.695: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.12.251:9080/dial?request=hostname&protocol=http&host=10.128.12.250&port=8083&tries=1'] Namespace:pod-network-test-1781 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:04.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:04.695: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:04.695: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1781/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.12.251%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.12.250%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:38:04.753: INFO: Waiting for responses: map[]
Jan 18 23:38:04.753: INFO: reached 10.128.12.250 after 0/1 tries
Jan 18 23:38:04.753: INFO: Breadth first check of 10.128.8.84 on host 10.0.236.5...
Jan 18 23:38:04.755: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.12.251:9080/dial?request=hostname&protocol=http&host=10.128.8.84&port=8083&tries=1'] Namespace:pod-network-test-1781 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:04.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:04.755: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:04.755: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1781/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.12.251%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.8.84%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:38:04.816: INFO: Waiting for responses: map[]
Jan 18 23:38:04.816: INFO: reached 10.128.8.84 after 0/1 tries
Jan 18 23:38:04.816: INFO: Breadth first check of 10.128.10.53 on host 10.0.253.152...
Jan 18 23:38:04.818: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.12.251:9080/dial?request=hostname&protocol=http&host=10.128.10.53&port=8083&tries=1'] Namespace:pod-network-test-1781 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:04.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:04.818: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:04.818: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1781/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.12.251%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.10.53%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:38:04.883: INFO: Waiting for responses: map[]
Jan 18 23:38:04.883: INFO: reached 10.128.10.53 after 0/1 tries
Jan 18 23:38:04.883: INFO: Going to retry 0 out of 6 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 18 23:38:04.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1781" for this suite.

• [SLOW TEST:26.777 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":356,"completed":237,"skipped":4448,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:38:04.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-172 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-172;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-172 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-172;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-172.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-172.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-172.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-172.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-172.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-172.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-172.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-172.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-172.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-172.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-172.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-172.svc;check="$$(dig +notcp +noall +answer +search 230.255.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.255.230_udp@PTR;check="$$(dig +tcp +noall +answer +search 230.255.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.255.230_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-172 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-172;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-172 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-172;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-172.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-172.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-172.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-172.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-172.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-172.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-172.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-172.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-172.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-172.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-172.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-172.svc;check="$$(dig +notcp +noall +answer +search 230.255.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.255.230_udp@PTR;check="$$(dig +tcp +noall +answer +search 230.255.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.255.230_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 18 23:38:09.007: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.010: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.012: INFO: Unable to read wheezy_udp@dns-test-service.dns-172 from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.014: INFO: Unable to read wheezy_tcp@dns-test-service.dns-172 from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.017: INFO: Unable to read wheezy_udp@dns-test-service.dns-172.svc from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.021: INFO: Unable to read wheezy_tcp@dns-test-service.dns-172.svc from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.023: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-172.svc from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.025: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-172.svc from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.038: INFO: Unable to read jessie_udp@dns-test-service from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.040: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.042: INFO: Unable to read jessie_udp@dns-test-service.dns-172 from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.044: INFO: Unable to read jessie_tcp@dns-test-service.dns-172 from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.047: INFO: Unable to read jessie_udp@dns-test-service.dns-172.svc from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.049: INFO: Unable to read jessie_tcp@dns-test-service.dns-172.svc from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.051: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-172.svc from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.053: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-172.svc from pod dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e: the server could not find the requested resource (get pods dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e)
Jan 18 23:38:09.062: INFO: Lookups using dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-172 wheezy_tcp@dns-test-service.dns-172 wheezy_udp@dns-test-service.dns-172.svc wheezy_tcp@dns-test-service.dns-172.svc wheezy_udp@_http._tcp.dns-test-service.dns-172.svc wheezy_tcp@_http._tcp.dns-test-service.dns-172.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-172 jessie_tcp@dns-test-service.dns-172 jessie_udp@dns-test-service.dns-172.svc jessie_tcp@dns-test-service.dns-172.svc jessie_udp@_http._tcp.dns-test-service.dns-172.svc jessie_tcp@_http._tcp.dns-test-service.dns-172.svc]

Jan 18 23:38:14.122: INFO: DNS probes using dns-172/dns-test-6deee4eb-0b96-47d6-b6af-0bf16df7741e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 18 23:38:14.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-172" for this suite.

• [SLOW TEST:9.307 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":356,"completed":238,"skipped":4471,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:38:14.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:38:14.266: INFO: Creating simple deployment test-new-deployment
Jan 18 23:38:14.304: INFO: deployment "test-new-deployment" doesn't have the required revision set
Jan 18 23:38:16.311: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 38, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 38, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-55df494869\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 23:38:18.459: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-254  e485fe48-856e-4d8d-b19e-fafe8fcc5220 283790 3 2023-01-18 23:38:14 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-18 23:38:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:38:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00dd66538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 23:38:17 +0000 UTC,LastTransitionTime:2023-01-18 23:38:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-55df494869" has successfully progressed.,LastUpdateTime:2023-01-18 23:38:17 +0000 UTC,LastTransitionTime:2023-01-18 23:38:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 23:38:18.460: INFO: New ReplicaSet "test-new-deployment-55df494869" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-55df494869  deployment-254  54174c6e-1bb1-4e2a-9084-b1308079db6a 283789 2 2023-01-18 23:38:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment e485fe48-856e-4d8d-b19e-fafe8fcc5220 0xc00dd66947 0xc00dd66948}] []  [{kube-controller-manager Update apps/v1 2023-01-18 23:38:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e485fe48-856e-4d8d-b19e-fafe8fcc5220\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:38:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00dd669d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:38:18.466: INFO: Pod "test-new-deployment-55df494869-bqcwm" is available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-bqcwm test-new-deployment-55df494869- deployment-254  32b837eb-ba15-475f-be36-4017e523a807 283774 0 2023-01-18 23:38:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.252/23"],"mac_address":"0a:58:0a:80:0c:fc","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.252/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.252"
    ],
    "mac": "0a:58:0a:80:0c:fc",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.252"
    ],
    "mac": "0a:58:0a:80:0c:fc",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-55df494869 54174c6e-1bb1-4e2a-9084-b1308079db6a 0xc008930f47 0xc008930f48}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:38:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:38:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54174c6e-1bb1-4e2a-9084-b1308079db6a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-18 23:38:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-18 23:38:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.12.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2k8jg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2k8jg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-219-147.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c61,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:38:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:38:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:38:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:38:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.219.147,PodIP:10.128.12.252,StartTime:2023-01-18 23:38:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:38:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://c7863035a9c70193b6ce14cfc06763f68f80c3ddde20ce284bbc1fcc4b4cd451,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.12.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:38:18.466: INFO: Pod "test-new-deployment-55df494869-fbq5q" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-fbq5q test-new-deployment-55df494869- deployment-254  60799a99-4b14-4c71-9e0e-b19520d46aa1 283792 0 2023-01-18 23:38:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-55df494869 54174c6e-1bb1-4e2a-9084-b1308079db6a 0xc0089311a7 0xc0089311a8}] []  [{kube-controller-manager Update v1 2023-01-18 23:38:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54174c6e-1bb1-4e2a-9084-b1308079db6a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8rqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8rqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c61,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-d7hls,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 18 23:38:18.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-254" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":356,"completed":239,"skipped":4523,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:38:18.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-5137/configmap-test-05b57a3b-9a64-456a-a49e-982e9083da79
STEP: Creating a pod to test consume configMaps
Jan 18 23:38:18.569: INFO: Waiting up to 5m0s for pod "pod-configmaps-7075e2e4-51f8-46c0-b100-65f479dbaf61" in namespace "configmap-5137" to be "Succeeded or Failed"
Jan 18 23:38:18.575: INFO: Pod "pod-configmaps-7075e2e4-51f8-46c0-b100-65f479dbaf61": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013204ms
Jan 18 23:38:20.579: INFO: Pod "pod-configmaps-7075e2e4-51f8-46c0-b100-65f479dbaf61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009257998s
Jan 18 23:38:22.581: INFO: Pod "pod-configmaps-7075e2e4-51f8-46c0-b100-65f479dbaf61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011786005s
Jan 18 23:38:24.584: INFO: Pod "pod-configmaps-7075e2e4-51f8-46c0-b100-65f479dbaf61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015156225s
STEP: Saw pod success
Jan 18 23:38:24.584: INFO: Pod "pod-configmaps-7075e2e4-51f8-46c0-b100-65f479dbaf61" satisfied condition "Succeeded or Failed"
Jan 18 23:38:24.588: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-configmaps-7075e2e4-51f8-46c0-b100-65f479dbaf61 container env-test: <nil>
STEP: delete the pod
Jan 18 23:38:24.604: INFO: Waiting for pod pod-configmaps-7075e2e4-51f8-46c0-b100-65f479dbaf61 to disappear
Jan 18 23:38:24.606: INFO: Pod pod-configmaps-7075e2e4-51f8-46c0-b100-65f479dbaf61 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 18 23:38:24.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5137" for this suite.

• [SLOW TEST:6.138 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":356,"completed":240,"skipped":4528,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:38:24.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jan 18 23:38:24.782: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:38:26.785: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:38:28.786: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jan 18 23:38:28.801: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:38:30.804: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 18 23:38:30.807: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9266 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:30.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:30.808: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:30.808: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 23:38:30.865: INFO: Exec stderr: ""
Jan 18 23:38:30.866: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9266 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:30.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:30.866: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:30.866: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 23:38:30.939: INFO: Exec stderr: ""
Jan 18 23:38:30.939: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9266 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:30.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:30.939: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:30.939: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 23:38:31.010: INFO: Exec stderr: ""
Jan 18 23:38:31.010: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9266 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:31.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:31.010: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:31.010: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 23:38:31.072: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 18 23:38:31.073: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9266 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:31.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:31.074: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:31.074: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 18 23:38:31.142: INFO: Exec stderr: ""
Jan 18 23:38:31.142: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9266 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:31.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:31.143: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:31.143: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 18 23:38:31.218: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 18 23:38:31.218: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9266 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:31.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:31.219: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:31.219: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 23:38:31.310: INFO: Exec stderr: ""
Jan 18 23:38:31.310: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9266 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:31.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:31.310: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:31.310: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 23:38:31.384: INFO: Exec stderr: ""
Jan 18 23:38:31.384: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9266 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:31.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:31.385: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:31.385: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 23:38:31.458: INFO: Exec stderr: ""
Jan 18 23:38:31.458: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9266 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:31.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:31.459: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:31.459: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 23:38:31.521: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:188
Jan 18 23:38:31.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9266" for this suite.

• [SLOW TEST:6.913 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":241,"skipped":4531,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:38:31.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting the proxy server
Jan 18 23:38:31.568: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-9104 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 23:38:31.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9104" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":356,"completed":242,"skipped":4531,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:38:31.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 18 23:38:31.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7317" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":356,"completed":243,"skipped":4570,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:38:31.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-114d3729-08bc-4118-980c-65a43358735d
STEP: Creating a pod to test consume configMaps
Jan 18 23:38:32.015: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3eab1734-70fb-40fd-ad5e-2445349ba54f" in namespace "projected-6523" to be "Succeeded or Failed"
Jan 18 23:38:32.025: INFO: Pod "pod-projected-configmaps-3eab1734-70fb-40fd-ad5e-2445349ba54f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.393432ms
Jan 18 23:38:34.029: INFO: Pod "pod-projected-configmaps-3eab1734-70fb-40fd-ad5e-2445349ba54f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013994039s
Jan 18 23:38:36.032: INFO: Pod "pod-projected-configmaps-3eab1734-70fb-40fd-ad5e-2445349ba54f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016612255s
Jan 18 23:38:38.037: INFO: Pod "pod-projected-configmaps-3eab1734-70fb-40fd-ad5e-2445349ba54f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021689653s
STEP: Saw pod success
Jan 18 23:38:38.037: INFO: Pod "pod-projected-configmaps-3eab1734-70fb-40fd-ad5e-2445349ba54f" satisfied condition "Succeeded or Failed"
Jan 18 23:38:38.039: INFO: Trying to get logs from node ip-10-0-128-7.ec2.internal pod pod-projected-configmaps-3eab1734-70fb-40fd-ad5e-2445349ba54f container agnhost-container: <nil>
STEP: delete the pod
Jan 18 23:38:38.062: INFO: Waiting for pod pod-projected-configmaps-3eab1734-70fb-40fd-ad5e-2445349ba54f to disappear
Jan 18 23:38:38.068: INFO: Pod pod-projected-configmaps-3eab1734-70fb-40fd-ad5e-2445349ba54f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 18 23:38:38.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6523" for this suite.

• [SLOW TEST:6.151 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":244,"skipped":4588,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:38:38.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 23:38:38.538: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 23:38:40.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 38, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 38, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 38, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 38, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 23:38:43.558: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:38:43.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2897" for this suite.
STEP: Destroying namespace "webhook-2897-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.613 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":356,"completed":245,"skipped":4665,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:38:43.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-projected-all-test-volume-b43085a0-703e-4377-9be2-95639d0c25ff
STEP: Creating secret with name secret-projected-all-test-volume-8a9a7289-18dc-440e-98ad-8fc1c92d5c81
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 18 23:38:43.863: INFO: Waiting up to 5m0s for pod "projected-volume-ad189b23-d735-4ed2-ac16-ecf55265b409" in namespace "projected-5839" to be "Succeeded or Failed"
Jan 18 23:38:43.873: INFO: Pod "projected-volume-ad189b23-d735-4ed2-ac16-ecf55265b409": Phase="Pending", Reason="", readiness=false. Elapsed: 9.681589ms
Jan 18 23:38:45.876: INFO: Pod "projected-volume-ad189b23-d735-4ed2-ac16-ecf55265b409": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012229974s
Jan 18 23:38:47.878: INFO: Pod "projected-volume-ad189b23-d735-4ed2-ac16-ecf55265b409": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014985715s
Jan 18 23:38:49.882: INFO: Pod "projected-volume-ad189b23-d735-4ed2-ac16-ecf55265b409": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01819226s
STEP: Saw pod success
Jan 18 23:38:49.882: INFO: Pod "projected-volume-ad189b23-d735-4ed2-ac16-ecf55265b409" satisfied condition "Succeeded or Failed"
Jan 18 23:38:49.884: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod projected-volume-ad189b23-d735-4ed2-ac16-ecf55265b409 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 18 23:38:49.897: INFO: Waiting for pod projected-volume-ad189b23-d735-4ed2-ac16-ecf55265b409 to disappear
Jan 18 23:38:49.899: INFO: Pod projected-volume-ad189b23-d735-4ed2-ac16-ecf55265b409 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:188
Jan 18 23:38:49.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5839" for this suite.

• [SLOW TEST:6.215 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":356,"completed":246,"skipped":4677,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:38:49.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jan 18 23:38:54.009: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1075 PodName:pod-sharedvolume-141f614a-bea8-4c58-b7ed-5c3294634a4f ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:38:54.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:38:54.009: INFO: ExecWithOptions: Clientset creation
Jan 18 23:38:54.009: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/emptydir-1075/pods/pod-sharedvolume-141f614a-bea8-4c58-b7ed-5c3294634a4f/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 18 23:38:54.098: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 18 23:38:54.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1075" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":356,"completed":247,"skipped":4693,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:38:54.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:38:54.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties
Jan 18 23:39:03.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-6119 --namespace=crd-publish-openapi-6119 create -f -'
Jan 18 23:39:05.281: INFO: stderr: ""
Jan 18 23:39:05.281: INFO: stdout: "e2e-test-crd-publish-openapi-3692-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 18 23:39:05.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-6119 --namespace=crd-publish-openapi-6119 delete e2e-test-crd-publish-openapi-3692-crds test-foo'
Jan 18 23:39:05.339: INFO: stderr: ""
Jan 18 23:39:05.339: INFO: stdout: "e2e-test-crd-publish-openapi-3692-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 18 23:39:05.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-6119 --namespace=crd-publish-openapi-6119 apply -f -'
Jan 18 23:39:05.595: INFO: stderr: ""
Jan 18 23:39:05.595: INFO: stdout: "e2e-test-crd-publish-openapi-3692-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 18 23:39:05.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-6119 --namespace=crd-publish-openapi-6119 delete e2e-test-crd-publish-openapi-3692-crds test-foo'
Jan 18 23:39:05.676: INFO: stderr: ""
Jan 18 23:39:05.676: INFO: stdout: "e2e-test-crd-publish-openapi-3692-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values
Jan 18 23:39:05.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-6119 --namespace=crd-publish-openapi-6119 create -f -'
Jan 18 23:39:05.944: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jan 18 23:39:05.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-6119 --namespace=crd-publish-openapi-6119 create -f -'
Jan 18 23:39:07.233: INFO: rc: 1
Jan 18 23:39:07.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-6119 --namespace=crd-publish-openapi-6119 apply -f -'
Jan 18 23:39:08.049: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties
Jan 18 23:39:08.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-6119 --namespace=crd-publish-openapi-6119 create -f -'
Jan 18 23:39:09.232: INFO: rc: 1
Jan 18 23:39:09.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-6119 --namespace=crd-publish-openapi-6119 apply -f -'
Jan 18 23:39:09.448: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jan 18 23:39:09.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-6119 explain e2e-test-crd-publish-openapi-3692-crds'
Jan 18 23:39:09.679: INFO: stderr: ""
Jan 18 23:39:09.679: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3692-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jan 18 23:39:09.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-6119 explain e2e-test-crd-publish-openapi-3692-crds.metadata'
Jan 18 23:39:09.924: INFO: stderr: ""
Jan 18 23:39:09.924: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3692-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     Deprecated: ClusterName is a legacy field that was always cleared by the\n     system and never used; it will be removed completely in 1.25.\n\n     The name in the go struct is changed to help clients detect accidental use.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 18 23:39:09.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-6119 explain e2e-test-crd-publish-openapi-3692-crds.spec'
Jan 18 23:39:10.151: INFO: stderr: ""
Jan 18 23:39:10.151: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3692-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 18 23:39:10.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-6119 explain e2e-test-crd-publish-openapi-3692-crds.spec.bars'
Jan 18 23:39:10.420: INFO: stderr: ""
Jan 18 23:39:10.420: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3692-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jan 18 23:39:10.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-6119 explain e2e-test-crd-publish-openapi-3692-crds.spec.bars2'
Jan 18 23:39:10.656: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:39:19.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6119" for this suite.

• [SLOW TEST:25.520 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":356,"completed":248,"skipped":4729,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:39:19.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
W0118 23:39:19.694535      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0118 23:39:29.712991      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0118 23:39:29.713006      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 23:39:29.713: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 18 23:39:29.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8041" for this suite.

• [SLOW TEST:10.096 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":356,"completed":249,"skipped":4747,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:39:29.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-fe784991-5925-4e7b-ae0f-b33aa8baf86f
STEP: Creating a pod to test consume configMaps
Jan 18 23:39:29.865: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-46c98338-6ab7-4889-b417-7af37a3ae3b2" in namespace "projected-6989" to be "Succeeded or Failed"
Jan 18 23:39:29.875: INFO: Pod "pod-projected-configmaps-46c98338-6ab7-4889-b417-7af37a3ae3b2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.958817ms
Jan 18 23:39:31.878: INFO: Pod "pod-projected-configmaps-46c98338-6ab7-4889-b417-7af37a3ae3b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012795455s
Jan 18 23:39:33.882: INFO: Pod "pod-projected-configmaps-46c98338-6ab7-4889-b417-7af37a3ae3b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016223231s
Jan 18 23:39:35.885: INFO: Pod "pod-projected-configmaps-46c98338-6ab7-4889-b417-7af37a3ae3b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01997416s
STEP: Saw pod success
Jan 18 23:39:35.885: INFO: Pod "pod-projected-configmaps-46c98338-6ab7-4889-b417-7af37a3ae3b2" satisfied condition "Succeeded or Failed"
Jan 18 23:39:35.888: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-projected-configmaps-46c98338-6ab7-4889-b417-7af37a3ae3b2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 18 23:39:35.905: INFO: Waiting for pod pod-projected-configmaps-46c98338-6ab7-4889-b417-7af37a3ae3b2 to disappear
Jan 18 23:39:35.906: INFO: Pod pod-projected-configmaps-46c98338-6ab7-4889-b417-7af37a3ae3b2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 18 23:39:35.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6989" for this suite.

• [SLOW TEST:6.190 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":250,"skipped":4794,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:39:35.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4122.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4122.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4122.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4122.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 18 23:39:40.108: INFO: DNS probes using dns-4122/dns-test-7b17eb53-1030-4b52-84cc-33a0e077482d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 18 23:39:40.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4122" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","total":356,"completed":251,"skipped":4801,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:39:40.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:39:40.206: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-481d0290-96b4-49e0-8ed6-ec79e0d7cec0
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 18 23:39:44.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8644" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":252,"skipped":4816,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:39:44.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 18 23:39:44.384: INFO: Waiting up to 5m0s for pod "pod-da9133a1-50d4-422d-85b3-cfbf9aec72e5" in namespace "emptydir-7077" to be "Succeeded or Failed"
Jan 18 23:39:44.399: INFO: Pod "pod-da9133a1-50d4-422d-85b3-cfbf9aec72e5": Phase="Pending", Reason="", readiness=false. Elapsed: 15.078701ms
Jan 18 23:39:46.403: INFO: Pod "pod-da9133a1-50d4-422d-85b3-cfbf9aec72e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019087169s
Jan 18 23:39:48.406: INFO: Pod "pod-da9133a1-50d4-422d-85b3-cfbf9aec72e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022305124s
Jan 18 23:39:50.409: INFO: Pod "pod-da9133a1-50d4-422d-85b3-cfbf9aec72e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024858194s
STEP: Saw pod success
Jan 18 23:39:50.409: INFO: Pod "pod-da9133a1-50d4-422d-85b3-cfbf9aec72e5" satisfied condition "Succeeded or Failed"
Jan 18 23:39:50.411: INFO: Trying to get logs from node ip-10-0-211-217.ec2.internal pod pod-da9133a1-50d4-422d-85b3-cfbf9aec72e5 container test-container: <nil>
STEP: delete the pod
Jan 18 23:39:50.431: INFO: Waiting for pod pod-da9133a1-50d4-422d-85b3-cfbf9aec72e5 to disappear
Jan 18 23:39:50.434: INFO: Pod pod-da9133a1-50d4-422d-85b3-cfbf9aec72e5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 18 23:39:50.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7077" for this suite.

• [SLOW TEST:6.153 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":253,"skipped":4817,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:39:50.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:39:50.563: INFO: created pod pod-service-account-defaultsa
Jan 18 23:39:50.563: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 18 23:39:50.574: INFO: created pod pod-service-account-mountsa
Jan 18 23:39:50.574: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 18 23:39:50.602: INFO: created pod pod-service-account-nomountsa
Jan 18 23:39:50.602: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 18 23:39:50.622: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 18 23:39:50.622: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 18 23:39:50.646: INFO: created pod pod-service-account-mountsa-mountspec
Jan 18 23:39:50.646: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 18 23:39:50.682: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 18 23:39:50.682: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 18 23:39:50.731: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 18 23:39:50.731: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 18 23:39:50.749: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 18 23:39:50.749: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 18 23:39:50.775: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 18 23:39:50.775: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 18 23:39:50.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9865" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":356,"completed":254,"skipped":4831,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:39:50.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 18 23:39:50.885: INFO: The status of Pod pod-update-bb0b9265-3b4f-4769-8871-c9028db236d1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:39:52.888: INFO: The status of Pod pod-update-bb0b9265-3b4f-4769-8871-c9028db236d1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:39:54.888: INFO: The status of Pod pod-update-bb0b9265-3b4f-4769-8871-c9028db236d1 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 18 23:39:55.407: INFO: Successfully updated pod "pod-update-bb0b9265-3b4f-4769-8871-c9028db236d1"
STEP: verifying the updated pod is in kubernetes
Jan 18 23:39:55.411: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 18 23:39:55.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5362" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":356,"completed":255,"skipped":4838,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:39:55.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jan 18 23:39:55.499: INFO: Waiting up to 5m0s for pod "security-context-29dffbb6-10c5-49ee-84a2-ca7c4919879f" in namespace "security-context-778" to be "Succeeded or Failed"
Jan 18 23:39:55.520: INFO: Pod "security-context-29dffbb6-10c5-49ee-84a2-ca7c4919879f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.472157ms
Jan 18 23:39:57.527: INFO: Pod "security-context-29dffbb6-10c5-49ee-84a2-ca7c4919879f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027884237s
Jan 18 23:39:59.531: INFO: Pod "security-context-29dffbb6-10c5-49ee-84a2-ca7c4919879f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031666328s
Jan 18 23:40:01.547: INFO: Pod "security-context-29dffbb6-10c5-49ee-84a2-ca7c4919879f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048602617s
STEP: Saw pod success
Jan 18 23:40:01.547: INFO: Pod "security-context-29dffbb6-10c5-49ee-84a2-ca7c4919879f" satisfied condition "Succeeded or Failed"
Jan 18 23:40:01.553: INFO: Trying to get logs from node ip-10-0-128-7.ec2.internal pod security-context-29dffbb6-10c5-49ee-84a2-ca7c4919879f container test-container: <nil>
STEP: delete the pod
Jan 18 23:40:01.602: INFO: Waiting for pod security-context-29dffbb6-10c5-49ee-84a2-ca7c4919879f to disappear
Jan 18 23:40:01.625: INFO: Pod security-context-29dffbb6-10c5-49ee-84a2-ca7c4919879f no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 18 23:40:01.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-778" for this suite.

• [SLOW TEST:6.219 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":256,"skipped":4849,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:40:01.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:40:01.679: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-a4eafba6-0c7b-4bc5-bd5f-a7db79da9055
STEP: Creating the pod
Jan 18 23:40:01.726: INFO: The status of Pod pod-configmaps-c2a6986d-af3e-496e-a16c-7bc5a23b839e is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:40:03.729: INFO: The status of Pod pod-configmaps-c2a6986d-af3e-496e-a16c-7bc5a23b839e is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:40:05.730: INFO: The status of Pod pod-configmaps-c2a6986d-af3e-496e-a16c-7bc5a23b839e is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-a4eafba6-0c7b-4bc5-bd5f-a7db79da9055
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 18 23:40:07.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7363" for this suite.

• [SLOW TEST:6.121 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":257,"skipped":4859,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:40:07.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service multi-endpoint-test in namespace services-2930
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2930 to expose endpoints map[]
Jan 18 23:40:07.809: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 18 23:40:08.836: INFO: successfully validated that service multi-endpoint-test in namespace services-2930 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2930
Jan 18 23:40:08.915: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:40:10.934: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:40:12.918: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2930 to expose endpoints map[pod1:[100]]
Jan 18 23:40:12.928: INFO: successfully validated that service multi-endpoint-test in namespace services-2930 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2930
Jan 18 23:40:12.947: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:40:14.951: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:40:16.950: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2930 to expose endpoints map[pod1:[100] pod2:[101]]
Jan 18 23:40:16.960: INFO: successfully validated that service multi-endpoint-test in namespace services-2930 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Jan 18 23:40:16.960: INFO: Creating new exec pod
Jan 18 23:40:21.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2930 exec execpodprb8m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 18 23:40:22.093: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 18 23:40:22.093: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:40:22.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2930 exec execpodprb8m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.94.70 80'
Jan 18 23:40:22.196: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.94.70 80\nConnection to 172.30.94.70 80 port [tcp/http] succeeded!\n"
Jan 18 23:40:22.196: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:40:22.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2930 exec execpodprb8m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 18 23:40:22.303: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 18 23:40:22.303: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:40:22.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2930 exec execpodprb8m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.94.70 81'
Jan 18 23:40:22.403: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.94.70 81\nConnection to 172.30.94.70 81 port [tcp/*] succeeded!\n"
Jan 18 23:40:22.403: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-2930
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2930 to expose endpoints map[pod2:[101]]
Jan 18 23:40:23.425: INFO: successfully validated that service multi-endpoint-test in namespace services-2930 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2930
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2930 to expose endpoints map[]
Jan 18 23:40:25.443: INFO: successfully validated that service multi-endpoint-test in namespace services-2930 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 18 23:40:25.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2930" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:17.722 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":356,"completed":258,"skipped":4859,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:40:25.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 18 23:40:25.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1252" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":356,"completed":259,"skipped":4859,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:40:25.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jan 18 23:40:25.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jan 18 23:40:56.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:41:05.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:41:37.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9787" for this suite.

• [SLOW TEST:72.173 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":356,"completed":260,"skipped":4867,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:41:37.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 18 23:41:37.924: INFO: Waiting up to 5m0s for pod "downward-api-0a426a14-6fc5-4136-8cbf-6712a8e706b0" in namespace "downward-api-4071" to be "Succeeded or Failed"
Jan 18 23:41:37.931: INFO: Pod "downward-api-0a426a14-6fc5-4136-8cbf-6712a8e706b0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.62953ms
Jan 18 23:41:39.935: INFO: Pod "downward-api-0a426a14-6fc5-4136-8cbf-6712a8e706b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011042965s
Jan 18 23:41:41.938: INFO: Pod "downward-api-0a426a14-6fc5-4136-8cbf-6712a8e706b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014292188s
Jan 18 23:41:43.941: INFO: Pod "downward-api-0a426a14-6fc5-4136-8cbf-6712a8e706b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016827008s
STEP: Saw pod success
Jan 18 23:41:43.941: INFO: Pod "downward-api-0a426a14-6fc5-4136-8cbf-6712a8e706b0" satisfied condition "Succeeded or Failed"
Jan 18 23:41:43.943: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downward-api-0a426a14-6fc5-4136-8cbf-6712a8e706b0 container dapi-container: <nil>
STEP: delete the pod
Jan 18 23:41:43.962: INFO: Waiting for pod downward-api-0a426a14-6fc5-4136-8cbf-6712a8e706b0 to disappear
Jan 18 23:41:43.964: INFO: Pod downward-api-0a426a14-6fc5-4136-8cbf-6712a8e706b0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 18 23:41:43.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4071" for this suite.

• [SLOW TEST:6.116 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":356,"completed":261,"skipped":4880,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:41:43.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 18 23:41:44.059: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:44.059: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:44.059: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:44.062: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:41:44.062: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 23:41:45.066: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:45.066: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:45.066: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:45.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:41:45.069: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 23:41:46.066: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:46.066: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:46.066: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:46.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:41:46.071: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 23:41:47.067: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:47.067: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:47.067: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:47.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 18 23:41:47.069: INFO: Node ip-10-0-236-5.ec2.internal is running 0 daemon pod, expected 1
Jan 18 23:41:48.066: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:48.066: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:48.066: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:48.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 18 23:41:48.069: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 18 23:41:48.084: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:48.084: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:48.084: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:41:48.087: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 18 23:41:48.087: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5175, will wait for the garbage collector to delete the pods
Jan 18 23:41:49.152: INFO: Deleting DaemonSet.extensions daemon-set took: 4.182784ms
Jan 18 23:41:49.252: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.460804ms
Jan 18 23:41:52.356: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:41:52.356: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 23:41:52.357: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"288671"},"items":null}

Jan 18 23:41:52.359: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"288671"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 18 23:41:52.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5175" for this suite.

• [SLOW TEST:8.413 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":356,"completed":262,"skipped":4940,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:41:52.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Jan 18 23:41:52.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-2632 create -f -'
Jan 18 23:41:53.867: INFO: stderr: ""
Jan 18 23:41:53.867: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 18 23:41:54.870: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 23:41:54.870: INFO: Found 0 / 1
Jan 18 23:41:55.869: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 23:41:55.869: INFO: Found 0 / 1
Jan 18 23:41:56.875: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 23:41:56.875: INFO: Found 1 / 1
Jan 18 23:41:56.875: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 18 23:41:56.885: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 23:41:56.885: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 23:41:56.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-2632 patch pod agnhost-primary-5qjg4 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 18 23:41:56.952: INFO: stderr: ""
Jan 18 23:41:56.952: INFO: stdout: "pod/agnhost-primary-5qjg4 patched\n"
STEP: checking annotations
Jan 18 23:41:56.954: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 23:41:56.955: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 23:41:56.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2632" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":356,"completed":263,"skipped":4960,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:41:56.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-downwardapi-mwgq
STEP: Creating a pod to test atomic-volume-subpath
Jan 18 23:41:57.097: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mwgq" in namespace "subpath-2177" to be "Succeeded or Failed"
Jan 18 23:41:57.101: INFO: Pod "pod-subpath-test-downwardapi-mwgq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.342909ms
Jan 18 23:41:59.105: INFO: Pod "pod-subpath-test-downwardapi-mwgq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007881422s
Jan 18 23:42:01.108: INFO: Pod "pod-subpath-test-downwardapi-mwgq": Phase="Running", Reason="", readiness=true. Elapsed: 4.011075382s
Jan 18 23:42:03.112: INFO: Pod "pod-subpath-test-downwardapi-mwgq": Phase="Running", Reason="", readiness=true. Elapsed: 6.014684337s
Jan 18 23:42:05.115: INFO: Pod "pod-subpath-test-downwardapi-mwgq": Phase="Running", Reason="", readiness=true. Elapsed: 8.018317404s
Jan 18 23:42:07.119: INFO: Pod "pod-subpath-test-downwardapi-mwgq": Phase="Running", Reason="", readiness=true. Elapsed: 10.021511916s
Jan 18 23:42:09.122: INFO: Pod "pod-subpath-test-downwardapi-mwgq": Phase="Running", Reason="", readiness=true. Elapsed: 12.025015924s
Jan 18 23:42:11.126: INFO: Pod "pod-subpath-test-downwardapi-mwgq": Phase="Running", Reason="", readiness=true. Elapsed: 14.029326148s
Jan 18 23:42:13.130: INFO: Pod "pod-subpath-test-downwardapi-mwgq": Phase="Running", Reason="", readiness=true. Elapsed: 16.032452947s
Jan 18 23:42:15.137: INFO: Pod "pod-subpath-test-downwardapi-mwgq": Phase="Running", Reason="", readiness=true. Elapsed: 18.040192065s
Jan 18 23:42:17.140: INFO: Pod "pod-subpath-test-downwardapi-mwgq": Phase="Running", Reason="", readiness=true. Elapsed: 20.043391601s
Jan 18 23:42:19.145: INFO: Pod "pod-subpath-test-downwardapi-mwgq": Phase="Running", Reason="", readiness=true. Elapsed: 22.048020995s
Jan 18 23:42:21.147: INFO: Pod "pod-subpath-test-downwardapi-mwgq": Phase="Running", Reason="", readiness=false. Elapsed: 24.050309705s
Jan 18 23:42:23.151: INFO: Pod "pod-subpath-test-downwardapi-mwgq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.053540542s
STEP: Saw pod success
Jan 18 23:42:23.151: INFO: Pod "pod-subpath-test-downwardapi-mwgq" satisfied condition "Succeeded or Failed"
Jan 18 23:42:23.153: INFO: Trying to get logs from node ip-10-0-211-217.ec2.internal pod pod-subpath-test-downwardapi-mwgq container test-container-subpath-downwardapi-mwgq: <nil>
STEP: delete the pod
Jan 18 23:42:23.170: INFO: Waiting for pod pod-subpath-test-downwardapi-mwgq to disappear
Jan 18 23:42:23.172: INFO: Pod pod-subpath-test-downwardapi-mwgq no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mwgq
Jan 18 23:42:23.172: INFO: Deleting pod "pod-subpath-test-downwardapi-mwgq" in namespace "subpath-2177"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 18 23:42:23.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2177" for this suite.

• [SLOW TEST:26.217 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","total":356,"completed":264,"skipped":5004,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:42:23.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 23:42:23.616: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 23:42:25.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 42, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 42, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 42, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 42, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 23:42:28.640: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:42:28.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1499" for this suite.
STEP: Destroying namespace "webhook-1499-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.558 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":356,"completed":265,"skipped":5004,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:42:28.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 23:42:28.834: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1d294e4-a1c8-4408-80f8-182ea8701688" in namespace "projected-6878" to be "Succeeded or Failed"
Jan 18 23:42:28.854: INFO: Pod "downwardapi-volume-d1d294e4-a1c8-4408-80f8-182ea8701688": Phase="Pending", Reason="", readiness=false. Elapsed: 19.518858ms
Jan 18 23:42:30.858: INFO: Pod "downwardapi-volume-d1d294e4-a1c8-4408-80f8-182ea8701688": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023639033s
Jan 18 23:42:32.861: INFO: Pod "downwardapi-volume-d1d294e4-a1c8-4408-80f8-182ea8701688": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026356869s
Jan 18 23:42:34.864: INFO: Pod "downwardapi-volume-d1d294e4-a1c8-4408-80f8-182ea8701688": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029337761s
STEP: Saw pod success
Jan 18 23:42:34.864: INFO: Pod "downwardapi-volume-d1d294e4-a1c8-4408-80f8-182ea8701688" satisfied condition "Succeeded or Failed"
Jan 18 23:42:34.866: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-d1d294e4-a1c8-4408-80f8-182ea8701688 container client-container: <nil>
STEP: delete the pod
Jan 18 23:42:34.879: INFO: Waiting for pod downwardapi-volume-d1d294e4-a1c8-4408-80f8-182ea8701688 to disappear
Jan 18 23:42:34.882: INFO: Pod downwardapi-volume-d1d294e4-a1c8-4408-80f8-182ea8701688 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 18 23:42:34.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6878" for this suite.

• [SLOW TEST:6.154 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":266,"skipped":5062,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:42:34.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a ReplicationController is created
W0118 23:42:34.970181      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: When the matched label of one of its pods change
Jan 18 23:42:34.977: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 18 23:42:39.979: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 18 23:42:40.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9975" for this suite.

• [SLOW TEST:6.115 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":356,"completed":267,"skipped":5076,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:42:41.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2205
[It] should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-2205
W0118 23:42:41.055813      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:42:41.062: INFO: Found 0 stateful pods, waiting for 1
Jan 18 23:42:51.066: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 23:42:51.084: INFO: Deleting all statefulset in ns statefulset-2205
Jan 18 23:42:51.085: INFO: Scaling statefulset ss to 0
Jan 18 23:43:01.102: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 23:43:01.104: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 18 23:43:01.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2205" for this suite.

• [SLOW TEST:20.128 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":356,"completed":268,"skipped":5092,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:43:01.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
W0118 23:43:01.186084      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 18 23:43:15.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5587" for this suite.

• [SLOW TEST:14.060 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":356,"completed":269,"skipped":5115,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:43:15.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jan 18 23:43:19.277: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7878 PodName:var-expansion-e8e75ff7-4245-4846-8a87-e27a4d1d9d6d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:43:19.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:43:19.277: INFO: ExecWithOptions: Clientset creation
Jan 18 23:43:19.278: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-7878/pods/var-expansion-e8e75ff7-4245-4846-8a87-e27a4d1d9d6d/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path
Jan 18 23:43:19.362: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7878 PodName:var-expansion-e8e75ff7-4245-4846-8a87-e27a4d1d9d6d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:43:19.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:43:19.362: INFO: ExecWithOptions: Clientset creation
Jan 18 23:43:19.362: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-7878/pods/var-expansion-e8e75ff7-4245-4846-8a87-e27a4d1d9d6d/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value
Jan 18 23:43:19.932: INFO: Successfully updated pod "var-expansion-e8e75ff7-4245-4846-8a87-e27a4d1d9d6d"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jan 18 23:43:19.935: INFO: Deleting pod "var-expansion-e8e75ff7-4245-4846-8a87-e27a4d1d9d6d" in namespace "var-expansion-7878"
Jan 18 23:43:19.940: INFO: Wait up to 5m0s for pod "var-expansion-e8e75ff7-4245-4846-8a87-e27a4d1d9d6d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 18 23:43:53.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7878" for this suite.

• [SLOW TEST:38.757 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":356,"completed":270,"skipped":5135,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:43:53.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
Jan 18 23:43:54.031: INFO: Creating simple deployment test-deployment-ckcbk
Jan 18 23:43:54.045: INFO: deployment "test-deployment-ckcbk" doesn't have the required revision set
Jan 18 23:43:56.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 43, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 43, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 43, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 43, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-ckcbk-688c4d6789\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Jan 18 23:43:58.060: INFO: Deployment test-deployment-ckcbk has Conditions: [{Available True 2023-01-18 23:43:56 +0000 UTC 2023-01-18 23:43:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-18 23:43:56 +0000 UTC 2023-01-18 23:43:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-ckcbk-688c4d6789" has successfully progressed.}]
STEP: updating Deployment Status
Jan 18 23:43:58.067: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 43, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 43, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 43, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 43, 54, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-ckcbk-688c4d6789\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Jan 18 23:43:58.069: INFO: Observed &Deployment event: ADDED
Jan 18 23:43:58.069: INFO: Observed Deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:43:54 +0000 UTC 2023-01-18 23:43:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-ckcbk-688c4d6789"}
Jan 18 23:43:58.069: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:43:58.069: INFO: Observed Deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:43:54 +0000 UTC 2023-01-18 23:43:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-ckcbk-688c4d6789"}
Jan 18 23:43:58.069: INFO: Observed Deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 23:43:54 +0000 UTC 2023-01-18 23:43:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 23:43:58.069: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:43:58.069: INFO: Observed Deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 23:43:54 +0000 UTC 2023-01-18 23:43:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 23:43:58.069: INFO: Observed Deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:43:54 +0000 UTC 2023-01-18 23:43:54 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-ckcbk-688c4d6789" is progressing.}
Jan 18 23:43:58.069: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:43:58.069: INFO: Observed Deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 23:43:56 +0000 UTC 2023-01-18 23:43:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 23:43:58.069: INFO: Observed Deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:43:56 +0000 UTC 2023-01-18 23:43:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-ckcbk-688c4d6789" has successfully progressed.}
Jan 18 23:43:58.069: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:43:58.069: INFO: Observed Deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 23:43:56 +0000 UTC 2023-01-18 23:43:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 23:43:58.069: INFO: Observed Deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:43:56 +0000 UTC 2023-01-18 23:43:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-ckcbk-688c4d6789" has successfully progressed.}
Jan 18 23:43:58.069: INFO: Found Deployment test-deployment-ckcbk in namespace deployment-9822 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 23:43:58.069: INFO: Deployment test-deployment-ckcbk has an updated status
STEP: patching the Statefulset Status
Jan 18 23:43:58.069: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 23:43:58.075: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Jan 18 23:43:58.077: INFO: Observed &Deployment event: ADDED
Jan 18 23:43:58.077: INFO: Observed deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:43:54 +0000 UTC 2023-01-18 23:43:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-ckcbk-688c4d6789"}
Jan 18 23:43:58.077: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:43:58.077: INFO: Observed deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:43:54 +0000 UTC 2023-01-18 23:43:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-ckcbk-688c4d6789"}
Jan 18 23:43:58.077: INFO: Observed deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 23:43:54 +0000 UTC 2023-01-18 23:43:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 23:43:58.077: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:43:58.077: INFO: Observed deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 23:43:54 +0000 UTC 2023-01-18 23:43:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 23:43:58.077: INFO: Observed deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:43:54 +0000 UTC 2023-01-18 23:43:54 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-ckcbk-688c4d6789" is progressing.}
Jan 18 23:43:58.077: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:43:58.077: INFO: Observed deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 23:43:56 +0000 UTC 2023-01-18 23:43:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 23:43:58.077: INFO: Observed deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:43:56 +0000 UTC 2023-01-18 23:43:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-ckcbk-688c4d6789" has successfully progressed.}
Jan 18 23:43:58.077: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:43:58.077: INFO: Observed deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 23:43:56 +0000 UTC 2023-01-18 23:43:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 23:43:58.077: INFO: Observed deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 23:43:56 +0000 UTC 2023-01-18 23:43:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-ckcbk-688c4d6789" has successfully progressed.}
Jan 18 23:43:58.077: INFO: Observed deployment test-deployment-ckcbk in namespace deployment-9822 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 23:43:58.077: INFO: Observed &Deployment event: MODIFIED
Jan 18 23:43:58.077: INFO: Found deployment test-deployment-ckcbk in namespace deployment-9822 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 18 23:43:58.077: INFO: Deployment test-deployment-ckcbk has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 23:43:58.079: INFO: Deployment "test-deployment-ckcbk":
&Deployment{ObjectMeta:{test-deployment-ckcbk  deployment-9822  14add8f8-28c6-4f3a-83c4-c326083ea31d 291155 1 2023-01-18 23:43:54 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2023-01-18 23:43:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:43:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-01-18 23:43:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00bbcf728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 23:43:58.082: INFO: New ReplicaSet "test-deployment-ckcbk-688c4d6789" of Deployment "test-deployment-ckcbk":
&ReplicaSet{ObjectMeta:{test-deployment-ckcbk-688c4d6789  deployment-9822  5e5d33f4-75f6-40e5-bb19-639ac85dd452 291131 1 2023-01-18 23:43:54 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-ckcbk 14add8f8-28c6-4f3a-83c4-c326083ea31d 0xc00bbcfaa7 0xc00bbcfaa8}] []  [{kube-controller-manager Update apps/v1 2023-01-18 23:43:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14add8f8-28c6-4f3a-83c4-c326083ea31d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:43:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 688c4d6789,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00bbcfb58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:43:58.084: INFO: Pod "test-deployment-ckcbk-688c4d6789-ztg89" is available:
&Pod{ObjectMeta:{test-deployment-ckcbk-688c4d6789-ztg89 test-deployment-ckcbk-688c4d6789- deployment-9822  e9eb2abe-4858-4204-8e08-d51d3c1b4161 291130 0 2023-01-18 23:43:54 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.13.21/23"],"mac_address":"0a:58:0a:80:0d:15","gateway_ips":["10.128.12.1"],"ip_address":"10.128.13.21/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.13.21"
    ],
    "mac": "0a:58:0a:80:0d:15",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.13.21"
    ],
    "mac": "0a:58:0a:80:0d:15",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-ckcbk-688c4d6789 5e5d33f4-75f6-40e5-bb19-639ac85dd452 0xc002afc197 0xc002afc198}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:43:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:43:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e5d33f4-75f6-40e5-bb19-639ac85dd452\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-18 23:43:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-18 23:43:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.13.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rg455,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rg455,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-219-147.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c57,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b7tdn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:43:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:43:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:43:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:43:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.219.147,PodIP:10.128.13.21,StartTime:2023-01-18 23:43:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:43:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4ff9af3ab72a59a4591773d3ca1e747e8aabbea8e003959545ffa96c3ba07798,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.13.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 18 23:43:58.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9822" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":356,"completed":271,"skipped":5136,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:43:58.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:43:58.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jan 18 23:44:07.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-786 --namespace=crd-publish-openapi-786 create -f -'
Jan 18 23:44:08.670: INFO: stderr: ""
Jan 18 23:44:08.670: INFO: stdout: "e2e-test-crd-publish-openapi-3126-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 18 23:44:08.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-786 --namespace=crd-publish-openapi-786 delete e2e-test-crd-publish-openapi-3126-crds test-cr'
Jan 18 23:44:08.723: INFO: stderr: ""
Jan 18 23:44:08.723: INFO: stdout: "e2e-test-crd-publish-openapi-3126-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 18 23:44:08.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-786 --namespace=crd-publish-openapi-786 apply -f -'
Jan 18 23:44:09.906: INFO: stderr: ""
Jan 18 23:44:09.906: INFO: stdout: "e2e-test-crd-publish-openapi-3126-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 18 23:44:09.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-786 --namespace=crd-publish-openapi-786 delete e2e-test-crd-publish-openapi-3126-crds test-cr'
Jan 18 23:44:09.960: INFO: stderr: ""
Jan 18 23:44:09.960: INFO: stdout: "e2e-test-crd-publish-openapi-3126-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 18 23:44:09.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=crd-publish-openapi-786 explain e2e-test-crd-publish-openapi-3126-crds'
Jan 18 23:44:11.149: INFO: stderr: ""
Jan 18 23:44:11.149: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3126-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:44:19.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-786" for this suite.

• [SLOW TEST:21.256 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":356,"completed":272,"skipped":5145,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:44:19.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override arguments
W0118 23:44:19.487281      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:44:19.487: INFO: Waiting up to 5m0s for pod "client-containers-618f1f32-07d3-4c1f-8c77-30a6f3207d4b" in namespace "containers-2658" to be "Succeeded or Failed"
Jan 18 23:44:19.500: INFO: Pod "client-containers-618f1f32-07d3-4c1f-8c77-30a6f3207d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.406292ms
Jan 18 23:44:21.504: INFO: Pod "client-containers-618f1f32-07d3-4c1f-8c77-30a6f3207d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017306304s
Jan 18 23:44:23.508: INFO: Pod "client-containers-618f1f32-07d3-4c1f-8c77-30a6f3207d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021042148s
Jan 18 23:44:25.510: INFO: Pod "client-containers-618f1f32-07d3-4c1f-8c77-30a6f3207d4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023531736s
STEP: Saw pod success
Jan 18 23:44:25.510: INFO: Pod "client-containers-618f1f32-07d3-4c1f-8c77-30a6f3207d4b" satisfied condition "Succeeded or Failed"
Jan 18 23:44:25.512: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod client-containers-618f1f32-07d3-4c1f-8c77-30a6f3207d4b container agnhost-container: <nil>
STEP: delete the pod
Jan 18 23:44:25.528: INFO: Waiting for pod client-containers-618f1f32-07d3-4c1f-8c77-30a6f3207d4b to disappear
Jan 18 23:44:25.530: INFO: Pod client-containers-618f1f32-07d3-4c1f-8c77-30a6f3207d4b no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 18 23:44:25.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2658" for this suite.

• [SLOW TEST:6.186 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","total":356,"completed":273,"skipped":5170,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:44:25.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-87809371-90a3-4945-a641-2a1ca533529a
STEP: Creating a pod to test consume configMaps
Jan 18 23:44:25.611: INFO: Waiting up to 5m0s for pod "pod-configmaps-91c7bdd9-76e7-42ec-a966-b25034791ca9" in namespace "configmap-1997" to be "Succeeded or Failed"
Jan 18 23:44:25.617: INFO: Pod "pod-configmaps-91c7bdd9-76e7-42ec-a966-b25034791ca9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.105215ms
Jan 18 23:44:27.619: INFO: Pod "pod-configmaps-91c7bdd9-76e7-42ec-a966-b25034791ca9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00876701s
Jan 18 23:44:29.623: INFO: Pod "pod-configmaps-91c7bdd9-76e7-42ec-a966-b25034791ca9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012310688s
Jan 18 23:44:31.626: INFO: Pod "pod-configmaps-91c7bdd9-76e7-42ec-a966-b25034791ca9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015142974s
STEP: Saw pod success
Jan 18 23:44:31.626: INFO: Pod "pod-configmaps-91c7bdd9-76e7-42ec-a966-b25034791ca9" satisfied condition "Succeeded or Failed"
Jan 18 23:44:31.628: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-configmaps-91c7bdd9-76e7-42ec-a966-b25034791ca9 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 18 23:44:31.639: INFO: Waiting for pod pod-configmaps-91c7bdd9-76e7-42ec-a966-b25034791ca9 to disappear
Jan 18 23:44:31.642: INFO: Pod pod-configmaps-91c7bdd9-76e7-42ec-a966-b25034791ca9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 18 23:44:31.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1997" for this suite.

• [SLOW TEST:6.112 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":274,"skipped":5194,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:44:31.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
W0118 23:44:31.673305      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:44:31.676: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 18 23:44:36.679: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Jan 18 23:44:36.681: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 18 23:44:36.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7733" for this suite.

• [SLOW TEST:5.066 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":356,"completed":275,"skipped":5205,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:44:36.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:44:36.734: INFO: Creating pod...
Jan 18 23:44:41.754: INFO: Creating service...
Jan 18 23:44:41.763: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9335/pods/agnhost/proxy?method=DELETE
Jan 18 23:44:41.775: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 23:44:41.775: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9335/pods/agnhost/proxy?method=OPTIONS
Jan 18 23:44:41.779: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 23:44:41.779: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9335/pods/agnhost/proxy?method=PATCH
Jan 18 23:44:41.781: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 23:44:41.781: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9335/pods/agnhost/proxy?method=POST
Jan 18 23:44:41.783: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 23:44:41.783: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9335/pods/agnhost/proxy?method=PUT
Jan 18 23:44:41.786: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 23:44:41.786: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9335/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 18 23:44:41.791: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 23:44:41.791: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9335/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 18 23:44:41.795: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 23:44:41.795: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9335/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 18 23:44:41.799: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 23:44:41.799: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9335/services/e2e-proxy-test-service/proxy?method=POST
Jan 18 23:44:41.802: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 23:44:41.803: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9335/services/e2e-proxy-test-service/proxy?method=PUT
Jan 18 23:44:41.806: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 23:44:41.806: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9335/pods/agnhost/proxy?method=GET
Jan 18 23:44:41.809: INFO: http.Client request:GET StatusCode:301
Jan 18 23:44:41.809: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9335/services/e2e-proxy-test-service/proxy?method=GET
Jan 18 23:44:41.813: INFO: http.Client request:GET StatusCode:301
Jan 18 23:44:41.813: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9335/pods/agnhost/proxy?method=HEAD
Jan 18 23:44:41.824: INFO: http.Client request:HEAD StatusCode:301
Jan 18 23:44:41.824: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9335/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 18 23:44:41.833: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jan 18 23:44:41.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9335" for this suite.

• [SLOW TEST:5.133 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","total":356,"completed":276,"skipped":5215,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:44:41.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:44:41.897: INFO: Creating ReplicaSet my-hostname-basic-30f476ff-814f-4c54-a504-35d9c5914315
W0118 23:44:41.915813      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-30f476ff-814f-4c54-a504-35d9c5914315" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-30f476ff-814f-4c54-a504-35d9c5914315" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-30f476ff-814f-4c54-a504-35d9c5914315" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-30f476ff-814f-4c54-a504-35d9c5914315" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:44:41.921: INFO: Pod name my-hostname-basic-30f476ff-814f-4c54-a504-35d9c5914315: Found 0 pods out of 1
Jan 18 23:44:46.924: INFO: Pod name my-hostname-basic-30f476ff-814f-4c54-a504-35d9c5914315: Found 1 pods out of 1
Jan 18 23:44:46.924: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-30f476ff-814f-4c54-a504-35d9c5914315" is running
Jan 18 23:44:46.926: INFO: Pod "my-hostname-basic-30f476ff-814f-4c54-a504-35d9c5914315-k5zh8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:44:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:44:45 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:44:45 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 23:44:41 +0000 UTC Reason: Message:}])
Jan 18 23:44:46.926: INFO: Trying to dial the pod
Jan 18 23:44:51.936: INFO: Controller my-hostname-basic-30f476ff-814f-4c54-a504-35d9c5914315: Got expected result from replica 1 [my-hostname-basic-30f476ff-814f-4c54-a504-35d9c5914315-k5zh8]: "my-hostname-basic-30f476ff-814f-4c54-a504-35d9c5914315-k5zh8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 18 23:44:51.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9415" for this suite.

• [SLOW TEST:10.095 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":277,"skipped":5234,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:44:51.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 23:44:52.060: INFO: Waiting up to 5m0s for pod "downwardapi-volume-528029b1-137a-4327-b73f-ec551d00ddbf" in namespace "projected-3546" to be "Succeeded or Failed"
Jan 18 23:44:52.095: INFO: Pod "downwardapi-volume-528029b1-137a-4327-b73f-ec551d00ddbf": Phase="Pending", Reason="", readiness=false. Elapsed: 34.226539ms
Jan 18 23:44:54.097: INFO: Pod "downwardapi-volume-528029b1-137a-4327-b73f-ec551d00ddbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036879522s
Jan 18 23:44:56.101: INFO: Pod "downwardapi-volume-528029b1-137a-4327-b73f-ec551d00ddbf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040342496s
Jan 18 23:44:58.104: INFO: Pod "downwardapi-volume-528029b1-137a-4327-b73f-ec551d00ddbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043141568s
STEP: Saw pod success
Jan 18 23:44:58.104: INFO: Pod "downwardapi-volume-528029b1-137a-4327-b73f-ec551d00ddbf" satisfied condition "Succeeded or Failed"
Jan 18 23:44:58.108: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-528029b1-137a-4327-b73f-ec551d00ddbf container client-container: <nil>
STEP: delete the pod
Jan 18 23:44:58.128: INFO: Waiting for pod downwardapi-volume-528029b1-137a-4327-b73f-ec551d00ddbf to disappear
Jan 18 23:44:58.129: INFO: Pod downwardapi-volume-528029b1-137a-4327-b73f-ec551d00ddbf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 18 23:44:58.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3546" for this suite.

• [SLOW TEST:6.193 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":278,"skipped":5243,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:44:58.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 18 23:44:58.202: INFO: Waiting up to 5m0s for pod "pod-2650e5f1-c624-4214-a012-cbb3bca31c1a" in namespace "emptydir-951" to be "Succeeded or Failed"
Jan 18 23:44:58.213: INFO: Pod "pod-2650e5f1-c624-4214-a012-cbb3bca31c1a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.379811ms
Jan 18 23:45:00.218: INFO: Pod "pod-2650e5f1-c624-4214-a012-cbb3bca31c1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015901406s
Jan 18 23:45:02.223: INFO: Pod "pod-2650e5f1-c624-4214-a012-cbb3bca31c1a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021137193s
Jan 18 23:45:04.226: INFO: Pod "pod-2650e5f1-c624-4214-a012-cbb3bca31c1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023928652s
STEP: Saw pod success
Jan 18 23:45:04.226: INFO: Pod "pod-2650e5f1-c624-4214-a012-cbb3bca31c1a" satisfied condition "Succeeded or Failed"
Jan 18 23:45:04.228: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-2650e5f1-c624-4214-a012-cbb3bca31c1a container test-container: <nil>
STEP: delete the pod
Jan 18 23:45:04.242: INFO: Waiting for pod pod-2650e5f1-c624-4214-a012-cbb3bca31c1a to disappear
Jan 18 23:45:04.245: INFO: Pod pod-2650e5f1-c624-4214-a012-cbb3bca31c1a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 18 23:45:04.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-951" for this suite.

• [SLOW TEST:6.113 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":279,"skipped":5300,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:45:04.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's command
W0118 23:45:04.301059      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:45:04.301: INFO: Waiting up to 5m0s for pod "var-expansion-bb8870c8-51ec-425d-90ea-561b4b19136c" in namespace "var-expansion-9091" to be "Succeeded or Failed"
Jan 18 23:45:04.309: INFO: Pod "var-expansion-bb8870c8-51ec-425d-90ea-561b4b19136c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.796268ms
Jan 18 23:45:06.313: INFO: Pod "var-expansion-bb8870c8-51ec-425d-90ea-561b4b19136c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011925831s
Jan 18 23:45:08.315: INFO: Pod "var-expansion-bb8870c8-51ec-425d-90ea-561b4b19136c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014461905s
Jan 18 23:45:10.319: INFO: Pod "var-expansion-bb8870c8-51ec-425d-90ea-561b4b19136c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017900094s
STEP: Saw pod success
Jan 18 23:45:10.319: INFO: Pod "var-expansion-bb8870c8-51ec-425d-90ea-561b4b19136c" satisfied condition "Succeeded or Failed"
Jan 18 23:45:10.321: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod var-expansion-bb8870c8-51ec-425d-90ea-561b4b19136c container dapi-container: <nil>
STEP: delete the pod
Jan 18 23:45:10.333: INFO: Waiting for pod var-expansion-bb8870c8-51ec-425d-90ea-561b4b19136c to disappear
Jan 18 23:45:10.335: INFO: Pod var-expansion-bb8870c8-51ec-425d-90ea-561b4b19136c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 18 23:45:10.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9091" for this suite.

• [SLOW TEST:6.094 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":356,"completed":280,"skipped":5304,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:45:10.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 23:45:10.873: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 23:45:12.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 45, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 45, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 45, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 45, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 23:45:15.894: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:45:16.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4163" for this suite.
STEP: Destroying namespace "webhook-4163-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.797 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":356,"completed":281,"skipped":5320,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:45:16.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jan 18 23:45:16.275: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8960  f20fb59d-9808-40a2-b2d3-577ae3807917 293229 0 2023-01-18 23:45:16 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2023-01-18 23:45:16 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kmz97,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kmz97,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 23:45:16.281: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:45:18.284: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:45:20.284: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jan 18 23:45:20.284: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8960 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:45:20.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:45:20.284: INFO: ExecWithOptions: Clientset creation
Jan 18 23:45:20.284: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-8960/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod...
Jan 18 23:45:20.359: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8960 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:45:20.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:45:20.360: INFO: ExecWithOptions: Clientset creation
Jan 18 23:45:20.360: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-8960/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 23:45:20.439: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 18 23:45:20.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8960" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":356,"completed":282,"skipped":5329,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:45:20.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 23:45:20.496: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 23:45:20.518: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 23:45:20.536: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-128-7.ec2.internal before test
Jan 18 23:45:20.591: INFO: addon-operator-manager-c8c4859bf-84wp8 from openshift-addon-operator started at 2023-01-18 21:36:25 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container manager ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container metrics-relay-server ready: true, restart count 0
Jan 18 23:45:20.591: INFO: addon-operator-webhooks-569bd4cfc5-dz6nk from openshift-addon-operator started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container webhook ready: true, restart count 0
Jan 18 23:45:20.591: INFO: aws-ebs-csi-driver-node-mq4qf from openshift-cluster-csi-drivers started at 2023-01-18 21:27:18 +0000 UTC (3 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:45:20.591: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:45:20.591: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:45:20.591: INFO: tuned-l9d9s from openshift-cluster-node-tuning-operator started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:45:20.591: INFO: dns-default-btls8 from openshift-dns started at 2023-01-18 22:06:24 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container dns ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:45:20.591: INFO: node-resolver-vhz2m from openshift-dns started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:45:20.591: INFO: node-ca-6cvvj from openshift-image-registry started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:45:20.591: INFO: ingress-canary-qw8v9 from openshift-ingress-canary started at 2023-01-18 21:28:03 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 23:45:20.591: INFO: router-default-7cff97cd98-dqgwn from openshift-ingress started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container router ready: true, restart count 0
Jan 18 23:45:20.591: INFO: machine-config-daemon-htf69 from openshift-machine-config-operator started at 2023-01-18 21:27:18 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:45:20.591: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:45:20.591: INFO: osd-patch-subscription-source-27901320-gz2jm from openshift-marketplace started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 18 23:45:20.591: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (6 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container alertmanager ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 23:45:20.591: INFO: kube-state-metrics-76877575d5-ptnh8 from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (3 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 18 23:45:20.591: INFO: node-exporter-spmbc from openshift-monitoring started at 2023-01-18 21:27:18 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.591: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:45:20.591: INFO: openshift-state-metrics-59fc669d8d-v25vz from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (3 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jan 18 23:45:20.591: INFO: prometheus-adapter-cf64f7f46-7bf79 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 18 23:45:20.591: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (6 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container prometheus ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 18 23:45:20.591: INFO: prometheus-operator-76957bb5bd-2pztb from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 18 23:45:20.591: INFO: prometheus-operator-admission-webhook-577cc9c956-d6fkv from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 23:45:20.591: INFO: sre-dns-latency-exporter-8dptc from openshift-monitoring started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container main ready: true, restart count 1
Jan 18 23:45:20.591: INFO: telemeter-client-6bb748465-zttx9 from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (3 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container reload ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container telemeter-client ready: true, restart count 0
Jan 18 23:45:20.591: INFO: thanos-querier-57f44c5498-xjsz4 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (6 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container thanos-query ready: true, restart count 0
Jan 18 23:45:20.591: INFO: multus-additional-cni-plugins-5lmsf from openshift-multus started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:45:20.591: INFO: multus-znqj6 from openshift-multus started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:45:20.591: INFO: network-metrics-daemon-ffdlk from openshift-multus started at 2023-01-18 21:27:18 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.591: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:45:20.591: INFO: network-check-target-xwp8p from openshift-network-diagnostics started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:45:20.591: INFO: ovnkube-node-mkzt4 from openshift-ovn-kubernetes started at 2023-01-18 21:27:18 +0000 UTC (5 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.591: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:45:20.591: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:45:20.591: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:45:20.591: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:45:20.591: INFO: splunkforwarder-ds-fbjlj from openshift-security started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 23:45:20.591: INFO: builds-pruner-27901380-cpft8 from openshift-sre-pruning started at 2023-01-18 23:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 18 23:45:20.591: INFO: deployments-pruner-27901320-vt4j8 from openshift-sre-pruning started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 18 23:45:20.591: INFO: deployments-pruner-27901380-b6sl8 from openshift-sre-pruning started at 2023-01-18 23:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 18 23:45:20.591: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-dwhpp from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.591: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:45:20.591: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-200-13.ec2.internal before test
Jan 18 23:45:20.647: INFO: aws-ebs-csi-driver-node-6nf8l from openshift-cluster-csi-drivers started at 2023-01-18 21:02:39 +0000 UTC (3 container statuses recorded)
Jan 18 23:45:20.647: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:45:20.647: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:45:20.647: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:45:20.647: INFO: tuned-t6mkr from openshift-cluster-node-tuning-operator started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.647: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:45:20.647: INFO: downloads-6f74f6fcbf-hcggr from openshift-console started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.647: INFO: 	Container download-server ready: true, restart count 0
Jan 18 23:45:20.647: INFO: dns-default-vkx5x from openshift-dns started at 2023-01-18 21:03:27 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container dns ready: true, restart count 1
Jan 18 23:45:20.648: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.648: INFO: node-resolver-gxvcl from openshift-dns started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:45:20.648: INFO: image-registry-7b8f8dcdc5-4lfps from openshift-image-registry started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container registry ready: true, restart count 0
Jan 18 23:45:20.648: INFO: node-ca-5tw9r from openshift-image-registry started at 2023-01-18 21:02:58 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:45:20.648: INFO: ingress-canary-ppjjg from openshift-ingress-canary started at 2023-01-18 21:03:27 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 23:45:20.648: INFO: machine-config-daemon-smj7g from openshift-machine-config-operator started at 2023-01-18 21:02:39 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:45:20.648: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:45:20.648: INFO: managed-node-metadata-operator-5d4c567575-cbrxh from openshift-managed-node-metadata-operator started at 2023-01-18 21:24:36 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container managed-node-metadata-operator ready: true, restart count 0
Jan 18 23:45:20.648: INFO: configure-alertmanager-operator-7565458cf4-pbmjl from openshift-monitoring started at 2023-01-18 21:24:36 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Jan 18 23:45:20.648: INFO: node-exporter-gzf8b from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.648: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:45:20.648: INFO: sre-dns-latency-exporter-s6nk6 from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container main ready: true, restart count 1
Jan 18 23:45:20.648: INFO: token-refresher-6d8f85f497-mhth9 from openshift-monitoring started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container token-refresher ready: true, restart count 0
Jan 18 23:45:20.648: INFO: multus-additional-cni-plugins-nm8v9 from openshift-multus started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:45:20.648: INFO: multus-cfrv2 from openshift-multus started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:45:20.648: INFO: network-metrics-daemon-m4c6r from openshift-multus started at 2023-01-18 21:02:39 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.648: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:45:20.648: INFO: network-check-target-hpzlx from openshift-network-diagnostics started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:45:20.648: INFO: obo-prometheus-operator-admission-webhook-5b4dc9bff5-l6qvc from openshift-observability-operator started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 23:45:20.648: INFO: ocm-agent-75d95f8dc7-gtjwg from openshift-ocm-agent-operator started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container ocm-agent ready: true, restart count 0
Jan 18 23:45:20.648: INFO: collect-profiles-27901395-s69sn from openshift-operator-lifecycle-manager started at 2023-01-18 23:15:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 18 23:45:20.648: INFO: collect-profiles-27901410-gd7tw from openshift-operator-lifecycle-manager started at 2023-01-18 23:30:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 18 23:45:20.648: INFO: collect-profiles-27901425-9wzjz from openshift-operator-lifecycle-manager started at 2023-01-18 23:45:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 18 23:45:20.648: INFO: osd-metrics-exporter-756f85967-8z4cz from openshift-osd-metrics started at 2023-01-18 21:25:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container osd-metrics-exporter ready: true, restart count 0
Jan 18 23:45:20.648: INFO: osd-metrics-exporter-registry-6rnzs from openshift-osd-metrics started at 2023-01-18 21:24:12 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:45:20.648: INFO: ovnkube-node-h62xt from openshift-ovn-kubernetes started at 2023-01-18 21:02:39 +0000 UTC (5 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.648: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:45:20.648: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:45:20.648: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:45:20.648: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:45:20.648: INFO: rbac-permissions-operator-7f6bc8977-vsvkh from openshift-rbac-permissions started at 2023-01-18 21:24:29 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Jan 18 23:45:20.648: INFO: blackbox-exporter-dfdd57dd6-tj4kz from openshift-route-monitor-operator started at 2023-01-18 21:24:50 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan 18 23:45:20.648: INFO: route-monitor-operator-controller-manager-cbc597f9d-84p87 from openshift-route-monitor-operator started at 2023-01-18 21:24:37 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container manager ready: true, restart count 0
Jan 18 23:45:20.648: INFO: route-monitor-operator-registry-cgj4n from openshift-route-monitor-operator started at 2023-01-18 21:24:36 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:45:20.648: INFO: splunkforwarder-ds-n7d7t from openshift-security started at 2023-01-18 21:24:58 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container splunk-uf ready: true, restart count 0
Jan 18 23:45:20.648: INFO: splunk-forwarder-operator-6f5bf57497-jwcz5 from openshift-splunk-forwarder-operator started at 2023-01-18 21:24:40 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Jan 18 23:45:20.648: INFO: managed-velero-operator-f9f4c8b45-xml88 from openshift-velero started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container managed-velero-operator ready: true, restart count 0
Jan 18 23:45:20.648: INFO: velero-749f7746d8-ds5cs from openshift-velero started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container velero ready: true, restart count 0
Jan 18 23:45:20.648: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-mdcrg from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.648: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:45:20.648: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:45:20.648: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-211-217.ec2.internal before test
Jan 18 23:45:20.689: INFO: addon-operator-webhooks-569bd4cfc5-mdklc from openshift-addon-operator started at 2023-01-18 21:39:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.689: INFO: 	Container webhook ready: true, restart count 0
Jan 18 23:45:20.689: INFO: osd-delete-ownerrefs-serviceaccounts-27901357-rd8sm from openshift-backplane-srep started at 2023-01-18 22:37:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.689: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 18 23:45:20.689: INFO: osd-delete-ownerrefs-serviceaccounts-27901387-lzh55 from openshift-backplane-srep started at 2023-01-18 23:07:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.689: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 18 23:45:20.689: INFO: osd-delete-ownerrefs-serviceaccounts-27901417-kbrf6 from openshift-backplane-srep started at 2023-01-18 23:37:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.689: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 18 23:45:20.689: INFO: osd-delete-backplane-serviceaccounts-27901400-q69vf from openshift-backplane started at 2023-01-18 23:20:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.689: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 18 23:45:20.689: INFO: osd-delete-backplane-serviceaccounts-27901410-lrs6c from openshift-backplane started at 2023-01-18 23:30:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.689: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 18 23:45:20.689: INFO: osd-delete-backplane-serviceaccounts-27901420-s8d8r from openshift-backplane started at 2023-01-18 23:40:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.689: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 18 23:45:20.689: INFO: sre-build-test-27901391-sgdf9 from openshift-build-test started at 2023-01-18 23:11:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.689: INFO: 	Container sre-build-test ready: false, restart count 0
Jan 18 23:45:20.689: INFO: aws-ebs-csi-driver-node-l4ng8 from openshift-cluster-csi-drivers started at 2023-01-18 21:27:42 +0000 UTC (3 container statuses recorded)
Jan 18 23:45:20.689: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:45:20.689: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:45:20.689: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:45:20.689: INFO: tuned-6pmzm from openshift-cluster-node-tuning-operator started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.689: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:45:20.689: INFO: dns-default-7sqw2 from openshift-dns started at 2023-01-18 22:06:24 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.689: INFO: 	Container dns ready: true, restart count 0
Jan 18 23:45:20.689: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:45:20.690: INFO: node-resolver-qsc9k from openshift-dns started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:45:20.690: INFO: image-pruner-27901320-rjnq8 from openshift-image-registry started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container image-pruner ready: false, restart count 0
Jan 18 23:45:20.690: INFO: image-pruner-27901380-mbpm4 from openshift-image-registry started at 2023-01-18 23:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container image-pruner ready: false, restart count 0
Jan 18 23:45:20.690: INFO: node-ca-9gnsm from openshift-image-registry started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:45:20.690: INFO: ingress-canary-k52kt from openshift-ingress-canary started at 2023-01-18 21:28:29 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 23:45:20.690: INFO: router-default-7cff97cd98-lstfd from openshift-ingress started at 2023-01-18 21:39:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container router ready: true, restart count 0
Jan 18 23:45:20.690: INFO: machine-config-daemon-krjvz from openshift-machine-config-operator started at 2023-01-18 21:27:42 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:45:20.690: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:45:20.690: INFO: osd-patch-subscription-source-27901380-gmtz6 from openshift-marketplace started at 2023-01-18 23:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 18 23:45:20.690: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-18 21:39:18 +0000 UTC (6 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container alertmanager ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 23:45:20.690: INFO: node-exporter-bpnc5 from openshift-monitoring started at 2023-01-18 21:27:42 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.690: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:45:20.690: INFO: osd-rebalance-infra-nodes-27901395-h44mq from openshift-monitoring started at 2023-01-18 23:15:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 18 23:45:20.690: INFO: osd-rebalance-infra-nodes-27901410-fkk69 from openshift-monitoring started at 2023-01-18 23:30:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 18 23:45:20.690: INFO: osd-rebalance-infra-nodes-27901425-6smbb from openshift-monitoring started at 2023-01-18 23:45:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 18 23:45:20.690: INFO: prometheus-adapter-cf64f7f46-jw64l from openshift-monitoring started at 2023-01-18 21:39:31 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 18 23:45:20.690: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-18 21:39:17 +0000 UTC (6 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container prometheus ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 18 23:45:20.690: INFO: prometheus-operator-admission-webhook-577cc9c956-lwfrr from openshift-monitoring started at 2023-01-18 21:39:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 23:45:20.690: INFO: sre-dns-latency-exporter-tlqwt from openshift-monitoring started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container main ready: true, restart count 1
Jan 18 23:45:20.690: INFO: thanos-querier-57f44c5498-fzjjb from openshift-monitoring started at 2023-01-18 21:39:18 +0000 UTC (6 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container thanos-query ready: true, restart count 0
Jan 18 23:45:20.690: INFO: multus-additional-cni-plugins-2q4s4 from openshift-multus started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:45:20.690: INFO: multus-cz5tw from openshift-multus started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:45:20.690: INFO: network-metrics-daemon-jnkxc from openshift-multus started at 2023-01-18 21:27:42 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.690: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:45:20.690: INFO: network-check-target-pjt5v from openshift-network-diagnostics started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:45:20.690: INFO: ovnkube-node-bf8j7 from openshift-ovn-kubernetes started at 2023-01-18 21:27:42 +0000 UTC (5 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.690: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:45:20.690: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:45:20.690: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:45:20.690: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:45:20.690: INFO: splunkforwarder-ds-gr4fb from openshift-security started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 23:45:20.690: INFO: builds-pruner-27901320-2bl9l from openshift-sre-pruning started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 18 23:45:20.690: INFO: sonobuoy-e2e-job-aed0e1be5f434192 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container e2e ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:45:20.690: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-tjlv7 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.690: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:45:20.690: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-219-147.ec2.internal before test
Jan 18 23:45:20.729: INFO: aws-ebs-csi-driver-node-qprtw from openshift-cluster-csi-drivers started at 2023-01-18 21:03:48 +0000 UTC (3 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:45:20.729: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:45:20.729: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:45:20.729: INFO: tuned-246td from openshift-cluster-node-tuning-operator started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:45:20.729: INFO: dns-default-hhrdx from openshift-dns started at 2023-01-18 23:05:43 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container dns ready: true, restart count 0
Jan 18 23:45:20.729: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:45:20.729: INFO: node-resolver-svmsb from openshift-dns started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:45:20.729: INFO: node-ca-nzbcp from openshift-image-registry started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:45:20.729: INFO: ingress-canary-fvl4z from openshift-ingress-canary started at 2023-01-18 23:05:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 18 23:45:20.729: INFO: machine-config-daemon-nb5xf from openshift-machine-config-operator started at 2023-01-18 21:03:48 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:45:20.729: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:45:20.729: INFO: node-exporter-6m8v9 from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.729: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:45:20.729: INFO: sre-dns-latency-exporter-rxh5d from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container main ready: true, restart count 1
Jan 18 23:45:20.729: INFO: multus-55cv4 from openshift-multus started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:45:20.729: INFO: multus-additional-cni-plugins-6n2cz from openshift-multus started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:45:20.729: INFO: network-metrics-daemon-mkvpn from openshift-multus started at 2023-01-18 21:03:48 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.729: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:45:20.729: INFO: network-check-target-cqk2p from openshift-network-diagnostics started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:45:20.729: INFO: ovnkube-node-7vtb8 from openshift-ovn-kubernetes started at 2023-01-18 21:03:48 +0000 UTC (5 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.729: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:45:20.729: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:45:20.729: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:45:20.729: INFO: 	Container ovnkube-node ready: true, restart count 4
Jan 18 23:45:20.729: INFO: splunkforwarder-ds-b6n2g from openshift-security started at 2023-01-18 21:24:58 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 23:45:20.729: INFO: sonobuoy from sonobuoy started at 2023-01-18 22:14:33 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 23:45:20.729: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-shjrl from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.729: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:45:20.729: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:45:20.729: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-236-5.ec2.internal before test
Jan 18 23:45:20.764: INFO: addon-operator-catalog-67nfj from openshift-addon-operator started at 2023-01-18 21:26:21 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:45:20.764: INFO: cloud-ingress-operator-registry-6f5sh from openshift-cloud-ingress-operator started at 2023-01-18 21:31:22 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:45:20.764: INFO: aws-ebs-csi-driver-node-cv9gd from openshift-cluster-csi-drivers started at 2023-01-18 21:03:23 +0000 UTC (3 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:45:20.764: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:45:20.764: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:45:20.764: INFO: tuned-b2gvw from openshift-cluster-node-tuning-operator started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:45:20.764: INFO: downloads-6f74f6fcbf-w6ht7 from openshift-console started at 2023-01-18 21:31:17 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container download-server ready: true, restart count 0
Jan 18 23:45:20.764: INFO: custom-domains-operator-7f97f586c8-czz24 from openshift-custom-domains-operator started at 2023-01-18 21:26:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container custom-domains-operator ready: true, restart count 0
Jan 18 23:45:20.764: INFO: custom-domains-operator-registry-sd6db from openshift-custom-domains-operator started at 2023-01-18 21:26:24 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:45:20.764: INFO: dns-default-4cddc from openshift-dns started at 2023-01-18 21:04:10 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container dns ready: true, restart count 1
Jan 18 23:45:20.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.764: INFO: node-resolver-8lsl6 from openshift-dns started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:45:20.764: INFO: node-ca-r29sw from openshift-image-registry started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:45:20.764: INFO: ingress-canary-d6jm7 from openshift-ingress-canary started at 2023-01-18 21:04:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 23:45:20.764: INFO: machine-config-daemon-xv85b from openshift-machine-config-operator started at 2023-01-18 21:03:23 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:45:20.764: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:45:20.764: INFO: managed-node-metadata-operator-registry-fn84g from openshift-managed-node-metadata-operator started at 2023-01-18 21:31:22 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:45:20.764: INFO: managed-upgrade-operator-catalog-g29pr from openshift-managed-upgrade-operator started at 2023-01-18 21:31:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:45:20.764: INFO: configure-alertmanager-operator-registry-vzl8c from openshift-monitoring started at 2023-01-18 21:31:25 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:45:20.764: INFO: node-exporter-6q46p from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.764: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:45:20.764: INFO: osd-cluster-ready-kfqk7 from openshift-monitoring started at 2023-01-18 21:26:19 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container osd-cluster-ready ready: false, restart count 31
Jan 18 23:45:20.764: INFO: sre-dns-latency-exporter-q7lp5 from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container main ready: true, restart count 1
Jan 18 23:45:20.764: INFO: sre-ebs-iops-reporter-1-l2chd from openshift-monitoring started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container main ready: true, restart count 0
Jan 18 23:45:20.764: INFO: sre-stuck-ebs-vols-1-mjmmn from openshift-monitoring started at 2023-01-18 21:26:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container main ready: true, restart count 0
Jan 18 23:45:20.764: INFO: multus-additional-cni-plugins-p78ph from openshift-multus started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:45:20.764: INFO: multus-wtvrb from openshift-multus started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:45:20.764: INFO: network-metrics-daemon-dxnrq from openshift-multus started at 2023-01-18 21:03:23 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.764: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:45:20.764: INFO: must-gather-operator-5f47db765d-zf7fj from openshift-must-gather-operator started at 2023-01-18 21:26:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container must-gather-operator ready: true, restart count 0
Jan 18 23:45:20.764: INFO: must-gather-operator-registry-gp8pc from openshift-must-gather-operator started at 2023-01-18 21:26:22 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:45:20.764: INFO: network-check-target-tr8pn from openshift-network-diagnostics started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:45:20.764: INFO: obo-prometheus-operator-6cb5cfc7b9-2rkdt from openshift-observability-operator started at 2023-01-18 21:26:20 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 18 23:45:20.764: INFO: obo-prometheus-operator-admission-webhook-5b4dc9bff5-f8pm5 from openshift-observability-operator started at 2023-01-18 21:26:21 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 23:45:20.764: INFO: observability-operator-5b467d8ccb-twlnn from openshift-observability-operator started at 2023-01-18 21:26:20 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container operator ready: true, restart count 0
Jan 18 23:45:20.764: INFO: observability-operator-catalog-khgpx from openshift-observability-operator started at 2023-01-18 21:26:25 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:45:20.764: INFO: ocm-agent-operator-9bd68bf49-z6qxl from openshift-ocm-agent-operator started at 2023-01-18 21:26:19 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container ocm-agent-operator ready: true, restart count 0
Jan 18 23:45:20.764: INFO: ocm-agent-operator-registry-qj69h from openshift-ocm-agent-operator started at 2023-01-18 21:26:24 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:45:20.764: INFO: ovnkube-node-g6c8t from openshift-ovn-kubernetes started at 2023-01-18 21:03:23 +0000 UTC (5 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.764: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:45:20.764: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:45:20.764: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:45:20.764: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:45:20.764: INFO: rbac-permissions-operator-registry-mqh7k from openshift-rbac-permissions started at 2023-01-18 21:26:26 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:45:20.764: INFO: splunkforwarder-ds-zhvhl from openshift-security started at 2023-01-18 21:25:56 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container splunk-uf ready: true, restart count 0
Jan 18 23:45:20.764: INFO: splunk-forwarder-operator-catalog-twzbw from openshift-splunk-forwarder-operator started at 2023-01-18 21:26:27 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:45:20.764: INFO: managed-velero-operator-registry-9j2kp from openshift-velero started at 2023-01-18 21:31:21 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:45:20.764: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-t5zp4 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.764: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:45:20.764: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:45:20.764: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-253-152.ec2.internal before test
Jan 18 23:45:20.795: INFO: cloud-ingress-operator-78d58985cd-9kp9q from openshift-cloud-ingress-operator started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Jan 18 23:45:20.795: INFO: aws-ebs-csi-driver-node-97pk2 from openshift-cluster-csi-drivers started at 2023-01-18 21:03:06 +0000 UTC (3 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:45:20.795: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:45:20.795: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:45:20.795: INFO: tuned-5wpdt from openshift-cluster-node-tuning-operator started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:45:20.795: INFO: dns-default-g4nw5 from openshift-dns started at 2023-01-18 21:03:53 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container dns ready: true, restart count 1
Jan 18 23:45:20.795: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.795: INFO: node-resolver-ffd24 from openshift-dns started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:45:20.795: INFO: image-registry-7b8f8dcdc5-2bvm7 from openshift-image-registry started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container registry ready: true, restart count 0
Jan 18 23:45:20.795: INFO: node-ca-bh7jz from openshift-image-registry started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:45:20.795: INFO: ingress-canary-vz8dj from openshift-ingress-canary started at 2023-01-18 21:03:53 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 23:45:20.795: INFO: machine-config-daemon-5zsxx from openshift-machine-config-operator started at 2023-01-18 21:03:06 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:45:20.795: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:45:20.795: INFO: node-exporter-z5k8l from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.795: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:45:20.795: INFO: sre-dns-latency-exporter-zsmln from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container main ready: true, restart count 1
Jan 18 23:45:20.795: INFO: multus-additional-cni-plugins-hsjtm from openshift-multus started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:45:20.795: INFO: multus-xzh2h from openshift-multus started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:45:20.795: INFO: network-metrics-daemon-cq5sl from openshift-multus started at 2023-01-18 21:03:06 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.795: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:45:20.795: INFO: network-check-source-5cb989cf6f-42gl2 from openshift-network-diagnostics started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container check-endpoints ready: true, restart count 0
Jan 18 23:45:20.795: INFO: network-check-target-9fq7l from openshift-network-diagnostics started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:45:20.795: INFO: ovnkube-node-mtfln from openshift-ovn-kubernetes started at 2023-01-18 21:03:06 +0000 UTC (5 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:45:20.795: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:45:20.795: INFO: 	Container ovn-acl-logging ready: true, restart count 2
Jan 18 23:45:20.795: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:45:20.795: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:45:20.795: INFO: splunkforwarder-ds-xsjtq from openshift-security started at 2023-01-18 21:24:58 +0000 UTC (1 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 23:45:20.795: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-ktxs4 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 23:45:20.795: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:45:20.795: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2b2683a1-efbb-4c8e-9c2f-028d9a64ffdf 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.219.147 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-2b2683a1-efbb-4c8e-9c2f-028d9a64ffdf off the node ip-10-0-219-147.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2b2683a1-efbb-4c8e-9c2f-028d9a64ffdf
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 18 23:50:28.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9439" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:308.497 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":356,"completed":283,"skipped":5375,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:50:28.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 18 23:50:29.130: INFO: Waiting up to 5m0s for pod "pod-d38838e4-74dd-43c5-8438-ad84eae92754" in namespace "emptydir-1408" to be "Succeeded or Failed"
Jan 18 23:50:29.156: INFO: Pod "pod-d38838e4-74dd-43c5-8438-ad84eae92754": Phase="Pending", Reason="", readiness=false. Elapsed: 25.56153ms
Jan 18 23:50:31.176: INFO: Pod "pod-d38838e4-74dd-43c5-8438-ad84eae92754": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046268134s
Jan 18 23:50:33.211: INFO: Pod "pod-d38838e4-74dd-43c5-8438-ad84eae92754": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081061429s
Jan 18 23:50:35.217: INFO: Pod "pod-d38838e4-74dd-43c5-8438-ad84eae92754": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.086937602s
STEP: Saw pod success
Jan 18 23:50:35.217: INFO: Pod "pod-d38838e4-74dd-43c5-8438-ad84eae92754" satisfied condition "Succeeded or Failed"
Jan 18 23:50:35.219: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-d38838e4-74dd-43c5-8438-ad84eae92754 container test-container: <nil>
STEP: delete the pod
Jan 18 23:50:35.236: INFO: Waiting for pod pod-d38838e4-74dd-43c5-8438-ad84eae92754 to disappear
Jan 18 23:50:35.238: INFO: Pod pod-d38838e4-74dd-43c5-8438-ad84eae92754 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 18 23:50:35.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1408" for this suite.

• [SLOW TEST:6.291 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":284,"skipped":5398,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:50:35.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Jan 18 23:50:35.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:51:17.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8452" for this suite.

• [SLOW TEST:42.069 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":356,"completed":285,"skipped":5408,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:51:17.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 18 23:51:17.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1256" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":356,"completed":286,"skipped":5410,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:51:17.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:51:17.482: INFO: Got root ca configmap in namespace "svcaccounts-2578"
Jan 18 23:51:17.527: INFO: Deleted root ca configmap in namespace "svcaccounts-2578"
STEP: waiting for a new root ca configmap created
Jan 18 23:51:18.029: INFO: Recreated root ca configmap in namespace "svcaccounts-2578"
Jan 18 23:51:18.035: INFO: Updated root ca configmap in namespace "svcaccounts-2578"
STEP: waiting for the root ca configmap reconciled
Jan 18 23:51:18.538: INFO: Reconciled root ca configmap in namespace "svcaccounts-2578"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 18 23:51:18.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2578" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":356,"completed":287,"skipped":5429,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:51:18.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-50e721b8-0443-4334-9035-8ad26627daea
STEP: Creating a pod to test consume secrets
Jan 18 23:51:18.624: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-855ebc89-d8bc-4d32-9854-f650b6f69846" in namespace "projected-9803" to be "Succeeded or Failed"
Jan 18 23:51:18.631: INFO: Pod "pod-projected-secrets-855ebc89-d8bc-4d32-9854-f650b6f69846": Phase="Pending", Reason="", readiness=false. Elapsed: 6.950133ms
Jan 18 23:51:20.634: INFO: Pod "pod-projected-secrets-855ebc89-d8bc-4d32-9854-f650b6f69846": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010023794s
Jan 18 23:51:22.638: INFO: Pod "pod-projected-secrets-855ebc89-d8bc-4d32-9854-f650b6f69846": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013676852s
Jan 18 23:51:24.641: INFO: Pod "pod-projected-secrets-855ebc89-d8bc-4d32-9854-f650b6f69846": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017186403s
STEP: Saw pod success
Jan 18 23:51:24.641: INFO: Pod "pod-projected-secrets-855ebc89-d8bc-4d32-9854-f650b6f69846" satisfied condition "Succeeded or Failed"
Jan 18 23:51:24.643: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-projected-secrets-855ebc89-d8bc-4d32-9854-f650b6f69846 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 18 23:51:24.662: INFO: Waiting for pod pod-projected-secrets-855ebc89-d8bc-4d32-9854-f650b6f69846 to disappear
Jan 18 23:51:24.664: INFO: Pod pod-projected-secrets-855ebc89-d8bc-4d32-9854-f650b6f69846 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 18 23:51:24.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9803" for this suite.

• [SLOW TEST:6.131 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":288,"skipped":5440,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:51:24.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:51:24.706: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jan 18 23:51:26.734: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 18 23:51:27.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3275" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":356,"completed":289,"skipped":5441,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:51:27.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-a1949467-dc64-4230-9378-6d5bfa180970
STEP: Creating a pod to test consume secrets
Jan 18 23:51:27.809: INFO: Waiting up to 5m0s for pod "pod-secrets-a1889f3a-daa7-43b2-9499-17a6173946e1" in namespace "secrets-6760" to be "Succeeded or Failed"
Jan 18 23:51:27.811: INFO: Pod "pod-secrets-a1889f3a-daa7-43b2-9499-17a6173946e1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.941871ms
Jan 18 23:51:29.816: INFO: Pod "pod-secrets-a1889f3a-daa7-43b2-9499-17a6173946e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00717967s
Jan 18 23:51:31.820: INFO: Pod "pod-secrets-a1889f3a-daa7-43b2-9499-17a6173946e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010391242s
Jan 18 23:51:33.825: INFO: Pod "pod-secrets-a1889f3a-daa7-43b2-9499-17a6173946e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01569782s
STEP: Saw pod success
Jan 18 23:51:33.825: INFO: Pod "pod-secrets-a1889f3a-daa7-43b2-9499-17a6173946e1" satisfied condition "Succeeded or Failed"
Jan 18 23:51:33.827: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-secrets-a1889f3a-daa7-43b2-9499-17a6173946e1 container secret-volume-test: <nil>
STEP: delete the pod
Jan 18 23:51:33.839: INFO: Waiting for pod pod-secrets-a1889f3a-daa7-43b2-9499-17a6173946e1 to disappear
Jan 18 23:51:33.841: INFO: Pod pod-secrets-a1889f3a-daa7-43b2-9499-17a6173946e1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 18 23:51:33.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6760" for this suite.

• [SLOW TEST:6.100 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":290,"skipped":5450,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:51:33.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 18 23:51:40.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1517" for this suite.

• [SLOW TEST:7.041 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":356,"completed":291,"skipped":5499,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:51:40.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 18 23:51:45.986: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 18 23:51:45.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4642" for this suite.

• [SLOW TEST:5.117 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":356,"completed":292,"skipped":5524,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:51:46.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 23:51:46.076: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb6143e5-c20f-46f1-8669-1690176a3fb3" in namespace "projected-9774" to be "Succeeded or Failed"
Jan 18 23:51:46.080: INFO: Pod "downwardapi-volume-fb6143e5-c20f-46f1-8669-1690176a3fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.129322ms
Jan 18 23:51:48.083: INFO: Pod "downwardapi-volume-fb6143e5-c20f-46f1-8669-1690176a3fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007171829s
Jan 18 23:51:50.087: INFO: Pod "downwardapi-volume-fb6143e5-c20f-46f1-8669-1690176a3fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010419849s
Jan 18 23:51:52.091: INFO: Pod "downwardapi-volume-fb6143e5-c20f-46f1-8669-1690176a3fb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014517523s
STEP: Saw pod success
Jan 18 23:51:52.091: INFO: Pod "downwardapi-volume-fb6143e5-c20f-46f1-8669-1690176a3fb3" satisfied condition "Succeeded or Failed"
Jan 18 23:51:52.093: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-fb6143e5-c20f-46f1-8669-1690176a3fb3 container client-container: <nil>
STEP: delete the pod
Jan 18 23:51:52.111: INFO: Waiting for pod downwardapi-volume-fb6143e5-c20f-46f1-8669-1690176a3fb3 to disappear
Jan 18 23:51:52.113: INFO: Pod downwardapi-volume-fb6143e5-c20f-46f1-8669-1690176a3fb3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 18 23:51:52.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9774" for this suite.

• [SLOW TEST:6.113 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":293,"skipped":5525,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:51:52.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:51:52.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:51:53.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7306" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":356,"completed":294,"skipped":5551,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:51:53.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 18 23:51:53.406: INFO: Waiting up to 5m0s for pod "pod-fa0fe8c4-4dc8-4d83-af79-f431c19acd70" in namespace "emptydir-1989" to be "Succeeded or Failed"
Jan 18 23:51:53.409: INFO: Pod "pod-fa0fe8c4-4dc8-4d83-af79-f431c19acd70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186236ms
Jan 18 23:51:55.411: INFO: Pod "pod-fa0fe8c4-4dc8-4d83-af79-f431c19acd70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005025417s
Jan 18 23:51:57.417: INFO: Pod "pod-fa0fe8c4-4dc8-4d83-af79-f431c19acd70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010164778s
Jan 18 23:51:59.420: INFO: Pod "pod-fa0fe8c4-4dc8-4d83-af79-f431c19acd70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013116727s
STEP: Saw pod success
Jan 18 23:51:59.420: INFO: Pod "pod-fa0fe8c4-4dc8-4d83-af79-f431c19acd70" satisfied condition "Succeeded or Failed"
Jan 18 23:51:59.421: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-fa0fe8c4-4dc8-4d83-af79-f431c19acd70 container test-container: <nil>
STEP: delete the pod
Jan 18 23:51:59.436: INFO: Waiting for pod pod-fa0fe8c4-4dc8-4d83-af79-f431c19acd70 to disappear
Jan 18 23:51:59.438: INFO: Pod pod-fa0fe8c4-4dc8-4d83-af79-f431c19acd70 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 18 23:51:59.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1989" for this suite.

• [SLOW TEST:6.142 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":295,"skipped":5552,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:51:59.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:51:59.837: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 18 23:51:59.838: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 18 23:51:59.838: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 18 23:51:59.838: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 18 23:51:59.838: INFO: Checking APIGroup: apps
Jan 18 23:51:59.839: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 18 23:51:59.839: INFO: Versions found [{apps/v1 v1}]
Jan 18 23:51:59.839: INFO: apps/v1 matches apps/v1
Jan 18 23:51:59.839: INFO: Checking APIGroup: events.k8s.io
Jan 18 23:51:59.840: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 18 23:51:59.840: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jan 18 23:51:59.840: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 18 23:51:59.840: INFO: Checking APIGroup: authentication.k8s.io
Jan 18 23:51:59.840: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 18 23:51:59.840: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 18 23:51:59.840: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 18 23:51:59.840: INFO: Checking APIGroup: authorization.k8s.io
Jan 18 23:51:59.841: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 18 23:51:59.841: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 18 23:51:59.841: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 18 23:51:59.841: INFO: Checking APIGroup: autoscaling
Jan 18 23:51:59.842: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 18 23:51:59.842: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jan 18 23:51:59.842: INFO: autoscaling/v2 matches autoscaling/v2
Jan 18 23:51:59.842: INFO: Checking APIGroup: batch
Jan 18 23:51:59.842: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 18 23:51:59.842: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jan 18 23:51:59.842: INFO: batch/v1 matches batch/v1
Jan 18 23:51:59.842: INFO: Checking APIGroup: certificates.k8s.io
Jan 18 23:51:59.843: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 18 23:51:59.843: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 18 23:51:59.843: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 18 23:51:59.843: INFO: Checking APIGroup: networking.k8s.io
Jan 18 23:51:59.843: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 18 23:51:59.843: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 18 23:51:59.843: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 18 23:51:59.843: INFO: Checking APIGroup: policy
Jan 18 23:51:59.844: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 18 23:51:59.844: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jan 18 23:51:59.844: INFO: policy/v1 matches policy/v1
Jan 18 23:51:59.844: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 18 23:51:59.844: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 18 23:51:59.844: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 18 23:51:59.844: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 18 23:51:59.844: INFO: Checking APIGroup: storage.k8s.io
Jan 18 23:51:59.845: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 18 23:51:59.845: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 18 23:51:59.845: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 18 23:51:59.845: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 18 23:51:59.846: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 18 23:51:59.846: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 18 23:51:59.846: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 18 23:51:59.846: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 18 23:51:59.846: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 18 23:51:59.846: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 18 23:51:59.846: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 18 23:51:59.846: INFO: Checking APIGroup: scheduling.k8s.io
Jan 18 23:51:59.847: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 18 23:51:59.847: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 18 23:51:59.847: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 18 23:51:59.847: INFO: Checking APIGroup: coordination.k8s.io
Jan 18 23:51:59.848: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 18 23:51:59.848: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 18 23:51:59.848: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 18 23:51:59.848: INFO: Checking APIGroup: node.k8s.io
Jan 18 23:51:59.848: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 18 23:51:59.848: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jan 18 23:51:59.848: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 18 23:51:59.848: INFO: Checking APIGroup: discovery.k8s.io
Jan 18 23:51:59.849: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 18 23:51:59.849: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jan 18 23:51:59.849: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 18 23:51:59.849: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 18 23:51:59.849: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 18 23:51:59.849: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 18 23:51:59.849: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 18 23:51:59.849: INFO: Checking APIGroup: apps.openshift.io
Jan 18 23:51:59.850: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Jan 18 23:51:59.850: INFO: Versions found [{apps.openshift.io/v1 v1}]
Jan 18 23:51:59.850: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Jan 18 23:51:59.850: INFO: Checking APIGroup: authorization.openshift.io
Jan 18 23:51:59.850: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Jan 18 23:51:59.850: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Jan 18 23:51:59.850: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Jan 18 23:51:59.850: INFO: Checking APIGroup: build.openshift.io
Jan 18 23:51:59.851: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Jan 18 23:51:59.851: INFO: Versions found [{build.openshift.io/v1 v1}]
Jan 18 23:51:59.851: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Jan 18 23:51:59.851: INFO: Checking APIGroup: image.openshift.io
Jan 18 23:51:59.851: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Jan 18 23:51:59.852: INFO: Versions found [{image.openshift.io/v1 v1}]
Jan 18 23:51:59.852: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Jan 18 23:51:59.852: INFO: Checking APIGroup: oauth.openshift.io
Jan 18 23:51:59.852: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Jan 18 23:51:59.852: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Jan 18 23:51:59.852: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Jan 18 23:51:59.852: INFO: Checking APIGroup: project.openshift.io
Jan 18 23:51:59.853: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Jan 18 23:51:59.853: INFO: Versions found [{project.openshift.io/v1 v1}]
Jan 18 23:51:59.853: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Jan 18 23:51:59.853: INFO: Checking APIGroup: quota.openshift.io
Jan 18 23:51:59.853: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Jan 18 23:51:59.853: INFO: Versions found [{quota.openshift.io/v1 v1}]
Jan 18 23:51:59.853: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Jan 18 23:51:59.853: INFO: Checking APIGroup: route.openshift.io
Jan 18 23:51:59.854: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Jan 18 23:51:59.854: INFO: Versions found [{route.openshift.io/v1 v1}]
Jan 18 23:51:59.854: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Jan 18 23:51:59.854: INFO: Checking APIGroup: security.openshift.io
Jan 18 23:51:59.854: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Jan 18 23:51:59.854: INFO: Versions found [{security.openshift.io/v1 v1}]
Jan 18 23:51:59.854: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Jan 18 23:51:59.854: INFO: Checking APIGroup: template.openshift.io
Jan 18 23:51:59.855: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Jan 18 23:51:59.855: INFO: Versions found [{template.openshift.io/v1 v1}]
Jan 18 23:51:59.855: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Jan 18 23:51:59.855: INFO: Checking APIGroup: user.openshift.io
Jan 18 23:51:59.856: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Jan 18 23:51:59.856: INFO: Versions found [{user.openshift.io/v1 v1}]
Jan 18 23:51:59.856: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Jan 18 23:51:59.856: INFO: Checking APIGroup: packages.operators.coreos.com
Jan 18 23:51:59.856: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Jan 18 23:51:59.856: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Jan 18 23:51:59.856: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Jan 18 23:51:59.856: INFO: Checking APIGroup: config.openshift.io
Jan 18 23:51:59.857: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Jan 18 23:51:59.857: INFO: Versions found [{config.openshift.io/v1 v1}]
Jan 18 23:51:59.857: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Jan 18 23:51:59.857: INFO: Checking APIGroup: operator.openshift.io
Jan 18 23:51:59.857: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Jan 18 23:51:59.857: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Jan 18 23:51:59.857: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Jan 18 23:51:59.857: INFO: Checking APIGroup: apiserver.openshift.io
Jan 18 23:51:59.858: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
Jan 18 23:51:59.858: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
Jan 18 23:51:59.858: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
Jan 18 23:51:59.858: INFO: Checking APIGroup: autoscaling.openshift.io
Jan 18 23:51:59.859: INFO: PreferredVersion.GroupVersion: autoscaling.openshift.io/v1
Jan 18 23:51:59.859: INFO: Versions found [{autoscaling.openshift.io/v1 v1} {autoscaling.openshift.io/v1beta1 v1beta1}]
Jan 18 23:51:59.859: INFO: autoscaling.openshift.io/v1 matches autoscaling.openshift.io/v1
Jan 18 23:51:59.859: INFO: Checking APIGroup: cloud.network.openshift.io
Jan 18 23:51:59.859: INFO: PreferredVersion.GroupVersion: cloud.network.openshift.io/v1
Jan 18 23:51:59.859: INFO: Versions found [{cloud.network.openshift.io/v1 v1}]
Jan 18 23:51:59.859: INFO: cloud.network.openshift.io/v1 matches cloud.network.openshift.io/v1
Jan 18 23:51:59.859: INFO: Checking APIGroup: cloudcredential.openshift.io
Jan 18 23:51:59.860: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Jan 18 23:51:59.860: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Jan 18 23:51:59.860: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Jan 18 23:51:59.860: INFO: Checking APIGroup: console.openshift.io
Jan 18 23:51:59.860: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Jan 18 23:51:59.860: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Jan 18 23:51:59.860: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Jan 18 23:51:59.860: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Jan 18 23:51:59.861: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Jan 18 23:51:59.861: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Jan 18 23:51:59.861: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Jan 18 23:51:59.861: INFO: Checking APIGroup: ingress.operator.openshift.io
Jan 18 23:51:59.862: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Jan 18 23:51:59.862: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Jan 18 23:51:59.862: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Jan 18 23:51:59.862: INFO: Checking APIGroup: k8s.cni.cncf.io
Jan 18 23:51:59.862: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Jan 18 23:51:59.862: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Jan 18 23:51:59.862: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Jan 18 23:51:59.862: INFO: Checking APIGroup: k8s.ovn.org
Jan 18 23:51:59.863: INFO: PreferredVersion.GroupVersion: k8s.ovn.org/v1
Jan 18 23:51:59.863: INFO: Versions found [{k8s.ovn.org/v1 v1}]
Jan 18 23:51:59.863: INFO: k8s.ovn.org/v1 matches k8s.ovn.org/v1
Jan 18 23:51:59.863: INFO: Checking APIGroup: machineconfiguration.openshift.io
Jan 18 23:51:59.863: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Jan 18 23:51:59.863: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Jan 18 23:51:59.863: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Jan 18 23:51:59.863: INFO: Checking APIGroup: monitoring.coreos.com
Jan 18 23:51:59.864: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jan 18 23:51:59.864: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Jan 18 23:51:59.864: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jan 18 23:51:59.864: INFO: Checking APIGroup: monitoring.rhobs
Jan 18 23:51:59.865: INFO: PreferredVersion.GroupVersion: monitoring.rhobs/v1
Jan 18 23:51:59.865: INFO: Versions found [{monitoring.rhobs/v1 v1} {monitoring.rhobs/v1alpha1 v1alpha1}]
Jan 18 23:51:59.865: INFO: monitoring.rhobs/v1 matches monitoring.rhobs/v1
Jan 18 23:51:59.865: INFO: Checking APIGroup: network.operator.openshift.io
Jan 18 23:51:59.865: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Jan 18 23:51:59.865: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Jan 18 23:51:59.865: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Jan 18 23:51:59.865: INFO: Checking APIGroup: operators.coreos.com
Jan 18 23:51:59.866: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Jan 18 23:51:59.866: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Jan 18 23:51:59.866: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Jan 18 23:51:59.866: INFO: Checking APIGroup: performance.openshift.io
Jan 18 23:51:59.866: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
Jan 18 23:51:59.866: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
Jan 18 23:51:59.866: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
Jan 18 23:51:59.866: INFO: Checking APIGroup: samples.operator.openshift.io
Jan 18 23:51:59.867: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Jan 18 23:51:59.867: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Jan 18 23:51:59.867: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Jan 18 23:51:59.867: INFO: Checking APIGroup: security.internal.openshift.io
Jan 18 23:51:59.887: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Jan 18 23:51:59.888: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Jan 18 23:51:59.888: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Jan 18 23:51:59.888: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jan 18 23:51:59.935: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jan 18 23:51:59.935: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Jan 18 23:51:59.935: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jan 18 23:51:59.935: INFO: Checking APIGroup: tuned.openshift.io
Jan 18 23:51:59.985: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Jan 18 23:51:59.985: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Jan 18 23:51:59.985: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Jan 18 23:51:59.985: INFO: Checking APIGroup: velero.io
Jan 18 23:52:00.035: INFO: PreferredVersion.GroupVersion: velero.io/v1
Jan 18 23:52:00.035: INFO: Versions found [{velero.io/v1 v1}]
Jan 18 23:52:00.035: INFO: velero.io/v1 matches velero.io/v1
Jan 18 23:52:00.035: INFO: Checking APIGroup: addons.managed.openshift.io
Jan 18 23:52:00.085: INFO: PreferredVersion.GroupVersion: addons.managed.openshift.io/v1alpha1
Jan 18 23:52:00.085: INFO: Versions found [{addons.managed.openshift.io/v1alpha1 v1alpha1}]
Jan 18 23:52:00.085: INFO: addons.managed.openshift.io/v1alpha1 matches addons.managed.openshift.io/v1alpha1
Jan 18 23:52:00.085: INFO: Checking APIGroup: cloudingress.managed.openshift.io
Jan 18 23:52:00.135: INFO: PreferredVersion.GroupVersion: cloudingress.managed.openshift.io/v1alpha1
Jan 18 23:52:00.135: INFO: Versions found [{cloudingress.managed.openshift.io/v1alpha1 v1alpha1}]
Jan 18 23:52:00.135: INFO: cloudingress.managed.openshift.io/v1alpha1 matches cloudingress.managed.openshift.io/v1alpha1
Jan 18 23:52:00.135: INFO: Checking APIGroup: controlplane.operator.openshift.io
Jan 18 23:52:00.185: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Jan 18 23:52:00.185: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Jan 18 23:52:00.185: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Jan 18 23:52:00.185: INFO: Checking APIGroup: managed.openshift.io
Jan 18 23:52:00.235: INFO: PreferredVersion.GroupVersion: managed.openshift.io/v1alpha2
Jan 18 23:52:00.235: INFO: Versions found [{managed.openshift.io/v1alpha2 v1alpha2} {managed.openshift.io/v1alpha1 v1alpha1}]
Jan 18 23:52:00.235: INFO: managed.openshift.io/v1alpha2 matches managed.openshift.io/v1alpha2
Jan 18 23:52:00.235: INFO: Checking APIGroup: metal3.io
Jan 18 23:52:00.285: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
Jan 18 23:52:00.285: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
Jan 18 23:52:00.285: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
Jan 18 23:52:00.285: INFO: Checking APIGroup: migration.k8s.io
Jan 18 23:52:00.335: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Jan 18 23:52:00.335: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Jan 18 23:52:00.335: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Jan 18 23:52:00.335: INFO: Checking APIGroup: monitoring.openshift.io
Jan 18 23:52:00.385: INFO: PreferredVersion.GroupVersion: monitoring.openshift.io/v1alpha1
Jan 18 23:52:00.385: INFO: Versions found [{monitoring.openshift.io/v1alpha1 v1alpha1}]
Jan 18 23:52:00.385: INFO: monitoring.openshift.io/v1alpha1 matches monitoring.openshift.io/v1alpha1
Jan 18 23:52:00.385: INFO: Checking APIGroup: ocmagent.managed.openshift.io
Jan 18 23:52:00.435: INFO: PreferredVersion.GroupVersion: ocmagent.managed.openshift.io/v1alpha1
Jan 18 23:52:00.435: INFO: Versions found [{ocmagent.managed.openshift.io/v1alpha1 v1alpha1}]
Jan 18 23:52:00.435: INFO: ocmagent.managed.openshift.io/v1alpha1 matches ocmagent.managed.openshift.io/v1alpha1
Jan 18 23:52:00.435: INFO: Checking APIGroup: splunkforwarder.managed.openshift.io
Jan 18 23:52:00.485: INFO: PreferredVersion.GroupVersion: splunkforwarder.managed.openshift.io/v1alpha1
Jan 18 23:52:00.485: INFO: Versions found [{splunkforwarder.managed.openshift.io/v1alpha1 v1alpha1}]
Jan 18 23:52:00.485: INFO: splunkforwarder.managed.openshift.io/v1alpha1 matches splunkforwarder.managed.openshift.io/v1alpha1
Jan 18 23:52:00.485: INFO: Checking APIGroup: upgrade.managed.openshift.io
Jan 18 23:52:00.535: INFO: PreferredVersion.GroupVersion: upgrade.managed.openshift.io/v1alpha1
Jan 18 23:52:00.535: INFO: Versions found [{upgrade.managed.openshift.io/v1alpha1 v1alpha1}]
Jan 18 23:52:00.535: INFO: upgrade.managed.openshift.io/v1alpha1 matches upgrade.managed.openshift.io/v1alpha1
Jan 18 23:52:00.535: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Jan 18 23:52:00.585: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Jan 18 23:52:00.585: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Jan 18 23:52:00.585: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Jan 18 23:52:00.585: INFO: Checking APIGroup: helm.openshift.io
Jan 18 23:52:00.635: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Jan 18 23:52:00.635: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Jan 18 23:52:00.635: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Jan 18 23:52:00.635: INFO: Checking APIGroup: machine.openshift.io
Jan 18 23:52:00.685: INFO: PreferredVersion.GroupVersion: machine.openshift.io/v1beta1
Jan 18 23:52:00.685: INFO: Versions found [{machine.openshift.io/v1beta1 v1beta1}]
Jan 18 23:52:00.685: INFO: machine.openshift.io/v1beta1 matches machine.openshift.io/v1beta1
Jan 18 23:52:00.685: INFO: Checking APIGroup: metrics.k8s.io
Jan 18 23:52:00.735: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 18 23:52:00.735: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 18 23:52:00.735: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:188
Jan 18 23:52:00.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-4098" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":356,"completed":296,"skipped":5557,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:52:00.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jan 18 23:52:00.915: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 23:52:00.915: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 23:52:00.927: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 23:52:00.927: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 23:52:00.935: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 23:52:00.935: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 23:52:00.982: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 23:52:00.983: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 23:52:03.416: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 18 23:52:03.416: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 18 23:52:04.445: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jan 18 23:52:04.454: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jan 18 23:52:04.455: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0
Jan 18 23:52:04.455: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0
Jan 18 23:52:04.455: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0
Jan 18 23:52:04.455: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0
Jan 18 23:52:04.455: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0
Jan 18 23:52:04.455: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0
Jan 18 23:52:04.456: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0
Jan 18 23:52:04.456: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 0
Jan 18 23:52:04.456: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1
Jan 18 23:52:04.456: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1
Jan 18 23:52:04.456: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2
Jan 18 23:52:04.456: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2
Jan 18 23:52:04.456: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2
Jan 18 23:52:04.456: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2
Jan 18 23:52:04.473: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2
Jan 18 23:52:04.473: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2
Jan 18 23:52:04.487: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2
Jan 18 23:52:04.487: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2
Jan 18 23:52:04.493: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1
Jan 18 23:52:04.493: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1
Jan 18 23:52:04.528: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1
Jan 18 23:52:04.528: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1
Jan 18 23:52:07.465: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2
Jan 18 23:52:07.465: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2
Jan 18 23:52:07.497: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1
STEP: listing Deployments
Jan 18 23:52:07.511: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jan 18 23:52:07.529: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jan 18 23:52:07.533: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 23:52:07.560: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 23:52:07.575: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 23:52:07.589: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 23:52:07.605: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 23:52:07.629: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 23:52:10.450: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 23:52:10.464: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 23:52:10.491: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 23:52:10.510: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 23:52:13.463: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jan 18 23:52:13.490: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1
Jan 18 23:52:13.490: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1
Jan 18 23:52:13.490: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1
Jan 18 23:52:13.490: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1
Jan 18 23:52:13.490: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1
Jan 18 23:52:13.490: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 1
Jan 18 23:52:13.490: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2
Jan 18 23:52:13.490: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 3
Jan 18 23:52:13.490: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2
Jan 18 23:52:13.490: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 2
Jan 18 23:52:13.490: INFO: observed Deployment test-deployment in namespace deployment-2081 with ReadyReplicas 3
STEP: deleting the Deployment
Jan 18 23:52:13.496: INFO: observed event type MODIFIED
Jan 18 23:52:13.497: INFO: observed event type MODIFIED
Jan 18 23:52:13.497: INFO: observed event type MODIFIED
Jan 18 23:52:13.497: INFO: observed event type MODIFIED
Jan 18 23:52:13.497: INFO: observed event type MODIFIED
Jan 18 23:52:13.497: INFO: observed event type MODIFIED
Jan 18 23:52:13.497: INFO: observed event type MODIFIED
Jan 18 23:52:13.497: INFO: observed event type MODIFIED
Jan 18 23:52:13.497: INFO: observed event type MODIFIED
Jan 18 23:52:13.497: INFO: observed event type MODIFIED
Jan 18 23:52:13.497: INFO: observed event type MODIFIED
Jan 18 23:52:13.497: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 23:52:13.499: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 18 23:52:13.502: INFO: ReplicaSet "test-deployment-6bdc46c995":
&ReplicaSet{ObjectMeta:{test-deployment-6bdc46c995  deployment-2081  05cfce99-1075-4ed7-8ebe-564061a143a0 298562 3 2023-01-18 23:52:00 +0000 UTC <nil> <nil> map[pod-template-hash:6bdc46c995 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment d2eaa94e-fb2d-40d7-b571-26797dc2e31c 0xc00944ada7 0xc00944ada8}] []  [{kube-controller-manager Update apps/v1 2023-01-18 23:52:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2eaa94e-fb2d-40d7-b571-26797dc2e31c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:52:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6bdc46c995,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6bdc46c995 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00944ae40 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 18 23:52:13.504: INFO: ReplicaSet "test-deployment-74c6dd549b":
&ReplicaSet{ObjectMeta:{test-deployment-74c6dd549b  deployment-2081  497fb554-2fad-4abb-9a2e-70a4ef2d101d 298693 2 2023-01-18 23:52:07 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment d2eaa94e-fb2d-40d7-b571-26797dc2e31c 0xc00944aea7 0xc00944aea8}] []  [{kube-controller-manager Update apps/v1 2023-01-18 23:52:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2eaa94e-fb2d-40d7-b571-26797dc2e31c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:52:10 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 74c6dd549b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00944af50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 18 23:52:13.507: INFO: pod: "test-deployment-74c6dd549b-7brgg":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-7brgg test-deployment-74c6dd549b- deployment-2081  9f291e11-38ab-4720-8a8d-73320f9849a7 298692 0 2023-01-18 23:52:10 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.16.190/23"],"mac_address":"0a:58:0a:80:10:be","gateway_ips":["10.128.16.1"],"ip_address":"10.128.16.190/23","gateway_ip":"10.128.16.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.16.190"
    ],
    "mac": "0a:58:0a:80:10:be",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.16.190"
    ],
    "mac": "0a:58:0a:80:10:be",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-74c6dd549b 497fb554-2fad-4abb-9a2e-70a4ef2d101d 0xc00953fc47 0xc00953fc48}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:52:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:52:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"497fb554-2fad-4abb-9a2e-70a4ef2d101d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-18 23:52:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-18 23:52:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.16.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t9rcc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t9rcc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-211-217.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c66,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wxbw4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:52:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:52:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:52:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:52:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.211.217,PodIP:10.128.16.190,StartTime:2023-01-18 23:52:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:52:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://26f13a59b07bf0ddc96699eec09cca07f0895d9c17a908f2d0b8f2220bdfabf6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.16.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 23:52:13.507: INFO: pod: "test-deployment-74c6dd549b-lcwfv":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-lcwfv test-deployment-74c6dd549b- deployment-2081  3a2082ed-da8e-4b8a-892e-524faebef032 298630 0 2023-01-18 23:52:07 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.13.42/23"],"mac_address":"0a:58:0a:80:0d:2a","gateway_ips":["10.128.12.1"],"ip_address":"10.128.13.42/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.13.42"
    ],
    "mac": "0a:58:0a:80:0d:2a",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.13.42"
    ],
    "mac": "0a:58:0a:80:0d:2a",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-74c6dd549b 497fb554-2fad-4abb-9a2e-70a4ef2d101d 0xc011042127 0xc011042128}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:52:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:52:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"497fb554-2fad-4abb-9a2e-70a4ef2d101d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-18 23:52:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-18 23:52:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.13.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sndp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sndp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-219-147.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c66,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wxbw4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:52:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:52:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:52:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:52:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.219.147,PodIP:10.128.13.42,StartTime:2023-01-18 23:52:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:52:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://639691648a9d017746e898f630777ddb23bdb16242eebdb889cb7f7ab97a2402,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.13.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 23:52:13.507: INFO: ReplicaSet "test-deployment-84b949bdfc":
&ReplicaSet{ObjectMeta:{test-deployment-84b949bdfc  deployment-2081  d43c9ae4-9f24-4526-853e-c653af70d725 298701 4 2023-01-18 23:52:04 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment d2eaa94e-fb2d-40d7-b571-26797dc2e31c 0xc00944afc7 0xc00944afc8}] []  [{kube-controller-manager Update apps/v1 2023-01-18 23:52:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2eaa94e-fb2d-40d7-b571-26797dc2e31c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:52:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 84b949bdfc,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.7 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00944b1b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 18 23:52:13.509: INFO: pod: "test-deployment-84b949bdfc-9jphb":
&Pod{ObjectMeta:{test-deployment-84b949bdfc-9jphb test-deployment-84b949bdfc- deployment-2081  3e2149c7-cd85-4973-858a-38ac0f938663 298695 0 2023-01-18 23:52:07 +0000 UTC 2023-01-18 23:52:14 +0000 UTC 0xc00301bc18 map[pod-template-hash:84b949bdfc test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.16.189/23"],"mac_address":"0a:58:0a:80:10:bd","gateway_ips":["10.128.16.1"],"ip_address":"10.128.16.189/23","gateway_ip":"10.128.16.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.16.189"
    ],
    "mac": "0a:58:0a:80:10:bd",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.16.189"
    ],
    "mac": "0a:58:0a:80:10:bd",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-84b949bdfc d43c9ae4-9f24-4526-853e-c653af70d725 0xc00301bc77 0xc00301bc78}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:52:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:52:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d43c9ae4-9f24-4526-853e-c653af70d725\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-18 23:52:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-18 23:52:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.16.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bmk74,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.7,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bmk74,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-211-217.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c66,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wxbw4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:52:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:52:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:52:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:52:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.211.217,PodIP:10.128.16.189,StartTime:2023-01-18 23:52:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:52:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.7,ImageID:k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c,ContainerID:cri-o://bef906ea4b5c5b2e7dc0988040cb0ba5b112c2254ad27406b840f6bcc5afed22,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.16.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 18 23:52:13.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2081" for this suite.

• [SLOW TEST:12.678 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":356,"completed":297,"skipped":5567,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:52:13.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 18 23:52:13.615: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:52:13.615: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:52:13.615: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:52:13.616: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:52:13.617: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 23:52:14.622: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:52:14.622: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:52:14.622: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:52:14.624: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:52:14.624: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 23:52:15.622: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:52:15.622: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:52:15.622: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:52:15.626: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:52:15.626: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 23:52:16.623: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:52:16.623: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:52:16.623: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:52:16.626: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 18 23:52:16.626: INFO: Node ip-10-0-219-147.ec2.internal is running 0 daemon pod, expected 1
Jan 18 23:52:17.621: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:52:17.621: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:52:17.621: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:52:17.624: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 18 23:52:17.624: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan 18 23:52:17.649: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"298885"},"items":null}

Jan 18 23:52:17.652: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"298891"},"items":[{"metadata":{"name":"daemon-set-8vt6k","generateName":"daemon-set-","namespace":"daemonsets-9380","uid":"cbed7bab-eae6-4994-9f2f-b44e932e657d","resourceVersion":"298890","creationTimestamp":"2023-01-18T23:52:13Z","deletionTimestamp":"2023-01-18T23:52:47Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.10.55/23\"],\"mac_address\":\"0a:58:0a:80:0a:37\",\"gateway_ips\":[\"10.128.10.1\"],\"ip_address\":\"10.128.10.55/23\",\"gateway_ip\":\"10.128.10.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.10.55\"\n    ],\n    \"mac\": \"0a:58:0a:80:0a:37\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.10.55\"\n    ],\n    \"mac\": \"0a:58:0a:80:0a:37\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cbd42ae8-496b-4a38-9677-21b638609560","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-176-170","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbd42ae8-496b-4a38-9677-21b638609560\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.10.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4mnpn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4mnpn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-253-152.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c66,c5"}},"imagePullSecrets":[{"name":"default-dockercfg-f5fwp"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-253-152.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:17Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:17Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:13Z"}],"hostIP":"10.0.253.152","podIP":"10.128.10.55","podIPs":[{"ip":"10.128.10.55"}],"startTime":"2023-01-18T23:52:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T23:52:16Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://fe10ab247938d9cedbd6f57be0b88e00060d5be5d4cb10d639eea0e91e36e18c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-chbnj","generateName":"daemon-set-","namespace":"daemonsets-9380","uid":"a96260ab-67bf-463b-b93c-9d7f22e0ec73","resourceVersion":"298888","creationTimestamp":"2023-01-18T23:52:13Z","deletionTimestamp":"2023-01-18T23:52:47Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.8.87/23\"],\"mac_address\":\"0a:58:0a:80:08:57\",\"gateway_ips\":[\"10.128.8.1\"],\"ip_address\":\"10.128.8.87/23\",\"gateway_ip\":\"10.128.8.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.8.87\"\n    ],\n    \"mac\": \"0a:58:0a:80:08:57\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.8.87\"\n    ],\n    \"mac\": \"0a:58:0a:80:08:57\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cbd42ae8-496b-4a38-9677-21b638609560","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-176-170","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbd42ae8-496b-4a38-9677-21b638609560\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4g2l2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4g2l2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-236-5.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c66,c5"}},"imagePullSecrets":[{"name":"default-dockercfg-f5fwp"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-236-5.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:13Z"}],"hostIP":"10.0.236.5","podIP":"10.128.8.87","podIPs":[{"ip":"10.128.8.87"}],"startTime":"2023-01-18T23:52:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T23:52:15Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://5d6fc04e5f3cf375ea894abe6320ceb932558d3994802c0a9e7294b2c17a14de","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kfmhm","generateName":"daemon-set-","namespace":"daemonsets-9380","uid":"af36af80-9a5a-4345-b51a-67374ee4ba3e","resourceVersion":"298891","creationTimestamp":"2023-01-18T23:52:13Z","deletionTimestamp":"2023-01-18T23:52:47Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.13.43/23\"],\"mac_address\":\"0a:58:0a:80:0d:2b\",\"gateway_ips\":[\"10.128.12.1\"],\"ip_address\":\"10.128.13.43/23\",\"gateway_ip\":\"10.128.12.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.13.43\"\n    ],\n    \"mac\": \"0a:58:0a:80:0d:2b\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.13.43\"\n    ],\n    \"mac\": \"0a:58:0a:80:0d:2b\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cbd42ae8-496b-4a38-9677-21b638609560","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-176-170","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbd42ae8-496b-4a38-9677-21b638609560\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.13.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-x5d6s","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-x5d6s","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-219-147.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c66,c5"}},"imagePullSecrets":[{"name":"default-dockercfg-f5fwp"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-219-147.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:17Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:17Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:13Z"}],"hostIP":"10.0.219.147","podIP":"10.128.13.43","podIPs":[{"ip":"10.128.13.43"}],"startTime":"2023-01-18T23:52:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T23:52:16Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://f843087818ac1e06a51892adaada6e4bb4727e75603fe779d0daa777b6f95dd4","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-nwqg2","generateName":"daemon-set-","namespace":"daemonsets-9380","uid":"245b4aa3-c094-4a43-b2ee-542025b158c5","resourceVersion":"298886","creationTimestamp":"2023-01-18T23:52:13Z","deletionTimestamp":"2023-01-18T23:52:47Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.16.191/23\"],\"mac_address\":\"0a:58:0a:80:10:bf\",\"gateway_ips\":[\"10.128.16.1\"],\"ip_address\":\"10.128.16.191/23\",\"gateway_ip\":\"10.128.16.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.16.191\"\n    ],\n    \"mac\": \"0a:58:0a:80:10:bf\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.16.191\"\n    ],\n    \"mac\": \"0a:58:0a:80:10:bf\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cbd42ae8-496b-4a38-9677-21b638609560","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-176-170","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbd42ae8-496b-4a38-9677-21b638609560\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.16.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-mcdwn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-mcdwn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-211-217.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c66,c5"}},"imagePullSecrets":[{"name":"default-dockercfg-f5fwp"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-211-217.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:16Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:16Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:13Z"}],"hostIP":"10.0.211.217","podIP":"10.128.16.191","podIPs":[{"ip":"10.128.16.191"}],"startTime":"2023-01-18T23:52:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T23:52:15Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://918cf5b5b828dd72b636eb24d30a58a723844348670a60e62f8d7cb30c19ed3b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rdxpr","generateName":"daemon-set-","namespace":"daemonsets-9380","uid":"c824dd37-010a-44fa-be00-25a9b30c5a80","resourceVersion":"298889","creationTimestamp":"2023-01-18T23:52:13Z","deletionTimestamp":"2023-01-18T23:52:47Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.6.88/23\"],\"mac_address\":\"0a:58:0a:80:06:58\",\"gateway_ips\":[\"10.128.6.1\"],\"ip_address\":\"10.128.6.88/23\",\"gateway_ip\":\"10.128.6.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.6.88\"\n    ],\n    \"mac\": \"0a:58:0a:80:06:58\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.6.88\"\n    ],\n    \"mac\": \"0a:58:0a:80:06:58\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cbd42ae8-496b-4a38-9677-21b638609560","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-176-170","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbd42ae8-496b-4a38-9677-21b638609560\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.6.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-npjwz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-npjwz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-200-13.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c66,c5"}},"imagePullSecrets":[{"name":"default-dockercfg-f5fwp"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-200-13.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:16Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:16Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:13Z"}],"hostIP":"10.0.200.13","podIP":"10.128.6.88","podIPs":[{"ip":"10.128.6.88"}],"startTime":"2023-01-18T23:52:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T23:52:15Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://d32ae2b63b8c42bc9680e666b97f20d461ddce372e2162bb362ecd193cc14fd7","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-t6gpp","generateName":"daemon-set-","namespace":"daemonsets-9380","uid":"14285a89-bdfc-4efd-93f2-fe370aaea04a","resourceVersion":"298885","creationTimestamp":"2023-01-18T23:52:13Z","deletionTimestamp":"2023-01-18T23:52:47Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.14.122/23\"],\"mac_address\":\"0a:58:0a:80:0e:7a\",\"gateway_ips\":[\"10.128.14.1\"],\"ip_address\":\"10.128.14.122/23\",\"gateway_ip\":\"10.128.14.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.14.122\"\n    ],\n    \"mac\": \"0a:58:0a:80:0e:7a\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.14.122\"\n    ],\n    \"mac\": \"0a:58:0a:80:0e:7a\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cbd42ae8-496b-4a38-9677-21b638609560","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-176-170","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbd42ae8-496b-4a38-9677-21b638609560\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T23:52:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.14.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8llvd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8llvd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-128-7.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c66,c5"}},"imagePullSecrets":[{"name":"default-dockercfg-f5fwp"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-128-7.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:16Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:16Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T23:52:13Z"}],"hostIP":"10.0.128.7","podIP":"10.128.14.122","podIPs":[{"ip":"10.128.14.122"}],"startTime":"2023-01-18T23:52:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T23:52:15Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://4385db13df3568ac95393b8d56ea5ba7ec64ed8ea464a049740c44b017d7b615","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 18 23:52:17.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9380" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":356,"completed":298,"skipped":5578,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:52:17.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating cluster-info
Jan 18 23:52:17.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-6765 cluster-info'
Jan 18 23:52:17.786: INFO: stderr: ""
Jan 18 23:52:17.786: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 23:52:17.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6765" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":356,"completed":299,"skipped":5600,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:52:17.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Jan 18 23:52:17.863: INFO: namespace kubectl-9911
Jan 18 23:52:17.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-9911 create -f -'
Jan 18 23:52:20.186: INFO: stderr: ""
Jan 18 23:52:20.186: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 18 23:52:21.189: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 23:52:21.189: INFO: Found 0 / 1
Jan 18 23:52:22.189: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 23:52:22.189: INFO: Found 0 / 1
Jan 18 23:52:23.189: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 23:52:23.189: INFO: Found 1 / 1
Jan 18 23:52:23.189: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 18 23:52:23.191: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 23:52:23.191: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 23:52:23.191: INFO: wait on agnhost-primary startup in kubectl-9911 
Jan 18 23:52:23.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-9911 logs agnhost-primary-7f7tv agnhost-primary'
Jan 18 23:52:23.247: INFO: stderr: ""
Jan 18 23:52:23.247: INFO: stdout: "Paused\n"
STEP: exposing RC
Jan 18 23:52:23.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-9911 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 18 23:52:23.312: INFO: stderr: ""
Jan 18 23:52:23.312: INFO: stdout: "service/rm2 exposed\n"
Jan 18 23:52:23.322: INFO: Service rm2 in namespace kubectl-9911 found.
STEP: exposing service
Jan 18 23:52:25.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-9911 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 18 23:52:25.398: INFO: stderr: ""
Jan 18 23:52:25.398: INFO: stdout: "service/rm3 exposed\n"
Jan 18 23:52:25.400: INFO: Service rm3 in namespace kubectl-9911 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 18 23:52:27.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9911" for this suite.

• [SLOW TEST:9.585 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1249
    should create services for rc  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":356,"completed":300,"skipped":5627,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:52:27.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-397.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-397.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-397.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-397.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 18 23:52:31.522: INFO: DNS probes using dns-test-fdb5a8f3-e205-44f1-a912-48be246aaff7 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-397.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-397.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-397.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-397.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 18 23:52:35.566: INFO: File wheezy_udp@dns-test-service-3.dns-397.svc.cluster.local from pod  dns-397/dns-test-7dbcbb64-a254-438f-ab45-736845bbd07b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 23:52:35.568: INFO: Lookups using dns-397/dns-test-7dbcbb64-a254-438f-ab45-736845bbd07b failed for: [wheezy_udp@dns-test-service-3.dns-397.svc.cluster.local]

Jan 18 23:52:40.575: INFO: DNS probes using dns-test-7dbcbb64-a254-438f-ab45-736845bbd07b succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-397.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-397.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-397.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-397.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 18 23:52:44.635: INFO: DNS probes using dns-test-71515ee5-5105-45c0-bf6c-e82029686d95 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 18 23:52:44.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-397" for this suite.

• [SLOW TEST:17.255 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":356,"completed":301,"skipped":5627,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:52:44.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 18 23:52:44.693: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 18 23:52:52.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7872" for this suite.

• [SLOW TEST:7.928 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":356,"completed":302,"skipped":5631,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:52:52.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: creating
W0118 23:52:52.647575      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: getting
STEP: listing
STEP: watching
Jan 18 23:52:52.666: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 18 23:52:52.679: INFO: starting watch
STEP: patching
STEP: updating
Jan 18 23:52:52.703: INFO: waiting for watch events with expected annotations
Jan 18 23:52:52.703: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 18 23:52:52.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-700" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":356,"completed":303,"skipped":5636,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:52:52.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-6m89
STEP: Creating a pod to test atomic-volume-subpath
Jan 18 23:52:52.846: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6m89" in namespace "subpath-5829" to be "Succeeded or Failed"
Jan 18 23:52:52.858: INFO: Pod "pod-subpath-test-configmap-6m89": Phase="Pending", Reason="", readiness=false. Elapsed: 11.825054ms
Jan 18 23:52:54.860: INFO: Pod "pod-subpath-test-configmap-6m89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01426689s
Jan 18 23:52:56.863: INFO: Pod "pod-subpath-test-configmap-6m89": Phase="Running", Reason="", readiness=true. Elapsed: 4.017171449s
Jan 18 23:52:58.866: INFO: Pod "pod-subpath-test-configmap-6m89": Phase="Running", Reason="", readiness=true. Elapsed: 6.019814694s
Jan 18 23:53:00.868: INFO: Pod "pod-subpath-test-configmap-6m89": Phase="Running", Reason="", readiness=true. Elapsed: 8.022646127s
Jan 18 23:53:02.871: INFO: Pod "pod-subpath-test-configmap-6m89": Phase="Running", Reason="", readiness=true. Elapsed: 10.025505164s
Jan 18 23:53:04.875: INFO: Pod "pod-subpath-test-configmap-6m89": Phase="Running", Reason="", readiness=true. Elapsed: 12.028962816s
Jan 18 23:53:06.878: INFO: Pod "pod-subpath-test-configmap-6m89": Phase="Running", Reason="", readiness=true. Elapsed: 14.032084186s
Jan 18 23:53:08.881: INFO: Pod "pod-subpath-test-configmap-6m89": Phase="Running", Reason="", readiness=true. Elapsed: 16.034789513s
Jan 18 23:53:10.887: INFO: Pod "pod-subpath-test-configmap-6m89": Phase="Running", Reason="", readiness=true. Elapsed: 18.04098943s
Jan 18 23:53:12.895: INFO: Pod "pod-subpath-test-configmap-6m89": Phase="Running", Reason="", readiness=true. Elapsed: 20.048959433s
Jan 18 23:53:14.900: INFO: Pod "pod-subpath-test-configmap-6m89": Phase="Running", Reason="", readiness=true. Elapsed: 22.053878807s
Jan 18 23:53:16.903: INFO: Pod "pod-subpath-test-configmap-6m89": Phase="Running", Reason="", readiness=false. Elapsed: 24.057353533s
Jan 18 23:53:18.909: INFO: Pod "pod-subpath-test-configmap-6m89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.062893171s
STEP: Saw pod success
Jan 18 23:53:18.909: INFO: Pod "pod-subpath-test-configmap-6m89" satisfied condition "Succeeded or Failed"
Jan 18 23:53:18.911: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-subpath-test-configmap-6m89 container test-container-subpath-configmap-6m89: <nil>
STEP: delete the pod
Jan 18 23:53:18.924: INFO: Waiting for pod pod-subpath-test-configmap-6m89 to disappear
Jan 18 23:53:18.926: INFO: Pod pod-subpath-test-configmap-6m89 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6m89
Jan 18 23:53:18.926: INFO: Deleting pod "pod-subpath-test-configmap-6m89" in namespace "subpath-5829"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 18 23:53:18.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5829" for this suite.

• [SLOW TEST:26.161 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","total":356,"completed":304,"skipped":5661,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:53:18.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0118 23:53:19.020271      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-user-65534-947a526f-57de-4cbe-b70c-ed71c8362d08" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-user-65534-947a526f-57de-4cbe-b70c-ed71c8362d08" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "busybox-user-65534-947a526f-57de-4cbe-b70c-ed71c8362d08" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:53:19.020: INFO: Waiting up to 5m0s for pod "busybox-user-65534-947a526f-57de-4cbe-b70c-ed71c8362d08" in namespace "security-context-test-1496" to be "Succeeded or Failed"
Jan 18 23:53:19.023: INFO: Pod "busybox-user-65534-947a526f-57de-4cbe-b70c-ed71c8362d08": Phase="Pending", Reason="", readiness=false. Elapsed: 3.047993ms
Jan 18 23:53:21.025: INFO: Pod "busybox-user-65534-947a526f-57de-4cbe-b70c-ed71c8362d08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005572083s
Jan 18 23:53:23.028: INFO: Pod "busybox-user-65534-947a526f-57de-4cbe-b70c-ed71c8362d08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007863995s
Jan 18 23:53:25.031: INFO: Pod "busybox-user-65534-947a526f-57de-4cbe-b70c-ed71c8362d08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010753501s
Jan 18 23:53:25.031: INFO: Pod "busybox-user-65534-947a526f-57de-4cbe-b70c-ed71c8362d08" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 18 23:53:25.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1496" for this suite.

• [SLOW TEST:6.105 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:52
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":305,"skipped":5711,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:53:25.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
W0118 23:53:25.089425      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:53:25.097: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 18 23:53:30.101: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 18 23:53:30.101: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 18 23:53:32.105: INFO: Creating deployment "test-rollover-deployment"
Jan 18 23:53:32.111: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 18 23:53:34.159: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 18 23:53:34.163: INFO: Ensure that both replica sets have 1 created replica
Jan 18 23:53:34.167: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 18 23:53:34.175: INFO: Updating deployment test-rollover-deployment
Jan 18 23:53:34.175: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 18 23:53:36.181: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 18 23:53:36.184: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 18 23:53:36.189: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 23:53:36.189: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 53, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:53:38.194: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 23:53:38.194: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 53, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:53:40.195: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 23:53:40.195: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 53, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:53:42.194: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 23:53:42.194: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 53, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:53:44.201: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 23:53:44.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 53, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:53:46.195: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 23:53:46.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 53, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 53, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:53:48.196: INFO: 
Jan 18 23:53:48.196: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 23:53:48.207: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6510  9f7e5b89-c69e-4e9a-ad76-187aa5bf1353 300817 2 2023-01-18 23:53:32 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-01-18 23:53:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:53:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067b52b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 23:53:32 +0000 UTC,LastTransitionTime:2023-01-18 23:53:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-77745f886c" has successfully progressed.,LastUpdateTime:2023-01-18 23:53:46 +0000 UTC,LastTransitionTime:2023-01-18 23:53:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 23:53:48.209: INFO: New ReplicaSet "test-rollover-deployment-77745f886c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-77745f886c  deployment-6510  eed6ae6f-271c-42ae-a898-5508fd6b7b2e 300807 2 2023-01-18 23:53:34 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77745f886c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 9f7e5b89-c69e-4e9a-ad76-187aa5bf1353 0xc0067b5777 0xc0067b5778}] []  [{kube-controller-manager Update apps/v1 2023-01-18 23:53:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f7e5b89-c69e-4e9a-ad76-187aa5bf1353\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:53:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 77745f886c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77745f886c] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067b5828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:53:48.209: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 18 23:53:48.209: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6510  92a1481b-bef3-4d32-8e20-a5dc65b13085 300816 2 2023-01-18 23:53:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 9f7e5b89-c69e-4e9a-ad76-187aa5bf1353 0xc0067b5647 0xc0067b5648}] []  [{e2e.test Update apps/v1 2023-01-18 23:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:53:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f7e5b89-c69e-4e9a-ad76-187aa5bf1353\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:53:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0067b5708 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:53:48.209: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-87f8f6dcf  deployment-6510  209f5bce-38c0-4945-8465-4ec0d2266f92 300693 2 2023-01-18 23:53:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 9f7e5b89-c69e-4e9a-ad76-187aa5bf1353 0xc0067b5890 0xc0067b5891}] []  [{kube-controller-manager Update apps/v1 2023-01-18 23:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f7e5b89-c69e-4e9a-ad76-187aa5bf1353\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:53:34 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 87f8f6dcf,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067b5938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:53:48.212: INFO: Pod "test-rollover-deployment-77745f886c-zjn7b" is available:
&Pod{ObjectMeta:{test-rollover-deployment-77745f886c-zjn7b test-rollover-deployment-77745f886c- deployment-6510  300ed5db-72f3-42ce-a8cb-00860a34eea0 300722 0 2023-01-18 23:53:34 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77745f886c] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.13.51/23"],"mac_address":"0a:58:0a:80:0d:33","gateway_ips":["10.128.12.1"],"ip_address":"10.128.13.51/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.13.51"
    ],
    "mac": "0a:58:0a:80:0d:33",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.13.51"
    ],
    "mac": "0a:58:0a:80:0d:33",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-77745f886c eed6ae6f-271c-42ae-a898-5508fd6b7b2e 0xc0067b5e97 0xc0067b5e98}] []  [{ip-10-0-176-170 Update v1 2023-01-18 23:53:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-18 23:53:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eed6ae6f-271c-42ae-a898-5508fd6b7b2e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 23:53:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.13.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-18 23:53:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b8dbg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b8dbg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-219-147.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c66,c45,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-qs8hv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:53:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:53:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:53:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:53:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.219.147,PodIP:10.128.13.51,StartTime:2023-01-18 23:53:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:53:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403,ContainerID:cri-o://c82d0f800c1bb8eec36bc03898c08cba3813789cce71de5569ce5987f055d222,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.13.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 18 23:53:48.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6510" for this suite.

• [SLOW TEST:23.180 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":356,"completed":306,"skipped":5733,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:53:48.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 23:53:48.307: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e9a0bddc-6ae6-488b-8d12-7d2e04c2bceb" in namespace "downward-api-245" to be "Succeeded or Failed"
Jan 18 23:53:48.326: INFO: Pod "downwardapi-volume-e9a0bddc-6ae6-488b-8d12-7d2e04c2bceb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.409526ms
Jan 18 23:53:50.329: INFO: Pod "downwardapi-volume-e9a0bddc-6ae6-488b-8d12-7d2e04c2bceb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022387458s
Jan 18 23:53:52.332: INFO: Pod "downwardapi-volume-e9a0bddc-6ae6-488b-8d12-7d2e04c2bceb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025741987s
Jan 18 23:53:54.336: INFO: Pod "downwardapi-volume-e9a0bddc-6ae6-488b-8d12-7d2e04c2bceb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028863567s
STEP: Saw pod success
Jan 18 23:53:54.336: INFO: Pod "downwardapi-volume-e9a0bddc-6ae6-488b-8d12-7d2e04c2bceb" satisfied condition "Succeeded or Failed"
Jan 18 23:53:54.337: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-e9a0bddc-6ae6-488b-8d12-7d2e04c2bceb container client-container: <nil>
STEP: delete the pod
Jan 18 23:53:54.349: INFO: Waiting for pod downwardapi-volume-e9a0bddc-6ae6-488b-8d12-7d2e04c2bceb to disappear
Jan 18 23:53:54.351: INFO: Pod downwardapi-volume-e9a0bddc-6ae6-488b-8d12-7d2e04c2bceb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 18 23:53:54.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-245" for this suite.

• [SLOW TEST:6.138 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":307,"skipped":5748,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:53:54.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name projected-secret-test-11baf1d5-e1f7-41fc-9393-840c846d01be
STEP: Creating a pod to test consume secrets
Jan 18 23:53:54.467: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-828e73f4-5796-4fa9-81e9-0e109f5a52d8" in namespace "projected-9969" to be "Succeeded or Failed"
Jan 18 23:53:54.483: INFO: Pod "pod-projected-secrets-828e73f4-5796-4fa9-81e9-0e109f5a52d8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013395ms
Jan 18 23:53:56.486: INFO: Pod "pod-projected-secrets-828e73f4-5796-4fa9-81e9-0e109f5a52d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018591046s
Jan 18 23:53:58.489: INFO: Pod "pod-projected-secrets-828e73f4-5796-4fa9-81e9-0e109f5a52d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021971831s
Jan 18 23:54:00.493: INFO: Pod "pod-projected-secrets-828e73f4-5796-4fa9-81e9-0e109f5a52d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02570637s
STEP: Saw pod success
Jan 18 23:54:00.493: INFO: Pod "pod-projected-secrets-828e73f4-5796-4fa9-81e9-0e109f5a52d8" satisfied condition "Succeeded or Failed"
Jan 18 23:54:00.495: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-projected-secrets-828e73f4-5796-4fa9-81e9-0e109f5a52d8 container secret-volume-test: <nil>
STEP: delete the pod
Jan 18 23:54:00.509: INFO: Waiting for pod pod-projected-secrets-828e73f4-5796-4fa9-81e9-0e109f5a52d8 to disappear
Jan 18 23:54:00.512: INFO: Pod pod-projected-secrets-828e73f4-5796-4fa9-81e9-0e109f5a52d8 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 18 23:54:00.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9969" for this suite.

• [SLOW TEST:6.161 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":308,"skipped":5758,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-apps] Job 
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:54:00.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Indexed job
W0118 23:54:00.556936      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions
STEP: Ensuring pods with index for job exist
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 18 23:54:14.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9911" for this suite.

• [SLOW TEST:14.050 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","total":356,"completed":309,"skipped":5766,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:54:14.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 18 23:54:14.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6897" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":356,"completed":310,"skipped":5771,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:54:14.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Deleting RuntimeClass runtimeclass-8749-delete-me
STEP: Waiting for the RuntimeClass to disappear
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 18 23:54:14.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8749" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":311,"skipped":5812,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:54:14.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:54:14.891: INFO: created pod
Jan 18 23:54:14.891: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6964" to be "Succeeded or Failed"
Jan 18 23:54:14.936: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 44.710621ms
Jan 18 23:54:16.938: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047233091s
Jan 18 23:54:18.941: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050292271s
Jan 18 23:54:20.945: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053943675s
STEP: Saw pod success
Jan 18 23:54:20.945: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 18 23:54:50.949: INFO: polling logs
Jan 18 23:54:50.953: INFO: Pod logs: 
I0118 23:54:17.019353       1 log.go:195] OK: Got token
I0118 23:54:17.019381       1 log.go:195] validating with in-cluster discovery
I0118 23:54:17.019789       1 log.go:195] OK: got issuer https://kubernetes.default.svc
I0118 23:54:17.019810       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6964:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674086655, NotBefore:1674086055, IssuedAt:1674086055, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6964", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"41961e34-15f5-4ce2-9f40-a9fb78e0eda6"}}}
I0118 23:54:17.030630       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0118 23:54:17.038813       1 log.go:195] OK: Validated signature on JWT
I0118 23:54:17.038896       1 log.go:195] OK: Got valid claims from token!
I0118 23:54:17.038928       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6964:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674086655, NotBefore:1674086055, IssuedAt:1674086055, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6964", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"41961e34-15f5-4ce2-9f40-a9fb78e0eda6"}}}

Jan 18 23:54:50.953: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 18 23:54:50.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6964" for this suite.

• [SLOW TEST:36.167 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":356,"completed":312,"skipped":5829,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:54:50.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 18 23:55:08.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7007" for this suite.

• [SLOW TEST:17.071 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":356,"completed":313,"skipped":5840,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:55:08.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 18 23:55:08.073: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 18 23:55:12.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-138" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":356,"completed":314,"skipped":5871,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:55:12.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-secret-qzkg
STEP: Creating a pod to test atomic-volume-subpath
Jan 18 23:55:13.013: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-qzkg" in namespace "subpath-8115" to be "Succeeded or Failed"
Jan 18 23:55:13.015: INFO: Pod "pod-subpath-test-secret-qzkg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.416736ms
Jan 18 23:55:15.025: INFO: Pod "pod-subpath-test-secret-qzkg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012116561s
Jan 18 23:55:17.028: INFO: Pod "pod-subpath-test-secret-qzkg": Phase="Running", Reason="", readiness=true. Elapsed: 4.014662075s
Jan 18 23:55:19.031: INFO: Pod "pod-subpath-test-secret-qzkg": Phase="Running", Reason="", readiness=true. Elapsed: 6.017806432s
Jan 18 23:55:21.033: INFO: Pod "pod-subpath-test-secret-qzkg": Phase="Running", Reason="", readiness=true. Elapsed: 8.020082852s
Jan 18 23:55:23.036: INFO: Pod "pod-subpath-test-secret-qzkg": Phase="Running", Reason="", readiness=true. Elapsed: 10.022928578s
Jan 18 23:55:25.039: INFO: Pod "pod-subpath-test-secret-qzkg": Phase="Running", Reason="", readiness=true. Elapsed: 12.026044661s
Jan 18 23:55:27.041: INFO: Pod "pod-subpath-test-secret-qzkg": Phase="Running", Reason="", readiness=true. Elapsed: 14.028388929s
Jan 18 23:55:29.045: INFO: Pod "pod-subpath-test-secret-qzkg": Phase="Running", Reason="", readiness=true. Elapsed: 16.032332774s
Jan 18 23:55:31.052: INFO: Pod "pod-subpath-test-secret-qzkg": Phase="Running", Reason="", readiness=true. Elapsed: 18.039544107s
Jan 18 23:55:33.057: INFO: Pod "pod-subpath-test-secret-qzkg": Phase="Running", Reason="", readiness=true. Elapsed: 20.043750121s
Jan 18 23:55:35.060: INFO: Pod "pod-subpath-test-secret-qzkg": Phase="Running", Reason="", readiness=true. Elapsed: 22.046654902s
Jan 18 23:55:37.063: INFO: Pod "pod-subpath-test-secret-qzkg": Phase="Running", Reason="", readiness=false. Elapsed: 24.050006947s
Jan 18 23:55:39.065: INFO: Pod "pod-subpath-test-secret-qzkg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.052585797s
STEP: Saw pod success
Jan 18 23:55:39.066: INFO: Pod "pod-subpath-test-secret-qzkg" satisfied condition "Succeeded or Failed"
Jan 18 23:55:39.068: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-subpath-test-secret-qzkg container test-container-subpath-secret-qzkg: <nil>
STEP: delete the pod
Jan 18 23:55:39.087: INFO: Waiting for pod pod-subpath-test-secret-qzkg to disappear
Jan 18 23:55:39.089: INFO: Pod pod-subpath-test-secret-qzkg no longer exists
STEP: Deleting pod pod-subpath-test-secret-qzkg
Jan 18 23:55:39.089: INFO: Deleting pod "pod-subpath-test-secret-qzkg" in namespace "subpath-8115"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 18 23:55:39.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8115" for this suite.

• [SLOW TEST:26.169 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","total":356,"completed":315,"skipped":5881,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:55:39.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:55:40.189: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"15ba19e9-297f-42de-9fe6-d272fc91780b", Controller:(*bool)(0xc005dbc8a6), BlockOwnerDeletion:(*bool)(0xc005dbc8a7)}}
Jan 18 23:55:40.199: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"cb37fcb0-b84e-4803-b163-3fa86ba3dcb9", Controller:(*bool)(0xc0020000c2), BlockOwnerDeletion:(*bool)(0xc0020000c3)}}
Jan 18 23:55:40.205: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"64856e51-5c35-4b80-8c38-dacaeee8ebee", Controller:(*bool)(0xc005dbcba2), BlockOwnerDeletion:(*bool)(0xc005dbcba3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 18 23:55:45.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-503" for this suite.

• [SLOW TEST:6.124 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":356,"completed":316,"skipped":5882,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:55:45.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1398
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1398
I0118 23:55:45.326818      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1398, replica count: 2
Jan 18 23:55:48.377: INFO: Creating new exec pod
I0118 23:55:48.377309      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 23:55:53.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-1398 exec execpod268z6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 18 23:55:53.516: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 23:55:53.516: INFO: stdout: "externalname-service-hc24m"
Jan 18 23:55:53.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-1398 exec execpod268z6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.63.41 80'
Jan 18 23:55:53.617: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.63.41 80\nConnection to 172.30.63.41 80 port [tcp/http] succeeded!\n"
Jan 18 23:55:53.617: INFO: stdout: "externalname-service-hc24m"
Jan 18 23:55:53.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-1398 exec execpod268z6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.128.7 30075'
Jan 18 23:55:53.727: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.128.7 30075\nConnection to 10.0.128.7 30075 port [tcp/*] succeeded!\n"
Jan 18 23:55:53.727: INFO: stdout: "externalname-service-54mq7"
Jan 18 23:55:53.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-1398 exec execpod268z6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.200.13 30075'
Jan 18 23:55:53.832: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.200.13 30075\nConnection to 10.0.200.13 30075 port [tcp/*] succeeded!\n"
Jan 18 23:55:53.832: INFO: stdout: "externalname-service-hc24m"
Jan 18 23:55:53.832: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 18 23:55:53.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1398" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:8.642 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":356,"completed":317,"skipped":5943,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:55:53.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 18 23:55:53.970: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9322  290dfc71-9b93-4817-b1fc-aa8b3cd97f48 303196 0 2023-01-18 23:55:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-18 23:55:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:55:53.970: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9322  290dfc71-9b93-4817-b1fc-aa8b3cd97f48 303203 0 2023-01-18 23:55:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-18 23:55:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 18 23:55:54.012: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9322  290dfc71-9b93-4817-b1fc-aa8b3cd97f48 303210 0 2023-01-18 23:55:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-18 23:55:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 23:55:54.012: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9322  290dfc71-9b93-4817-b1fc-aa8b3cd97f48 303212 0 2023-01-18 23:55:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-18 23:55:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 18 23:55:54.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9322" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":356,"completed":318,"skipped":5964,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:55:54.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-7939
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 18 23:55:54.091: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 23:55:54.290: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:55:56.293: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:55:58.292: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:56:00.293: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:56:02.292: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:56:04.294: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:56:06.294: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:56:08.293: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:56:10.294: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:56:12.298: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:56:14.293: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:56:16.296: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 18 23:56:16.305: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 18 23:56:16.310: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jan 18 23:56:16.331: INFO: The status of Pod netserver-3 is Running (Ready = true)
Jan 18 23:56:16.345: INFO: The status of Pod netserver-4 is Running (Ready = true)
Jan 18 23:56:16.351: INFO: The status of Pod netserver-5 is Running (Ready = true)
STEP: Creating test pods
Jan 18 23:56:20.396: INFO: Setting MaxTries for pod polling to 66 for networking test based on endpoint count 6
Jan 18 23:56:20.396: INFO: Going to poll 10.128.14.123 on port 8083 at least 0 times, with a maximum of 66 tries before failing
Jan 18 23:56:20.400: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.14.123:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7939 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:56:20.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:56:20.400: INFO: ExecWithOptions: Clientset creation
Jan 18 23:56:20.400: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-7939/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.14.123%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 23:56:20.507: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 18 23:56:20.507: INFO: Going to poll 10.128.6.89 on port 8083 at least 0 times, with a maximum of 66 tries before failing
Jan 18 23:56:20.509: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.6.89:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7939 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:56:20.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:56:20.509: INFO: ExecWithOptions: Clientset creation
Jan 18 23:56:20.509: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-7939/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.6.89%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 23:56:20.610: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 18 23:56:20.610: INFO: Going to poll 10.128.16.197 on port 8083 at least 0 times, with a maximum of 66 tries before failing
Jan 18 23:56:20.612: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.16.197:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7939 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:56:20.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:56:20.613: INFO: ExecWithOptions: Clientset creation
Jan 18 23:56:20.613: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-7939/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.16.197%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 23:56:20.713: INFO: Found all 1 expected endpoints: [netserver-2]
Jan 18 23:56:20.713: INFO: Going to poll 10.128.13.64 on port 8083 at least 0 times, with a maximum of 66 tries before failing
Jan 18 23:56:20.715: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.13.64:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7939 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:56:20.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:56:20.715: INFO: ExecWithOptions: Clientset creation
Jan 18 23:56:20.716: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-7939/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.13.64%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 23:56:20.806: INFO: Found all 1 expected endpoints: [netserver-3]
Jan 18 23:56:20.806: INFO: Going to poll 10.128.8.88 on port 8083 at least 0 times, with a maximum of 66 tries before failing
Jan 18 23:56:20.808: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.8.88:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7939 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:56:20.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:56:20.809: INFO: ExecWithOptions: Clientset creation
Jan 18 23:56:20.809: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-7939/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.8.88%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 23:56:20.900: INFO: Found all 1 expected endpoints: [netserver-4]
Jan 18 23:56:20.900: INFO: Going to poll 10.128.10.56 on port 8083 at least 0 times, with a maximum of 66 tries before failing
Jan 18 23:56:20.902: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.10.56:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7939 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:56:20.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:56:20.903: INFO: ExecWithOptions: Clientset creation
Jan 18 23:56:20.903: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-7939/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.10.56%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 23:56:20.986: INFO: Found all 1 expected endpoints: [netserver-5]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 18 23:56:20.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7939" for this suite.

• [SLOW TEST:26.972 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":319,"skipped":5969,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:56:20.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 23:56:21.106: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca8cf1ec-2b3b-415b-9744-1a318bc0b48f" in namespace "downward-api-6223" to be "Succeeded or Failed"
Jan 18 23:56:21.108: INFO: Pod "downwardapi-volume-ca8cf1ec-2b3b-415b-9744-1a318bc0b48f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027301ms
Jan 18 23:56:23.111: INFO: Pod "downwardapi-volume-ca8cf1ec-2b3b-415b-9744-1a318bc0b48f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005037812s
Jan 18 23:56:25.116: INFO: Pod "downwardapi-volume-ca8cf1ec-2b3b-415b-9744-1a318bc0b48f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010444266s
Jan 18 23:56:27.119: INFO: Pod "downwardapi-volume-ca8cf1ec-2b3b-415b-9744-1a318bc0b48f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013567827s
STEP: Saw pod success
Jan 18 23:56:27.119: INFO: Pod "downwardapi-volume-ca8cf1ec-2b3b-415b-9744-1a318bc0b48f" satisfied condition "Succeeded or Failed"
Jan 18 23:56:27.121: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-ca8cf1ec-2b3b-415b-9744-1a318bc0b48f container client-container: <nil>
STEP: delete the pod
Jan 18 23:56:27.134: INFO: Waiting for pod downwardapi-volume-ca8cf1ec-2b3b-415b-9744-1a318bc0b48f to disappear
Jan 18 23:56:27.136: INFO: Pod downwardapi-volume-ca8cf1ec-2b3b-415b-9744-1a318bc0b48f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 18 23:56:27.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6223" for this suite.

• [SLOW TEST:6.148 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":320,"skipped":5972,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:56:27.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 18 23:56:27.603: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 23:56:29.611: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 56, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 56, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 56, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 56, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 18 23:56:32.623: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 18 23:56:32.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3878" for this suite.
STEP: Destroying namespace "webhook-3878-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.638 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":356,"completed":321,"skipped":5986,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:56:32.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-a10f432d-bf49-4bfb-b150-06e7baa2f285
STEP: Creating a pod to test consume configMaps
Jan 18 23:56:32.937: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1c61db9b-8ba9-4080-aff5-aa2d084cb5ef" in namespace "projected-3010" to be "Succeeded or Failed"
Jan 18 23:56:32.940: INFO: Pod "pod-projected-configmaps-1c61db9b-8ba9-4080-aff5-aa2d084cb5ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.530877ms
Jan 18 23:56:34.943: INFO: Pod "pod-projected-configmaps-1c61db9b-8ba9-4080-aff5-aa2d084cb5ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00566289s
Jan 18 23:56:36.946: INFO: Pod "pod-projected-configmaps-1c61db9b-8ba9-4080-aff5-aa2d084cb5ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008434185s
Jan 18 23:56:38.949: INFO: Pod "pod-projected-configmaps-1c61db9b-8ba9-4080-aff5-aa2d084cb5ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011468539s
STEP: Saw pod success
Jan 18 23:56:38.949: INFO: Pod "pod-projected-configmaps-1c61db9b-8ba9-4080-aff5-aa2d084cb5ef" satisfied condition "Succeeded or Failed"
Jan 18 23:56:38.951: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-projected-configmaps-1c61db9b-8ba9-4080-aff5-aa2d084cb5ef container agnhost-container: <nil>
STEP: delete the pod
Jan 18 23:56:38.963: INFO: Waiting for pod pod-projected-configmaps-1c61db9b-8ba9-4080-aff5-aa2d084cb5ef to disappear
Jan 18 23:56:38.965: INFO: Pod pod-projected-configmaps-1c61db9b-8ba9-4080-aff5-aa2d084cb5ef no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 18 23:56:38.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3010" for this suite.

• [SLOW TEST:6.190 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":322,"skipped":6032,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:56:38.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 18 23:56:39.049: INFO: Waiting up to 5m0s for pod "downwardapi-volume-244342b3-69c8-49c7-a03f-1bec5902600e" in namespace "downward-api-6942" to be "Succeeded or Failed"
Jan 18 23:56:39.060: INFO: Pod "downwardapi-volume-244342b3-69c8-49c7-a03f-1bec5902600e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.237454ms
Jan 18 23:56:41.064: INFO: Pod "downwardapi-volume-244342b3-69c8-49c7-a03f-1bec5902600e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014600273s
Jan 18 23:56:43.068: INFO: Pod "downwardapi-volume-244342b3-69c8-49c7-a03f-1bec5902600e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019228095s
Jan 18 23:56:45.071: INFO: Pod "downwardapi-volume-244342b3-69c8-49c7-a03f-1bec5902600e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022516238s
STEP: Saw pod success
Jan 18 23:56:45.071: INFO: Pod "downwardapi-volume-244342b3-69c8-49c7-a03f-1bec5902600e" satisfied condition "Succeeded or Failed"
Jan 18 23:56:45.074: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod downwardapi-volume-244342b3-69c8-49c7-a03f-1bec5902600e container client-container: <nil>
STEP: delete the pod
Jan 18 23:56:45.102: INFO: Waiting for pod downwardapi-volume-244342b3-69c8-49c7-a03f-1bec5902600e to disappear
Jan 18 23:56:45.104: INFO: Pod downwardapi-volume-244342b3-69c8-49c7-a03f-1bec5902600e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 18 23:56:45.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6942" for this suite.

• [SLOW TEST:6.138 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":323,"skipped":6050,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:56:45.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 18 23:56:45.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7684" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":356,"completed":324,"skipped":6095,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:56:45.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:56:49.366: INFO: Deleting pod "var-expansion-6e399d5b-057e-452b-914a-cae8154bb64b" in namespace "var-expansion-6525"
Jan 18 23:56:49.373: INFO: Wait up to 5m0s for pod "var-expansion-6e399d5b-057e-452b-914a-cae8154bb64b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 18 23:56:51.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6525" for this suite.

• [SLOW TEST:6.202 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":356,"completed":325,"skipped":6111,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:56:51.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
W0118 23:56:51.467482      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 18 23:56:51.467: INFO: Waiting up to 5m0s for pod "security-context-91646efa-252f-42ab-b8dd-5907bbff25bf" in namespace "security-context-4509" to be "Succeeded or Failed"
Jan 18 23:56:51.469: INFO: Pod "security-context-91646efa-252f-42ab-b8dd-5907bbff25bf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.653751ms
Jan 18 23:56:53.472: INFO: Pod "security-context-91646efa-252f-42ab-b8dd-5907bbff25bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004837193s
Jan 18 23:56:55.474: INFO: Pod "security-context-91646efa-252f-42ab-b8dd-5907bbff25bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007183609s
Jan 18 23:56:57.477: INFO: Pod "security-context-91646efa-252f-42ab-b8dd-5907bbff25bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010380906s
STEP: Saw pod success
Jan 18 23:56:57.478: INFO: Pod "security-context-91646efa-252f-42ab-b8dd-5907bbff25bf" satisfied condition "Succeeded or Failed"
Jan 18 23:56:57.480: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod security-context-91646efa-252f-42ab-b8dd-5907bbff25bf container test-container: <nil>
STEP: delete the pod
Jan 18 23:56:57.491: INFO: Waiting for pod security-context-91646efa-252f-42ab-b8dd-5907bbff25bf to disappear
Jan 18 23:56:57.493: INFO: Pod security-context-91646efa-252f-42ab-b8dd-5907bbff25bf no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 18 23:56:57.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4509" for this suite.

• [SLOW TEST:6.106 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":326,"skipped":6115,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:56:57.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
Jan 18 23:56:57.602: INFO: Create a RollingUpdate DaemonSet
Jan 18 23:56:57.608: INFO: Check that daemon pods launch on every node of the cluster
Jan 18 23:56:57.616: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:56:57.616: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:56:57.617: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:56:57.619: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:56:57.619: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 23:56:58.623: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:56:58.623: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:56:58.623: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:56:58.627: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:56:58.627: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 23:56:59.625: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:56:59.625: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:56:59.625: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:56:59.628: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:56:59.628: INFO: Node ip-10-0-128-7.ec2.internal is running 0 daemon pod, expected 1
Jan 18 23:57:00.623: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:00.624: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:00.624: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:00.626: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 18 23:57:00.626: INFO: Node ip-10-0-219-147.ec2.internal is running 0 daemon pod, expected 1
Jan 18 23:57:01.623: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:01.623: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:01.623: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:01.626: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 18 23:57:01.626: INFO: Node ip-10-0-219-147.ec2.internal is running 0 daemon pod, expected 1
Jan 18 23:57:02.625: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:02.626: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:02.626: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:02.631: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 18 23:57:02.631: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
Jan 18 23:57:02.631: INFO: Update the DaemonSet to trigger a rollout
Jan 18 23:57:02.652: INFO: Updating DaemonSet daemon-set
Jan 18 23:57:04.664: INFO: Roll back the DaemonSet before rollout is complete
Jan 18 23:57:04.671: INFO: Updating DaemonSet daemon-set
Jan 18 23:57:04.671: INFO: Make sure DaemonSet rollback is complete
Jan 18 23:57:04.674: INFO: Wrong image for pod: daemon-set-ltmgf. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 18 23:57:04.674: INFO: Pod daemon-set-ltmgf is not available
Jan 18 23:57:04.677: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:04.677: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:04.677: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:05.685: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:05.685: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:05.685: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:06.684: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:06.684: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:06.684: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:07.681: INFO: Pod daemon-set-95brb is not available
Jan 18 23:57:07.684: INFO: DaemonSet pods can't tolerate node ip-10-0-164-47.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:07.684: INFO: DaemonSet pods can't tolerate node ip-10-0-176-161.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 23:57:07.684: INFO: DaemonSet pods can't tolerate node ip-10-0-176-170.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1283, will wait for the garbage collector to delete the pods
Jan 18 23:57:07.745: INFO: Deleting DaemonSet.extensions daemon-set took: 4.456353ms
Jan 18 23:57:07.846: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.345474ms
Jan 18 23:57:11.449: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 23:57:11.449: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 23:57:11.452: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"305284"},"items":null}

Jan 18 23:57:11.454: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"305284"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 18 23:57:11.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1283" for this suite.

• [SLOW TEST:13.976 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":356,"completed":327,"skipped":6115,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:57:11.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-4051
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 18 23:57:11.510: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 23:57:11.676: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:57:13.679: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:57:15.680: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:57:17.678: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:57:19.679: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:57:21.679: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:57:23.680: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:57:25.681: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:57:27.680: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:57:29.706: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:57:31.679: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 18 23:57:33.680: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 18 23:57:33.684: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 18 23:57:33.688: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jan 18 23:57:33.692: INFO: The status of Pod netserver-3 is Running (Ready = true)
Jan 18 23:57:33.696: INFO: The status of Pod netserver-4 is Running (Ready = true)
Jan 18 23:57:33.699: INFO: The status of Pod netserver-5 is Running (Ready = true)
STEP: Creating test pods
Jan 18 23:57:37.720: INFO: Setting MaxTries for pod polling to 66 for networking test based on endpoint count 6
Jan 18 23:57:37.720: INFO: Breadth first check of 10.128.14.127 on host 10.0.128.7...
Jan 18 23:57:37.722: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.13.73:9080/dial?request=hostname&protocol=udp&host=10.128.14.127&port=8081&tries=1'] Namespace:pod-network-test-4051 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:57:37.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:57:37.722: INFO: ExecWithOptions: Clientset creation
Jan 18 23:57:37.722: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-4051/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.13.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.14.127%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:57:37.807: INFO: Waiting for responses: map[]
Jan 18 23:57:37.807: INFO: reached 10.128.14.127 after 0/1 tries
Jan 18 23:57:37.807: INFO: Breadth first check of 10.128.6.91 on host 10.0.200.13...
Jan 18 23:57:37.810: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.13.73:9080/dial?request=hostname&protocol=udp&host=10.128.6.91&port=8081&tries=1'] Namespace:pod-network-test-4051 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:57:37.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:57:37.810: INFO: ExecWithOptions: Clientset creation
Jan 18 23:57:37.810: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-4051/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.13.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.6.91%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:57:37.871: INFO: Waiting for responses: map[]
Jan 18 23:57:37.871: INFO: reached 10.128.6.91 after 0/1 tries
Jan 18 23:57:37.871: INFO: Breadth first check of 10.128.16.200 on host 10.0.211.217...
Jan 18 23:57:37.873: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.13.73:9080/dial?request=hostname&protocol=udp&host=10.128.16.200&port=8081&tries=1'] Namespace:pod-network-test-4051 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:57:37.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:57:37.874: INFO: ExecWithOptions: Clientset creation
Jan 18 23:57:37.874: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-4051/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.13.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.16.200%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:57:37.934: INFO: Waiting for responses: map[]
Jan 18 23:57:37.934: INFO: reached 10.128.16.200 after 0/1 tries
Jan 18 23:57:37.934: INFO: Breadth first check of 10.128.13.72 on host 10.0.219.147...
Jan 18 23:57:37.937: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.13.73:9080/dial?request=hostname&protocol=udp&host=10.128.13.72&port=8081&tries=1'] Namespace:pod-network-test-4051 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:57:37.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:57:37.937: INFO: ExecWithOptions: Clientset creation
Jan 18 23:57:37.937: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-4051/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.13.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.13.72%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:57:37.994: INFO: Waiting for responses: map[]
Jan 18 23:57:37.994: INFO: reached 10.128.13.72 after 0/1 tries
Jan 18 23:57:37.994: INFO: Breadth first check of 10.128.8.90 on host 10.0.236.5...
Jan 18 23:57:37.997: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.13.73:9080/dial?request=hostname&protocol=udp&host=10.128.8.90&port=8081&tries=1'] Namespace:pod-network-test-4051 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:57:37.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:57:37.997: INFO: ExecWithOptions: Clientset creation
Jan 18 23:57:37.997: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-4051/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.13.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.8.90%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:57:38.055: INFO: Waiting for responses: map[]
Jan 18 23:57:38.055: INFO: reached 10.128.8.90 after 0/1 tries
Jan 18 23:57:38.055: INFO: Breadth first check of 10.128.10.58 on host 10.0.253.152...
Jan 18 23:57:38.057: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.13.73:9080/dial?request=hostname&protocol=udp&host=10.128.10.58&port=8081&tries=1'] Namespace:pod-network-test-4051 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 23:57:38.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 18 23:57:38.058: INFO: ExecWithOptions: Clientset creation
Jan 18 23:57:38.058: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-4051/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.13.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.10.58%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 23:57:38.119: INFO: Waiting for responses: map[]
Jan 18 23:57:38.119: INFO: reached 10.128.10.58 after 0/1 tries
Jan 18 23:57:38.119: INFO: Going to retry 0 out of 6 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 18 23:57:38.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4051" for this suite.

• [SLOW TEST:26.651 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":356,"completed":328,"skipped":6128,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSS
------------------------------
[sig-apps] Job 
  should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:57:38.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
W0118 23:57:38.174743      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensure pods equal to paralellism count is attached to the job
STEP: patching /status
STEP: updating /status
STEP: get /status
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 18 23:57:42.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9753" for this suite.
•{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","total":356,"completed":329,"skipped":6135,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:57:42.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 23:57:42.248: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 23:57:42.268: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 23:57:42.278: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-128-7.ec2.internal before test
Jan 18 23:57:42.324: INFO: addon-operator-manager-c8c4859bf-84wp8 from openshift-addon-operator started at 2023-01-18 21:36:25 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.324: INFO: 	Container manager ready: true, restart count 0
Jan 18 23:57:42.324: INFO: 	Container metrics-relay-server ready: true, restart count 0
Jan 18 23:57:42.324: INFO: addon-operator-webhooks-569bd4cfc5-dz6nk from openshift-addon-operator started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.324: INFO: 	Container webhook ready: true, restart count 0
Jan 18 23:57:42.324: INFO: aws-ebs-csi-driver-node-mq4qf from openshift-cluster-csi-drivers started at 2023-01-18 21:27:18 +0000 UTC (3 container statuses recorded)
Jan 18 23:57:42.324: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:57:42.324: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:57:42.324: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:57:42.324: INFO: tuned-l9d9s from openshift-cluster-node-tuning-operator started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.324: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:57:42.324: INFO: dns-default-btls8 from openshift-dns started at 2023-01-18 22:06:24 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.324: INFO: 	Container dns ready: true, restart count 0
Jan 18 23:57:42.324: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:57:42.324: INFO: node-resolver-vhz2m from openshift-dns started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.324: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:57:42.324: INFO: node-ca-6cvvj from openshift-image-registry started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.324: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:57:42.324: INFO: ingress-canary-qw8v9 from openshift-ingress-canary started at 2023-01-18 21:28:03 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.324: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 23:57:42.324: INFO: router-default-7cff97cd98-dqgwn from openshift-ingress started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container router ready: true, restart count 0
Jan 18 23:57:42.325: INFO: machine-config-daemon-htf69 from openshift-machine-config-operator started at 2023-01-18 21:27:18 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:57:42.325: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:57:42.325: INFO: osd-patch-subscription-source-27901320-gz2jm from openshift-marketplace started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 18 23:57:42.325: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (6 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container alertmanager ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 23:57:42.325: INFO: kube-state-metrics-76877575d5-ptnh8 from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (3 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 18 23:57:42.325: INFO: node-exporter-spmbc from openshift-monitoring started at 2023-01-18 21:27:18 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.325: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:57:42.325: INFO: openshift-state-metrics-59fc669d8d-v25vz from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (3 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jan 18 23:57:42.325: INFO: prometheus-adapter-cf64f7f46-7bf79 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 18 23:57:42.325: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (6 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container prometheus ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 18 23:57:42.325: INFO: prometheus-operator-76957bb5bd-2pztb from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 18 23:57:42.325: INFO: prometheus-operator-admission-webhook-577cc9c956-d6fkv from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 23:57:42.325: INFO: sre-dns-latency-exporter-8dptc from openshift-monitoring started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container main ready: true, restart count 1
Jan 18 23:57:42.325: INFO: telemeter-client-6bb748465-zttx9 from openshift-monitoring started at 2023-01-18 21:36:25 +0000 UTC (3 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container reload ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container telemeter-client ready: true, restart count 0
Jan 18 23:57:42.325: INFO: thanos-querier-57f44c5498-xjsz4 from openshift-monitoring started at 2023-01-18 21:36:15 +0000 UTC (6 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container thanos-query ready: true, restart count 0
Jan 18 23:57:42.325: INFO: multus-additional-cni-plugins-5lmsf from openshift-multus started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:57:42.325: INFO: multus-znqj6 from openshift-multus started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:57:42.325: INFO: network-metrics-daemon-ffdlk from openshift-multus started at 2023-01-18 21:27:18 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.325: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:57:42.325: INFO: network-check-target-xwp8p from openshift-network-diagnostics started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:57:42.325: INFO: ovnkube-node-mkzt4 from openshift-ovn-kubernetes started at 2023-01-18 21:27:18 +0000 UTC (5 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.325: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:57:42.325: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:57:42.325: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:57:42.325: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:57:42.325: INFO: splunkforwarder-ds-fbjlj from openshift-security started at 2023-01-18 21:27:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 23:57:42.325: INFO: builds-pruner-27901380-cpft8 from openshift-sre-pruning started at 2023-01-18 23:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 18 23:57:42.325: INFO: deployments-pruner-27901320-vt4j8 from openshift-sre-pruning started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 18 23:57:42.325: INFO: deployments-pruner-27901380-b6sl8 from openshift-sre-pruning started at 2023-01-18 23:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 18 23:57:42.325: INFO: netserver-0 from pod-network-test-4051 started at 2023-01-18 23:57:11 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container webserver ready: true, restart count 0
Jan 18 23:57:42.325: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-dwhpp from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.325: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:57:42.325: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-200-13.ec2.internal before test
Jan 18 23:57:42.372: INFO: aws-ebs-csi-driver-node-6nf8l from openshift-cluster-csi-drivers started at 2023-01-18 21:02:39 +0000 UTC (3 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:57:42.372: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:57:42.372: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:57:42.372: INFO: tuned-t6mkr from openshift-cluster-node-tuning-operator started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:57:42.372: INFO: downloads-6f74f6fcbf-hcggr from openshift-console started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container download-server ready: true, restart count 0
Jan 18 23:57:42.372: INFO: dns-default-vkx5x from openshift-dns started at 2023-01-18 21:03:27 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container dns ready: true, restart count 1
Jan 18 23:57:42.372: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.372: INFO: node-resolver-gxvcl from openshift-dns started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:57:42.372: INFO: image-registry-7b8f8dcdc5-4lfps from openshift-image-registry started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container registry ready: true, restart count 0
Jan 18 23:57:42.372: INFO: node-ca-5tw9r from openshift-image-registry started at 2023-01-18 21:02:58 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:57:42.372: INFO: ingress-canary-ppjjg from openshift-ingress-canary started at 2023-01-18 21:03:27 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 23:57:42.372: INFO: machine-config-daemon-smj7g from openshift-machine-config-operator started at 2023-01-18 21:02:39 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:57:42.372: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:57:42.372: INFO: managed-node-metadata-operator-5d4c567575-cbrxh from openshift-managed-node-metadata-operator started at 2023-01-18 21:24:36 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container managed-node-metadata-operator ready: true, restart count 0
Jan 18 23:57:42.372: INFO: configure-alertmanager-operator-7565458cf4-pbmjl from openshift-monitoring started at 2023-01-18 21:24:36 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Jan 18 23:57:42.372: INFO: node-exporter-gzf8b from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.372: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:57:42.372: INFO: sre-dns-latency-exporter-s6nk6 from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container main ready: true, restart count 1
Jan 18 23:57:42.372: INFO: token-refresher-6d8f85f497-mhth9 from openshift-monitoring started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container token-refresher ready: true, restart count 0
Jan 18 23:57:42.372: INFO: multus-additional-cni-plugins-nm8v9 from openshift-multus started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:57:42.372: INFO: multus-cfrv2 from openshift-multus started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:57:42.372: INFO: network-metrics-daemon-m4c6r from openshift-multus started at 2023-01-18 21:02:39 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.372: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:57:42.372: INFO: network-check-target-hpzlx from openshift-network-diagnostics started at 2023-01-18 21:02:39 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:57:42.372: INFO: obo-prometheus-operator-admission-webhook-5b4dc9bff5-l6qvc from openshift-observability-operator started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 23:57:42.372: INFO: ocm-agent-75d95f8dc7-gtjwg from openshift-ocm-agent-operator started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container ocm-agent ready: true, restart count 0
Jan 18 23:57:42.372: INFO: collect-profiles-27901395-s69sn from openshift-operator-lifecycle-manager started at 2023-01-18 23:15:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 18 23:57:42.372: INFO: collect-profiles-27901410-gd7tw from openshift-operator-lifecycle-manager started at 2023-01-18 23:30:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 18 23:57:42.372: INFO: collect-profiles-27901425-9wzjz from openshift-operator-lifecycle-manager started at 2023-01-18 23:45:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 18 23:57:42.372: INFO: osd-metrics-exporter-756f85967-8z4cz from openshift-osd-metrics started at 2023-01-18 21:25:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container osd-metrics-exporter ready: true, restart count 0
Jan 18 23:57:42.372: INFO: osd-metrics-exporter-registry-6rnzs from openshift-osd-metrics started at 2023-01-18 21:24:12 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:57:42.372: INFO: ovnkube-node-h62xt from openshift-ovn-kubernetes started at 2023-01-18 21:02:39 +0000 UTC (5 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.372: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:57:42.372: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:57:42.372: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:57:42.372: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:57:42.372: INFO: rbac-permissions-operator-7f6bc8977-vsvkh from openshift-rbac-permissions started at 2023-01-18 21:24:29 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Jan 18 23:57:42.372: INFO: blackbox-exporter-dfdd57dd6-tj4kz from openshift-route-monitor-operator started at 2023-01-18 21:24:50 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.372: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan 18 23:57:42.372: INFO: route-monitor-operator-controller-manager-cbc597f9d-84p87 from openshift-route-monitor-operator started at 2023-01-18 21:24:37 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.373: INFO: 	Container manager ready: true, restart count 0
Jan 18 23:57:42.373: INFO: route-monitor-operator-registry-cgj4n from openshift-route-monitor-operator started at 2023-01-18 21:24:36 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.373: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:57:42.373: INFO: splunkforwarder-ds-n7d7t from openshift-security started at 2023-01-18 21:24:58 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.373: INFO: 	Container splunk-uf ready: true, restart count 0
Jan 18 23:57:42.373: INFO: splunk-forwarder-operator-6f5bf57497-jwcz5 from openshift-splunk-forwarder-operator started at 2023-01-18 21:24:40 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.373: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Jan 18 23:57:42.373: INFO: managed-velero-operator-f9f4c8b45-xml88 from openshift-velero started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.373: INFO: 	Container managed-velero-operator ready: true, restart count 0
Jan 18 23:57:42.373: INFO: velero-749f7746d8-ds5cs from openshift-velero started at 2023-01-18 21:24:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.373: INFO: 	Container velero ready: true, restart count 0
Jan 18 23:57:42.373: INFO: netserver-1 from pod-network-test-4051 started at 2023-01-18 23:57:11 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.373: INFO: 	Container webserver ready: true, restart count 0
Jan 18 23:57:42.373: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-mdcrg from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.373: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:57:42.373: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:57:42.373: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-211-217.ec2.internal before test
Jan 18 23:57:42.419: INFO: suspend-false-to-true-dfb8t from job-9753 started at 2023-01-18 23:57:38 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container c ready: true, restart count 0
Jan 18 23:57:42.419: INFO: addon-operator-webhooks-569bd4cfc5-mdklc from openshift-addon-operator started at 2023-01-18 21:39:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container webhook ready: true, restart count 0
Jan 18 23:57:42.419: INFO: osd-delete-ownerrefs-serviceaccounts-27901357-rd8sm from openshift-backplane-srep started at 2023-01-18 22:37:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 18 23:57:42.419: INFO: osd-delete-ownerrefs-serviceaccounts-27901387-lzh55 from openshift-backplane-srep started at 2023-01-18 23:07:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 18 23:57:42.419: INFO: osd-delete-ownerrefs-serviceaccounts-27901417-kbrf6 from openshift-backplane-srep started at 2023-01-18 23:37:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 18 23:57:42.419: INFO: osd-delete-backplane-serviceaccounts-27901410-lrs6c from openshift-backplane started at 2023-01-18 23:30:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 18 23:57:42.419: INFO: osd-delete-backplane-serviceaccounts-27901420-s8d8r from openshift-backplane started at 2023-01-18 23:40:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 18 23:57:42.419: INFO: osd-delete-backplane-serviceaccounts-27901430-sjgvz from openshift-backplane started at 2023-01-18 23:50:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 18 23:57:42.419: INFO: sre-build-test-27901391-sgdf9 from openshift-build-test started at 2023-01-18 23:11:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container sre-build-test ready: false, restart count 0
Jan 18 23:57:42.419: INFO: aws-ebs-csi-driver-node-l4ng8 from openshift-cluster-csi-drivers started at 2023-01-18 21:27:42 +0000 UTC (3 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:57:42.419: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:57:42.419: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:57:42.419: INFO: tuned-6pmzm from openshift-cluster-node-tuning-operator started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:57:42.419: INFO: dns-default-7sqw2 from openshift-dns started at 2023-01-18 22:06:24 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container dns ready: true, restart count 0
Jan 18 23:57:42.419: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:57:42.419: INFO: node-resolver-qsc9k from openshift-dns started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:57:42.419: INFO: image-pruner-27901320-rjnq8 from openshift-image-registry started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container image-pruner ready: false, restart count 0
Jan 18 23:57:42.419: INFO: image-pruner-27901380-mbpm4 from openshift-image-registry started at 2023-01-18 23:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container image-pruner ready: false, restart count 0
Jan 18 23:57:42.419: INFO: node-ca-9gnsm from openshift-image-registry started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:57:42.419: INFO: ingress-canary-k52kt from openshift-ingress-canary started at 2023-01-18 21:28:29 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 23:57:42.419: INFO: router-default-7cff97cd98-lstfd from openshift-ingress started at 2023-01-18 21:39:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container router ready: true, restart count 0
Jan 18 23:57:42.419: INFO: machine-config-daemon-krjvz from openshift-machine-config-operator started at 2023-01-18 21:27:42 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:57:42.419: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:57:42.419: INFO: osd-patch-subscription-source-27901380-gmtz6 from openshift-marketplace started at 2023-01-18 23:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 18 23:57:42.419: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-18 21:39:18 +0000 UTC (6 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container alertmanager ready: true, restart count 0
Jan 18 23:57:42.419: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 18 23:57:42.419: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 23:57:42.419: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:57:42.419: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 18 23:57:42.419: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 23:57:42.419: INFO: node-exporter-bpnc5 from openshift-monitoring started at 2023-01-18 21:27:42 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.419: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:57:42.419: INFO: osd-rebalance-infra-nodes-27901395-h44mq from openshift-monitoring started at 2023-01-18 23:15:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 18 23:57:42.419: INFO: osd-rebalance-infra-nodes-27901410-fkk69 from openshift-monitoring started at 2023-01-18 23:30:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 18 23:57:42.419: INFO: osd-rebalance-infra-nodes-27901425-6smbb from openshift-monitoring started at 2023-01-18 23:45:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 18 23:57:42.419: INFO: prometheus-adapter-cf64f7f46-jw64l from openshift-monitoring started at 2023-01-18 21:39:31 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.419: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 18 23:57:42.419: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-18 21:39:17 +0000 UTC (6 container statuses recorded)
Jan 18 23:57:42.420: INFO: 	Container config-reloader ready: true, restart count 0
Jan 18 23:57:42.420: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:57:42.420: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 18 23:57:42.420: INFO: 	Container prometheus ready: true, restart count 0
Jan 18 23:57:42.420: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 18 23:57:42.420: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 18 23:57:42.420: INFO: prometheus-operator-admission-webhook-577cc9c956-lwfrr from openshift-monitoring started at 2023-01-18 21:39:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.420: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 23:57:42.420: INFO: sre-dns-latency-exporter-tlqwt from openshift-monitoring started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.420: INFO: 	Container main ready: true, restart count 1
Jan 18 23:57:42.420: INFO: thanos-querier-57f44c5498-fzjjb from openshift-monitoring started at 2023-01-18 21:39:18 +0000 UTC (6 container statuses recorded)
Jan 18 23:57:42.420: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:57:42.420: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 18 23:57:42.420: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 18 23:57:42.420: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 18 23:57:42.420: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 18 23:57:42.420: INFO: 	Container thanos-query ready: true, restart count 0
Jan 18 23:57:42.420: INFO: multus-additional-cni-plugins-2q4s4 from openshift-multus started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.420: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:57:42.420: INFO: multus-cz5tw from openshift-multus started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.420: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:57:42.420: INFO: network-metrics-daemon-jnkxc from openshift-multus started at 2023-01-18 21:27:42 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.420: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.420: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:57:42.420: INFO: network-check-target-pjt5v from openshift-network-diagnostics started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.420: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:57:42.420: INFO: ovnkube-node-bf8j7 from openshift-ovn-kubernetes started at 2023-01-18 21:27:42 +0000 UTC (5 container statuses recorded)
Jan 18 23:57:42.420: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.420: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:57:42.420: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:57:42.420: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:57:42.420: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:57:42.420: INFO: splunkforwarder-ds-gr4fb from openshift-security started at 2023-01-18 21:27:42 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.420: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 23:57:42.420: INFO: builds-pruner-27901320-2bl9l from openshift-sre-pruning started at 2023-01-18 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.420: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 18 23:57:42.420: INFO: netserver-2 from pod-network-test-4051 started at 2023-01-18 23:57:11 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.420: INFO: 	Container webserver ready: true, restart count 0
Jan 18 23:57:42.420: INFO: sonobuoy-e2e-job-aed0e1be5f434192 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.420: INFO: 	Container e2e ready: true, restart count 0
Jan 18 23:57:42.420: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:57:42.420: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-tjlv7 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.420: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:57:42.420: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:57:42.420: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-219-147.ec2.internal before test
Jan 18 23:57:42.454: INFO: suspend-false-to-true-hmvl8 from job-9753 started at 2023-01-18 23:57:38 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container c ready: true, restart count 0
Jan 18 23:57:42.454: INFO: aws-ebs-csi-driver-node-qprtw from openshift-cluster-csi-drivers started at 2023-01-18 21:03:48 +0000 UTC (3 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:57:42.454: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:57:42.454: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:57:42.454: INFO: tuned-246td from openshift-cluster-node-tuning-operator started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:57:42.454: INFO: dns-default-hhrdx from openshift-dns started at 2023-01-18 23:05:43 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container dns ready: true, restart count 0
Jan 18 23:57:42.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 18 23:57:42.454: INFO: node-resolver-svmsb from openshift-dns started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:57:42.454: INFO: node-ca-nzbcp from openshift-image-registry started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:57:42.454: INFO: ingress-canary-fvl4z from openshift-ingress-canary started at 2023-01-18 23:05:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 18 23:57:42.454: INFO: machine-config-daemon-nb5xf from openshift-machine-config-operator started at 2023-01-18 21:03:48 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:57:42.454: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:57:42.454: INFO: node-exporter-6m8v9 from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.454: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:57:42.454: INFO: sre-dns-latency-exporter-rxh5d from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container main ready: true, restart count 1
Jan 18 23:57:42.454: INFO: multus-55cv4 from openshift-multus started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:57:42.454: INFO: multus-additional-cni-plugins-6n2cz from openshift-multus started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:57:42.454: INFO: network-metrics-daemon-mkvpn from openshift-multus started at 2023-01-18 21:03:48 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.454: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:57:42.454: INFO: network-check-target-cqk2p from openshift-network-diagnostics started at 2023-01-18 21:03:48 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:57:42.454: INFO: ovnkube-node-7vtb8 from openshift-ovn-kubernetes started at 2023-01-18 21:03:48 +0000 UTC (5 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.454: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:57:42.454: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:57:42.454: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:57:42.454: INFO: 	Container ovnkube-node ready: true, restart count 4
Jan 18 23:57:42.454: INFO: splunkforwarder-ds-b6n2g from openshift-security started at 2023-01-18 21:24:58 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 23:57:42.454: INFO: netserver-3 from pod-network-test-4051 started at 2023-01-18 23:57:11 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container webserver ready: true, restart count 0
Jan 18 23:57:42.454: INFO: test-container-pod from pod-network-test-4051 started at 2023-01-18 23:57:33 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container webserver ready: true, restart count 0
Jan 18 23:57:42.454: INFO: sonobuoy from sonobuoy started at 2023-01-18 22:14:33 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 23:57:42.454: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-shjrl from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.454: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:57:42.454: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:57:42.454: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-236-5.ec2.internal before test
Jan 18 23:57:42.486: INFO: addon-operator-catalog-67nfj from openshift-addon-operator started at 2023-01-18 21:26:21 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.486: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:57:42.486: INFO: cloud-ingress-operator-registry-6f5sh from openshift-cloud-ingress-operator started at 2023-01-18 21:31:22 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.486: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:57:42.486: INFO: aws-ebs-csi-driver-node-cv9gd from openshift-cluster-csi-drivers started at 2023-01-18 21:03:23 +0000 UTC (3 container statuses recorded)
Jan 18 23:57:42.486: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:57:42.486: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:57:42.486: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:57:42.486: INFO: tuned-b2gvw from openshift-cluster-node-tuning-operator started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.486: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:57:42.486: INFO: downloads-6f74f6fcbf-w6ht7 from openshift-console started at 2023-01-18 21:31:17 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.486: INFO: 	Container download-server ready: true, restart count 0
Jan 18 23:57:42.486: INFO: custom-domains-operator-7f97f586c8-czz24 from openshift-custom-domains-operator started at 2023-01-18 21:26:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.486: INFO: 	Container custom-domains-operator ready: true, restart count 0
Jan 18 23:57:42.486: INFO: custom-domains-operator-registry-sd6db from openshift-custom-domains-operator started at 2023-01-18 21:26:24 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.486: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:57:42.486: INFO: dns-default-4cddc from openshift-dns started at 2023-01-18 21:04:10 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.486: INFO: 	Container dns ready: true, restart count 1
Jan 18 23:57:42.486: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.486: INFO: node-resolver-8lsl6 from openshift-dns started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.486: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:57:42.486: INFO: node-ca-r29sw from openshift-image-registry started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.486: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:57:42.486: INFO: ingress-canary-d6jm7 from openshift-ingress-canary started at 2023-01-18 21:04:10 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.486: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 23:57:42.486: INFO: machine-config-daemon-xv85b from openshift-machine-config-operator started at 2023-01-18 21:03:23 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:57:42.487: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:57:42.487: INFO: managed-node-metadata-operator-registry-fn84g from openshift-managed-node-metadata-operator started at 2023-01-18 21:31:22 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:57:42.487: INFO: managed-upgrade-operator-catalog-g29pr from openshift-managed-upgrade-operator started at 2023-01-18 21:31:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:57:42.487: INFO: configure-alertmanager-operator-registry-vzl8c from openshift-monitoring started at 2023-01-18 21:31:25 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:57:42.487: INFO: node-exporter-6q46p from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.487: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:57:42.487: INFO: osd-cluster-ready-kfqk7 from openshift-monitoring started at 2023-01-18 21:26:19 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container osd-cluster-ready ready: false, restart count 34
Jan 18 23:57:42.487: INFO: sre-dns-latency-exporter-q7lp5 from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container main ready: true, restart count 1
Jan 18 23:57:42.487: INFO: sre-ebs-iops-reporter-1-l2chd from openshift-monitoring started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container main ready: true, restart count 0
Jan 18 23:57:42.487: INFO: sre-stuck-ebs-vols-1-mjmmn from openshift-monitoring started at 2023-01-18 21:26:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container main ready: true, restart count 0
Jan 18 23:57:42.487: INFO: multus-additional-cni-plugins-p78ph from openshift-multus started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:57:42.487: INFO: multus-wtvrb from openshift-multus started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:57:42.487: INFO: network-metrics-daemon-dxnrq from openshift-multus started at 2023-01-18 21:03:23 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.487: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:57:42.487: INFO: must-gather-operator-5f47db765d-zf7fj from openshift-must-gather-operator started at 2023-01-18 21:26:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container must-gather-operator ready: true, restart count 0
Jan 18 23:57:42.487: INFO: must-gather-operator-registry-gp8pc from openshift-must-gather-operator started at 2023-01-18 21:26:22 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:57:42.487: INFO: network-check-target-tr8pn from openshift-network-diagnostics started at 2023-01-18 21:03:23 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:57:42.487: INFO: obo-prometheus-operator-6cb5cfc7b9-2rkdt from openshift-observability-operator started at 2023-01-18 21:26:20 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 18 23:57:42.487: INFO: obo-prometheus-operator-admission-webhook-5b4dc9bff5-f8pm5 from openshift-observability-operator started at 2023-01-18 21:26:21 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 18 23:57:42.487: INFO: observability-operator-5b467d8ccb-twlnn from openshift-observability-operator started at 2023-01-18 21:26:20 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container operator ready: true, restart count 0
Jan 18 23:57:42.487: INFO: observability-operator-catalog-khgpx from openshift-observability-operator started at 2023-01-18 21:26:25 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:57:42.487: INFO: ocm-agent-operator-9bd68bf49-z6qxl from openshift-ocm-agent-operator started at 2023-01-18 21:26:19 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container ocm-agent-operator ready: true, restart count 0
Jan 18 23:57:42.487: INFO: ocm-agent-operator-registry-qj69h from openshift-ocm-agent-operator started at 2023-01-18 21:26:24 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:57:42.487: INFO: ovnkube-node-g6c8t from openshift-ovn-kubernetes started at 2023-01-18 21:03:23 +0000 UTC (5 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.487: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:57:42.487: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 18 23:57:42.487: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:57:42.487: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:57:42.487: INFO: rbac-permissions-operator-registry-mqh7k from openshift-rbac-permissions started at 2023-01-18 21:26:26 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:57:42.487: INFO: splunkforwarder-ds-zhvhl from openshift-security started at 2023-01-18 21:25:56 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container splunk-uf ready: true, restart count 0
Jan 18 23:57:42.487: INFO: splunk-forwarder-operator-catalog-twzbw from openshift-splunk-forwarder-operator started at 2023-01-18 21:26:27 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:57:42.487: INFO: managed-velero-operator-registry-9j2kp from openshift-velero started at 2023-01-18 21:31:21 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container registry-server ready: true, restart count 0
Jan 18 23:57:42.487: INFO: netserver-4 from pod-network-test-4051 started at 2023-01-18 23:57:11 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container webserver ready: true, restart count 0
Jan 18 23:57:42.487: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-t5zp4 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.487: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:57:42.487: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 23:57:42.487: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-253-152.ec2.internal before test
Jan 18 23:57:42.517: INFO: cloud-ingress-operator-78d58985cd-9kp9q from openshift-cloud-ingress-operator started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Jan 18 23:57:42.517: INFO: aws-ebs-csi-driver-node-97pk2 from openshift-cluster-csi-drivers started at 2023-01-18 21:03:06 +0000 UTC (3 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container csi-driver ready: true, restart count 1
Jan 18 23:57:42.517: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 18 23:57:42.517: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 18 23:57:42.517: INFO: tuned-5wpdt from openshift-cluster-node-tuning-operator started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container tuned ready: true, restart count 1
Jan 18 23:57:42.517: INFO: dns-default-g4nw5 from openshift-dns started at 2023-01-18 21:03:53 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container dns ready: true, restart count 1
Jan 18 23:57:42.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.517: INFO: node-resolver-ffd24 from openshift-dns started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 18 23:57:42.517: INFO: image-registry-7b8f8dcdc5-2bvm7 from openshift-image-registry started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container registry ready: true, restart count 0
Jan 18 23:57:42.517: INFO: node-ca-bh7jz from openshift-image-registry started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container node-ca ready: true, restart count 1
Jan 18 23:57:42.517: INFO: ingress-canary-vz8dj from openshift-ingress-canary started at 2023-01-18 21:03:53 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 18 23:57:42.517: INFO: machine-config-daemon-5zsxx from openshift-machine-config-operator started at 2023-01-18 21:03:06 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 18 23:57:42.517: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 18 23:57:42.517: INFO: node-exporter-z5k8l from openshift-monitoring started at 2023-01-18 21:08:32 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.517: INFO: 	Container node-exporter ready: true, restart count 1
Jan 18 23:57:42.517: INFO: sre-dns-latency-exporter-zsmln from openshift-monitoring started at 2023-01-18 21:18:07 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container main ready: true, restart count 1
Jan 18 23:57:42.517: INFO: multus-additional-cni-plugins-hsjtm from openshift-multus started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 18 23:57:42.517: INFO: multus-xzh2h from openshift-multus started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container kube-multus ready: true, restart count 1
Jan 18 23:57:42.517: INFO: network-metrics-daemon-cq5sl from openshift-multus started at 2023-01-18 21:03:06 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.517: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 18 23:57:42.517: INFO: network-check-source-5cb989cf6f-42gl2 from openshift-network-diagnostics started at 2023-01-18 21:31:18 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container check-endpoints ready: true, restart count 0
Jan 18 23:57:42.517: INFO: network-check-target-9fq7l from openshift-network-diagnostics started at 2023-01-18 21:03:06 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 18 23:57:42.517: INFO: ovnkube-node-mtfln from openshift-ovn-kubernetes started at 2023-01-18 21:03:06 +0000 UTC (5 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 18 23:57:42.517: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 18 23:57:42.517: INFO: 	Container ovn-acl-logging ready: true, restart count 2
Jan 18 23:57:42.517: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 18 23:57:42.517: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 18 23:57:42.517: INFO: splunkforwarder-ds-xsjtq from openshift-security started at 2023-01-18 21:24:58 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 18 23:57:42.517: INFO: netserver-5 from pod-network-test-4051 started at 2023-01-18 23:57:11 +0000 UTC (1 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container webserver ready: true, restart count 0
Jan 18 23:57:42.517: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-ktxs4 from sonobuoy started at 2023-01-18 22:14:37 +0000 UTC (2 container statuses recorded)
Jan 18 23:57:42.517: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 23:57:42.517: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173b8c960e74fd54], Reason = [FailedScheduling], Message = [0/9 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 9 node(s) didn't match Pod's node affinity/selector. preemption: 0/9 nodes are available: 9 Preemption is not helpful for scheduling.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 18 23:57:43.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4027" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":356,"completed":330,"skipped":6157,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:57:43.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 18 23:57:44.185: INFO: Pod name wrapped-volume-race-a0e293cf-1f93-4550-b7d6-abcc71de8cc4: Found 0 pods out of 5
Jan 18 23:57:49.192: INFO: Pod name wrapped-volume-race-a0e293cf-1f93-4550-b7d6-abcc71de8cc4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a0e293cf-1f93-4550-b7d6-abcc71de8cc4 in namespace emptydir-wrapper-261, will wait for the garbage collector to delete the pods
Jan 18 23:57:49.261: INFO: Deleting ReplicationController wrapped-volume-race-a0e293cf-1f93-4550-b7d6-abcc71de8cc4 took: 5.411923ms
Jan 18 23:57:49.362: INFO: Terminating ReplicationController wrapped-volume-race-a0e293cf-1f93-4550-b7d6-abcc71de8cc4 pods took: 100.924957ms
STEP: Creating RC which spawns configmap-volume pods
Jan 18 23:57:51.284: INFO: Pod name wrapped-volume-race-bdccdec4-59df-4056-b86d-3df988e271a6: Found 0 pods out of 5
Jan 18 23:57:56.291: INFO: Pod name wrapped-volume-race-bdccdec4-59df-4056-b86d-3df988e271a6: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-bdccdec4-59df-4056-b86d-3df988e271a6 in namespace emptydir-wrapper-261, will wait for the garbage collector to delete the pods
Jan 18 23:57:56.359: INFO: Deleting ReplicationController wrapped-volume-race-bdccdec4-59df-4056-b86d-3df988e271a6 took: 4.625447ms
Jan 18 23:57:56.459: INFO: Terminating ReplicationController wrapped-volume-race-bdccdec4-59df-4056-b86d-3df988e271a6 pods took: 100.102501ms
STEP: Creating RC which spawns configmap-volume pods
Jan 18 23:57:58.476: INFO: Pod name wrapped-volume-race-42d690f8-cff0-4a30-ab7b-3b2d1d8d8706: Found 0 pods out of 5
Jan 18 23:58:03.480: INFO: Pod name wrapped-volume-race-42d690f8-cff0-4a30-ab7b-3b2d1d8d8706: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-42d690f8-cff0-4a30-ab7b-3b2d1d8d8706 in namespace emptydir-wrapper-261, will wait for the garbage collector to delete the pods
Jan 18 23:58:03.550: INFO: Deleting ReplicationController wrapped-volume-race-42d690f8-cff0-4a30-ab7b-3b2d1d8d8706 took: 5.014171ms
Jan 18 23:58:03.650: INFO: Terminating ReplicationController wrapped-volume-race-42d690f8-cff0-4a30-ab7b-3b2d1d8d8706 pods took: 100.790513ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Jan 18 23:58:06.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-261" for this suite.

• [SLOW TEST:22.489 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":356,"completed":331,"skipped":6173,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:58:06.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-9eed0338-d7c7-4157-9a77-c46a83f34992
STEP: Creating a pod to test consume configMaps
Jan 18 23:58:06.211: INFO: Waiting up to 5m0s for pod "pod-configmaps-7a683ef4-8e9d-4864-b89c-f55ccb1de58e" in namespace "configmap-8937" to be "Succeeded or Failed"
Jan 18 23:58:06.217: INFO: Pod "pod-configmaps-7a683ef4-8e9d-4864-b89c-f55ccb1de58e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.375573ms
Jan 18 23:58:08.220: INFO: Pod "pod-configmaps-7a683ef4-8e9d-4864-b89c-f55ccb1de58e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008990771s
Jan 18 23:58:10.235: INFO: Pod "pod-configmaps-7a683ef4-8e9d-4864-b89c-f55ccb1de58e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023610511s
Jan 18 23:58:12.245: INFO: Pod "pod-configmaps-7a683ef4-8e9d-4864-b89c-f55ccb1de58e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033595882s
STEP: Saw pod success
Jan 18 23:58:12.245: INFO: Pod "pod-configmaps-7a683ef4-8e9d-4864-b89c-f55ccb1de58e" satisfied condition "Succeeded or Failed"
Jan 18 23:58:12.258: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-configmaps-7a683ef4-8e9d-4864-b89c-f55ccb1de58e container agnhost-container: <nil>
STEP: delete the pod
Jan 18 23:58:12.278: INFO: Waiting for pod pod-configmaps-7a683ef4-8e9d-4864-b89c-f55ccb1de58e to disappear
Jan 18 23:58:12.282: INFO: Pod pod-configmaps-7a683ef4-8e9d-4864-b89c-f55ccb1de58e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 18 23:58:12.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8937" for this suite.

• [SLOW TEST:6.195 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":332,"skipped":6176,"failed":1,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 18 23:58:12.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-2068
Jan 18 23:58:12.393: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:58:14.395: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 18 23:58:14.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2068 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 18 23:58:14.578: INFO: rc: 7
Jan 18 23:58:14.588: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 18 23:58:14.590: INFO: Pod kube-proxy-mode-detector no longer exists
Jan 18 23:58:14.590: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2068 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-2068
STEP: creating replication controller affinity-clusterip-timeout in namespace services-2068
I0118 23:58:14.603196      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-2068, replica count: 3
I0118 23:58:17.654784      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 23:58:20.654921      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 23:58:20.660: INFO: Creating new exec pod
Jan 18 23:58:25.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2068 exec execpod-affinityjxvkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 18 23:58:25.808: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 18 23:58:25.808: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:58:25.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2068 exec execpod-affinityjxvkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.90.7 80'
Jan 18 23:58:25.909: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.90.7 80\nConnection to 172.30.90.7 80 port [tcp/http] succeeded!\n"
Jan 18 23:58:25.909: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:58:25.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2068 exec execpod-affinityjxvkd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.90.7:80/ ; done'
Jan 18 23:58:26.082: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n"
Jan 18 23:58:26.082: INFO: stdout: "\naffinity-clusterip-timeout-2hrwq\naffinity-clusterip-timeout-2hrwq\naffinity-clusterip-timeout-2hrwq\naffinity-clusterip-timeout-2hrwq\naffinity-clusterip-timeout-2hrwq\naffinity-clusterip-timeout-2hrwq\naffinity-clusterip-timeout-2hrwq\naffinity-clusterip-timeout-2hrwq\naffinity-clusterip-timeout-2hrwq\naffinity-clusterip-timeout-2hrwq\naffinity-clusterip-timeout-2hrwq\naffinity-clusterip-timeout-2hrwq\naffinity-clusterip-timeout-2hrwq\naffinity-clusterip-timeout-2hrwq\naffinity-clusterip-timeout-2hrwq\naffinity-clusterip-timeout-2hrwq"
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Received response from host: affinity-clusterip-timeout-2hrwq
Jan 18 23:58:26.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2068 exec execpod-affinityjxvkd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.90.7:80/'
Jan 18 23:58:26.186: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n"
Jan 18 23:58:26.186: INFO: stdout: "affinity-clusterip-timeout-2hrwq"
Jan 18 23:58:46.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2068 exec execpod-affinityjxvkd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.90.7:80/'
Jan 18 23:58:46.315: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n"
Jan 18 23:58:46.315: INFO: stdout: "affinity-clusterip-timeout-2hrwq"
Jan 18 23:59:06.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2068 exec execpod-affinityjxvkd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.90.7:80/'
Jan 18 23:59:06.421: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n"
Jan 18 23:59:06.421: INFO: stdout: "affinity-clusterip-timeout-2hrwq"
Jan 18 23:59:26.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2068 exec execpod-affinityjxvkd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.90.7:80/'
Jan 18 23:59:26.543: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n"
Jan 18 23:59:26.543: INFO: stdout: "affinity-clusterip-timeout-2hrwq"
Jan 18 23:59:46.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2068 exec execpod-affinityjxvkd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.90.7:80/'
Jan 18 23:59:46.670: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n"
Jan 18 23:59:46.670: INFO: stdout: "affinity-clusterip-timeout-2hrwq"
Jan 19 00:00:06.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2068 exec execpod-affinityjxvkd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.90.7:80/'
Jan 19 00:00:06.805: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n"
Jan 19 00:00:06.805: INFO: stdout: "affinity-clusterip-timeout-2hrwq"
Jan 19 00:00:26.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2068 exec execpod-affinityjxvkd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.90.7:80/'
Jan 19 00:00:26.953: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n"
Jan 19 00:00:26.953: INFO: stdout: "affinity-clusterip-timeout-2hrwq"
Jan 19 00:00:46.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2068 exec execpod-affinityjxvkd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.90.7:80/'
Jan 19 00:00:47.106: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n"
Jan 19 00:00:47.106: INFO: stdout: "affinity-clusterip-timeout-2hrwq"
Jan 19 00:01:07.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2068 exec execpod-affinityjxvkd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.90.7:80/'
Jan 19 00:01:07.246: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n"
Jan 19 00:01:07.246: INFO: stdout: "affinity-clusterip-timeout-2hrwq"
Jan 19 00:01:27.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-2068 exec execpod-affinityjxvkd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.90.7:80/'
Jan 19 00:01:27.383: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.90.7:80/\n"
Jan 19 00:01:27.383: INFO: stdout: "affinity-clusterip-timeout-2hrwq"
Jan 19 00:01:47.384: FAIL: Session is sticky after reaching the timeout

Full Stack Trace
k8s.io/kubernetes/test/e2e/network.execAffinityTestForSessionAffinityTimeout(0xc000c1bc80, {0x7a6bf58, 0xc002d3a300}, 0xc002486a00)
	test/e2e/network/service.go:3367 +0xb2e
k8s.io/kubernetes/test/e2e/network.glob..func25.23()
	test/e2e/network/service.go:1829 +0x8b
k8s.io/kubernetes/test/e2e.RunE2ETests(0x257e5b7?)
	test/e2e/e2e.go:130 +0x686
k8s.io/kubernetes/test/e2e.TestE2E(0x24efd19?)
	test/e2e/e2e_test.go:136 +0x19
testing.tRunner(0xc00072f6c0, 0x73fa160)
	/usr/local/go/src/testing/testing.go:1439 +0x102
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:1486 +0x35f
Jan 19 00:01:47.384: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-2068, will wait for the garbage collector to delete the pods
Jan 19 00:01:47.453: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.512093ms
Jan 19 00:01:47.554: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.02945ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
STEP: Collecting events from namespace "services-2068".
STEP: Found 32 events.
Jan 19 00:01:50.080: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for affinity-clusterip-timeout-2hrwq: { } Scheduled: Successfully assigned services-2068/affinity-clusterip-timeout-2hrwq to ip-10-0-128-7.ec2.internal by ip-10-0-176-161
Jan 19 00:01:50.080: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for affinity-clusterip-timeout-clr92: { } Scheduled: Successfully assigned services-2068/affinity-clusterip-timeout-clr92 to ip-10-0-219-147.ec2.internal by ip-10-0-176-161
Jan 19 00:01:50.080: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for affinity-clusterip-timeout-gdhzv: { } Scheduled: Successfully assigned services-2068/affinity-clusterip-timeout-gdhzv to ip-10-0-211-217.ec2.internal by ip-10-0-176-161
Jan 19 00:01:50.080: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for execpod-affinityjxvkd: { } Scheduled: Successfully assigned services-2068/execpod-affinityjxvkd to ip-10-0-219-147.ec2.internal by ip-10-0-176-161
Jan 19 00:01:50.080: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for kube-proxy-mode-detector: { } Scheduled: Successfully assigned services-2068/kube-proxy-mode-detector to ip-10-0-219-147.ec2.internal by ip-10-0-176-161
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:12 +0000 UTC - event for kube-proxy-mode-detector: {kubelet ip-10-0-219-147.ec2.internal} Started: Started container agnhost-container
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:12 +0000 UTC - event for kube-proxy-mode-detector: {kubelet ip-10-0-219-147.ec2.internal} Created: Created container agnhost-container
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:12 +0000 UTC - event for kube-proxy-mode-detector: {kubelet ip-10-0-219-147.ec2.internal} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.36" already present on machine
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:14 +0000 UTC - event for affinity-clusterip-timeout: {replication-controller } SuccessfulCreate: Created pod: affinity-clusterip-timeout-2hrwq
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:14 +0000 UTC - event for affinity-clusterip-timeout: {replication-controller } SuccessfulCreate: Created pod: affinity-clusterip-timeout-gdhzv
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:14 +0000 UTC - event for affinity-clusterip-timeout: {replication-controller } SuccessfulCreate: Created pod: affinity-clusterip-timeout-clr92
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:15 +0000 UTC - event for kube-proxy-mode-detector: {kubelet ip-10-0-219-147.ec2.internal} Killing: Stopping container agnhost-container
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:16 +0000 UTC - event for affinity-clusterip-timeout-2hrwq: {multus } AddedInterface: Add eth0 [10.128.14.128/23] from ovn-kubernetes
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:16 +0000 UTC - event for affinity-clusterip-timeout-2hrwq: {kubelet ip-10-0-128-7.ec2.internal} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.36" already present on machine
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:16 +0000 UTC - event for affinity-clusterip-timeout-2hrwq: {kubelet ip-10-0-128-7.ec2.internal} Started: Started container affinity-clusterip-timeout
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:16 +0000 UTC - event for affinity-clusterip-timeout-2hrwq: {kubelet ip-10-0-128-7.ec2.internal} Created: Created container affinity-clusterip-timeout
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:16 +0000 UTC - event for affinity-clusterip-timeout-gdhzv: {kubelet ip-10-0-211-217.ec2.internal} Started: Started container affinity-clusterip-timeout
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:16 +0000 UTC - event for affinity-clusterip-timeout-gdhzv: {kubelet ip-10-0-211-217.ec2.internal} Created: Created container affinity-clusterip-timeout
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:16 +0000 UTC - event for affinity-clusterip-timeout-gdhzv: {kubelet ip-10-0-211-217.ec2.internal} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.36" already present on machine
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:16 +0000 UTC - event for affinity-clusterip-timeout-gdhzv: {multus } AddedInterface: Add eth0 [10.128.16.207/23] from ovn-kubernetes
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:17 +0000 UTC - event for affinity-clusterip-timeout-clr92: {kubelet ip-10-0-219-147.ec2.internal} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.36" already present on machine
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:17 +0000 UTC - event for affinity-clusterip-timeout-clr92: {multus } AddedInterface: Add eth0 [10.128.13.76/23] from ovn-kubernetes
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:17 +0000 UTC - event for affinity-clusterip-timeout-clr92: {kubelet ip-10-0-219-147.ec2.internal} Started: Started container affinity-clusterip-timeout
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:17 +0000 UTC - event for affinity-clusterip-timeout-clr92: {kubelet ip-10-0-219-147.ec2.internal} Created: Created container affinity-clusterip-timeout
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:22 +0000 UTC - event for execpod-affinityjxvkd: {kubelet ip-10-0-219-147.ec2.internal} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.36" already present on machine
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:22 +0000 UTC - event for execpod-affinityjxvkd: {multus } AddedInterface: Add eth0 [10.128.13.77/23] from ovn-kubernetes
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:23 +0000 UTC - event for execpod-affinityjxvkd: {kubelet ip-10-0-219-147.ec2.internal} Started: Started container agnhost-container
Jan 19 00:01:50.080: INFO: At 2023-01-18 23:58:23 +0000 UTC - event for execpod-affinityjxvkd: {kubelet ip-10-0-219-147.ec2.internal} Created: Created container agnhost-container
Jan 19 00:01:50.080: INFO: At 2023-01-19 00:01:47 +0000 UTC - event for affinity-clusterip-timeout-2hrwq: {kubelet ip-10-0-128-7.ec2.internal} Killing: Stopping container affinity-clusterip-timeout
Jan 19 00:01:50.080: INFO: At 2023-01-19 00:01:47 +0000 UTC - event for affinity-clusterip-timeout-clr92: {kubelet ip-10-0-219-147.ec2.internal} Killing: Stopping container affinity-clusterip-timeout
Jan 19 00:01:50.080: INFO: At 2023-01-19 00:01:47 +0000 UTC - event for affinity-clusterip-timeout-gdhzv: {kubelet ip-10-0-211-217.ec2.internal} Killing: Stopping container affinity-clusterip-timeout
Jan 19 00:01:50.080: INFO: At 2023-01-19 00:01:47 +0000 UTC - event for execpod-affinityjxvkd: {kubelet ip-10-0-219-147.ec2.internal} Killing: Stopping container agnhost-container
Jan 19 00:01:50.082: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
Jan 19 00:01:50.082: INFO: 
Jan 19 00:01:50.087: INFO: 
Logging node info for node ip-10-0-128-7.ec2.internal
Jan 19 00:01:50.099: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-128-7.ec2.internal    0e2fac99-ae24-4675-b8b4-0c59f1661da4 308602 0 2023-01-18 21:27:09 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:r5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-128-7.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io:infra node-role.kubernetes.io/infra: node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:r5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-010e853cf354cb872","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0abfe3af21c04dd6c"} k8s.ovn.org/host-addresses:["10.0.128.7"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-128-7.ec2.internal","mac-address":"02:a3:9c:91:ae:55","ip-addresses":["10.0.128.7/17"],"ip-address":"10.0.128.7/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:52724080-26ac-4555-b2d7-40245661c309 k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.9/16"} k8s.ovn.org/node-mgmt-port-mac-address:ae:4c:ac:9d:28:d3 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.128.7/17"} k8s.ovn.org/node-subnets:{"default":"10.128.14.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-infra-us-east-1a-f5bmn machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done managed.openshift.com/customlabels:node-role.kubernetes.io,node-role.kubernetes.io/infra volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{ancient-changes Update v1 2023-01-18 21:27:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {nodelink-controller Update v1 2023-01-18 21:27:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}},"f:labels":{"f:node-role.kubernetes.io":{},"f:node-role.kubernetes.io/infra":{}}}} } {cloud-network-config-controller Update v1 2023-01-18 21:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{}}}} } {machine-config-daemon Update v1 2023-01-18 21:27:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {ip-10-0-128-7 Update v1 2023-01-18 21:27:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {machine-config-controller Update v1 2023-01-18 21:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {manager Update v1 2023-01-18 21:36:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:managed.openshift.com/customlabels":{}}}} } {kube-controller-manager Update v1 2023-01-18 21:36:17 +0000 UTC FieldsV1 {"f:status":{"f:volumesAttached":{}}} status} {kubelet Update v1 2023-01-18 22:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}},"f:volumesInUse":{}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-0abfe3af21c04dd6c,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321574121472 0} {<nil>} 314037228Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{33222410240 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3920 -3} {<nil>} 3920m DecimalSI},ephemeral-storage: {{289416708846 0} {<nil>} 289416708846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{29423880765440 -3} {<nil>} 29423880765440m DecimalSI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-19 00:00:32 +0000 UTC,LastTransitionTime:2023-01-18 21:36:02 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-19 00:00:32 +0000 UTC,LastTransitionTime:2023-01-18 21:36:02 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-19 00:00:32 +0000 UTC,LastTransitionTime:2023-01-18 21:36:02 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-19 00:00:32 +0000 UTC,LastTransitionTime:2023-01-18 21:36:02 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.128.7,},NodeAddress{Type:Hostname,Address:ip-10-0-128-7.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-128-7.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec21c658414c8d2d06336e7d6a9b3742,SystemUUID:ec21c658-414c-8d2d-0633-6e7d6a9b3742,BootID:c7eefbc8-7dc5-4eb8-9fc3-8763853c85cf,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[image-registry.openshift-image-registry.svc:5000/openshift/cli@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45 image-registry.openshift-image-registry.svc:5000/openshift/cli:latest],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c7df53b796e81ba8301ba74d02317226329bd5752fd31c1b44d028e4832f21c3],SizeBytes:442026998,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f1134e2e28f44375c3bd9a6ee34d7f9972fdbd3305a3ee2b95e6ed3cede02140],SizeBytes:409661973,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:36fc214537c763b3a3f0a9dc7a1bd4378a80428c31b2629df8786a9b09155e6d],SizeBytes:398604719,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab0d2f0b6d01a4a3b9952710cc49e787ebb0e8649a288438774c1ffe0fc3fae0],SizeBytes:392510137,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5813d3728229c2a09a05bc454753e0128ac2bbd203e7000a4d954daab12fbb79],SizeBytes:377171632,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c956edcee9b9ba5462572b65b6a92983b20ace63dae50e3237bfdbd6d8c0b972],SizeBytes:375087838,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5e1c69d005727e3245604cfca7a63e4f9bc6e15128c7489e41d5e967305089e],SizeBytes:371248553,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:77c5db690d9438ac077736cad8f28c04de476c04c3a97f39910ed86b6c395b85],SizeBytes:368328246,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f3ebf99182733dff2666e6a5e7e217085e06029362036ad79c91278e69dcbf7],SizeBytes:366270626,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2269a2b10b3ba2a6783dde2a97451f57f5716a20ebef4a82bea20d25df8761fa],SizeBytes:365147114,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:32fa95a3d4ecfebe96152242926addf27789555b12413203927f255a712c8a0c],SizeBytes:358569445,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:91308d35c1e56463f55c1aaa519ff4de7335d43b254c21abdb845fc8c72821a1],SizeBytes:347460509,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:255a95e8b514dcdc4fa12436789115739355c13fae93eccbba660298746d9aa1],SizeBytes:346372629,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a4fdcff8b0038d858d46740c9622d5dc428074dcf7e662deacd9e6c0aff18286],SizeBytes:344400730,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:209e20410ec2d3d7a502f568d2b7fe1cd1beadcb36fff2d1e6f59d77be3200e3],SizeBytes:339844669,},ContainerImage{Names:[quay.io/openshift/origin-kube-rbac-proxy@sha256:baedb268ac66456018fb30af395bb3d69af5fff3252ff5d549f0231b1ebb6901 quay.io/openshift/origin-kube-rbac-proxy:4.10.0],SizeBytes:337627888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:140f8947593d92e1517e50a201e83bdef8eb965b552a21d3caf346a250d0cf6e],SizeBytes:335139477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01de581f7f0c7e43f809e9346ced7e60c0ecdd7c0078e97a8f64b24d4d3fa0b7],SizeBytes:332949901,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ed634acd9b29e16b578c74b38d5425156cd39bb72c0884a9564331f95f80911],SizeBytes:330558591,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs@sha256:83da6a30accf7d97453051973ba8c2ae1f78fa0472d2aeb0f8fcfdd8ba04e11a docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:324891489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:923999744ad1557510fc5c3a95cf978b27c453988489ada899c574ab1d71a218],SizeBytes:312722401,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-base@sha256:8da4f871cc18d3ac288a2fbb5c37ea27eabea4314dabca68f3f8469c7d7a4a21 quay.io/app-sre/managed-prometheus-exporter-base:latest],SizeBytes:303857745,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2550b2cbdf864515b1edacf43c25eb6b6f179713c1df34e51f6e9bba48d6430a],SizeBytes:303075904,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9260bbcc285a9fd4c1c7df865280261723eb7679050905da4acbfe728f96ee1b],SizeBytes:298386295,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder@sha256:afc413cd2504b586b6f28c16355db243c8bfdef536cc80e44f61e03c01e12235],SizeBytes:229564388,},ContainerImage{Names:[quay.io/app-sre/addon-operator-manager@sha256:3a5f9c6073f3b3542e53be155f1364bcba10233ed69011eaadbc73318776b775 quay.io/app-sre/addon-operator-manager:a45b2b8],SizeBytes:176376786,},ContainerImage{Names:[quay.io/app-sre/addon-operator-webhook@sha256:16a258633afc63916d84a447a6dd8a234e1803a4dfa22a15887ff1dc71781e75 quay.io/app-sre/addon-operator-webhook:a45b2b8],SizeBytes:173194705,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:20f25f275d46aa728f7615a1ccc19c78b2ed89435bf943a44b339f70f45508e6 k8s.gcr.io/e2e-test-images/httpd@sha256:5d28f127fae41261c56abccd96df481f8f4fba2a3305fece212e11aafe646943 k8s.gcr.io/e2e-test-images/httpd:2.4.39-2],SizeBytes:132295599,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 k8s.gcr.io/e2e-test-images/httpd@sha256:6589a0d3a4b40f996ab554f0d58e8c567c2850c41bc99d05498378a50b7e1cb4 k8s.gcr.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:128894651,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403 k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8 k8s.gcr.io/e2e-test-images/agnhost:2.36],SizeBytes:126252387,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:7f3ae784f32d750e63495dacd39c3bc57a9a0fc3e7ffc4b6f65e36be03cb368c docker.io/sonobuoy/sonobuoy@sha256:eaa42dd0660ece6c18d06199b78a41bd532a4851fc32a5a99acfafc03556852e docker.io/sonobuoy/sonobuoy:v0.56.14],SizeBytes:49637374,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nginx@sha256:13616070e3f29de4417eee434a8ef472221c9e51b3d037b5a6b46cef08eb7443 k8s.gcr.io/e2e-test-images/nginx@sha256:eee1822ee5bafc780db34f9ab68456c5fdcc0f994e1c1e01d5cde593cb3897a1 k8s.gcr.io/e2e-test-images/nginx:1.14-2],SizeBytes:17245363,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf k8s.gcr.io/e2e-test-images/busybox@sha256:f1fa97a4e1dab565da2790e6c76f8aecf87c44c9ac7ba53a9c65af96ec472c4a k8s.gcr.io/e2e-test-images/busybox:1.29-2],SizeBytes:1374588,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c k8s.gcr.io/pause@sha256:f81611a21cf91214c1ea751c5b525931a0e2ebabe62b3937b6158039ff6f922d k8s.gcr.io/pause:3.7],SizeBytes:717997,},},VolumesInUse:[kubernetes.io/csi/ebs.csi.aws.com^vol-03be0e2c9156fe2c9 kubernetes.io/csi/ebs.csi.aws.com^vol-0d9cb1f50abc74bfa],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/csi/ebs.csi.aws.com^vol-03be0e2c9156fe2c9,DevicePath:,},AttachedVolume{Name:kubernetes.io/csi/ebs.csi.aws.com^vol-0d9cb1f50abc74bfa,DevicePath:,},},Config:nil,},}
Jan 19 00:01:50.099: INFO: 
Logging kubelet events for node ip-10-0-128-7.ec2.internal
Jan 19 00:01:50.103: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-128-7.ec2.internal
Jan 19 00:01:50.137: INFO: machine-config-daemon-htf69 started at 2023-01-18 21:27:18 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 00:01:50.137: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 00:01:50.137: INFO: kube-state-metrics-76877575d5-ptnh8 started at 2023-01-18 21:36:25 +0000 UTC (0+3 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 19 00:01:50.137: INFO: deployments-pruner-27901380-b6sl8 started at 2023-01-18 23:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 19 00:01:50.137: INFO: network-metrics-daemon-ffdlk started at 2023-01-18 21:27:18 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.137: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 00:01:50.137: INFO: aws-ebs-csi-driver-node-mq4qf started at 2023-01-18 21:27:18 +0000 UTC (0+3 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 00:01:50.137: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 00:01:50.137: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 00:01:50.137: INFO: prometheus-k8s-1 started at 2023-01-18 21:36:15 +0000 UTC (1+6 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Init container init-config-reloader ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 00:01:50.137: INFO: addon-operator-webhooks-569bd4cfc5-dz6nk started at 2023-01-18 21:36:15 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container webhook ready: true, restart count 0
Jan 19 00:01:50.137: INFO: prometheus-operator-76957bb5bd-2pztb started at 2023-01-18 21:36:25 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 19 00:01:50.137: INFO: openshift-state-metrics-59fc669d8d-v25vz started at 2023-01-18 21:36:25 +0000 UTC (0+3 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jan 19 00:01:50.137: INFO: deployments-pruner-27901320-vt4j8 started at 2023-01-18 22:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 19 00:01:50.137: INFO: sre-dns-latency-exporter-8dptc started at 2023-01-18 21:27:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container main ready: true, restart count 1
Jan 19 00:01:50.137: INFO: thanos-querier-57f44c5498-xjsz4 started at 2023-01-18 21:36:15 +0000 UTC (0+6 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container thanos-query ready: true, restart count 0
Jan 19 00:01:50.137: INFO: prometheus-operator-admission-webhook-577cc9c956-d6fkv started at 2023-01-18 21:36:15 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 00:01:50.137: INFO: alertmanager-main-1 started at 2023-01-18 21:36:15 +0000 UTC (0+6 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container alertmanager ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 00:01:50.137: INFO: osd-delete-backplane-serviceaccounts-27901440-fcmzj started at 2023-01-19 00:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 19 00:01:50.137: INFO: node-ca-6cvvj started at 2023-01-18 21:27:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 00:01:50.137: INFO: dns-default-btls8 started at 2023-01-18 22:06:24 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container dns ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.137: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-dwhpp started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 00:01:50.137: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 00:01:50.137: INFO: osd-patch-subscription-source-27901320-gz2jm started at 2023-01-18 22:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 19 00:01:50.137: INFO: prometheus-adapter-cf64f7f46-7bf79 started at 2023-01-18 21:36:15 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 19 00:01:50.137: INFO: router-default-7cff97cd98-dqgwn started at 2023-01-18 21:36:15 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container router ready: true, restart count 0
Jan 19 00:01:50.137: INFO: ovnkube-node-mkzt4 started at 2023-01-18 21:27:18 +0000 UTC (0+5 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.137: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 00:01:50.137: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 00:01:50.137: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 00:01:50.137: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 00:01:50.137: INFO: osd-patch-subscription-source-27901440-96chq started at 2023-01-19 00:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 19 00:01:50.137: INFO: builds-pruner-27901380-cpft8 started at 2023-01-18 23:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.137: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 19 00:01:50.137: INFO: multus-additional-cni-plugins-5lmsf started at 2023-01-18 21:27:18 +0000 UTC (6+1 container statuses recorded)
Jan 19 00:01:50.138: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 19 00:01:50.138: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 19 00:01:50.138: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 19 00:01:50.138: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 19 00:01:50.138: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 19 00:01:50.138: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 19 00:01:50.138: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 00:01:50.138: INFO: node-exporter-spmbc started at 2023-01-18 21:27:18 +0000 UTC (1+2 container statuses recorded)
Jan 19 00:01:50.138: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 19 00:01:50.138: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.138: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 00:01:50.138: INFO: tuned-l9d9s started at 2023-01-18 21:27:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.138: INFO: 	Container tuned ready: true, restart count 1
Jan 19 00:01:50.138: INFO: ingress-canary-qw8v9 started at 2023-01-18 21:28:03 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.138: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 00:01:50.138: INFO: addon-operator-manager-c8c4859bf-84wp8 started at 2023-01-18 21:36:25 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.138: INFO: 	Container manager ready: true, restart count 0
Jan 19 00:01:50.138: INFO: 	Container metrics-relay-server ready: true, restart count 0
Jan 19 00:01:50.138: INFO: splunkforwarder-ds-fbjlj started at 2023-01-18 21:27:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.138: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 00:01:50.138: INFO: multus-znqj6 started at 2023-01-18 21:27:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.138: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 00:01:50.138: INFO: node-resolver-vhz2m started at 2023-01-18 21:27:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.138: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 00:01:50.138: INFO: telemeter-client-6bb748465-zttx9 started at 2023-01-18 21:36:25 +0000 UTC (0+3 container statuses recorded)
Jan 19 00:01:50.138: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.138: INFO: 	Container reload ready: true, restart count 0
Jan 19 00:01:50.138: INFO: 	Container telemeter-client ready: true, restart count 0
Jan 19 00:01:50.138: INFO: network-check-target-xwp8p started at 2023-01-18 21:27:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.138: INFO: 	Container network-check-target-container ready: true, restart count 1
W0119 00:01:50.140439      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 19 00:01:50.202: INFO: 
Latency metrics for node ip-10-0-128-7.ec2.internal
Jan 19 00:01:50.202: INFO: 
Logging node info for node ip-10-0-164-47.ec2.internal
Jan 19 00:01:50.205: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-164-47.ec2.internal    e59b7a45-572f-4e61-9a5b-f3f783808f02 307855 0 2023-01-18 20:50:38 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.2xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-164-47.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io/master: node.kubernetes.io/instance-type:m5.2xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-0b3b3d8daf4bd7665","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-001d375a61a9acedb"} k8s.ovn.org/host-addresses:["10.0.164.47"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-164-47.ec2.internal","mac-address":"02:eb:d1:75:d1:d3","ip-addresses":["10.0.164.47/17"],"ip-address":"10.0.164.47/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:8ae32da4-7cb3-4d76-8e05-755f3c313edb k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.3/16"} k8s.ovn.org/node-mgmt-port-mac-address:22:25:b3:75:3a:91 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.164.47/17"} k8s.ovn.org/node-subnets:{"default":"10.128.2.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-master-1 machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/desiredConfig:rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubelet Update v1 2023-01-18 20:50:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/master":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {ip-10-0-164-47 Update v1 2023-01-18 20:56:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {cloud-network-config-controller Update v1 2023-01-18 20:58:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{}}}} } {nodelink-controller Update v1 2023-01-18 20:58:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}} } {machine-config-daemon Update v1 2023-01-18 20:59:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {machine-config-controller Update v1 2023-01-18 21:24:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {kube-controller-manager Update v1 2023-01-18 21:28:43 +0000 UTC FieldsV1 {"f:spec":{"f:taints":{}}} } {kubelet Update v1 2023-01-18 21:30:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:capacity":{"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-001d375a61a9acedb,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{8 0} {<nil>} 8 DecimalSI},ephemeral-storage: {{375261212672 0} {<nil>} 366466028Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{33221505024 0} {<nil>} 32442876Ki BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{7910 -3} {<nil>} 7910m DecimalSI},ephemeral-storage: {{337735090846 0} {<nil>} 337735090846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{29422975549440 -3} {<nil>} 29422975549440m DecimalSI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-18 23:59:27 +0000 UTC,LastTransitionTime:2023-01-18 21:30:27 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-18 23:59:27 +0000 UTC,LastTransitionTime:2023-01-18 21:30:27 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-18 23:59:27 +0000 UTC,LastTransitionTime:2023-01-18 21:30:27 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-18 23:59:27 +0000 UTC,LastTransitionTime:2023-01-18 21:30:27 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.164.47,},NodeAddress{Type:Hostname,Address:ip-10-0-164-47.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-164-47.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec28c4af2b916b7eacf9cea932ffe6d0,SystemUUID:ec28c4af-2b91-6b7e-acf9-cea932ffe6d0,BootID:5e0ae34f-f696-4bf5-9841-70e984334861,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[registry.redhat.io/redhat/redhat-operator-index@sha256:19e8a9a4d95111fd4db559838dd26ffa823b26dcac8004cd553a2ce39388b31f registry.redhat.io/redhat/redhat-operator-index@sha256:efdc74d42cfb372d6df37bb383af9585db327f2b47366818118addbe2e5825c1 registry.redhat.io/redhat/redhat-operator-index:v4.11],SizeBytes:1393216950,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[registry.redhat.io/redhat/community-operator-index@sha256:9d33175d7dcd7fd04bb53aea6e96e4bd902b866a09eef7fc7b5982c340929929 registry.redhat.io/redhat/community-operator-index@sha256:b36e3032da5f096768c703d5112a06325083783602ae8186b923def349a15578 registry.redhat.io/redhat/community-operator-index:v4.11],SizeBytes:960075608,},ContainerImage{Names:[registry.redhat.io/redhat/certified-operator-index@sha256:08d51898f4a8b59ec52f51c835f7f46871905001248d2f91a8c50c5c6ae04a43 registry.redhat.io/redhat/certified-operator-index@sha256:e3617454b09c589d685c05e3f16dcc9fb73c02688cf720599f3fe453f718da74 registry.redhat.io/redhat/certified-operator-index:v4.11],SizeBytes:932364392,},ContainerImage{Names:[registry.redhat.io/redhat/redhat-marketplace-index@sha256:8bba3a2e3edd687602c0702153f5a2c2f5055c4fb51093152fca24252fca87c4 registry.redhat.io/redhat/redhat-marketplace-index@sha256:ab8a83a2a96ad8efdd2b923b615867953cf746d779320facab9d533e0ea00c9a registry.redhat.io/redhat/redhat-marketplace-index:v4.11],SizeBytes:826959445,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c32772c6199a054c863126285a8b30cdadd6b1296cff1e4f22d6d3e3f9c84a7c],SizeBytes:823606942,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08ab62da9265ef67f4eb4b526b38346a3ae3d35e2d5bfbdd58a0858b76b21f77],SizeBytes:681959546,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:43612fe6e941c7292ed3ab0933e60703ce9115357a44aa20be25ed07b7cc7959],SizeBytes:612295182,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:515d4b9ff441191257d7b9ff99666737682d3b5e5a1879ba7abdd1e019f1dff2],SizeBytes:480023620,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c305cab192dd40b909dbb56981ad14c8460a6eac9bed9cbeebcd569a5a2eb4f0],SizeBytes:466400791,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:865e66be494fc077d12492a7dc204616fc8a13bf4c906f4081ab3555ce2cc970],SizeBytes:454669803,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72befefa8585db143140a682046ee43f4a62d6b0b5fc16a20b48186dc2b1dcb3],SizeBytes:450775130,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fead5da1351abee9e77f5c46247c7947d6f9cf76b5bf7a8b4905222fe2564665],SizeBytes:434647633,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cdf42132f29ddf587b7da228d83d184b152d81ed7cdff83e5b4ab20eb79d42b4],SizeBytes:434254012,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:26686a92917f2c24599df67391f6774cbf6637ccff23aa47a9ed0761d951291e],SizeBytes:427979869,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:856a2097e01dbaad614b0f57570a5616d99e5348ea6f35ad4cea2fdd6d18126b],SizeBytes:415053325,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:252813800adf8140c408e6aa0956ccac8f9a3ddd4cf743f140fcaef61b23762f],SizeBytes:414242392,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e5a9884f8bb84ee5256d0e0701c1f45d2a4e39b3a12d5befbbf2c83daf4d174],SizeBytes:412745337,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6da5f65dc70f3308d7dc9d3f64e7462458e2778268f291c3b55601b5f6f13e9d],SizeBytes:412482144,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:09d0eae43aa689add7aee2704b547a10c1423f4d065b777ac40260578ebdfcdd],SizeBytes:412124822,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f3ab7facf3d8b744025ac716a731ca8ec3c3fb6aaf7f4030fffe4b27fb9b3167],SizeBytes:411670500,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:06c9b59044c11816be5e21b6a98b941ff5b4dbac525b552f260eaf29b58d506c],SizeBytes:411066934,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1e17af5c01fa12b7703ff1bf4f568c3c9871a595d93712cb0bc0644de261c8b2],SizeBytes:410568245,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8307057acd3af8dadf87323bfe9e2d4d9e74a5910cfe17373f9b30a96c7a796f],SizeBytes:409308238,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e591be349fc890140870a9203b7ad17732ba3793986184696dd6573c02f7512a],SizeBytes:408835766,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d35bbe6e090f32f26efabbe515cf115709859021559568593e0cf683c448f9a2],SizeBytes:408565287,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fafa9771e41b970fb0ebf8cdbfe159b8dd8b48bfbc1acd225bb3d72838be3d07],SizeBytes:408260136,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:676243be57bc8a7bb88ced454753032c2c0709a3d04c4378c625576e2956c274],SizeBytes:407648939,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:064c7c0257d1de0cad1d8e883b505853bed53fd835686d4105eca3c4b734e0b5],SizeBytes:407165482,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:506acacd57b6cf51722a41278c0278a28df68ac86d131c59ff8b687d6254aca3],SizeBytes:404880817,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2912cae3175adb7b70e41632ac83ede3d4af241a937d84bfce4cd446697616c1],SizeBytes:403284531,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9abc51cf4faf2d8f2a85c70ca6e48e28ff04d94714dab5f332a51284dd18868b],SizeBytes:401406468,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-release@sha256:300bce8246cf880e792e106607925de0a404484637627edf5f517375517d54a4],SizeBytes:395682633,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c2a9eff33dd53c4e0dcf9fdd5ef471263f621424652368b819c4e4180b4bc197],SizeBytes:394107296,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:77a20c01792b26383b3365b8cf3cbe4d62bb4044be8372247d82a33bb8db680e],SizeBytes:389399190,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1cb2a173c991cb3521bbed849cc2d72d94f57700f68726516d10b674eeb89fef],SizeBytes:387320849,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:56d1f8f14d2c423be54b468489b901f0986334daa52d933f6d8c393f0f3294c1],SizeBytes:385729777,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e538dd11b15508ec363b8855e5496b51f65365ecfa62419899d3cdb21252f44],SizeBytes:385715621,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jan 19 00:01:50.205: INFO: 
Logging kubelet events for node ip-10-0-164-47.ec2.internal
Jan 19 00:01:50.209: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-164-47.ec2.internal
Jan 19 00:01:50.250: INFO: node-exporter-jpjhd started at 2023-01-18 21:08:32 +0000 UTC (1+2 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 19 00:01:50.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.250: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 00:01:50.250: INFO: cluster-monitoring-operator-54dd78cc74-5kf77 started at 2023-01-18 21:31:01 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jan 19 00:01:50.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.250: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-hgd9k started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 00:01:50.250: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 00:01:50.250: INFO: installer-10-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:31:55 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container installer ready: false, restart count 0
Jan 19 00:01:50.250: INFO: kube-apiserver-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:11:38 +0000 UTC (1+5 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Init container setup ready: true, restart count 0
Jan 19 00:01:50.250: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 19 00:01:50.250: INFO: 	Container kube-apiserver-cert-regeneration-controller ready: true, restart count 0
Jan 19 00:01:50.250: INFO: 	Container kube-apiserver-cert-syncer ready: true, restart count 0
Jan 19 00:01:50.250: INFO: 	Container kube-apiserver-check-endpoints ready: true, restart count 0
Jan 19 00:01:50.250: INFO: 	Container kube-apiserver-insecure-readyz ready: true, restart count 0
Jan 19 00:01:50.250: INFO: multus-qhptl started at 2023-01-18 20:55:14 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 00:01:50.250: INFO: multus-admission-controller-jdrvf started at 2023-01-18 20:57:06 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.250: INFO: 	Container multus-admission-controller ready: true, restart count 1
Jan 19 00:01:50.250: INFO: validation-webhook-ptpcg started at 2023-01-18 21:18:05 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container webhooks ready: true, restart count 1
Jan 19 00:01:50.250: INFO: installer-6-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:26:42 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container installer ready: false, restart count 0
Jan 19 00:01:50.250: INFO: apiserver-66fc6ccdbf-mdrrn started at 2023-01-18 21:30:42 +0000 UTC (1+2 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Jan 19 00:01:50.250: INFO: 	Container openshift-apiserver ready: true, restart count 0
Jan 19 00:01:50.250: INFO: 	Container openshift-apiserver-check-endpoints ready: true, restart count 0
Jan 19 00:01:50.250: INFO: catalog-operator-d8bc95486-bckt7 started at 2023-01-18 21:31:05 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container catalog-operator ready: true, restart count 1
Jan 19 00:01:50.250: INFO: insights-operator-ddf4f58bc-6tvtt started at 2023-01-18 21:31:07 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container insights-operator ready: true, restart count 0
Jan 19 00:01:50.250: INFO: ovnkube-master-87l2b started at 2023-01-18 20:55:24 +0000 UTC (0+6 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.250: INFO: 	Container nbdb ready: true, restart count 1
Jan 19 00:01:50.250: INFO: 	Container northd ready: true, restart count 1
Jan 19 00:01:50.250: INFO: 	Container ovn-dbchecker ready: true, restart count 1
Jan 19 00:01:50.250: INFO: 	Container ovnkube-master ready: true, restart count 2
Jan 19 00:01:50.250: INFO: 	Container sbdb ready: true, restart count 1
Jan 19 00:01:50.250: INFO: sre-dns-latency-exporter-skzfn started at 2023-01-18 21:18:07 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container main ready: true, restart count 1
Jan 19 00:01:50.250: INFO: audit-exporter-swhwq started at 2023-01-18 21:19:38 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container audit-exporter ready: true, restart count 1
Jan 19 00:01:50.250: INFO: dns-operator-69db46cc47-prtl4 started at 2023-01-18 21:30:58 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container dns-operator ready: true, restart count 0
Jan 19 00:01:50.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.250: INFO: openshift-controller-manager-operator-598f9758bb-vjtl2 started at 2023-01-18 21:30:59 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container openshift-controller-manager-operator ready: true, restart count 0
Jan 19 00:01:50.250: INFO: etcd-guard-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:30:45 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container guard ready: true, restart count 0
Jan 19 00:01:50.250: INFO: cluster-storage-operator-58f544dddc-8w72f started at 2023-01-18 21:30:57 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Jan 19 00:01:50.250: INFO: installer-7-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:40:31 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container installer ready: false, restart count 0
Jan 19 00:01:50.250: INFO: csi-snapshot-controller-64fc8cb68f-5xx4c started at 2023-01-18 21:30:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 19 00:01:50.250: INFO: kube-apiserver-operator-87c785995-rdmfm started at 2023-01-18 21:31:03 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container kube-apiserver-operator ready: true, restart count 0
Jan 19 00:01:50.250: INFO: authentication-operator-7449fd4467-x9fzx started at 2023-01-18 21:30:57 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container authentication-operator ready: true, restart count 0
Jan 19 00:01:50.250: INFO: cloud-network-config-controller-c6495b94b-49k5d started at 2023-01-18 21:30:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container controller ready: true, restart count 0
Jan 19 00:01:50.250: INFO: cloud-credential-operator-5ff5b4df7c-67ctf started at 2023-01-18 21:30:59 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container cloud-credential-operator ready: true, restart count 0
Jan 19 00:01:50.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.250: INFO: olm-operator-84c588b5d4-d2bgw started at 2023-01-18 21:31:06 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container olm-operator ready: true, restart count 0
Jan 19 00:01:50.250: INFO: dns-default-dqbtt started at 2023-01-18 20:58:10 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container dns ready: true, restart count 1
Jan 19 00:01:50.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.250: INFO: splunkforwarder-ds-c6bl8 started at 2023-01-18 21:24:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 00:01:50.250: INFO: openshift-config-operator-6c4d478fcc-v4lzl started at 2023-01-18 21:30:57 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container openshift-config-operator ready: true, restart count 0
Jan 19 00:01:50.250: INFO: cluster-image-registry-operator-8654985b9-tlbrt started at 2023-01-18 21:31:02 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jan 19 00:01:50.250: INFO: kube-controller-manager-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:01:39 +0000 UTC (0+4 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container cluster-policy-controller ready: true, restart count 0
Jan 19 00:01:50.250: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan 19 00:01:50.250: INFO: 	Container kube-controller-manager-cert-syncer ready: true, restart count 0
Jan 19 00:01:50.250: INFO: 	Container kube-controller-manager-recovery-controller ready: true, restart count 0
Jan 19 00:01:50.250: INFO: controller-manager-wsvkm started at 2023-01-18 21:26:39 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container controller-manager ready: true, restart count 1
Jan 19 00:01:50.250: INFO: revision-pruner-9-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:26:56 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container pruner ready: false, restart count 0
Jan 19 00:01:50.250: INFO: openshift-apiserver-operator-749c855c4b-p78ll started at 2023-01-18 21:30:57 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container openshift-apiserver-operator ready: true, restart count 0
Jan 19 00:01:50.250: INFO: csi-snapshot-webhook-f7487cb9f-26nwq started at 2023-01-18 21:30:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container webhook ready: true, restart count 0
Jan 19 00:01:50.250: INFO: ingress-operator-8646cf484d-wfqwx started at 2023-01-18 21:31:02 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container ingress-operator ready: true, restart count 0
Jan 19 00:01:50.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.250: INFO: console-67d8cf9f45-dxkwr started at 2023-01-18 21:30:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container console ready: true, restart count 0
Jan 19 00:01:50.250: INFO: revision-pruner-7-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:40:22 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container pruner ready: false, restart count 0
Jan 19 00:01:50.250: INFO: network-metrics-daemon-vcckr started at 2023-01-18 20:55:15 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.250: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 00:01:50.250: INFO: aws-ebs-csi-driver-node-zfr78 started at 2023-01-18 20:57:36 +0000 UTC (0+3 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 00:01:50.250: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 00:01:50.250: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 00:01:50.250: INFO: tuned-hp4hs started at 2023-01-18 20:58:22 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.250: INFO: 	Container tuned ready: true, restart count 1
Jan 19 00:01:50.251: INFO: installer-6-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:30:28 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container installer ready: false, restart count 0
Jan 19 00:01:50.251: INFO: kube-apiserver-guard-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:30:50 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container guard ready: true, restart count 0
Jan 19 00:01:50.251: INFO: ovnkube-node-9z5m7 started at 2023-01-18 20:55:24 +0000 UTC (0+5 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 00:01:50.251: INFO: machine-config-server-52vnt started at 2023-01-18 20:57:49 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container machine-config-server ready: true, restart count 1
Jan 19 00:01:50.251: INFO: etcd-operator-5d7bb49b6f-mr69f started at 2023-01-18 21:30:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container etcd-operator ready: true, restart count 0
Jan 19 00:01:50.251: INFO: packageserver-749dbf8d56-f68km started at 2023-01-18 21:31:06 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container packageserver ready: true, restart count 0
Jan 19 00:01:50.251: INFO: machine-config-daemon-9c9lc started at 2023-01-18 20:57:06 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 00:01:50.251: INFO: node-ca-jb8rs started at 2023-01-18 21:02:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 00:01:50.251: INFO: apiserver-58dcc4c5df-xjj54 started at 2023-01-18 21:30:42 +0000 UTC (1+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container oauth-apiserver ready: true, restart count 0
Jan 19 00:01:50.251: INFO: kube-controller-manager-guard-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:30:49 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container guard ready: true, restart count 0
Jan 19 00:01:50.251: INFO: machine-approver-5c7fdbf98f-p8nf9 started at 2023-01-18 21:30:58 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container machine-approver-controller ready: true, restart count 0
Jan 19 00:01:50.251: INFO: installer-7-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:39:16 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container installer ready: false, restart count 0
Jan 19 00:01:50.251: INFO: openshift-kube-scheduler-ip-10-0-164-47.ec2.internal started at 2023-01-18 20:59:04 +0000 UTC (1+3 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Init container wait-for-host-port ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container kube-scheduler-cert-syncer ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container kube-scheduler-recovery-controller ready: true, restart count 0
Jan 19 00:01:50.251: INFO: etcd-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:00:48 +0000 UTC (3+5 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Init container setup ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Init container etcd-ensure-env-vars ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Init container etcd-resources-copy ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Container etcd ready: true, restart count 3
Jan 19 00:01:50.251: INFO: 	Container etcd-health-monitor ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Container etcd-metrics ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Container etcd-readyz ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Container etcdctl ready: true, restart count 1
Jan 19 00:01:50.251: INFO: installer-11-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:34:21 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container installer ready: false, restart count 0
Jan 19 00:01:50.251: INFO: revision-pruner-10-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:30:28 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container pruner ready: false, restart count 0
Jan 19 00:01:50.251: INFO: aws-ebs-csi-driver-controller-6f56cff656-hvllp started at 2023-01-18 21:30:58 +0000 UTC (0+11 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container attacher-kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container csi-driver ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container driver-kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container provisioner-kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container resizer-kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container snapshotter-kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.251: INFO: kube-controller-manager-operator-7dddcdbdcf-z7kth started at 2023-01-18 21:31:07 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container kube-controller-manager-operator ready: true, restart count 0
Jan 19 00:01:50.251: INFO: revision-pruner-9-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:26:24 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container pruner ready: false, restart count 0
Jan 19 00:01:50.251: INFO: network-check-target-7sxbt started at 2023-01-18 20:55:26 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 00:01:50.251: INFO: oauth-openshift-7cd8db5c84-tnrnn started at 2023-01-18 21:30:42 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container oauth-openshift ready: true, restart count 0
Jan 19 00:01:50.251: INFO: openshift-kube-scheduler-guard-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:30:44 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container guard ready: true, restart count 0
Jan 19 00:01:50.251: INFO: cluster-baremetal-operator-5444b4778c-h7zf6 started at 2023-01-18 21:31:04 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container baremetal-kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.251: INFO: 	Container cluster-baremetal-operator ready: true, restart count 0
Jan 19 00:01:50.251: INFO: revision-pruner-11-ip-10-0-164-47.ec2.internal started at 2023-01-18 21:34:12 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container pruner ready: false, restart count 0
Jan 19 00:01:50.251: INFO: openshift-kube-scheduler-operator-f5fdbcc7f-mh7l5 started at 2023-01-18 21:31:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container kube-scheduler-operator-container ready: true, restart count 0
Jan 19 00:01:50.251: INFO: multus-additional-cni-plugins-mdbtj started at 2023-01-18 20:55:14 +0000 UTC (6+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 19 00:01:50.251: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 00:01:50.251: INFO: node-resolver-wmqhb started at 2023-01-18 20:58:11 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 00:01:50.251: INFO: cluster-node-tuning-operator-759d66c58c-84fll started at 2023-01-18 21:30:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jan 19 00:01:50.251: INFO: marketplace-operator-587887dd98-cbnd7 started at 2023-01-18 21:31:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container marketplace-operator ready: true, restart count 0
Jan 19 00:01:50.251: INFO: kube-storage-version-migrator-operator-6868b77b99-jdp6z started at 2023-01-18 21:31:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.251: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 0
W0119 00:01:50.253232      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 19 00:01:50.281: INFO: 
Latency metrics for node ip-10-0-164-47.ec2.internal
Jan 19 00:01:50.281: INFO: 
Logging node info for node ip-10-0-176-161.ec2.internal
Jan 19 00:01:50.283: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-176-161.ec2.internal    b5315323-0061-47b2-b15b-c33b1d1b7b4d 308012 0 2023-01-18 20:55:08 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.2xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-176-161.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io/master: node.kubernetes.io/instance-type:m5.2xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-014476fb4887fbb3c","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0fa2697d329ce1dd7"} k8s.ovn.org/host-addresses:["10.0.176.161"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-176-161.ec2.internal","mac-address":"02:4b:a7:84:a8:1b","ip-addresses":["10.0.176.161/17"],"ip-address":"10.0.176.161/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:da8863b7-8575-4776-881c-d426297d05fd k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.4/16"} k8s.ovn.org/node-mgmt-port-mac-address:82:f3:04:f6:5f:98 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.176.161/17"} k8s.ovn.org/node-subnets:{"default":"10.128.4.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-master-0 machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/desiredConfig:rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubelet Update v1 2023-01-18 20:55:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/master":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {ip-10-0-164-47 Update v1 2023-01-18 20:56:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {ip-10-0-176-161 Update v1 2023-01-18 20:56:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {machine-config-daemon Update v1 2023-01-18 20:58:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {cloud-network-config-controller Update v1 2023-01-18 20:58:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{}}}} } {nodelink-controller Update v1 2023-01-18 20:58:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}} } {kube-controller-manager Update v1 2023-01-18 21:30:55 +0000 UTC FieldsV1 {"f:spec":{"f:taints":{}}} } {machine-config-controller Update v1 2023-01-18 21:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {kubelet Update v1 2023-01-18 21:36:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-0fa2697d329ce1dd7,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{8 0} {<nil>} 8 DecimalSI},ephemeral-storage: {{375261212672 0} {<nil>} 366466028Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{33221505024 0} {<nil>} 32442876Ki BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{7910 -3} {<nil>} 7910m DecimalSI},ephemeral-storage: {{337735090846 0} {<nil>} 337735090846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{29422975549440 -3} {<nil>} 29422975549440m DecimalSI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-18 23:59:42 +0000 UTC,LastTransitionTime:2023-01-18 21:36:48 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-18 23:59:42 +0000 UTC,LastTransitionTime:2023-01-18 21:36:48 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-18 23:59:42 +0000 UTC,LastTransitionTime:2023-01-18 21:36:48 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-18 23:59:42 +0000 UTC,LastTransitionTime:2023-01-18 21:36:48 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.176.161,},NodeAddress{Type:Hostname,Address:ip-10-0-176-161.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-176-161.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec28a0ad2d0b01eb828efbed3def1995,SystemUUID:ec28a0ad-2d0b-01eb-828e-fbed3def1995,BootID:c3c8cea8-86ac-4752-8b78-3b121525af7d,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c32772c6199a054c863126285a8b30cdadd6b1296cff1e4f22d6d3e3f9c84a7c],SizeBytes:823606942,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08ab62da9265ef67f4eb4b526b38346a3ae3d35e2d5bfbdd58a0858b76b21f77],SizeBytes:681959546,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:43612fe6e941c7292ed3ab0933e60703ce9115357a44aa20be25ed07b7cc7959],SizeBytes:612295182,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:515d4b9ff441191257d7b9ff99666737682d3b5e5a1879ba7abdd1e019f1dff2],SizeBytes:480023620,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c305cab192dd40b909dbb56981ad14c8460a6eac9bed9cbeebcd569a5a2eb4f0],SizeBytes:466400791,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:865e66be494fc077d12492a7dc204616fc8a13bf4c906f4081ab3555ce2cc970],SizeBytes:454669803,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fead5da1351abee9e77f5c46247c7947d6f9cf76b5bf7a8b4905222fe2564665],SizeBytes:434647633,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:26686a92917f2c24599df67391f6774cbf6637ccff23aa47a9ed0761d951291e],SizeBytes:427979869,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:856a2097e01dbaad614b0f57570a5616d99e5348ea6f35ad4cea2fdd6d18126b],SizeBytes:415053325,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:252813800adf8140c408e6aa0956ccac8f9a3ddd4cf743f140fcaef61b23762f],SizeBytes:414242392,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e5a9884f8bb84ee5256d0e0701c1f45d2a4e39b3a12d5befbbf2c83daf4d174],SizeBytes:412745337,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6da5f65dc70f3308d7dc9d3f64e7462458e2778268f291c3b55601b5f6f13e9d],SizeBytes:412482144,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:09d0eae43aa689add7aee2704b547a10c1423f4d065b777ac40260578ebdfcdd],SizeBytes:412124822,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f3ab7facf3d8b744025ac716a731ca8ec3c3fb6aaf7f4030fffe4b27fb9b3167],SizeBytes:411670500,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1e17af5c01fa12b7703ff1bf4f568c3c9871a595d93712cb0bc0644de261c8b2],SizeBytes:410568245,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8307057acd3af8dadf87323bfe9e2d4d9e74a5910cfe17373f9b30a96c7a796f],SizeBytes:409308238,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e591be349fc890140870a9203b7ad17732ba3793986184696dd6573c02f7512a],SizeBytes:408835766,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3634a2d516f4455d78a54952446bcf7cdebb913c5fcf736de59e48d067479b1e],SizeBytes:408726214,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e9013f41f06bd4c609f94b4ff8d4087961e0af67e4317f2cb03f45fec08892f6],SizeBytes:408639000,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d35bbe6e090f32f26efabbe515cf115709859021559568593e0cf683c448f9a2],SizeBytes:408565287,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:676243be57bc8a7bb88ced454753032c2c0709a3d04c4378c625576e2956c274],SizeBytes:407648939,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:064c7c0257d1de0cad1d8e883b505853bed53fd835686d4105eca3c4b734e0b5],SizeBytes:407165482,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:506acacd57b6cf51722a41278c0278a28df68ac86d131c59ff8b687d6254aca3],SizeBytes:404880817,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2912cae3175adb7b70e41632ac83ede3d4af241a937d84bfce4cd446697616c1],SizeBytes:403284531,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9abc51cf4faf2d8f2a85c70ca6e48e28ff04d94714dab5f332a51284dd18868b],SizeBytes:401406468,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c2a9eff33dd53c4e0dcf9fdd5ef471263f621424652368b819c4e4180b4bc197],SizeBytes:394107296,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:77a20c01792b26383b3365b8cf3cbe4d62bb4044be8372247d82a33bb8db680e],SizeBytes:389399190,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1cb2a173c991cb3521bbed849cc2d72d94f57700f68726516d10b674eeb89fef],SizeBytes:387320849,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:56d1f8f14d2c423be54b468489b901f0986334daa52d933f6d8c393f0f3294c1],SizeBytes:385729777,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e538dd11b15508ec363b8855e5496b51f65365ecfa62419899d3cdb21252f44],SizeBytes:385715621,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e463f54f6a78c0ff0f995dc9b6557fd3af350cd20f7116f56160cdfa8ddc6dc],SizeBytes:382829546,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:90616b372816c6b9e06c670b613dd6292b0b078d78c835f1c5da8374f908f39a],SizeBytes:382625292,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8aae812f23337d5143f2be66cc79c512d422e5e3b3b11307a81db299da31764c],SizeBytes:382218627,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cca1847c9a763bfa7e79fcbcc4d7e82918461e7ade26ec61b681937e9c92ff23],SizeBytes:380734570,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0cc5bd656b0355619fdac6c8920a2388edabbe14cdc7ab93069bacdc1e2db1d1],SizeBytes:380104717,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f7e1af0b22f2e5943636d18fba82b080cf70828a5e62866e4891a97f8f7113b5],SizeBytes:377151511,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b5be929f4ccae9b9f3b1c5f19281c9a24f1adb54bf9c358691af988556dce27],SizeBytes:376385161,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jan 19 00:01:50.284: INFO: 
Logging kubelet events for node ip-10-0-176-161.ec2.internal
Jan 19 00:01:50.287: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-176-161.ec2.internal
Jan 19 00:01:50.314: INFO: apiserver-66fc6ccdbf-n4lfr started at 2023-01-18 21:37:10 +0000 UTC (1+2 container statuses recorded)
Jan 19 00:01:50.314: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Jan 19 00:01:50.314: INFO: 	Container openshift-apiserver ready: true, restart count 0
Jan 19 00:01:50.314: INFO: 	Container openshift-apiserver-check-endpoints ready: true, restart count 0
Jan 19 00:01:50.314: INFO: kube-apiserver-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:13:11 +0000 UTC (1+5 container statuses recorded)
Jan 19 00:01:50.314: INFO: 	Init container setup ready: true, restart count 0
Jan 19 00:01:50.314: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 19 00:01:50.314: INFO: 	Container kube-apiserver-cert-regeneration-controller ready: true, restart count 0
Jan 19 00:01:50.314: INFO: 	Container kube-apiserver-cert-syncer ready: true, restart count 0
Jan 19 00:01:50.314: INFO: 	Container kube-apiserver-check-endpoints ready: true, restart count 0
Jan 19 00:01:50.314: INFO: 	Container kube-apiserver-insecure-readyz ready: true, restart count 0
Jan 19 00:01:50.314: INFO: revision-pruner-11-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:36:48 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.314: INFO: 	Container pruner ready: false, restart count 0
Jan 19 00:01:50.314: INFO: revision-pruner-9-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:36:48 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.314: INFO: 	Container pruner ready: false, restart count 0
Jan 19 00:01:50.314: INFO: openshift-kube-scheduler-guard-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:37:11 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.314: INFO: 	Container guard ready: true, restart count 0
Jan 19 00:01:50.314: INFO: kube-controller-manager-guard-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:37:11 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.314: INFO: 	Container guard ready: true, restart count 0
Jan 19 00:01:50.314: INFO: etcd-guard-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:37:11 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.314: INFO: 	Container guard ready: true, restart count 0
Jan 19 00:01:50.314: INFO: machine-config-daemon-kjrpw started at 2023-01-18 20:57:06 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.314: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 00:01:50.314: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 00:01:50.314: INFO: apiserver-58dcc4c5df-jftb5 started at 2023-01-18 21:37:10 +0000 UTC (1+1 container statuses recorded)
Jan 19 00:01:50.314: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Jan 19 00:01:50.314: INFO: 	Container oauth-apiserver ready: true, restart count 0
Jan 19 00:01:50.314: INFO: installer-11-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:38:15 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.314: INFO: 	Container installer ready: false, restart count 0
Jan 19 00:01:50.314: INFO: validation-webhook-fnxnh started at 2023-01-18 21:18:05 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.314: INFO: 	Container webhooks ready: true, restart count 1
Jan 19 00:01:50.314: INFO: multus-ljzjp started at 2023-01-18 20:55:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.314: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 00:01:50.314: INFO: ovnkube-node-6f4f4 started at 2023-01-18 20:55:24 +0000 UTC (0+5 container statuses recorded)
Jan 19 00:01:50.314: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 00:01:50.315: INFO: sre-dns-latency-exporter-pqxpd started at 2023-01-18 21:18:07 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container main ready: true, restart count 1
Jan 19 00:01:50.315: INFO: controller-manager-6chhz started at 2023-01-18 21:26:39 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container controller-manager ready: true, restart count 1
Jan 19 00:01:50.315: INFO: kube-apiserver-guard-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:37:13 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container guard ready: true, restart count 0
Jan 19 00:01:50.315: INFO: installer-7-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:36:48 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container installer ready: false, restart count 0
Jan 19 00:01:50.315: INFO: oauth-openshift-7cd8db5c84-n8xww started at 2023-01-18 21:37:10 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container oauth-openshift ready: true, restart count 0
Jan 19 00:01:50.315: INFO: etcd-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:02:31 +0000 UTC (3+5 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Init container setup ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Init container etcd-ensure-env-vars ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Init container etcd-resources-copy ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container etcd ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container etcd-health-monitor ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container etcd-metrics ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container etcd-readyz ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container etcdctl ready: true, restart count 1
Jan 19 00:01:50.315: INFO: network-check-target-6lqkp started at 2023-01-18 20:55:26 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 00:01:50.315: INFO: multus-admission-controller-bcql5 started at 2023-01-18 20:56:59 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container multus-admission-controller ready: true, restart count 1
Jan 19 00:01:50.315: INFO: dns-default-kmk24 started at 2023-01-18 20:58:10 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container dns ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.315: INFO: splunkforwarder-ds-rb44w started at 2023-01-18 21:24:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 00:01:50.315: INFO: machine-config-server-x47zb started at 2023-01-18 20:57:49 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container machine-config-server ready: true, restart count 1
Jan 19 00:01:50.315: INFO: node-resolver-2hwdq started at 2023-01-18 20:58:11 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 00:01:50.315: INFO: node-ca-w59x4 started at 2023-01-18 21:02:57 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 00:01:50.315: INFO: openshift-kube-scheduler-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:00:43 +0000 UTC (1+3 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Init container wait-for-host-port ready: true, restart count 0
Jan 19 00:01:50.315: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan 19 00:01:50.315: INFO: 	Container kube-scheduler-cert-syncer ready: true, restart count 0
Jan 19 00:01:50.315: INFO: 	Container kube-scheduler-recovery-controller ready: true, restart count 0
Jan 19 00:01:50.315: INFO: revision-pruner-7-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:40:25 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container pruner ready: false, restart count 0
Jan 19 00:01:50.315: INFO: multus-additional-cni-plugins-qrxcd started at 2023-01-18 20:55:18 +0000 UTC (6+1 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 00:01:50.315: INFO: ovnkube-master-lcjzg started at 2023-01-18 20:55:24 +0000 UTC (0+6 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container nbdb ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container northd ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container ovn-dbchecker ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container ovnkube-master ready: true, restart count 4
Jan 19 00:01:50.315: INFO: 	Container sbdb ready: true, restart count 1
Jan 19 00:01:50.315: INFO: aws-ebs-csi-driver-node-8n9wm started at 2023-01-18 20:57:42 +0000 UTC (0+3 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 00:01:50.315: INFO: tuned-r9x6w started at 2023-01-18 20:58:22 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container tuned ready: true, restart count 1
Jan 19 00:01:50.315: INFO: node-exporter-xxq2s started at 2023-01-18 21:08:32 +0000 UTC (1+2 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 00:01:50.315: INFO: audit-exporter-cn4nn started at 2023-01-18 21:19:38 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container audit-exporter ready: true, restart count 1
Jan 19 00:01:50.315: INFO: installer-7-ip-10-0-176-161.ec2.internal started at 2023-01-18 21:36:48 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container installer ready: false, restart count 0
Jan 19 00:01:50.315: INFO: kube-controller-manager-ip-10-0-176-161.ec2.internal started at 2023-01-18 20:59:27 +0000 UTC (0+4 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container cluster-policy-controller ready: true, restart count 0
Jan 19 00:01:50.315: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan 19 00:01:50.315: INFO: 	Container kube-controller-manager-cert-syncer ready: true, restart count 0
Jan 19 00:01:50.315: INFO: 	Container kube-controller-manager-recovery-controller ready: true, restart count 0
Jan 19 00:01:50.315: INFO: network-metrics-daemon-75l4s started at 2023-01-18 20:55:18 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.315: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 00:01:50.315: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-6psbg started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.315: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 00:01:50.315: INFO: 	Container systemd-logs ready: true, restart count 0
W0119 00:01:50.317459      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 19 00:01:50.340: INFO: 
Latency metrics for node ip-10-0-176-161.ec2.internal
Jan 19 00:01:50.340: INFO: 
Logging node info for node ip-10-0-176-170.ec2.internal
Jan 19 00:01:50.342: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-176-170.ec2.internal    a7fcbe04-baee-4285-bb14-33b17935a43d 307962 0 2023-01-18 20:50:23 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.2xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-176-170.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io/master: node.kubernetes.io/instance-type:m5.2xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-01989b7189c1e0adb","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0020e87e2124183e6"} k8s.ovn.org/host-addresses:["10.0.176.170"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-176-170.ec2.internal","mac-address":"02:cb:07:d5:72:d5","ip-addresses":["10.0.176.170/17"],"ip-address":"10.0.176.170/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:0ce8e931-99cf-445b-b666-7a3e580e351d k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.2/16"} k8s.ovn.org/node-mgmt-port-mac-address:a6:4c:04:da:60:d8 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.176.170/17"} k8s.ovn.org/node-subnets:{"default":"10.128.0.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-master-2 machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/desiredConfig:rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-master-13ae5a95ba10d24dc4fab52083044f9d machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubelet Update v1 2023-01-18 20:50:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/master":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {ip-10-0-164-47 Update v1 2023-01-18 20:56:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {ip-10-0-176-170 Update v1 2023-01-18 20:56:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {cloud-network-config-controller Update v1 2023-01-18 20:58:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{}}}} } {machine-config-daemon Update v1 2023-01-18 20:58:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {nodelink-controller Update v1 2023-01-18 20:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}} } {kube-controller-manager Update v1 2023-01-18 21:18:19 +0000 UTC FieldsV1 {"f:spec":{"f:taints":{}}} } {machine-config-controller Update v1 2023-01-18 21:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {kubelet Update v1 2023-01-18 21:23:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-0020e87e2124183e6,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{8 0} {<nil>} 8 DecimalSI},ephemeral-storage: {{375261212672 0} {<nil>} 366466028Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{33221505024 0} {<nil>} 32442876Ki BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{7910 -3} {<nil>} 7910m DecimalSI},ephemeral-storage: {{337735090846 0} {<nil>} 337735090846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{29422975549440 -3} {<nil>} 29422975549440m DecimalSI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-18 23:59:37 +0000 UTC,LastTransitionTime:2023-01-18 20:50:23 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-18 23:59:37 +0000 UTC,LastTransitionTime:2023-01-18 20:50:23 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-18 23:59:37 +0000 UTC,LastTransitionTime:2023-01-18 20:50:23 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-18 23:59:37 +0000 UTC,LastTransitionTime:2023-01-18 20:57:01 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.176.170,},NodeAddress{Type:Hostname,Address:ip-10-0-176-170.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-176-170.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec28eb65055ec31071286c18c22a5e4c,SystemUUID:ec28eb65-055e-c310-7128-6c18c22a5e4c,BootID:46a64a14-b2eb-454a-8d74-74eda176c814,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[registry.redhat.io/redhat/redhat-operator-index@sha256:19e8a9a4d95111fd4db559838dd26ffa823b26dcac8004cd553a2ce39388b31f registry.redhat.io/redhat/redhat-operator-index@sha256:efdc74d42cfb372d6df37bb383af9585db327f2b47366818118addbe2e5825c1 registry.redhat.io/redhat/redhat-operator-index:v4.11],SizeBytes:1393216950,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[registry.redhat.io/redhat/community-operator-index@sha256:9d33175d7dcd7fd04bb53aea6e96e4bd902b866a09eef7fc7b5982c340929929 registry.redhat.io/redhat/community-operator-index@sha256:b36e3032da5f096768c703d5112a06325083783602ae8186b923def349a15578 registry.redhat.io/redhat/community-operator-index:v4.11],SizeBytes:960075608,},ContainerImage{Names:[registry.redhat.io/redhat/certified-operator-index@sha256:08d51898f4a8b59ec52f51c835f7f46871905001248d2f91a8c50c5c6ae04a43 registry.redhat.io/redhat/certified-operator-index@sha256:e3617454b09c589d685c05e3f16dcc9fb73c02688cf720599f3fe453f718da74 registry.redhat.io/redhat/certified-operator-index:v4.11],SizeBytes:932364392,},ContainerImage{Names:[registry.redhat.io/redhat/redhat-marketplace-index@sha256:8bba3a2e3edd687602c0702153f5a2c2f5055c4fb51093152fca24252fca87c4 registry.redhat.io/redhat/redhat-marketplace-index@sha256:ab8a83a2a96ad8efdd2b923b615867953cf746d779320facab9d533e0ea00c9a registry.redhat.io/redhat/redhat-marketplace-index:v4.11],SizeBytes:826959445,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c32772c6199a054c863126285a8b30cdadd6b1296cff1e4f22d6d3e3f9c84a7c],SizeBytes:823606942,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08ab62da9265ef67f4eb4b526b38346a3ae3d35e2d5bfbdd58a0858b76b21f77],SizeBytes:681959546,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:43612fe6e941c7292ed3ab0933e60703ce9115357a44aa20be25ed07b7cc7959],SizeBytes:612295182,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c305cab192dd40b909dbb56981ad14c8460a6eac9bed9cbeebcd569a5a2eb4f0],SizeBytes:466400791,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:865e66be494fc077d12492a7dc204616fc8a13bf4c906f4081ab3555ce2cc970],SizeBytes:454669803,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72befefa8585db143140a682046ee43f4a62d6b0b5fc16a20b48186dc2b1dcb3],SizeBytes:450775130,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fead5da1351abee9e77f5c46247c7947d6f9cf76b5bf7a8b4905222fe2564665],SizeBytes:434647633,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cdf42132f29ddf587b7da228d83d184b152d81ed7cdff83e5b4ab20eb79d42b4],SizeBytes:434254012,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:856a2097e01dbaad614b0f57570a5616d99e5348ea6f35ad4cea2fdd6d18126b],SizeBytes:415053325,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:252813800adf8140c408e6aa0956ccac8f9a3ddd4cf743f140fcaef61b23762f],SizeBytes:414242392,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:09d0eae43aa689add7aee2704b547a10c1423f4d065b777ac40260578ebdfcdd],SizeBytes:412124822,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f3ab7facf3d8b744025ac716a731ca8ec3c3fb6aaf7f4030fffe4b27fb9b3167],SizeBytes:411670500,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:06c9b59044c11816be5e21b6a98b941ff5b4dbac525b552f260eaf29b58d506c],SizeBytes:411066934,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8307057acd3af8dadf87323bfe9e2d4d9e74a5910cfe17373f9b30a96c7a796f],SizeBytes:409308238,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3634a2d516f4455d78a54952446bcf7cdebb913c5fcf736de59e48d067479b1e],SizeBytes:408726214,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e9013f41f06bd4c609f94b4ff8d4087961e0af67e4317f2cb03f45fec08892f6],SizeBytes:408639000,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fafa9771e41b970fb0ebf8cdbfe159b8dd8b48bfbc1acd225bb3d72838be3d07],SizeBytes:408260136,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:064c7c0257d1de0cad1d8e883b505853bed53fd835686d4105eca3c4b734e0b5],SizeBytes:407165482,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:506acacd57b6cf51722a41278c0278a28df68ac86d131c59ff8b687d6254aca3],SizeBytes:404880817,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-release@sha256:300bce8246cf880e792e106607925de0a404484637627edf5f517375517d54a4],SizeBytes:395682633,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c2a9eff33dd53c4e0dcf9fdd5ef471263f621424652368b819c4e4180b4bc197],SizeBytes:394107296,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e538dd11b15508ec363b8855e5496b51f65365ecfa62419899d3cdb21252f44],SizeBytes:385715621,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e463f54f6a78c0ff0f995dc9b6557fd3af350cd20f7116f56160cdfa8ddc6dc],SizeBytes:382829546,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:90616b372816c6b9e06c670b613dd6292b0b078d78c835f1c5da8374f908f39a],SizeBytes:382625292,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8aae812f23337d5143f2be66cc79c512d422e5e3b3b11307a81db299da31764c],SizeBytes:382218627,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0cc5bd656b0355619fdac6c8920a2388edabbe14cdc7ab93069bacdc1e2db1d1],SizeBytes:380104717,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5813d3728229c2a09a05bc454753e0128ac2bbd203e7000a4d954daab12fbb79],SizeBytes:377171632,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b5be929f4ccae9b9f3b1c5f19281c9a24f1adb54bf9c358691af988556dce27],SizeBytes:376385161,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e403f3e14cbbf2e91ce0d5ed7ad90d88f95e617a446e907b973c8c7f4916cbe1],SizeBytes:372710918,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:abcba83a0a4224a97f080480404808da1e5f42de24b6ded0062aad7a598dfc87],SizeBytes:371310125,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5e1c69d005727e3245604cfca7a63e4f9bc6e15128c7489e41d5e967305089e],SizeBytes:371248553,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:212f5bba67dd176f7843b808bc04d7680709f5c8a177fe39a204412aeacf3c5a],SizeBytes:368625226,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f3ebf99182733dff2666e6a5e7e217085e06029362036ad79c91278e69dcbf7],SizeBytes:366270626,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f5228e2d779565e500aed3a80e7f8f8fafc45a4af65acbd9e47a7e21239ba709],SizeBytes:364674190,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jan 19 00:01:50.342: INFO: 
Logging kubelet events for node ip-10-0-176-170.ec2.internal
Jan 19 00:01:50.345: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-176-170.ec2.internal
Jan 19 00:01:50.392: INFO: cluster-samples-operator-665bf757cb-jntc9 started at 2023-01-18 21:24:12 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.392: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jan 19 00:01:50.392: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jan 19 00:01:50.392: INFO: packageserver-749dbf8d56-cntts started at 2023-01-18 21:24:13 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.392: INFO: 	Container packageserver ready: true, restart count 0
Jan 19 00:01:50.392: INFO: installer-6-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:32:47 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.392: INFO: 	Container installer ready: false, restart count 0
Jan 19 00:01:50.392: INFO: tuned-tdrgh started at 2023-01-18 20:58:22 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.392: INFO: 	Container tuned ready: true, restart count 1
Jan 19 00:01:50.392: INFO: node-exporter-xsg8v started at 2023-01-18 21:08:32 +0000 UTC (1+2 container statuses recorded)
Jan 19 00:01:50.392: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 19 00:01:50.392: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.392: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 00:01:50.392: INFO: apiserver-58dcc4c5df-tlmzz started at 2023-01-18 21:24:08 +0000 UTC (1+1 container statuses recorded)
Jan 19 00:01:50.392: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Jan 19 00:01:50.392: INFO: 	Container oauth-apiserver ready: true, restart count 0
Jan 19 00:01:50.392: INFO: openshift-kube-scheduler-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:02:13 +0000 UTC (1+3 container statuses recorded)
Jan 19 00:01:50.392: INFO: 	Init container wait-for-host-port ready: true, restart count 0
Jan 19 00:01:50.392: INFO: 	Container kube-scheduler ready: true, restart count 0
Jan 19 00:01:50.392: INFO: 	Container kube-scheduler-cert-syncer ready: true, restart count 0
Jan 19 00:01:50.392: INFO: 	Container kube-scheduler-recovery-controller ready: true, restart count 0
Jan 19 00:01:50.392: INFO: etcd-ip-10-0-176-170.ec2.internal started at 2023-01-18 20:59:00 +0000 UTC (3+5 container statuses recorded)
Jan 19 00:01:50.392: INFO: 	Init container setup ready: true, restart count 1
Jan 19 00:01:50.392: INFO: 	Init container etcd-ensure-env-vars ready: true, restart count 1
Jan 19 00:01:50.392: INFO: 	Init container etcd-resources-copy ready: true, restart count 1
Jan 19 00:01:50.392: INFO: 	Container etcd ready: true, restart count 1
Jan 19 00:01:50.392: INFO: 	Container etcd-health-monitor ready: true, restart count 1
Jan 19 00:01:50.392: INFO: 	Container etcd-metrics ready: true, restart count 1
Jan 19 00:01:50.392: INFO: 	Container etcd-readyz ready: true, restart count 1
Jan 19 00:01:50.392: INFO: 	Container etcdctl ready: true, restart count 1
Jan 19 00:01:50.392: INFO: dns-default-hz2tr started at 2023-01-18 20:58:10 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.392: INFO: 	Container dns ready: true, restart count 1
Jan 19 00:01:50.392: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.392: INFO: migrator-59f7fcfc8f-7shcf started at 2023-01-18 21:24:13 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.392: INFO: 	Container migrator ready: true, restart count 0
Jan 19 00:01:50.392: INFO: splunkforwarder-ds-bbtz4 started at 2023-01-18 21:24:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.392: INFO: 	Container splunk-uf ready: true, restart count 0
Jan 19 00:01:50.392: INFO: service-ca-d5df49fff-lhrcw started at 2023-01-18 21:30:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.392: INFO: 	Container service-ca-controller ready: false, restart count 0
Jan 19 00:01:50.392: INFO: oauth-openshift-7cd8db5c84-f5qw5 started at 2023-01-18 21:37:42 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container oauth-openshift ready: true, restart count 0
Jan 19 00:01:50.393: INFO: audit-exporter-zgvd7 started at 2023-01-18 21:19:38 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container audit-exporter ready: true, restart count 0
Jan 19 00:01:50.393: INFO: pod-identity-webhook-679b695d64-9v6dr started at 2023-01-18 21:24:14 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container pod-identity-webhook ready: true, restart count 0
Jan 19 00:01:50.393: INFO: community-operators-km2p8 started at 2023-01-18 21:24:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.393: INFO: controller-manager-cd92f started at 2023-01-18 21:26:39 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container controller-manager ready: true, restart count 0
Jan 19 00:01:50.393: INFO: installer-11-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:42:12 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container installer ready: false, restart count 0
Jan 19 00:01:50.393: INFO: csi-snapshot-controller-operator-6c49b58f57-99q68 started at 2023-01-18 21:31:06 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Jan 19 00:01:50.393: INFO: csi-snapshot-controller-64fc8cb68f-zlghz started at 2023-01-18 21:24:17 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 19 00:01:50.393: INFO: aws-ebs-csi-driver-operator-cf879859c-txgsp started at 2023-01-18 21:24:17 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container aws-ebs-csi-driver-operator ready: true, restart count 0
Jan 19 00:01:50.393: INFO: package-server-manager-7bd9967fd7-hftlg started at 2023-01-18 21:31:01 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container package-server-manager ready: true, restart count 0
Jan 19 00:01:50.393: INFO: cluster-autoscaler-operator-6f86dcbd8d-r826n started at 2023-01-18 21:31:03 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container cluster-autoscaler-operator ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.393: INFO: machine-api-operator-84bc76c576-ldbg4 started at 2023-01-18 21:31:05 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container machine-api-operator ready: true, restart count 0
Jan 19 00:01:50.393: INFO: service-ca-operator-688c645b8c-gzbtx started at 2023-01-18 21:31:06 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container service-ca-operator ready: true, restart count 0
Jan 19 00:01:50.393: INFO: installer-7-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:38:09 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container installer ready: false, restart count 0
Jan 19 00:01:50.393: INFO: multus-c6bxn started at 2023-01-18 20:55:14 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 00:01:50.393: INFO: cluster-cloud-controller-manager-operator-5bbd997669-hdb8j started at 2023-01-18 21:24:12 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container cluster-cloud-controller-manager ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container config-sync-controllers ready: true, restart count 0
Jan 19 00:01:50.393: INFO: machine-config-operator-59b6ff77f9-8k25m started at 2023-01-18 21:31:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container machine-config-operator ready: true, restart count 0
Jan 19 00:01:50.393: INFO: console-operator-7498759ddc-jnrvj started at 2023-01-18 21:24:12 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container console-operator ready: true, restart count 0
Jan 19 00:01:50.393: INFO: revision-pruner-9-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:26:21 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container pruner ready: false, restart count 0
Jan 19 00:01:50.393: INFO: kube-controller-manager-guard-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:26:39 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container guard ready: true, restart count 0
Jan 19 00:01:50.393: INFO: network-operator-84894578b9-l8pbp started at 2023-01-18 21:24:15 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container network-operator ready: true, restart count 0
Jan 19 00:01:50.393: INFO: kube-apiserver-guard-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:27:01 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container guard ready: true, restart count 0
Jan 19 00:01:50.393: INFO: revision-pruner-10-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:27:19 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container pruner ready: false, restart count 0
Jan 19 00:01:50.393: INFO: multus-admission-controller-mvgmc started at 2023-01-18 20:57:01 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Container multus-admission-controller ready: true, restart count 1
Jan 19 00:01:50.393: INFO: node-resolver-f7pfq started at 2023-01-18 20:58:11 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 00:01:50.393: INFO: validation-webhook-8pdq4 started at 2023-01-18 21:18:05 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container webhooks ready: true, restart count 1
Jan 19 00:01:50.393: INFO: revision-pruner-7-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:40:28 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container pruner ready: false, restart count 0
Jan 19 00:01:50.393: INFO: network-check-target-58b7z started at 2023-01-18 20:55:26 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 00:01:50.393: INFO: machine-config-daemon-9wdbz started at 2023-01-18 20:57:06 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 00:01:50.393: INFO: installer-10-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:27:48 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container installer ready: false, restart count 0
Jan 19 00:01:50.393: INFO: installer-7-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:38:46 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container installer ready: false, restart count 0
Jan 19 00:01:50.393: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-k8rgl started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 00:01:50.393: INFO: etcd-guard-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:26:27 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container guard ready: true, restart count 0
Jan 19 00:01:50.393: INFO: cluster-version-operator-796d5bc86b-m6s67 started at 2023-01-18 21:24:16 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container cluster-version-operator ready: true, restart count 0
Jan 19 00:01:50.393: INFO: certified-operators-kz76t started at 2023-01-18 21:24:17 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.393: INFO: kube-apiserver-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:00:53 +0000 UTC (1+5 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Init container setup ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container kube-apiserver-cert-regeneration-controller ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container kube-apiserver-cert-syncer ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container kube-apiserver-check-endpoints ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container kube-apiserver-insecure-readyz ready: true, restart count 0
Jan 19 00:01:50.393: INFO: ovnkube-master-ljq6z started at 2023-01-18 20:55:24 +0000 UTC (0+6 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Container nbdb ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Container northd ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Container ovn-dbchecker ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Container ovnkube-master ready: true, restart count 5
Jan 19 00:01:50.393: INFO: 	Container sbdb ready: true, restart count 1
Jan 19 00:01:50.393: INFO: ovnkube-node-w2dnv started at 2023-01-18 20:55:24 +0000 UTC (0+5 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 00:01:50.393: INFO: machine-config-controller-fdb8b6c8d-jvpq4 started at 2023-01-18 20:57:44 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container machine-config-controller ready: true, restart count 2
Jan 19 00:01:50.393: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 00:01:50.393: INFO: aws-ebs-csi-driver-controller-6f56cff656-vxff5 started at 2023-01-18 21:24:17 +0000 UTC (0+11 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container attacher-kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container csi-driver ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container driver-kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container provisioner-kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container resizer-kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container snapshotter-kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.393: INFO: console-67d8cf9f45-k9j6z started at 2023-01-18 21:27:16 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container console ready: true, restart count 0
Jan 19 00:01:50.393: INFO: kube-controller-manager-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:00:32 +0000 UTC (0+4 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container cluster-policy-controller ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container kube-controller-manager-cert-syncer ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container kube-controller-manager-recovery-controller ready: true, restart count 0
Jan 19 00:01:50.393: INFO: machine-config-server-7skcj started at 2023-01-18 20:57:49 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container machine-config-server ready: true, restart count 1
Jan 19 00:01:50.393: INFO: revision-pruner-9-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:23:46 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container pruner ready: false, restart count 0
Jan 19 00:01:50.393: INFO: apiserver-66fc6ccdbf-7psdj started at 2023-01-18 21:32:17 +0000 UTC (1+2 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container openshift-apiserver ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container openshift-apiserver-check-endpoints ready: true, restart count 0
Jan 19 00:01:50.393: INFO: network-metrics-daemon-qqj8q started at 2023-01-18 20:55:15 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 00:01:50.393: INFO: node-ca-dlqnz started at 2023-01-18 21:02:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 00:01:50.393: INFO: redhat-marketplace-ks27p started at 2023-01-18 21:24:22 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.393: INFO: multus-additional-cni-plugins-rm96m started at 2023-01-18 20:55:14 +0000 UTC (6+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 00:01:50.393: INFO: revision-pruner-11-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:34:08 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container pruner ready: false, restart count 0
Jan 19 00:01:50.393: INFO: openshift-kube-scheduler-guard-ip-10-0-176-170.ec2.internal started at 2023-01-18 21:26:22 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container guard ready: true, restart count 0
Jan 19 00:01:50.393: INFO: machine-api-controllers-65b8bc9f-ffb75 started at 2023-01-18 21:24:12 +0000 UTC (0+7 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container kube-rbac-proxy-machine-mtrc ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container kube-rbac-proxy-machineset-mtrc ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container kube-rbac-proxy-mhc-mtrc ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container machine-controller ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container machine-healthcheck-controller ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container machineset-controller ready: true, restart count 0
Jan 19 00:01:50.393: INFO: 	Container nodelink-controller ready: true, restart count 0
Jan 19 00:01:50.393: INFO: csi-snapshot-webhook-f7487cb9f-shngp started at 2023-01-18 21:24:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container webhook ready: true, restart count 0
Jan 19 00:01:50.393: INFO: redhat-operators-4nktc started at 2023-01-18 21:24:20 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.393: INFO: managed-upgrade-operator-6748688f8b-qcjzq started at 2023-01-18 21:24:23 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container managed-upgrade-operator ready: true, restart count 0
Jan 19 00:01:50.393: INFO: aws-ebs-csi-driver-node-dvvjd started at 2023-01-18 20:57:36 +0000 UTC (0+3 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 00:01:50.393: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 00:01:50.393: INFO: sre-dns-latency-exporter-pgd7n started at 2023-01-18 21:18:07 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.393: INFO: 	Container main ready: true, restart count 1
W0119 00:01:50.395857      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 19 00:01:50.473: INFO: 
Latency metrics for node ip-10-0-176-170.ec2.internal
Jan 19 00:01:50.473: INFO: 
Logging node info for node ip-10-0-200-13.ec2.internal
Jan 19 00:01:50.476: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-200-13.ec2.internal    1e41ad69-39ff-411d-ae35-02a8ab8f162a 305964 0 2023-01-18 21:02:39 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-200-13.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-08a1adf43d92b1771","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0494590192945872e"} k8s.ovn.org/host-addresses:["10.0.200.13"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-200-13.ec2.internal","mac-address":"02:54:53:77:0c:0f","ip-addresses":["10.0.200.13/17"],"ip-address":"10.0.200.13/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:5761956d-9599-4f6a-a0bd-9faad9420cd7 k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.5/16"} k8s.ovn.org/node-mgmt-port-mac-address:82:8d:90:60:54:7c k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.200.13/17"} k8s.ovn.org/node-subnets:{"default":"10.128.6.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-worker-us-east-1a-q8ss7 machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done managed.openshift.com/customlabels: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{cloud-network-config-controller Update v1 2023-01-18 21:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{}}}} } {ip-10-0-164-47 Update v1 2023-01-18 21:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {kubelet Update v1 2023-01-18 21:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {nodelink-controller Update v1 2023-01-18 21:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}} } {ip-10-0-200-13 Update v1 2023-01-18 21:03:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {machine-config-daemon Update v1 2023-01-18 21:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {machine-config-controller Update v1 2023-01-18 21:18:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {manager Update v1 2023-01-18 21:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:managed.openshift.com/customlabels":{}}}} } {kubelet Update v1 2023-01-18 22:51:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-0494590192945872e,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321574121472 0} {<nil>} 314037228Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16487145472 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3920 -3} {<nil>} 3920m DecimalSI},ephemeral-storage: {{289416708846 0} {<nil>} 289416708846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{13697933312 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-18 23:57:43 +0000 UTC,LastTransitionTime:2023-01-18 21:02:39 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-18 23:57:43 +0000 UTC,LastTransitionTime:2023-01-18 21:02:39 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-18 23:57:43 +0000 UTC,LastTransitionTime:2023-01-18 21:02:39 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-18 23:57:43 +0000 UTC,LastTransitionTime:2023-01-18 21:03:20 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.200.13,},NodeAddress{Type:Hostname,Address:ip-10-0-200-13.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-200-13.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec2e839fdd9360a6cb050f8bfd6a64a2,SystemUUID:ec2e839f-dd93-60a6-cb05-0f8bfd6a64a2,BootID:37f61e92-0a53-4e5c-8366-d0abf103fe13,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e682077b6cbbe34d4c5e45ed10ffa9bc37e113e17e92a1d5330a7a9fffeab8a3],SizeBytes:1294522975,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08ab62da9265ef67f4eb4b526b38346a3ae3d35e2d5bfbdd58a0858b76b21f77],SizeBytes:681959546,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c7df53b796e81ba8301ba74d02317226329bd5752fd31c1b44d028e4832f21c3],SizeBytes:442026998,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f1134e2e28f44375c3bd9a6ee34d7f9972fdbd3305a3ee2b95e6ed3cede02140],SizeBytes:409661973,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:36fc214537c763b3a3f0a9dc7a1bd4378a80428c31b2629df8786a9b09155e6d],SizeBytes:398604719,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab0d2f0b6d01a4a3b9952710cc49e787ebb0e8649a288438774c1ffe0fc3fae0],SizeBytes:392510137,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5e1c69d005727e3245604cfca7a63e4f9bc6e15128c7489e41d5e967305089e],SizeBytes:371248553,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:77c5db690d9438ac077736cad8f28c04de476c04c3a97f39910ed86b6c395b85],SizeBytes:368328246,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f3ebf99182733dff2666e6a5e7e217085e06029362036ad79c91278e69dcbf7],SizeBytes:366270626,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2269a2b10b3ba2a6783dde2a97451f57f5716a20ebef4a82bea20d25df8761fa],SizeBytes:365147114,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:32fa95a3d4ecfebe96152242926addf27789555b12413203927f255a712c8a0c],SizeBytes:358569445,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:91308d35c1e56463f55c1aaa519ff4de7335d43b254c21abdb845fc8c72821a1],SizeBytes:347460509,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:255a95e8b514dcdc4fa12436789115739355c13fae93eccbba660298746d9aa1],SizeBytes:346372629,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a4fdcff8b0038d858d46740c9622d5dc428074dcf7e662deacd9e6c0aff18286],SizeBytes:344400730,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:209e20410ec2d3d7a502f568d2b7fe1cd1beadcb36fff2d1e6f59d77be3200e3],SizeBytes:339844669,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:140f8947593d92e1517e50a201e83bdef8eb965b552a21d3caf346a250d0cf6e],SizeBytes:335139477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01de581f7f0c7e43f809e9346ced7e60c0ecdd7c0078e97a8f64b24d4d3fa0b7],SizeBytes:332949901,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ed634acd9b29e16b578c74b38d5425156cd39bb72c0884a9564331f95f80911],SizeBytes:330558591,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs@sha256:83da6a30accf7d97453051973ba8c2ae1f78fa0472d2aeb0f8fcfdd8ba04e11a docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:324891489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:923999744ad1557510fc5c3a95cf978b27c453988489ada899c574ab1d71a218],SizeBytes:312722401,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-base@sha256:8da4f871cc18d3ac288a2fbb5c37ea27eabea4314dabca68f3f8469c7d7a4a21 quay.io/app-sre/managed-prometheus-exporter-base:latest],SizeBytes:303857745,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2550b2cbdf864515b1edacf43c25eb6b6f179713c1df34e51f6e9bba48d6430a],SizeBytes:303075904,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9260bbcc285a9fd4c1c7df865280261723eb7679050905da4acbfe728f96ee1b],SizeBytes:298386295,},ContainerImage{Names:[quay.io/konveyor/velero@sha256:6abd52244096680eeb3c80289999a00f642069edcd3d2e6c6948317b4bdd9bcd],SizeBytes:230325612,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder@sha256:afc413cd2504b586b6f28c16355db243c8bfdef536cc80e44f61e03c01e12235],SizeBytes:229564388,},ContainerImage{Names:[quay.io/app-sre/managed-velero-operator@sha256:587741a4e6772774f01efd91ed9b93ed5d0a21fcf9ff1f3bebaf14eb98466132],SizeBytes:155001069,},ContainerImage{Names:[quay.io/konveyor/velero-plugin-for-aws@sha256:7c22d5ae59862a66bac77e3fb48e6cd9c1556e4c9d7277aad4f093a198cb4373],SizeBytes:149334789,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder-operator@sha256:6c775fd9bfbbdabc114aaf28fa765b8290775030bbc8ddf36ea9a7522e73e804],SizeBytes:144945179,},ContainerImage{Names:[quay.io/app-sre/configure-alertmanager-operator@sha256:53691c2d7c4ff4314ceecdd01386a1260c85d4a4012d7434af7b61ead4d27eda],SizeBytes:144363279,},ContainerImage{Names:[quay.io/app-sre/osd-metrics-exporter@sha256:d413e14c899d4160c9be3d7854f2f5d8899ac649f6214f5405bd37cb98b3fc3d],SizeBytes:144088719,},ContainerImage{Names:[quay.io/app-sre/managed-node-metadata-operator@sha256:81cb5d1fcc8261858727876036c86fcf45b5b3cf9f5129450019c6f107cd11da],SizeBytes:143306922,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 k8s.gcr.io/e2e-test-images/httpd@sha256:6589a0d3a4b40f996ab554f0d58e8c567c2850c41bc99d05498378a50b7e1cb4 k8s.gcr.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:128894651,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403 k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8 k8s.gcr.io/e2e-test-images/agnhost:2.36],SizeBytes:126252387,},ContainerImage{Names:[quay.io/app-sre/custom-domains-operator-registry@sha256:365524ae13f1b5e5b252190c828e7ce9a5c1916a6aba32566326240d676059dc],SizeBytes:118428594,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator-registry@sha256:22c7691bceb31faeb7413d4e0fa56a52afb9d1bb64c11890874100d15fce0e9c],SizeBytes:117297595,},ContainerImage{Names:[quay.io/app-sre/route-monitor-operator-registry@sha256:bedb73a971e8a4094ea296e5df7279587edb6b293fa4b9227072792773cab1dc],SizeBytes:111841208,},ContainerImage{Names:[quay.io/app-sre/osd-metrics-exporter-registry@sha256:c9fa231e704f53d0e7b69e4bf1f746e53f916998f451da26fdfbb3e74dc9b197],SizeBytes:108399030,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jan 19 00:01:50.476: INFO: 
Logging kubelet events for node ip-10-0-200-13.ec2.internal
Jan 19 00:01:50.480: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-200-13.ec2.internal
Jan 19 00:01:50.504: INFO: ovnkube-node-h62xt started at 2023-01-18 21:02:39 +0000 UTC (0+5 container statuses recorded)
Jan 19 00:01:50.504: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.504: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 00:01:50.504: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 00:01:50.504: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 00:01:50.504: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 00:01:50.504: INFO: machine-config-daemon-smj7g started at 2023-01-18 21:02:39 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.504: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 00:01:50.504: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 00:01:50.504: INFO: network-metrics-daemon-m4c6r started at 2023-01-18 21:02:39 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.505: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 00:01:50.505: INFO: node-resolver-gxvcl started at 2023-01-18 21:02:39 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 00:01:50.505: INFO: downloads-6f74f6fcbf-hcggr started at 2023-01-18 21:24:10 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container download-server ready: true, restart count 0
Jan 19 00:01:50.505: INFO: rbac-permissions-operator-7f6bc8977-vsvkh started at 2023-01-18 21:24:29 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Jan 19 00:01:50.505: INFO: collect-profiles-27901425-9wzjz started at 2023-01-18 23:45:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 19 00:01:50.505: INFO: multus-cfrv2 started at 2023-01-18 21:02:39 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 00:01:50.505: INFO: ingress-canary-ppjjg started at 2023-01-18 21:03:27 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 00:01:50.505: INFO: ocm-agent-75d95f8dc7-gtjwg started at 2023-01-18 21:24:10 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container ocm-agent ready: true, restart count 0
Jan 19 00:01:50.505: INFO: aws-ebs-csi-driver-node-6nf8l started at 2023-01-18 21:02:39 +0000 UTC (0+3 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 00:01:50.505: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 00:01:50.505: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 00:01:50.505: INFO: managed-velero-operator-f9f4c8b45-xml88 started at 2023-01-18 21:24:10 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container managed-velero-operator ready: true, restart count 0
Jan 19 00:01:50.505: INFO: velero-749f7746d8-ds5cs started at 2023-01-18 21:24:10 +0000 UTC (1+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Init container konveyor-velero-plugin-for-aws ready: true, restart count 0
Jan 19 00:01:50.505: INFO: 	Container velero ready: true, restart count 0
Jan 19 00:01:50.505: INFO: network-check-target-hpzlx started at 2023-01-18 21:02:39 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 00:01:50.505: INFO: splunk-forwarder-operator-6f5bf57497-jwcz5 started at 2023-01-18 21:24:40 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Jan 19 00:01:50.505: INFO: node-exporter-gzf8b started at 2023-01-18 21:08:32 +0000 UTC (1+2 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 19 00:01:50.505: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.505: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 00:01:50.505: INFO: image-registry-7b8f8dcdc5-4lfps started at 2023-01-18 21:24:10 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container registry ready: true, restart count 0
Jan 19 00:01:50.505: INFO: osd-metrics-exporter-registry-6rnzs started at 2023-01-18 21:24:12 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.505: INFO: managed-node-metadata-operator-5d4c567575-cbrxh started at 2023-01-18 21:24:36 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container managed-node-metadata-operator ready: true, restart count 0
Jan 19 00:01:50.505: INFO: collect-profiles-27901440-zs9zb started at 2023-01-19 00:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 19 00:01:50.505: INFO: tuned-t6mkr started at 2023-01-18 21:02:39 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container tuned ready: true, restart count 1
Jan 19 00:01:50.505: INFO: dns-default-vkx5x started at 2023-01-18 21:03:27 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container dns ready: true, restart count 1
Jan 19 00:01:50.505: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.505: INFO: sre-dns-latency-exporter-s6nk6 started at 2023-01-18 21:18:07 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container main ready: true, restart count 1
Jan 19 00:01:50.505: INFO: splunkforwarder-ds-n7d7t started at 2023-01-18 21:24:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container splunk-uf ready: true, restart count 0
Jan 19 00:01:50.505: INFO: route-monitor-operator-registry-cgj4n started at 2023-01-18 21:24:36 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.505: INFO: node-ca-5tw9r started at 2023-01-18 21:02:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 00:01:50.505: INFO: token-refresher-6d8f85f497-mhth9 started at 2023-01-18 21:24:10 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container token-refresher ready: true, restart count 0
Jan 19 00:01:50.505: INFO: obo-prometheus-operator-admission-webhook-5b4dc9bff5-l6qvc started at 2023-01-18 21:24:10 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 00:01:50.505: INFO: configure-alertmanager-operator-7565458cf4-pbmjl started at 2023-01-18 21:24:36 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Jan 19 00:01:50.505: INFO: route-monitor-operator-controller-manager-cbc597f9d-84p87 started at 2023-01-18 21:24:37 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container manager ready: true, restart count 0
Jan 19 00:01:50.505: INFO: blackbox-exporter-dfdd57dd6-tj4kz started at 2023-01-18 21:24:50 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan 19 00:01:50.505: INFO: multus-additional-cni-plugins-nm8v9 started at 2023-01-18 21:02:39 +0000 UTC (6+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 19 00:01:50.505: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 19 00:01:50.505: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 19 00:01:50.505: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 19 00:01:50.505: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 19 00:01:50.505: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 19 00:01:50.505: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 00:01:50.505: INFO: osd-metrics-exporter-756f85967-8z4cz started at 2023-01-18 21:25:10 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container osd-metrics-exporter ready: true, restart count 0
Jan 19 00:01:50.505: INFO: collect-profiles-27901410-gd7tw started at 2023-01-18 23:30:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 19 00:01:50.505: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-mdcrg started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.505: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 00:01:50.505: INFO: 	Container systemd-logs ready: true, restart count 0
W0119 00:01:50.507204      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 19 00:01:50.556: INFO: 
Latency metrics for node ip-10-0-200-13.ec2.internal
Jan 19 00:01:50.556: INFO: 
Logging node info for node ip-10-0-211-217.ec2.internal
Jan 19 00:01:50.558: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-211-217.ec2.internal    8dc19be1-68f9-4ba1-8988-e3dd116688ce 308584 0 2023-01-18 21:27:17 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:r5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-211-217.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io:infra node-role.kubernetes.io/infra: node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:r5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-04da74abf7ad6ed49","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-05b843346e3d96659"} k8s.ovn.org/host-addresses:["10.0.211.217"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-211-217.ec2.internal","mac-address":"02:33:6b:05:74:ad","ip-addresses":["10.0.211.217/17"],"ip-address":"10.0.211.217/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:68435c44-ccc8-48aa-9dca-17a66621f9dd k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.10/16"} k8s.ovn.org/node-mgmt-port-mac-address:b6:30:0a:a5:94:93 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.211.217/17"} k8s.ovn.org/node-subnets:{"default":"10.128.16.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-infra-us-east-1a-htq2c machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done managed.openshift.com/customlabels:node-role.kubernetes.io,node-role.kubernetes.io/infra volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{ancient-changes Update v1 2023-01-18 21:27:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{},"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {kubelet Update v1 2023-01-18 21:27:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {nodelink-controller Update v1 2023-01-18 21:27:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}},"f:labels":{"f:node-role.kubernetes.io":{},"f:node-role.kubernetes.io/infra":{}}}} } {ip-10-0-211-217 Update v1 2023-01-18 21:28:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {machine-config-daemon Update v1 2023-01-18 21:28:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {machine-config-controller Update v1 2023-01-18 21:36:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {manager Update v1 2023-01-18 21:39:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:managed.openshift.com/customlabels":{}}}} } {kube-controller-manager Update v1 2023-01-18 21:39:20 +0000 UTC FieldsV1 {"f:status":{"f:volumesAttached":{}}} status} {kubelet Update v1 2023-01-18 22:51:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}},"f:volumesInUse":{}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-05b843346e3d96659,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321574121472 0} {<nil>} 314037228Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{33222410240 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3920 -3} {<nil>} 3920m DecimalSI},ephemeral-storage: {{289416708846 0} {<nil>} 289416708846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{29423880765440 -3} {<nil>} 29423880765440m DecimalSI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-19 00:00:29 +0000 UTC,LastTransitionTime:2023-01-18 21:39:04 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-19 00:00:29 +0000 UTC,LastTransitionTime:2023-01-18 21:39:04 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-19 00:00:29 +0000 UTC,LastTransitionTime:2023-01-18 21:39:04 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-19 00:00:29 +0000 UTC,LastTransitionTime:2023-01-18 21:39:04 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.211.217,},NodeAddress{Type:Hostname,Address:ip-10-0-211-217.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-211-217.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec25a10687a71ed601256f15b22bf2ae,SystemUUID:ec25a106-87a7-1ed6-0125-6f15b22bf2ae,BootID:f5f35c2c-ec0c-49ff-a53f-fc5203750137,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[image-registry.openshift-image-registry.svc:5000/openshift/cli@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45 image-registry.openshift-image-registry.svc:5000/openshift/cli:latest],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c7df53b796e81ba8301ba74d02317226329bd5752fd31c1b44d028e4832f21c3],SizeBytes:442026998,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f1134e2e28f44375c3bd9a6ee34d7f9972fdbd3305a3ee2b95e6ed3cede02140],SizeBytes:409661973,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:36fc214537c763b3a3f0a9dc7a1bd4378a80428c31b2629df8786a9b09155e6d],SizeBytes:398604719,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab0d2f0b6d01a4a3b9952710cc49e787ebb0e8649a288438774c1ffe0fc3fae0],SizeBytes:392510137,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5813d3728229c2a09a05bc454753e0128ac2bbd203e7000a4d954daab12fbb79],SizeBytes:377171632,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c956edcee9b9ba5462572b65b6a92983b20ace63dae50e3237bfdbd6d8c0b972],SizeBytes:375087838,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5e1c69d005727e3245604cfca7a63e4f9bc6e15128c7489e41d5e967305089e],SizeBytes:371248553,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:77c5db690d9438ac077736cad8f28c04de476c04c3a97f39910ed86b6c395b85],SizeBytes:368328246,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f3ebf99182733dff2666e6a5e7e217085e06029362036ad79c91278e69dcbf7],SizeBytes:366270626,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2269a2b10b3ba2a6783dde2a97451f57f5716a20ebef4a82bea20d25df8761fa],SizeBytes:365147114,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:32fa95a3d4ecfebe96152242926addf27789555b12413203927f255a712c8a0c],SizeBytes:358569445,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:91308d35c1e56463f55c1aaa519ff4de7335d43b254c21abdb845fc8c72821a1],SizeBytes:347460509,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:255a95e8b514dcdc4fa12436789115739355c13fae93eccbba660298746d9aa1],SizeBytes:346372629,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a4fdcff8b0038d858d46740c9622d5dc428074dcf7e662deacd9e6c0aff18286],SizeBytes:344400730,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:209e20410ec2d3d7a502f568d2b7fe1cd1beadcb36fff2d1e6f59d77be3200e3],SizeBytes:339844669,},ContainerImage{Names:[quay.io/openshift/origin-kube-rbac-proxy@sha256:baedb268ac66456018fb30af395bb3d69af5fff3252ff5d549f0231b1ebb6901 quay.io/openshift/origin-kube-rbac-proxy:4.10.0],SizeBytes:337627888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:140f8947593d92e1517e50a201e83bdef8eb965b552a21d3caf346a250d0cf6e],SizeBytes:335139477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01de581f7f0c7e43f809e9346ced7e60c0ecdd7c0078e97a8f64b24d4d3fa0b7],SizeBytes:332949901,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ed634acd9b29e16b578c74b38d5425156cd39bb72c0884a9564331f95f80911],SizeBytes:330558591,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs@sha256:83da6a30accf7d97453051973ba8c2ae1f78fa0472d2aeb0f8fcfdd8ba04e11a docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:324891489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:923999744ad1557510fc5c3a95cf978b27c453988489ada899c574ab1d71a218],SizeBytes:312722401,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-base@sha256:8da4f871cc18d3ac288a2fbb5c37ea27eabea4314dabca68f3f8469c7d7a4a21 quay.io/app-sre/managed-prometheus-exporter-base:latest],SizeBytes:303857745,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2550b2cbdf864515b1edacf43c25eb6b6f179713c1df34e51f6e9bba48d6430a],SizeBytes:303075904,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9260bbcc285a9fd4c1c7df865280261723eb7679050905da4acbfe728f96ee1b],SizeBytes:298386295,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/jessie-dnsutils@sha256:11e6a66017ba4e4b938c1612b7a54a3befcefd354796c04e1dba76873a13518e k8s.gcr.io/e2e-test-images/jessie-dnsutils@sha256:a11a6fb43a910882e547b12b511b70b912834b9d3fc1a1d00b374c173b156d87 k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.5],SizeBytes:260976825,},ContainerImage{Names:[registry.k8s.io/conformance@sha256:77097c50d228096fc7af733bb98aff4a66e7846fb6ab62e3d46f84299cf8c235 registry.k8s.io/conformance@sha256:b9f91081bf4b080ad3b29977b15594ba68f0380242e35f31850616c12cedd323 registry.k8s.io/conformance:v1.24.0],SizeBytes:252262821,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder@sha256:afc413cd2504b586b6f28c16355db243c8bfdef536cc80e44f61e03c01e12235],SizeBytes:229564388,},ContainerImage{Names:[quay.io/app-sre/addon-operator-manager@sha256:3a5f9c6073f3b3542e53be155f1364bcba10233ed69011eaadbc73318776b775 quay.io/app-sre/addon-operator-manager:a45b2b8],SizeBytes:176376786,},ContainerImage{Names:[quay.io/app-sre/addon-operator-webhook@sha256:16a258633afc63916d84a447a6dd8a234e1803a4dfa22a15887ff1dc71781e75 quay.io/app-sre/addon-operator-webhook:a45b2b8],SizeBytes:173194705,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:20f25f275d46aa728f7615a1ccc19c78b2ed89435bf943a44b339f70f45508e6 k8s.gcr.io/e2e-test-images/httpd@sha256:5d28f127fae41261c56abccd96df481f8f4fba2a3305fece212e11aafe646943 k8s.gcr.io/e2e-test-images/httpd:2.4.39-2],SizeBytes:132295599,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 k8s.gcr.io/e2e-test-images/httpd@sha256:6589a0d3a4b40f996ab554f0d58e8c567c2850c41bc99d05498378a50b7e1cb4 k8s.gcr.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:128894651,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403 k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8 k8s.gcr.io/e2e-test-images/agnhost:2.36],SizeBytes:126252387,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nautilus@sha256:99c0d6f1ad24a1aa1905d9c6534d193f268f7b23f9add2ae6bb41f31094bdd5c k8s.gcr.io/e2e-test-images/nautilus@sha256:d3190bb7fb53d99b11da40aab7ca81b5f40379345deb8c09349d080e243c9e0c k8s.gcr.io/e2e-test-images/nautilus:1.5],SizeBytes:125565966,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:7f3ae784f32d750e63495dacd39c3bc57a9a0fc3e7ffc4b6f65e36be03cb368c docker.io/sonobuoy/sonobuoy@sha256:eaa42dd0660ece6c18d06199b78a41bd532a4851fc32a5a99acfafc03556852e docker.io/sonobuoy/sonobuoy:v0.56.14],SizeBytes:49637374,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nginx@sha256:13616070e3f29de4417eee434a8ef472221c9e51b3d037b5a6b46cef08eb7443 k8s.gcr.io/e2e-test-images/nginx@sha256:eee1822ee5bafc780db34f9ab68456c5fdcc0f994e1c1e01d5cde593cb3897a1 k8s.gcr.io/e2e-test-images/nginx:1.14-2],SizeBytes:17245363,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf k8s.gcr.io/e2e-test-images/busybox@sha256:f1fa97a4e1dab565da2790e6c76f8aecf87c44c9ac7ba53a9c65af96ec472c4a k8s.gcr.io/e2e-test-images/busybox:1.29-2],SizeBytes:1374588,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c k8s.gcr.io/pause@sha256:f81611a21cf91214c1ea751c5b525931a0e2ebabe62b3937b6158039ff6f922d k8s.gcr.io/pause:3.7],SizeBytes:717997,},},VolumesInUse:[kubernetes.io/csi/ebs.csi.aws.com^vol-00a181b5d3d011a60 kubernetes.io/csi/ebs.csi.aws.com^vol-04efe798559a2c542],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/csi/ebs.csi.aws.com^vol-04efe798559a2c542,DevicePath:,},AttachedVolume{Name:kubernetes.io/csi/ebs.csi.aws.com^vol-00a181b5d3d011a60,DevicePath:,},},Config:nil,},}
Jan 19 00:01:50.559: INFO: 
Logging kubelet events for node ip-10-0-211-217.ec2.internal
Jan 19 00:01:50.562: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-211-217.ec2.internal
Jan 19 00:01:50.590: INFO: node-resolver-qsc9k started at 2023-01-18 21:27:42 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 00:01:50.590: INFO: sre-dns-latency-exporter-tlqwt started at 2023-01-18 21:27:42 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container main ready: true, restart count 1
Jan 19 00:01:50.590: INFO: sonobuoy-e2e-job-aed0e1be5f434192 started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container e2e ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 00:01:50.590: INFO: osd-delete-backplane-serviceaccounts-27901420-s8d8r started at 2023-01-18 23:40:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 19 00:01:50.590: INFO: node-exporter-bpnc5 started at 2023-01-18 21:27:42 +0000 UTC (1+2 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 00:01:50.590: INFO: addon-operator-webhooks-569bd4cfc5-mdklc started at 2023-01-18 21:39:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container webhook ready: true, restart count 0
Jan 19 00:01:50.590: INFO: network-metrics-daemon-jnkxc started at 2023-01-18 21:27:42 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 00:01:50.590: INFO: ovnkube-node-bf8j7 started at 2023-01-18 21:27:42 +0000 UTC (0+5 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 00:01:50.590: INFO: ingress-canary-k52kt started at 2023-01-18 21:28:29 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 00:01:50.590: INFO: router-default-7cff97cd98-lstfd started at 2023-01-18 21:39:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container router ready: true, restart count 0
Jan 19 00:01:50.590: INFO: multus-cz5tw started at 2023-01-18 21:27:42 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 00:01:50.590: INFO: tuned-6pmzm started at 2023-01-18 21:27:42 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container tuned ready: true, restart count 1
Jan 19 00:01:50.590: INFO: prometheus-k8s-0 started at 2023-01-18 21:39:17 +0000 UTC (1+6 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Init container init-config-reloader ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 00:01:50.590: INFO: osd-delete-ownerrefs-serviceaccounts-27901387-lzh55 started at 2023-01-18 23:07:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 19 00:01:50.590: INFO: builds-pruner-27901440-q4l8s started at 2023-01-19 00:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 19 00:01:50.590: INFO: node-ca-9gnsm started at 2023-01-18 21:27:42 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 00:01:50.590: INFO: multus-additional-cni-plugins-2q4s4 started at 2023-01-18 21:27:42 +0000 UTC (6+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 00:01:50.590: INFO: image-pruner-27901380-mbpm4 started at 2023-01-18 23:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container image-pruner ready: false, restart count 0
Jan 19 00:01:50.590: INFO: network-check-target-pjt5v started at 2023-01-18 21:27:42 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 00:01:50.590: INFO: aws-ebs-csi-driver-node-l4ng8 started at 2023-01-18 21:27:42 +0000 UTC (0+3 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 00:01:50.590: INFO: prometheus-operator-admission-webhook-577cc9c956-lwfrr started at 2023-01-18 21:39:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 00:01:50.590: INFO: prometheus-adapter-cf64f7f46-jw64l started at 2023-01-18 21:39:31 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 19 00:01:50.590: INFO: osd-rebalance-infra-nodes-27901425-6smbb started at 2023-01-18 23:45:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 19 00:01:50.590: INFO: image-pruner-27901440-qvbp2 started at 2023-01-19 00:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container image-pruner ready: false, restart count 0
Jan 19 00:01:50.590: INFO: alertmanager-main-0 started at 2023-01-18 21:39:18 +0000 UTC (0+6 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container alertmanager ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 00:01:50.590: INFO: dns-default-7sqw2 started at 2023-01-18 22:06:24 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container dns ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.590: INFO: osd-delete-ownerrefs-serviceaccounts-27901357-rd8sm started at 2023-01-18 22:37:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 19 00:01:50.590: INFO: osd-delete-backplane-serviceaccounts-27901430-sjgvz started at 2023-01-18 23:50:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 19 00:01:50.590: INFO: osd-rebalance-infra-nodes-27901440-rt25s started at 2023-01-19 00:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 19 00:01:50.590: INFO: osd-patch-subscription-source-27901380-gmtz6 started at 2023-01-18 23:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 19 00:01:50.590: INFO: osd-rebalance-infra-nodes-27901410-fkk69 started at 2023-01-18 23:30:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 19 00:01:50.590: INFO: image-pruner-27901320-rjnq8 started at 2023-01-18 22:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container image-pruner ready: false, restart count 0
Jan 19 00:01:50.590: INFO: splunkforwarder-ds-gr4fb started at 2023-01-18 21:27:42 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 00:01:50.590: INFO: builds-pruner-27901320-2bl9l started at 2023-01-18 22:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 19 00:01:50.590: INFO: osd-delete-ownerrefs-serviceaccounts-27901417-kbrf6 started at 2023-01-18 23:37:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 19 00:01:50.590: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-tjlv7 started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 00:01:50.590: INFO: machine-config-daemon-krjvz started at 2023-01-18 21:27:42 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 00:01:50.590: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 00:01:50.590: INFO: thanos-querier-57f44c5498-fzjjb started at 2023-01-18 21:39:18 +0000 UTC (0+6 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 00:01:50.590: INFO: 	Container thanos-query ready: true, restart count 0
Jan 19 00:01:50.590: INFO: deployments-pruner-27901440-4fwkx started at 2023-01-19 00:00:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 19 00:01:50.590: INFO: sre-build-test-27901391-sgdf9 started at 2023-01-18 23:11:00 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.590: INFO: 	Container sre-build-test ready: false, restart count 0
W0119 00:01:50.595212      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 19 00:01:50.619: INFO: 
Latency metrics for node ip-10-0-211-217.ec2.internal
Jan 19 00:01:50.619: INFO: 
Logging node info for node ip-10-0-219-147.ec2.internal
Jan 19 00:01:50.622: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-219-147.ec2.internal    7f02bac5-3479-43f2-9907-d04fbea8675c 308767 0 2023-01-18 21:03:48 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-219-147.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-0dd82b6977e2626fa","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0910f24871101dbf4"} k8s.ovn.org/host-addresses:["10.0.219.147"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-219-147.ec2.internal","mac-address":"02:8a:81:6a:3c:b1","ip-addresses":["10.0.219.147/17"],"ip-address":"10.0.219.147/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:0ddbb2d7-7bcd-4adb-91e4-f6546b32776d k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.8/16"} k8s.ovn.org/node-mgmt-port-mac-address:f6:73:8e:97:41:a3 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.219.147/17"} k8s.ovn.org/node-subnets:{"default":"10.128.12.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-worker-us-east-1a-5kt8g machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done managed.openshift.com/customlabels: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{ancient-changes Update v1 2023-01-18 21:03:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{},"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {kubelet Update v1 2023-01-18 21:03:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {nodelink-controller Update v1 2023-01-18 21:03:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}} } {machine-config-daemon Update v1 2023-01-18 21:07:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {ip-10-0-219-147 Update v1 2023-01-18 21:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {manager Update v1 2023-01-18 21:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:managed.openshift.com/customlabels":{}}}} } {machine-config-controller Update v1 2023-01-18 21:31:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {e2e.test Update v1 2023-01-18 23:09:00 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:example.com/fakecpu":{}}}} status} {kubelet Update v1 2023-01-18 23:09:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:example.com/fakecpu":{},"f:memory":{}},"f:capacity":{"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-0910f24871101dbf4,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321574121472 0} {<nil>} 314037228Ki BinarySI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16487145472 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3920 -3} {<nil>} 3920m DecimalSI},ephemeral-storage: {{289416708846 0} {<nil>} 289416708846 DecimalSI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{13697933312 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-19 00:00:50 +0000 UTC,LastTransitionTime:2023-01-18 21:33:15 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-19 00:00:50 +0000 UTC,LastTransitionTime:2023-01-18 21:33:15 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-19 00:00:50 +0000 UTC,LastTransitionTime:2023-01-18 21:33:15 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-19 00:00:50 +0000 UTC,LastTransitionTime:2023-01-18 21:33:15 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.219.147,},NodeAddress{Type:Hostname,Address:ip-10-0-219-147.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-219-147.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec22e883e23ff437129dfb17e11a2173,SystemUUID:ec22e883-e23f-f437-129d-fb17e11a2173,BootID:e31e18d4-9c53-4050-87d7-b931ed6c39ae,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e682077b6cbbe34d4c5e45ed10ffa9bc37e113e17e92a1d5330a7a9fffeab8a3],SizeBytes:1294522975,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ca84d407cbe329b9efad6656408b6c6cade8899633305d6fe45d9aaf079cc45c],SizeBytes:574313810,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c7df53b796e81ba8301ba74d02317226329bd5752fd31c1b44d028e4832f21c3],SizeBytes:442026998,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:36fc214537c763b3a3f0a9dc7a1bd4378a80428c31b2629df8786a9b09155e6d],SizeBytes:398604719,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab0d2f0b6d01a4a3b9952710cc49e787ebb0e8649a288438774c1ffe0fc3fae0],SizeBytes:392510137,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5e1c69d005727e3245604cfca7a63e4f9bc6e15128c7489e41d5e967305089e],SizeBytes:371248553,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f3ebf99182733dff2666e6a5e7e217085e06029362036ad79c91278e69dcbf7],SizeBytes:366270626,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:91308d35c1e56463f55c1aaa519ff4de7335d43b254c21abdb845fc8c72821a1],SizeBytes:347460509,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:255a95e8b514dcdc4fa12436789115739355c13fae93eccbba660298746d9aa1],SizeBytes:346372629,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a4fdcff8b0038d858d46740c9622d5dc428074dcf7e662deacd9e6c0aff18286],SizeBytes:344400730,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:209e20410ec2d3d7a502f568d2b7fe1cd1beadcb36fff2d1e6f59d77be3200e3],SizeBytes:339844669,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:140f8947593d92e1517e50a201e83bdef8eb965b552a21d3caf346a250d0cf6e],SizeBytes:335139477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01de581f7f0c7e43f809e9346ced7e60c0ecdd7c0078e97a8f64b24d4d3fa0b7],SizeBytes:332949901,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ed634acd9b29e16b578c74b38d5425156cd39bb72c0884a9564331f95f80911],SizeBytes:330558591,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs@sha256:83da6a30accf7d97453051973ba8c2ae1f78fa0472d2aeb0f8fcfdd8ba04e11a docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:324891489,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-initcontainer@sha256:f859874cf8ef92e8e806ff615f33472992917545ec94d461caa8e6e13b8a1983 quay.io/app-sre/managed-prometheus-exporter-initcontainer:latest],SizeBytes:315381355,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:923999744ad1557510fc5c3a95cf978b27c453988489ada899c574ab1d71a218],SizeBytes:312722401,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-base@sha256:8da4f871cc18d3ac288a2fbb5c37ea27eabea4314dabca68f3f8469c7d7a4a21 quay.io/app-sre/managed-prometheus-exporter-base:latest],SizeBytes:303857745,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2550b2cbdf864515b1edacf43c25eb6b6f179713c1df34e51f6e9bba48d6430a],SizeBytes:303075904,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:13f53ed1d91e2e11aac476ee9a0269fdda6cc4874eba903efd40daf50c55eee5 k8s.gcr.io/etcd@sha256:678382ed340f6996ad40cdba4a4745a2ada41ed9c322c026a2a695338a93dcbe k8s.gcr.io/etcd:3.5.3-0],SizeBytes:300857875,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9260bbcc285a9fd4c1c7df865280261723eb7679050905da4acbfe728f96ee1b],SizeBytes:298386295,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/jessie-dnsutils@sha256:11e6a66017ba4e4b938c1612b7a54a3befcefd354796c04e1dba76873a13518e k8s.gcr.io/e2e-test-images/jessie-dnsutils@sha256:a11a6fb43a910882e547b12b511b70b912834b9d3fc1a1d00b374c173b156d87 k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.5],SizeBytes:260976825,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder@sha256:afc413cd2504b586b6f28c16355db243c8bfdef536cc80e44f61e03c01e12235],SizeBytes:229564388,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:20f25f275d46aa728f7615a1ccc19c78b2ed89435bf943a44b339f70f45508e6 k8s.gcr.io/e2e-test-images/httpd@sha256:5d28f127fae41261c56abccd96df481f8f4fba2a3305fece212e11aafe646943 k8s.gcr.io/e2e-test-images/httpd:2.4.39-2],SizeBytes:132295599,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 k8s.gcr.io/e2e-test-images/httpd@sha256:6589a0d3a4b40f996ab554f0d58e8c567c2850c41bc99d05498378a50b7e1cb4 k8s.gcr.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:128894651,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403 k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8 k8s.gcr.io/e2e-test-images/agnhost:2.36],SizeBytes:126252387,},ContainerImage{Names:[quay.io/app-sre/managed-upgrade-operator-registry@sha256:1584ad8c5f28533cad5a72bcf7fc69d03fc7eac87a580897553bdcc01c89f1c6],SizeBytes:126047670,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nautilus@sha256:99c0d6f1ad24a1aa1905d9c6534d193f268f7b23f9add2ae6bb41f31094bdd5c k8s.gcr.io/e2e-test-images/nautilus@sha256:d3190bb7fb53d99b11da40aab7ca81b5f40379345deb8c09349d080e243c9e0c k8s.gcr.io/e2e-test-images/nautilus:1.5],SizeBytes:125565966,},ContainerImage{Names:[quay.io/app-sre/managed-velero-operator-registry@sha256:70df9cc14d07e0212d2f16b2237483a1fe12bb4b0f3aa3fc4240db0d18387aee],SizeBytes:118411702,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator-registry@sha256:22c7691bceb31faeb7413d4e0fa56a52afb9d1bb64c11890874100d15fce0e9c],SizeBytes:117297595,},ContainerImage{Names:[quay.io/app-sre/managed-node-metadata-operator-registry@sha256:bfa8c6e594b330941714269ce0e0dbbc88e65642ce1141496fd88329255f2e04],SizeBytes:117254580,},ContainerImage{Names:[quay.io/app-sre/configure-alertmanager-operator-registry@sha256:3aa6059809b09b01557a31227e624ead2e52867a7e0249f06819a1e554dbc1ae],SizeBytes:111741880,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator@sha256:cfb005c1b5144447dad26c5e7ccbf486bfa82d52673488356fc9522dc8420f69],SizeBytes:111630880,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/sample-apiserver@sha256:48c8ebf147fc7255093e4e8527e99f9a0c74a6d3ea5afd1c1c10b2a8fda9e6f7 k8s.gcr.io/e2e-test-images/sample-apiserver@sha256:f9c93b92b6ff750b41a93c4e4fe0bfe384597aeb841e2539d5444815c55b2d8f k8s.gcr.io/e2e-test-images/sample-apiserver:1.17.5],SizeBytes:56769717,},ContainerImage{Names:[docker.io/sonobuoy/sonobuoy@sha256:7f3ae784f32d750e63495dacd39c3bc57a9a0fc3e7ffc4b6f65e36be03cb368c docker.io/sonobuoy/sonobuoy@sha256:eaa42dd0660ece6c18d06199b78a41bd532a4851fc32a5a99acfafc03556852e docker.io/sonobuoy/sonobuoy:v0.56.14],SizeBytes:49637374,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nginx@sha256:13616070e3f29de4417eee434a8ef472221c9e51b3d037b5a6b46cef08eb7443 k8s.gcr.io/e2e-test-images/nginx@sha256:eee1822ee5bafc780db34f9ab68456c5fdcc0f994e1c1e01d5cde593cb3897a1 k8s.gcr.io/e2e-test-images/nginx:1.14-2],SizeBytes:17245363,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nonewprivs@sha256:8ac1264691820febacf3aea5d152cbde6d10685731ec14966a9401c6f47a68ac k8s.gcr.io/e2e-test-images/nonewprivs@sha256:f6b1c4aef11b116c2a065ea60ed071a8f205444f1897bed9aa2e98a5d78cbdae k8s.gcr.io/e2e-test-images/nonewprivs:1.3],SizeBytes:7373984,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jan 19 00:01:50.622: INFO: 
Logging kubelet events for node ip-10-0-219-147.ec2.internal
Jan 19 00:01:50.626: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-219-147.ec2.internal
Jan 19 00:01:50.644: INFO: ovnkube-node-7vtb8 started at 2023-01-18 21:03:48 +0000 UTC (0+5 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Container ovnkube-node ready: true, restart count 4
Jan 19 00:01:50.644: INFO: ingress-canary-fvl4z started at 2023-01-18 23:05:23 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 19 00:01:50.644: INFO: dns-default-hhrdx started at 2023-01-18 23:05:43 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Container dns ready: true, restart count 0
Jan 19 00:01:50.644: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 00:01:50.644: INFO: node-resolver-svmsb started at 2023-01-18 21:03:48 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 00:01:50.644: INFO: aws-ebs-csi-driver-node-qprtw started at 2023-01-18 21:03:48 +0000 UTC (0+3 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 00:01:50.644: INFO: multus-additional-cni-plugins-6n2cz started at 2023-01-18 21:03:48 +0000 UTC (6+1 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 00:01:50.644: INFO: machine-config-daemon-nb5xf started at 2023-01-18 21:03:48 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 00:01:50.644: INFO: node-exporter-6m8v9 started at 2023-01-18 21:08:32 +0000 UTC (1+2 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 00:01:50.644: INFO: network-metrics-daemon-mkvpn started at 2023-01-18 21:03:48 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.644: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 00:01:50.644: INFO: multus-55cv4 started at 2023-01-18 21:03:48 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 00:01:50.644: INFO: sre-dns-latency-exporter-rxh5d started at 2023-01-18 21:18:07 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Container main ready: true, restart count 1
Jan 19 00:01:50.644: INFO: splunkforwarder-ds-b6n2g started at 2023-01-18 21:24:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 00:01:50.644: INFO: sonobuoy started at 2023-01-18 22:14:33 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 19 00:01:50.644: INFO: tuned-246td started at 2023-01-18 21:03:48 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Container tuned ready: true, restart count 1
Jan 19 00:01:50.644: INFO: network-check-target-cqk2p started at 2023-01-18 21:03:48 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 00:01:50.644: INFO: node-ca-nzbcp started at 2023-01-18 21:03:48 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 00:01:50.644: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-shjrl started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.644: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 00:01:50.644: INFO: 	Container systemd-logs ready: true, restart count 0
W0119 00:01:50.646577      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 19 00:01:50.672: INFO: 
Latency metrics for node ip-10-0-219-147.ec2.internal
Jan 19 00:01:50.672: INFO: 
Logging node info for node ip-10-0-236-5.ec2.internal
Jan 19 00:01:50.674: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-236-5.ec2.internal    3857b2bb-1ebc-43ac-88d7-3457c64fe28c 307761 0 2023-01-18 21:02:48 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-236-5.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-003fca968413295a7","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0d3be9ff8902e98c7"} k8s.ovn.org/host-addresses:["10.0.236.5"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-236-5.ec2.internal","mac-address":"02:6c:41:55:cb:7d","ip-addresses":["10.0.236.5/17"],"ip-address":"10.0.236.5/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:dd4a5019-6dcb-4e50-a381-1ed2a7495c37 k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.6/16"} k8s.ovn.org/node-mgmt-port-mac-address:8a:d5:1e:e7:7c:71 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.236.5/17"} k8s.ovn.org/node-subnets:{"default":"10.128.8.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-worker-us-east-1a-2tqnm machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done managed.openshift.com/customlabels: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{ancient-changes Update v1 2023-01-18 21:02:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:machine.openshift.io/machine":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {cloud-network-config-controller Update v1 2023-01-18 21:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{}}}} } {ip-10-0-164-47 Update v1 2023-01-18 21:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {ip-10-0-236-5 Update v1 2023-01-18 21:03:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {machine-config-daemon Update v1 2023-01-18 21:03:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {machine-config-controller Update v1 2023-01-18 21:24:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {manager Update v1 2023-01-18 21:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:managed.openshift.com/customlabels":{}}}} } {kubelet Update v1 2023-01-18 22:51:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-0d3be9ff8902e98c7,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321574121472 0} {<nil>} 314037228Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16487145472 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3920 -3} {<nil>} 3920m DecimalSI},ephemeral-storage: {{289416708846 0} {<nil>} 289416708846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{13697933312 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-18 23:59:16 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-18 23:59:16 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-18 23:59:16 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-18 23:59:16 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.236.5,},NodeAddress{Type:Hostname,Address:ip-10-0-236-5.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-236-5.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec2af1d21417952ad207fe06f4d3ae06,SystemUUID:ec2af1d2-1417-952a-d207-fe06f4d3ae06,BootID:176a37bd-3afd-4445-ab83-1d05946c1425,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e682077b6cbbe34d4c5e45ed10ffa9bc37e113e17e92a1d5330a7a9fffeab8a3],SizeBytes:1294522975,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cad43cf9f6fbf861e09237e05c5765d15af9a8bd7ee82e557443d8ecba381f56],SizeBytes:454652760,},ContainerImage{Names:[image-registry.openshift-image-registry.svc:5000/openshift/cli@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45 image-registry.openshift-image-registry.svc:5000/openshift/cli:latest],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c7df53b796e81ba8301ba74d02317226329bd5752fd31c1b44d028e4832f21c3],SizeBytes:442026998,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f1134e2e28f44375c3bd9a6ee34d7f9972fdbd3305a3ee2b95e6ed3cede02140],SizeBytes:409661973,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:36fc214537c763b3a3f0a9dc7a1bd4378a80428c31b2629df8786a9b09155e6d],SizeBytes:398604719,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab0d2f0b6d01a4a3b9952710cc49e787ebb0e8649a288438774c1ffe0fc3fae0],SizeBytes:392510137,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c956edcee9b9ba5462572b65b6a92983b20ace63dae50e3237bfdbd6d8c0b972],SizeBytes:375087838,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5e1c69d005727e3245604cfca7a63e4f9bc6e15128c7489e41d5e967305089e],SizeBytes:371248553,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f3ebf99182733dff2666e6a5e7e217085e06029362036ad79c91278e69dcbf7],SizeBytes:366270626,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2269a2b10b3ba2a6783dde2a97451f57f5716a20ebef4a82bea20d25df8761fa],SizeBytes:365147114,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:32fa95a3d4ecfebe96152242926addf27789555b12413203927f255a712c8a0c],SizeBytes:358569445,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:91308d35c1e56463f55c1aaa519ff4de7335d43b254c21abdb845fc8c72821a1],SizeBytes:347460509,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:255a95e8b514dcdc4fa12436789115739355c13fae93eccbba660298746d9aa1],SizeBytes:346372629,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a4fdcff8b0038d858d46740c9622d5dc428074dcf7e662deacd9e6c0aff18286],SizeBytes:344400730,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:209e20410ec2d3d7a502f568d2b7fe1cd1beadcb36fff2d1e6f59d77be3200e3],SizeBytes:339844669,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:140f8947593d92e1517e50a201e83bdef8eb965b552a21d3caf346a250d0cf6e],SizeBytes:335139477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01de581f7f0c7e43f809e9346ced7e60c0ecdd7c0078e97a8f64b24d4d3fa0b7],SizeBytes:332949901,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ed634acd9b29e16b578c74b38d5425156cd39bb72c0884a9564331f95f80911],SizeBytes:330558591,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs@sha256:83da6a30accf7d97453051973ba8c2ae1f78fa0472d2aeb0f8fcfdd8ba04e11a docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:324891489,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-initcontainer@sha256:f859874cf8ef92e8e806ff615f33472992917545ec94d461caa8e6e13b8a1983 quay.io/app-sre/managed-prometheus-exporter-initcontainer:latest],SizeBytes:315381355,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:923999744ad1557510fc5c3a95cf978b27c453988489ada899c574ab1d71a218],SizeBytes:312722401,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-base@sha256:8da4f871cc18d3ac288a2fbb5c37ea27eabea4314dabca68f3f8469c7d7a4a21 quay.io/app-sre/managed-prometheus-exporter-base:latest],SizeBytes:303857745,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2550b2cbdf864515b1edacf43c25eb6b6f179713c1df34e51f6e9bba48d6430a],SizeBytes:303075904,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9260bbcc285a9fd4c1c7df865280261723eb7679050905da4acbfe728f96ee1b],SizeBytes:298386295,},ContainerImage{Names:[quay.io/konveyor/velero@sha256:6abd52244096680eeb3c80289999a00f642069edcd3d2e6c6948317b4bdd9bcd],SizeBytes:230325612,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder@sha256:afc413cd2504b586b6f28c16355db243c8bfdef536cc80e44f61e03c01e12235],SizeBytes:229564388,},ContainerImage{Names:[quay.io/app-sre/must-gather-operator@sha256:2e9c61f5bb3d7f95805130843b0368853d38934756e892954cdf3e251941ac43],SizeBytes:187166704,},ContainerImage{Names:[quay.io/app-sre/managed-velero-operator@sha256:587741a4e6772774f01efd91ed9b93ed5d0a21fcf9ff1f3bebaf14eb98466132],SizeBytes:155001069,},ContainerImage{Names:[quay.io/konveyor/velero-plugin-for-aws@sha256:7c22d5ae59862a66bac77e3fb48e6cd9c1556e4c9d7277aad4f093a198cb4373],SizeBytes:149334789,},ContainerImage{Names:[quay.io/app-sre/custom-domains-operator@sha256:59158acb6a90927f89d85ffaf448e4831b8e8b352d4539b597b97e5f5ff92f6b],SizeBytes:147473321,},ContainerImage{Names:[quay.io/app-sre/ocm-agent-operator@sha256:d697996b5051ee1fdf174a7d23fdd0f0fe612876e8b0d8e94a0c92fee72ea011],SizeBytes:143355987,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 k8s.gcr.io/e2e-test-images/httpd@sha256:6589a0d3a4b40f996ab554f0d58e8c567c2850c41bc99d05498378a50b7e1cb4 k8s.gcr.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:128894651,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403 k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8 k8s.gcr.io/e2e-test-images/agnhost:2.36],SizeBytes:126252387,},ContainerImage{Names:[quay.io/app-sre/managed-upgrade-operator-registry@sha256:1584ad8c5f28533cad5a72bcf7fc69d03fc7eac87a580897553bdcc01c89f1c6],SizeBytes:126047670,},ContainerImage{Names:[quay.io/app-sre/ocm-agent-operator-registry@sha256:1b80b1e0493e7803f6a498ab5617b30009e9b2bcccca93e82057317eadf1a2cb],SizeBytes:118500277,},ContainerImage{Names:[quay.io/app-sre/custom-domains-operator-registry@sha256:365524ae13f1b5e5b252190c828e7ce9a5c1916a6aba32566326240d676059dc],SizeBytes:118428594,},ContainerImage{Names:[quay.io/app-sre/managed-velero-operator-registry@sha256:70df9cc14d07e0212d2f16b2237483a1fe12bb4b0f3aa3fc4240db0d18387aee],SizeBytes:118411702,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jan 19 00:01:50.674: INFO: 
Logging kubelet events for node ip-10-0-236-5.ec2.internal
Jan 19 00:01:50.677: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-236-5.ec2.internal
Jan 19 00:01:50.701: INFO: network-metrics-daemon-dxnrq started at 2023-01-18 21:03:23 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 00:01:50.701: INFO: multus-additional-cni-plugins-p78ph started at 2023-01-18 21:03:23 +0000 UTC (6+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 00:01:50.701: INFO: observability-operator-catalog-khgpx started at 2023-01-18 21:26:25 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.701: INFO: splunk-forwarder-operator-catalog-twzbw started at 2023-01-18 21:26:27 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.701: INFO: tuned-b2gvw started at 2023-01-18 21:03:23 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container tuned ready: true, restart count 1
Jan 19 00:01:50.701: INFO: custom-domains-operator-7f97f586c8-czz24 started at 2023-01-18 21:26:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container custom-domains-operator ready: true, restart count 0
Jan 19 00:01:50.701: INFO: managed-velero-operator-registry-9j2kp started at 2023-01-18 21:31:21 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.701: INFO: must-gather-operator-registry-gp8pc started at 2023-01-18 21:26:22 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.701: INFO: custom-domains-operator-registry-sd6db started at 2023-01-18 21:26:24 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.701: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-t5zp4 started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 00:01:50.701: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 00:01:50.701: INFO: node-exporter-6q46p started at 2023-01-18 21:08:32 +0000 UTC (1+2 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 00:01:50.701: INFO: cloud-ingress-operator-registry-6f5sh started at 2023-01-18 21:31:22 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.701: INFO: ingress-canary-d6jm7 started at 2023-01-18 21:04:10 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 00:01:50.701: INFO: must-gather-operator-5f47db765d-zf7fj started at 2023-01-18 21:26:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container must-gather-operator ready: true, restart count 0
Jan 19 00:01:50.701: INFO: obo-prometheus-operator-6cb5cfc7b9-2rkdt started at 2023-01-18 21:26:20 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 19 00:01:50.701: INFO: sre-ebs-iops-reporter-1-l2chd started at 2023-01-18 21:31:18 +0000 UTC (1+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Init container setupcreds ready: true, restart count 0
Jan 19 00:01:50.701: INFO: 	Container main ready: true, restart count 0
Jan 19 00:01:50.701: INFO: managed-upgrade-operator-catalog-g29pr started at 2023-01-18 21:31:23 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.701: INFO: dns-default-4cddc started at 2023-01-18 21:04:10 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container dns ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.701: INFO: splunkforwarder-ds-zhvhl started at 2023-01-18 21:25:56 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container splunk-uf ready: true, restart count 0
Jan 19 00:01:50.701: INFO: addon-operator-catalog-67nfj started at 2023-01-18 21:26:21 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.701: INFO: ocm-agent-operator-9bd68bf49-z6qxl started at 2023-01-18 21:26:19 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container ocm-agent-operator ready: true, restart count 0
Jan 19 00:01:50.701: INFO: rbac-permissions-operator-registry-mqh7k started at 2023-01-18 21:26:26 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.701: INFO: machine-config-daemon-xv85b started at 2023-01-18 21:03:23 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 00:01:50.701: INFO: observability-operator-5b467d8ccb-twlnn started at 2023-01-18 21:26:20 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container operator ready: true, restart count 0
Jan 19 00:01:50.701: INFO: multus-wtvrb started at 2023-01-18 21:03:23 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 00:01:50.701: INFO: node-ca-r29sw started at 2023-01-18 21:03:23 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 00:01:50.701: INFO: osd-cluster-ready-kfqk7 started at 2023-01-18 21:26:19 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container osd-cluster-ready ready: false, restart count 35
Jan 19 00:01:50.701: INFO: obo-prometheus-operator-admission-webhook-5b4dc9bff5-f8pm5 started at 2023-01-18 21:26:21 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 00:01:50.701: INFO: ocm-agent-operator-registry-qj69h started at 2023-01-18 21:26:24 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.701: INFO: network-check-target-tr8pn started at 2023-01-18 21:03:23 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 00:01:50.701: INFO: sre-stuck-ebs-vols-1-mjmmn started at 2023-01-18 21:26:18 +0000 UTC (1+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Init container setupcreds ready: true, restart count 0
Jan 19 00:01:50.701: INFO: 	Container main ready: true, restart count 0
Jan 19 00:01:50.701: INFO: sre-dns-latency-exporter-q7lp5 started at 2023-01-18 21:18:07 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container main ready: true, restart count 1
Jan 19 00:01:50.701: INFO: aws-ebs-csi-driver-node-cv9gd started at 2023-01-18 21:03:23 +0000 UTC (0+3 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 00:01:50.701: INFO: ovnkube-node-g6c8t started at 2023-01-18 21:03:23 +0000 UTC (0+5 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 00:01:50.701: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 00:01:50.701: INFO: node-resolver-8lsl6 started at 2023-01-18 21:03:23 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 00:01:50.701: INFO: configure-alertmanager-operator-registry-vzl8c started at 2023-01-18 21:31:25 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 00:01:50.701: INFO: downloads-6f74f6fcbf-w6ht7 started at 2023-01-18 21:31:17 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container download-server ready: true, restart count 0
Jan 19 00:01:50.701: INFO: managed-node-metadata-operator-registry-fn84g started at 2023-01-18 21:31:22 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.701: INFO: 	Container registry-server ready: true, restart count 0
W0119 00:01:50.703796      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 19 00:01:50.726: INFO: 
Latency metrics for node ip-10-0-236-5.ec2.internal
Jan 19 00:01:50.726: INFO: 
Logging node info for node ip-10-0-253-152.ec2.internal
Jan 19 00:01:50.729: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-253-152.ec2.internal    97b1bb86-6430-4e5f-9977-2fd85f6253db 306176 0 2023-01-18 21:03:02 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-1 failure-domain.beta.kubernetes.io/zone:us-east-1a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-253-152.ec2.internal kubernetes.io/os:linux node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-1a topology.kubernetes.io/region:us-east-1 topology.kubernetes.io/zone:us-east-1a] map[cloud.network.openshift.io/egress-ipconfig:[{"interface":"eni-0a8bc1206b380c762","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ipv4":14,"ipv6":15}}] csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0e74592248ae3d3ce"} k8s.ovn.org/host-addresses:["10.0.253.152"] k8s.ovn.org/l3-gateway-config:{"default":{"mode":"shared","interface-id":"br-ex_ip-10-0-253-152.ec2.internal","mac-address":"02:e2:0c:32:ec:b1","ip-addresses":["10.0.253.152/17"],"ip-address":"10.0.253.152/17","next-hops":["10.0.128.1"],"next-hop":"10.0.128.1","node-port-enable":"true","vlan-id":"0"}} k8s.ovn.org/node-chassis-id:8e77c1cf-2007-41db-be37-b3782a8818cd k8s.ovn.org/node-gateway-router-lrp-ifaddr:{"ipv4":"100.64.0.7/16"} k8s.ovn.org/node-mgmt-port-mac-address:3a:95:1e:aa:38:c3 k8s.ovn.org/node-primary-ifaddr:{"ipv4":"10.0.253.152/17"} k8s.ovn.org/node-subnets:{"default":"10.128.10.0/23"} machine.openshift.io/machine:openshift-machine-api/sdcicd-cncf-l8h7s-worker-us-east-1a-wrmcr machineconfiguration.openshift.io/controlPlaneTopology:HighlyAvailable machineconfiguration.openshift.io/currentConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredConfig:rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/desiredDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/lastAppliedDrain:uncordon-rendered-worker-af0537f98f02ab1d77a12933e3698455 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done managed.openshift.com/customlabels: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{ancient-changes Update v1 2023-01-18 21:03:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cloud.network.openshift.io/egress-ipconfig":{},"f:k8s.ovn.org/node-gateway-router-lrp-ifaddr":{},"f:k8s.ovn.org/node-subnets":{}}}} } {kubelet Update v1 2023-01-18 21:03:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/instance-type":{},"f:beta.kubernetes.io/os":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}}} } {nodelink-controller Update v1 2023-01-18 21:03:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}} } {ip-10-0-253-152 Update v1 2023-01-18 21:03:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/host-addresses":{},"f:k8s.ovn.org/l3-gateway-config":{},"f:k8s.ovn.org/node-chassis-id":{},"f:k8s.ovn.org/node-mgmt-port-mac-address":{},"f:k8s.ovn.org/node-primary-ifaddr":{}}}} } {machine-config-daemon Update v1 2023-01-18 21:03:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/desiredDrain":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}} } {manager Update v1 2023-01-18 21:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:managed.openshift.com/customlabels":{}}}} } {machine-config-controller Update v1 2023-01-18 21:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/controlPlaneTopology":{},"f:machineconfiguration.openshift.io/desiredConfig":{},"f:machineconfiguration.openshift.io/lastAppliedDrain":{}}}} } {kubelet Update v1 2023-01-18 22:51:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}},"f:labels":{"f:topology.ebs.csi.aws.com/zone":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{}}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:aws:///us-east-1a/i-0e74592248ae3d3ce,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321574121472 0} {<nil>} 314037228Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16487137280 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3920 -3} {<nil>} 3920m DecimalSI},ephemeral-storage: {{289416708846 0} {<nil>} 289416708846 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{13697925120 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2023-01-18 23:57:47 +0000 UTC,LastTransitionTime:2023-01-18 21:30:40 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2023-01-18 23:57:47 +0000 UTC,LastTransitionTime:2023-01-18 21:30:40 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2023-01-18 23:57:47 +0000 UTC,LastTransitionTime:2023-01-18 21:30:40 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2023-01-18 23:57:47 +0000 UTC,LastTransitionTime:2023-01-18 21:30:40 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.253.152,},NodeAddress{Type:Hostname,Address:ip-10-0-253-152.ec2.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-253-152.ec2.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec24fa9bda8e441ce667ebd3ad1fbec6,SystemUUID:ec24fa9b-da8e-441c-e667-ebd3ad1fbec6,BootID:0478c97d-2400-438e-8e0d-403cc784e17a,KernelVersion:4.18.0-372.19.1.el8_6.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 411.86.202208031059-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.24.1-11.rhaos4.11.gitb0d2ef3.el8,KubeletVersion:v1.24.0+9546431,KubeProxyVersion:v1.24.0+9546431,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc0a54cd1e11e92cfefc261305b3d74c9d74c88fa9a98884da8140436ec2ad3],SizeBytes:1057821807,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b741193c0e8f5dd0b254c4185b936a9dd2907f53296a82d2db26bb290a5040a2],SizeBytes:733158564,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08ab62da9265ef67f4eb4b526b38346a3ae3d35e2d5bfbdd58a0858b76b21f77],SizeBytes:681959546,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2075075c139cc7e067d21078981f9bb0a1299bb64a17af2971a95cce92b890],SizeBytes:548228687,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3780cc6fb81b9b81169398ed95ace94cc38bf43a3c9684a019ee791ff49b343b],SizeBytes:520897179,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fce157944c5b0cb0a8ad7c6df7bc78c265e70b547a0e630efe7dd409bf90340d],SizeBytes:503580749,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5a7a916ddb0e80c917ef4a79dba4b1bc5a9ce0c7a81ff4e17d513c314b34328],SizeBytes:491467400,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b01556f148e521a9a8c3ce7ee22ef0456aa2dc86587bfa80ccf8b99d06fdd6d5],SizeBytes:466890183,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8836b1df44fc883c9c0369487c9f9e37ce4339ac735b937f6823bca22e887fdf],SizeBytes:459839859,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cad43cf9f6fbf861e09237e05c5765d15af9a8bd7ee82e557443d8ecba381f56],SizeBytes:454652760,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4fda9e768456eb600936637f1a2a1457d8403f7295d3585b2ca3bd97d94eb45],SizeBytes:454639046,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:44d6ec5354600c25d5cbfa43e2365a0971d6d6e109bf3729e676009c7db670cf],SizeBytes:420100955,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3abe65df892d91b6f219983bd4ecc71ef24a78bbc4c779ef11c5cebf3bff6879],SizeBytes:410815079,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f1134e2e28f44375c3bd9a6ee34d7f9972fdbd3305a3ee2b95e6ed3cede02140],SizeBytes:409661973,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a30dcfcc48b7d53fa6ed89d93e7e45cf8633499f03b7a52417a86913991269f2],SizeBytes:408153575,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9b2b7b7dc13abc6c7791940ffd0f0e74c11a9929f8eff4df33810234f70c8a1],SizeBytes:408114990,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:36fc214537c763b3a3f0a9dc7a1bd4378a80428c31b2629df8786a9b09155e6d],SizeBytes:398604719,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:251710917b12b11b2fd04fe5f7b25e0d493c41c2486fd097b40c1a6ceab0d7ff],SizeBytes:397867888,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab0d2f0b6d01a4a3b9952710cc49e787ebb0e8649a288438774c1ffe0fc3fae0],SizeBytes:392510137,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c956edcee9b9ba5462572b65b6a92983b20ace63dae50e3237bfdbd6d8c0b972],SizeBytes:375087838,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5e1c69d005727e3245604cfca7a63e4f9bc6e15128c7489e41d5e967305089e],SizeBytes:371248553,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:77c5db690d9438ac077736cad8f28c04de476c04c3a97f39910ed86b6c395b85],SizeBytes:368328246,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f3ebf99182733dff2666e6a5e7e217085e06029362036ad79c91278e69dcbf7],SizeBytes:366270626,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:91308d35c1e56463f55c1aaa519ff4de7335d43b254c21abdb845fc8c72821a1],SizeBytes:347460509,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:255a95e8b514dcdc4fa12436789115739355c13fae93eccbba660298746d9aa1],SizeBytes:346372629,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a4fdcff8b0038d858d46740c9622d5dc428074dcf7e662deacd9e6c0aff18286],SizeBytes:344400730,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:209e20410ec2d3d7a502f568d2b7fe1cd1beadcb36fff2d1e6f59d77be3200e3],SizeBytes:339844669,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:140f8947593d92e1517e50a201e83bdef8eb965b552a21d3caf346a250d0cf6e],SizeBytes:335139477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01de581f7f0c7e43f809e9346ced7e60c0ecdd7c0078e97a8f64b24d4d3fa0b7],SizeBytes:332949901,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ed634acd9b29e16b578c74b38d5425156cd39bb72c0884a9564331f95f80911],SizeBytes:330558591,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs@sha256:83da6a30accf7d97453051973ba8c2ae1f78fa0472d2aeb0f8fcfdd8ba04e11a docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:324891489,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-initcontainer@sha256:f859874cf8ef92e8e806ff615f33472992917545ec94d461caa8e6e13b8a1983 quay.io/app-sre/managed-prometheus-exporter-initcontainer:latest],SizeBytes:315381355,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:923999744ad1557510fc5c3a95cf978b27c453988489ada899c574ab1d71a218],SizeBytes:312722401,},ContainerImage{Names:[quay.io/app-sre/managed-prometheus-exporter-base@sha256:8da4f871cc18d3ac288a2fbb5c37ea27eabea4314dabca68f3f8469c7d7a4a21 quay.io/app-sre/managed-prometheus-exporter-base:latest],SizeBytes:303857745,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2550b2cbdf864515b1edacf43c25eb6b6f179713c1df34e51f6e9bba48d6430a],SizeBytes:303075904,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9260bbcc285a9fd4c1c7df865280261723eb7679050905da4acbfe728f96ee1b],SizeBytes:298386295,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder@sha256:afc413cd2504b586b6f28c16355db243c8bfdef536cc80e44f61e03c01e12235],SizeBytes:229564388,},ContainerImage{Names:[quay.io/app-sre/must-gather-operator@sha256:2e9c61f5bb3d7f95805130843b0368853d38934756e892954cdf3e251941ac43],SizeBytes:187166704,},ContainerImage{Names:[quay.io/app-sre/custom-domains-operator@sha256:59158acb6a90927f89d85ffaf448e4831b8e8b352d4539b597b97e5f5ff92f6b],SizeBytes:147473321,},ContainerImage{Names:[quay.io/app-sre/ocm-agent-operator@sha256:d697996b5051ee1fdf174a7d23fdd0f0fe612876e8b0d8e94a0c92fee72ea011],SizeBytes:143355987,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 k8s.gcr.io/e2e-test-images/httpd@sha256:6589a0d3a4b40f996ab554f0d58e8c567c2850c41bc99d05498378a50b7e1cb4 k8s.gcr.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:128894651,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403 k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8 k8s.gcr.io/e2e-test-images/agnhost:2.36],SizeBytes:126252387,},ContainerImage{Names:[quay.io/app-sre/ocm-agent-operator-registry@sha256:1b80b1e0493e7803f6a498ab5617b30009e9b2bcccca93e82057317eadf1a2cb],SizeBytes:118500277,},ContainerImage{Names:[quay.io/app-sre/custom-domains-operator-registry@sha256:365524ae13f1b5e5b252190c828e7ce9a5c1916a6aba32566326240d676059dc],SizeBytes:118428594,},ContainerImage{Names:[quay.io/app-sre/must-gather-operator-registry@sha256:b8833c045365b8f0eaa0ca353672d3bf117bf856fcfc66369c438cfb1fc39649],SizeBytes:117960118,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator@sha256:cfb005c1b5144447dad26c5e7ccbf486bfa82d52673488356fc9522dc8420f69],SizeBytes:111630880,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder-operator-registry@sha256:883305a3920f97afac14ab10d39b74c4a34d30c342e35f8fa225a1d9f01c3730],SizeBytes:110390713,},ContainerImage{Names:[quay.io/app-sre/rbac-permissions-operator-registry@sha256:75a26e1a178730f3e8d81d3b41aae78a5b05b8812569d4252b42a1fbe2b1452d],SizeBytes:108948408,},ContainerImage{Names:[quay.io/app-sre/osd-cluster-ready@sha256:5a346ccfce791546a717d8b1a156707a3a1a9b0efd2f833d60ca4d32f0ff751c],SizeBytes:97521241,},ContainerImage{Names:[quay.io/rhobs/observability-operator-catalog@sha256:f1caec528bfdf530da8d99456b293185715712989484ffa3df9c21f4e8d77030],SizeBytes:78895244,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Jan 19 00:01:50.729: INFO: 
Logging kubelet events for node ip-10-0-253-152.ec2.internal
Jan 19 00:01:50.734: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-253-152.ec2.internal
Jan 19 00:01:50.752: INFO: ovnkube-node-mtfln started at 2023-01-18 21:03:06 +0000 UTC (0+5 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Container ovn-acl-logging ready: true, restart count 2
Jan 19 00:01:50.752: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 00:01:50.752: INFO: ingress-canary-vz8dj started at 2023-01-18 21:03:53 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 00:01:50.752: INFO: network-check-source-5cb989cf6f-42gl2 started at 2023-01-18 21:31:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container check-endpoints ready: true, restart count 0
Jan 19 00:01:50.752: INFO: machine-config-daemon-5zsxx started at 2023-01-18 21:03:06 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 00:01:50.752: INFO: node-ca-bh7jz started at 2023-01-18 21:03:06 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 00:01:50.752: INFO: network-metrics-daemon-cq5sl started at 2023-01-18 21:03:06 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 00:01:50.752: INFO: network-check-target-9fq7l started at 2023-01-18 21:03:06 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 00:01:50.752: INFO: image-registry-7b8f8dcdc5-2bvm7 started at 2023-01-18 21:31:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container registry ready: true, restart count 0
Jan 19 00:01:50.752: INFO: sonobuoy-systemd-logs-daemon-set-034419193cd64c26-ktxs4 started at 2023-01-18 22:14:37 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 00:01:50.752: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 00:01:50.752: INFO: tuned-5wpdt started at 2023-01-18 21:03:06 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container tuned ready: true, restart count 1
Jan 19 00:01:50.752: INFO: multus-xzh2h started at 2023-01-18 21:03:06 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 00:01:50.752: INFO: aws-ebs-csi-driver-node-97pk2 started at 2023-01-18 21:03:06 +0000 UTC (0+3 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 00:01:50.752: INFO: dns-default-g4nw5 started at 2023-01-18 21:03:53 +0000 UTC (0+2 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container dns ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.752: INFO: node-exporter-z5k8l started at 2023-01-18 21:08:32 +0000 UTC (1+2 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Init container init-textfile ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 00:01:50.752: INFO: sre-dns-latency-exporter-zsmln started at 2023-01-18 21:18:07 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container main ready: true, restart count 1
Jan 19 00:01:50.752: INFO: multus-additional-cni-plugins-hsjtm started at 2023-01-18 21:03:06 +0000 UTC (6+1 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Init container egress-router-binary-copy ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Init container cni-plugins ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Init container bond-cni-plugin ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Init container routeoverride-cni ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Init container whereabouts-cni ready: true, restart count 1
Jan 19 00:01:50.752: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 00:01:50.752: INFO: node-resolver-ffd24 started at 2023-01-18 21:03:06 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 00:01:50.752: INFO: splunkforwarder-ds-xsjtq started at 2023-01-18 21:24:58 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 00:01:50.752: INFO: cloud-ingress-operator-78d58985cd-9kp9q started at 2023-01-18 21:31:18 +0000 UTC (0+1 container statuses recorded)
Jan 19 00:01:50.752: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
W0119 00:01:50.754662      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
Jan 19 00:01:50.807: INFO: 
Latency metrics for node ip-10-0-253-152.ec2.internal
Jan 19 00:01:50.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2068" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• Failure [218.498 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance] [It]
  test/e2e/framework/framework.go:652

  Jan 19 00:01:47.384: Session is sticky after reaching the timeout

  test/e2e/network/service.go:3367
------------------------------
{"msg":"FAILED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":332,"skipped":6192,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:01:50.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jan 19 00:01:51.902: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0119 00:01:51.902332      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0119 00:01:51.902350      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 19 00:01:51.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1670" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":356,"completed":333,"skipped":6212,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:01:51.921: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Starting the proxy
Jan 19 00:01:51.944: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=kubectl-7829 proxy --unix-socket=/tmp/kubectl-proxy-unix3162298738/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 19 00:01:51.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7829" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":356,"completed":334,"skipped":6296,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:01:51.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Jan 19 00:01:52.098: INFO: pods: 0 < 3
Jan 19 00:01:54.101: INFO: running pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jan 19 00:02:00.159: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 19 00:02:02.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4833" for this suite.

• [SLOW TEST:10.202 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":356,"completed":335,"skipped":6305,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:02:02.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 19 00:02:15.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4272" for this suite.

• [SLOW TEST:13.100 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":356,"completed":336,"skipped":6316,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:02:15.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 19 00:02:15.391: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 00:02:17.394: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 00:02:19.394: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 19 00:02:19.408: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 19 00:02:21.413: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 19 00:02:23.412: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 19 00:02:23.420: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 19 00:02:23.422: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 19 00:02:25.423: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 19 00:02:25.425: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 19 00:02:25.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9134" for this suite.

• [SLOW TEST:10.142 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":356,"completed":337,"skipped":6324,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:02:25.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 19 00:02:31.551: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 19 00:02:31.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-308" for this suite.

• [SLOW TEST:6.135 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":338,"skipped":6327,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:02:31.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Jan 19 00:02:32.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9163" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":339,"skipped":6348,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:02:32.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test service account token: 
W0119 00:02:32.227099      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 00:02:32.227: INFO: Waiting up to 5m0s for pod "test-pod-b9eed30e-8efa-429e-856f-ceae61a8fd5c" in namespace "svcaccounts-9273" to be "Succeeded or Failed"
Jan 19 00:02:32.279: INFO: Pod "test-pod-b9eed30e-8efa-429e-856f-ceae61a8fd5c": Phase="Pending", Reason="", readiness=false. Elapsed: 52.742354ms
Jan 19 00:02:34.283: INFO: Pod "test-pod-b9eed30e-8efa-429e-856f-ceae61a8fd5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056114621s
Jan 19 00:02:36.287: INFO: Pod "test-pod-b9eed30e-8efa-429e-856f-ceae61a8fd5c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059852092s
Jan 19 00:02:38.290: INFO: Pod "test-pod-b9eed30e-8efa-429e-856f-ceae61a8fd5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06293663s
STEP: Saw pod success
Jan 19 00:02:38.290: INFO: Pod "test-pod-b9eed30e-8efa-429e-856f-ceae61a8fd5c" satisfied condition "Succeeded or Failed"
Jan 19 00:02:38.292: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod test-pod-b9eed30e-8efa-429e-856f-ceae61a8fd5c container agnhost-container: <nil>
STEP: delete the pod
Jan 19 00:02:38.306: INFO: Waiting for pod test-pod-b9eed30e-8efa-429e-856f-ceae61a8fd5c to disappear
Jan 19 00:02:38.308: INFO: Pod test-pod-b9eed30e-8efa-429e-856f-ceae61a8fd5c no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 19 00:02:38.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9273" for this suite.

• [SLOW TEST:6.217 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":356,"completed":340,"skipped":6353,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:02:38.319: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-25de5774-3bec-4b23-9989-c95c6808c358
STEP: Creating a pod to test consume secrets
Jan 19 00:02:38.400: INFO: Waiting up to 5m0s for pod "pod-secrets-16a25c05-fc28-4c8a-b8a0-ebb081f5caee" in namespace "secrets-4988" to be "Succeeded or Failed"
Jan 19 00:02:38.404: INFO: Pod "pod-secrets-16a25c05-fc28-4c8a-b8a0-ebb081f5caee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.972317ms
Jan 19 00:02:40.406: INFO: Pod "pod-secrets-16a25c05-fc28-4c8a-b8a0-ebb081f5caee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006070102s
Jan 19 00:02:42.410: INFO: Pod "pod-secrets-16a25c05-fc28-4c8a-b8a0-ebb081f5caee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00990705s
Jan 19 00:02:44.413: INFO: Pod "pod-secrets-16a25c05-fc28-4c8a-b8a0-ebb081f5caee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013011221s
STEP: Saw pod success
Jan 19 00:02:44.413: INFO: Pod "pod-secrets-16a25c05-fc28-4c8a-b8a0-ebb081f5caee" satisfied condition "Succeeded or Failed"
Jan 19 00:02:44.415: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-secrets-16a25c05-fc28-4c8a-b8a0-ebb081f5caee container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 00:02:44.426: INFO: Waiting for pod pod-secrets-16a25c05-fc28-4c8a-b8a0-ebb081f5caee to disappear
Jan 19 00:02:44.428: INFO: Pod pod-secrets-16a25c05-fc28-4c8a-b8a0-ebb081f5caee no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 19 00:02:44.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4988" for this suite.

• [SLOW TEST:6.118 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":341,"skipped":6363,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:02:44.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-6415
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 19 00:02:44.462: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 19 00:02:44.694: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 00:02:46.696: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 00:02:48.696: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 00:02:50.697: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 00:02:52.697: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 00:02:54.697: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 00:02:56.696: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 00:02:58.698: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 00:03:00.697: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 00:03:02.697: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 00:03:04.698: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 00:03:06.698: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 19 00:03:06.702: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 19 00:03:06.707: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jan 19 00:03:06.710: INFO: The status of Pod netserver-3 is Running (Ready = true)
Jan 19 00:03:06.715: INFO: The status of Pod netserver-4 is Running (Ready = true)
Jan 19 00:03:06.718: INFO: The status of Pod netserver-5 is Running (Ready = true)
STEP: Creating test pods
Jan 19 00:03:10.754: INFO: Setting MaxTries for pod polling to 66 for networking test based on endpoint count 6
Jan 19 00:03:10.754: INFO: Going to poll 10.128.14.132 on port 8081 at least 0 times, with a maximum of 66 tries before failing
Jan 19 00:03:10.756: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.14.132 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 00:03:10.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 19 00:03:10.756: INFO: ExecWithOptions: Clientset creation
Jan 19 00:03:10.757: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-6415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.14.132+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 00:03:11.875: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 19 00:03:11.875: INFO: Going to poll 10.128.6.98 on port 8081 at least 0 times, with a maximum of 66 tries before failing
Jan 19 00:03:11.877: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.6.98 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 00:03:11.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 19 00:03:11.878: INFO: ExecWithOptions: Clientset creation
Jan 19 00:03:11.878: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-6415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.6.98+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 00:03:12.979: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 19 00:03:12.979: INFO: Going to poll 10.128.16.216 on port 8081 at least 0 times, with a maximum of 66 tries before failing
Jan 19 00:03:12.982: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.16.216 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 00:03:12.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 19 00:03:12.982: INFO: ExecWithOptions: Clientset creation
Jan 19 00:03:12.982: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-6415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.16.216+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 00:03:14.082: INFO: Found all 1 expected endpoints: [netserver-2]
Jan 19 00:03:14.082: INFO: Going to poll 10.128.13.84 on port 8081 at least 0 times, with a maximum of 66 tries before failing
Jan 19 00:03:14.087: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.13.84 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 00:03:14.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 19 00:03:14.088: INFO: ExecWithOptions: Clientset creation
Jan 19 00:03:14.088: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-6415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.13.84+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 00:03:15.201: INFO: Found all 1 expected endpoints: [netserver-3]
Jan 19 00:03:15.201: INFO: Going to poll 10.128.8.96 on port 8081 at least 0 times, with a maximum of 66 tries before failing
Jan 19 00:03:15.205: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.8.96 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 00:03:15.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 19 00:03:15.205: INFO: ExecWithOptions: Clientset creation
Jan 19 00:03:15.205: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-6415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.8.96+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 00:03:16.321: INFO: Found all 1 expected endpoints: [netserver-4]
Jan 19 00:03:16.321: INFO: Going to poll 10.128.10.59 on port 8081 at least 0 times, with a maximum of 66 tries before failing
Jan 19 00:03:16.323: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.10.59 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 00:03:16.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
Jan 19 00:03:16.323: INFO: ExecWithOptions: Clientset creation
Jan 19 00:03:16.323: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-6415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.10.59+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 00:03:17.455: INFO: Found all 1 expected endpoints: [netserver-5]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 19 00:03:17.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6415" for this suite.

• [SLOW TEST:33.027 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":342,"skipped":6401,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:03:17.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Jan 19 00:03:17.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3812" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":343,"skipped":6455,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:03:17.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-a055d3db-37ce-42f8-9296-a90f1127a112
STEP: Creating a pod to test consume configMaps
Jan 19 00:03:18.040: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1ebd31d7-3f6c-4aef-8e61-fbfc7674704e" in namespace "projected-2284" to be "Succeeded or Failed"
Jan 19 00:03:18.043: INFO: Pod "pod-projected-configmaps-1ebd31d7-3f6c-4aef-8e61-fbfc7674704e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.882661ms
Jan 19 00:03:20.046: INFO: Pod "pod-projected-configmaps-1ebd31d7-3f6c-4aef-8e61-fbfc7674704e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005319704s
Jan 19 00:03:22.048: INFO: Pod "pod-projected-configmaps-1ebd31d7-3f6c-4aef-8e61-fbfc7674704e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007914627s
Jan 19 00:03:24.052: INFO: Pod "pod-projected-configmaps-1ebd31d7-3f6c-4aef-8e61-fbfc7674704e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011344443s
STEP: Saw pod success
Jan 19 00:03:24.052: INFO: Pod "pod-projected-configmaps-1ebd31d7-3f6c-4aef-8e61-fbfc7674704e" satisfied condition "Succeeded or Failed"
Jan 19 00:03:24.055: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-projected-configmaps-1ebd31d7-3f6c-4aef-8e61-fbfc7674704e container agnhost-container: <nil>
STEP: delete the pod
Jan 19 00:03:24.069: INFO: Waiting for pod pod-projected-configmaps-1ebd31d7-3f6c-4aef-8e61-fbfc7674704e to disappear
Jan 19 00:03:24.071: INFO: Pod pod-projected-configmaps-1ebd31d7-3f6c-4aef-8e61-fbfc7674704e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 19 00:03:24.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2284" for this suite.

• [SLOW TEST:6.111 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":344,"skipped":6468,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:03:24.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 19 00:03:24.122: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 19 00:03:29.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7258" for this suite.

• [SLOW TEST:5.085 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":356,"completed":345,"skipped":6475,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:03:29.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 19 00:04:30.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7925" for this suite.

• [SLOW TEST:61.051 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":356,"completed":346,"skipped":6487,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:04:30.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8437
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Jan 19 00:04:30.306: INFO: Found 0 stateful pods, waiting for 3
Jan 19 00:04:40.309: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 00:04:40.309: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 00:04:40.309: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan 19 00:04:50.313: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 00:04:50.313: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 00:04:50.313: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jan 19 00:04:50.337: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 19 00:05:00.364: INFO: Updating stateful set ss2
Jan 19 00:05:00.374: INFO: Waiting for Pod statefulset-8437/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Jan 19 00:05:10.406: INFO: Found 1 stateful pods, waiting for 3
Jan 19 00:05:20.413: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 00:05:20.413: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 00:05:20.413: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 19 00:05:20.435: INFO: Updating stateful set ss2
Jan 19 00:05:20.441: INFO: Waiting for Pod statefulset-8437/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Jan 19 00:05:30.468: INFO: Updating stateful set ss2
Jan 19 00:05:30.473: INFO: Waiting for StatefulSet statefulset-8437/ss2 to complete update
Jan 19 00:05:30.473: INFO: Waiting for Pod statefulset-8437/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 00:05:40.479: INFO: Deleting all statefulset in ns statefulset-8437
Jan 19 00:05:40.481: INFO: Scaling statefulset ss2 to 0
Jan 19 00:05:50.499: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 00:05:50.501: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 19 00:05:50.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8437" for this suite.

• [SLOW TEST:80.301 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":356,"completed":347,"skipped":6494,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:05:50.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0119 00:06:30.961875      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0119 00:06:30.979812      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 19 00:06:30.993: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 19 00:06:30.993: INFO: Deleting pod "simpletest.rc-25mdb" in namespace "gc-1172"
Jan 19 00:06:31.028: INFO: Deleting pod "simpletest.rc-2bqms" in namespace "gc-1172"
Jan 19 00:06:31.044: INFO: Deleting pod "simpletest.rc-2ckhx" in namespace "gc-1172"
Jan 19 00:06:31.091: INFO: Deleting pod "simpletest.rc-2kkhs" in namespace "gc-1172"
Jan 19 00:06:31.127: INFO: Deleting pod "simpletest.rc-2xhdp" in namespace "gc-1172"
Jan 19 00:06:31.164: INFO: Deleting pod "simpletest.rc-44hdt" in namespace "gc-1172"
Jan 19 00:06:31.204: INFO: Deleting pod "simpletest.rc-45xrw" in namespace "gc-1172"
Jan 19 00:06:31.226: INFO: Deleting pod "simpletest.rc-46mbw" in namespace "gc-1172"
Jan 19 00:06:31.283: INFO: Deleting pod "simpletest.rc-48knj" in namespace "gc-1172"
Jan 19 00:06:31.323: INFO: Deleting pod "simpletest.rc-4tv8q" in namespace "gc-1172"
Jan 19 00:06:31.372: INFO: Deleting pod "simpletest.rc-4vbpw" in namespace "gc-1172"
Jan 19 00:06:31.417: INFO: Deleting pod "simpletest.rc-58zgj" in namespace "gc-1172"
Jan 19 00:06:31.447: INFO: Deleting pod "simpletest.rc-5dhw9" in namespace "gc-1172"
Jan 19 00:06:31.465: INFO: Deleting pod "simpletest.rc-5gk7m" in namespace "gc-1172"
Jan 19 00:06:31.476: INFO: Deleting pod "simpletest.rc-5lr5k" in namespace "gc-1172"
Jan 19 00:06:31.499: INFO: Deleting pod "simpletest.rc-5n5g7" in namespace "gc-1172"
Jan 19 00:06:31.507: INFO: Deleting pod "simpletest.rc-5pljt" in namespace "gc-1172"
Jan 19 00:06:31.521: INFO: Deleting pod "simpletest.rc-5qzv5" in namespace "gc-1172"
Jan 19 00:06:31.545: INFO: Deleting pod "simpletest.rc-64c9q" in namespace "gc-1172"
Jan 19 00:06:31.553: INFO: Deleting pod "simpletest.rc-655cr" in namespace "gc-1172"
Jan 19 00:06:31.567: INFO: Deleting pod "simpletest.rc-69f9r" in namespace "gc-1172"
Jan 19 00:06:31.612: INFO: Deleting pod "simpletest.rc-6ss85" in namespace "gc-1172"
Jan 19 00:06:31.622: INFO: Deleting pod "simpletest.rc-7zwqr" in namespace "gc-1172"
Jan 19 00:06:31.634: INFO: Deleting pod "simpletest.rc-8fsgd" in namespace "gc-1172"
Jan 19 00:06:31.649: INFO: Deleting pod "simpletest.rc-8ml5d" in namespace "gc-1172"
Jan 19 00:06:31.662: INFO: Deleting pod "simpletest.rc-8qb5d" in namespace "gc-1172"
Jan 19 00:06:31.670: INFO: Deleting pod "simpletest.rc-92cs2" in namespace "gc-1172"
Jan 19 00:06:31.678: INFO: Deleting pod "simpletest.rc-9bd4x" in namespace "gc-1172"
Jan 19 00:06:31.695: INFO: Deleting pod "simpletest.rc-9k8m5" in namespace "gc-1172"
Jan 19 00:06:31.710: INFO: Deleting pod "simpletest.rc-9nmcx" in namespace "gc-1172"
Jan 19 00:06:31.737: INFO: Deleting pod "simpletest.rc-b95zj" in namespace "gc-1172"
Jan 19 00:06:31.752: INFO: Deleting pod "simpletest.rc-bbmn4" in namespace "gc-1172"
Jan 19 00:06:31.768: INFO: Deleting pod "simpletest.rc-bmzft" in namespace "gc-1172"
Jan 19 00:06:31.780: INFO: Deleting pod "simpletest.rc-brs5k" in namespace "gc-1172"
Jan 19 00:06:31.799: INFO: Deleting pod "simpletest.rc-ccp2b" in namespace "gc-1172"
Jan 19 00:06:31.821: INFO: Deleting pod "simpletest.rc-cpq54" in namespace "gc-1172"
Jan 19 00:06:31.834: INFO: Deleting pod "simpletest.rc-csggs" in namespace "gc-1172"
Jan 19 00:06:31.853: INFO: Deleting pod "simpletest.rc-cwvbg" in namespace "gc-1172"
Jan 19 00:06:31.864: INFO: Deleting pod "simpletest.rc-cxfct" in namespace "gc-1172"
Jan 19 00:06:31.875: INFO: Deleting pod "simpletest.rc-d2mj4" in namespace "gc-1172"
Jan 19 00:06:31.896: INFO: Deleting pod "simpletest.rc-d8b8t" in namespace "gc-1172"
Jan 19 00:06:31.909: INFO: Deleting pod "simpletest.rc-dhnzv" in namespace "gc-1172"
Jan 19 00:06:31.924: INFO: Deleting pod "simpletest.rc-dmdg5" in namespace "gc-1172"
Jan 19 00:06:31.933: INFO: Deleting pod "simpletest.rc-dwjd5" in namespace "gc-1172"
Jan 19 00:06:31.942: INFO: Deleting pod "simpletest.rc-f72h7" in namespace "gc-1172"
Jan 19 00:06:31.955: INFO: Deleting pod "simpletest.rc-g7vvm" in namespace "gc-1172"
Jan 19 00:06:31.968: INFO: Deleting pod "simpletest.rc-gnmgh" in namespace "gc-1172"
Jan 19 00:06:31.982: INFO: Deleting pod "simpletest.rc-gpwpb" in namespace "gc-1172"
Jan 19 00:06:31.996: INFO: Deleting pod "simpletest.rc-h6mz7" in namespace "gc-1172"
Jan 19 00:06:32.010: INFO: Deleting pod "simpletest.rc-hl4kp" in namespace "gc-1172"
Jan 19 00:06:32.025: INFO: Deleting pod "simpletest.rc-hlm4h" in namespace "gc-1172"
Jan 19 00:06:32.052: INFO: Deleting pod "simpletest.rc-hr6bw" in namespace "gc-1172"
Jan 19 00:06:32.142: INFO: Deleting pod "simpletest.rc-hscfq" in namespace "gc-1172"
Jan 19 00:06:32.151: INFO: Deleting pod "simpletest.rc-hwkm4" in namespace "gc-1172"
Jan 19 00:06:32.167: INFO: Deleting pod "simpletest.rc-jcbrq" in namespace "gc-1172"
Jan 19 00:06:32.175: INFO: Deleting pod "simpletest.rc-k2m4c" in namespace "gc-1172"
Jan 19 00:06:32.189: INFO: Deleting pod "simpletest.rc-kn85h" in namespace "gc-1172"
Jan 19 00:06:32.202: INFO: Deleting pod "simpletest.rc-kq5pf" in namespace "gc-1172"
Jan 19 00:06:32.212: INFO: Deleting pod "simpletest.rc-kr6wt" in namespace "gc-1172"
Jan 19 00:06:32.222: INFO: Deleting pod "simpletest.rc-lh84q" in namespace "gc-1172"
Jan 19 00:06:32.230: INFO: Deleting pod "simpletest.rc-lqdh9" in namespace "gc-1172"
Jan 19 00:06:32.241: INFO: Deleting pod "simpletest.rc-lxc69" in namespace "gc-1172"
Jan 19 00:06:32.317: INFO: Deleting pod "simpletest.rc-m77wx" in namespace "gc-1172"
Jan 19 00:06:32.327: INFO: Deleting pod "simpletest.rc-mqpjg" in namespace "gc-1172"
Jan 19 00:06:32.336: INFO: Deleting pod "simpletest.rc-n69d4" in namespace "gc-1172"
Jan 19 00:06:32.346: INFO: Deleting pod "simpletest.rc-npkj4" in namespace "gc-1172"
Jan 19 00:06:32.365: INFO: Deleting pod "simpletest.rc-nsqpb" in namespace "gc-1172"
Jan 19 00:06:32.377: INFO: Deleting pod "simpletest.rc-p4ts4" in namespace "gc-1172"
Jan 19 00:06:32.386: INFO: Deleting pod "simpletest.rc-pc4fq" in namespace "gc-1172"
Jan 19 00:06:32.395: INFO: Deleting pod "simpletest.rc-pkp5h" in namespace "gc-1172"
Jan 19 00:06:32.406: INFO: Deleting pod "simpletest.rc-ppfxw" in namespace "gc-1172"
Jan 19 00:06:32.419: INFO: Deleting pod "simpletest.rc-ptcvv" in namespace "gc-1172"
Jan 19 00:06:32.443: INFO: Deleting pod "simpletest.rc-qbjm2" in namespace "gc-1172"
Jan 19 00:06:32.462: INFO: Deleting pod "simpletest.rc-qc4rb" in namespace "gc-1172"
Jan 19 00:06:32.472: INFO: Deleting pod "simpletest.rc-qmgs9" in namespace "gc-1172"
Jan 19 00:06:32.486: INFO: Deleting pod "simpletest.rc-qn5mq" in namespace "gc-1172"
Jan 19 00:06:32.495: INFO: Deleting pod "simpletest.rc-r2tr5" in namespace "gc-1172"
Jan 19 00:06:32.506: INFO: Deleting pod "simpletest.rc-rdbqr" in namespace "gc-1172"
Jan 19 00:06:32.519: INFO: Deleting pod "simpletest.rc-rg4cl" in namespace "gc-1172"
Jan 19 00:06:32.532: INFO: Deleting pod "simpletest.rc-rsjxj" in namespace "gc-1172"
Jan 19 00:06:32.542: INFO: Deleting pod "simpletest.rc-sjp4t" in namespace "gc-1172"
Jan 19 00:06:32.575: INFO: Deleting pod "simpletest.rc-srgsm" in namespace "gc-1172"
Jan 19 00:06:32.617: INFO: Deleting pod "simpletest.rc-sx4mm" in namespace "gc-1172"
Jan 19 00:06:32.664: INFO: Deleting pod "simpletest.rc-td47j" in namespace "gc-1172"
Jan 19 00:06:32.719: INFO: Deleting pod "simpletest.rc-tjq4f" in namespace "gc-1172"
Jan 19 00:06:32.777: INFO: Deleting pod "simpletest.rc-tptdn" in namespace "gc-1172"
Jan 19 00:06:32.815: INFO: Deleting pod "simpletest.rc-tt2j5" in namespace "gc-1172"
Jan 19 00:06:32.865: INFO: Deleting pod "simpletest.rc-twj6r" in namespace "gc-1172"
Jan 19 00:06:32.915: INFO: Deleting pod "simpletest.rc-v4c64" in namespace "gc-1172"
Jan 19 00:06:32.965: INFO: Deleting pod "simpletest.rc-vk6l5" in namespace "gc-1172"
Jan 19 00:06:33.016: INFO: Deleting pod "simpletest.rc-w48dh" in namespace "gc-1172"
Jan 19 00:06:33.064: INFO: Deleting pod "simpletest.rc-w7rpd" in namespace "gc-1172"
Jan 19 00:06:33.114: INFO: Deleting pod "simpletest.rc-wbvkq" in namespace "gc-1172"
Jan 19 00:06:33.167: INFO: Deleting pod "simpletest.rc-wfcg9" in namespace "gc-1172"
Jan 19 00:06:33.219: INFO: Deleting pod "simpletest.rc-wlthx" in namespace "gc-1172"
Jan 19 00:06:33.267: INFO: Deleting pod "simpletest.rc-xbpw2" in namespace "gc-1172"
Jan 19 00:06:33.322: INFO: Deleting pod "simpletest.rc-xxhbv" in namespace "gc-1172"
Jan 19 00:06:33.367: INFO: Deleting pod "simpletest.rc-zp98l" in namespace "gc-1172"
Jan 19 00:06:33.418: INFO: Deleting pod "simpletest.rc-zqvcx" in namespace "gc-1172"
Jan 19 00:06:33.463: INFO: Deleting pod "simpletest.rc-zrm6x" in namespace "gc-1172"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 19 00:06:33.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1172" for this suite.

• [SLOW TEST:43.091 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":356,"completed":348,"skipped":6494,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:06:33.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9269
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9269
STEP: creating replication controller externalsvc in namespace services-9269
I0119 00:06:33.779632      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9269, replica count: 2
I0119 00:06:36.832314      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 00:06:39.832451      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jan 19 00:06:39.848: INFO: Creating new exec pod
Jan 19 00:06:43.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3152582312 --namespace=services-9269 exec execpodrwlgx -- /bin/sh -x -c nslookup clusterip-service.services-9269.svc.cluster.local'
Jan 19 00:06:44.039: INFO: stderr: "+ nslookup clusterip-service.services-9269.svc.cluster.local\n"
Jan 19 00:06:44.039: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nclusterip-service.services-9269.svc.cluster.local\tcanonical name = externalsvc.services-9269.svc.cluster.local.\nName:\texternalsvc.services-9269.svc.cluster.local\nAddress: 172.30.42.174\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9269, will wait for the garbage collector to delete the pods
Jan 19 00:06:44.099: INFO: Deleting ReplicationController externalsvc took: 6.249893ms
Jan 19 00:06:44.199: INFO: Terminating ReplicationController externalsvc pods took: 100.503367ms
Jan 19 00:06:46.426: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 19 00:06:46.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9269" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:12.846 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":356,"completed":349,"skipped":6531,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:06:46.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
STEP: mirroring a new custom Endpoint
Jan 19 00:06:46.547: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Jan 19 00:06:48.555: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Jan 19 00:06:50.567: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:188
Jan 19 00:06:52.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-6233" for this suite.

• [SLOW TEST:6.105 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":356,"completed":350,"skipped":6549,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:06:52.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 19 00:06:52.703: INFO: The status of Pod busybox-scheduling-073175b1-b00e-4987-a0f2-7803316f9842 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 00:06:54.706: INFO: The status of Pod busybox-scheduling-073175b1-b00e-4987-a0f2-7803316f9842 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 00:06:56.706: INFO: The status of Pod busybox-scheduling-073175b1-b00e-4987-a0f2-7803316f9842 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 19 00:06:56.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5731" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":356,"completed":351,"skipped":6557,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:06:56.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 19 00:06:56.798: INFO: Waiting up to 5m0s for pod "pod-82276a14-6157-46fc-a0c1-fc2a1ff8c30b" in namespace "emptydir-6548" to be "Succeeded or Failed"
Jan 19 00:06:56.809: INFO: Pod "pod-82276a14-6157-46fc-a0c1-fc2a1ff8c30b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.392577ms
Jan 19 00:06:58.813: INFO: Pod "pod-82276a14-6157-46fc-a0c1-fc2a1ff8c30b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014760185s
Jan 19 00:07:00.816: INFO: Pod "pod-82276a14-6157-46fc-a0c1-fc2a1ff8c30b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018009648s
Jan 19 00:07:02.819: INFO: Pod "pod-82276a14-6157-46fc-a0c1-fc2a1ff8c30b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020538168s
STEP: Saw pod success
Jan 19 00:07:02.819: INFO: Pod "pod-82276a14-6157-46fc-a0c1-fc2a1ff8c30b" satisfied condition "Succeeded or Failed"
Jan 19 00:07:02.821: INFO: Trying to get logs from node ip-10-0-219-147.ec2.internal pod pod-82276a14-6157-46fc-a0c1-fc2a1ff8c30b container test-container: <nil>
STEP: delete the pod
Jan 19 00:07:02.832: INFO: Waiting for pod pod-82276a14-6157-46fc-a0c1-fc2a1ff8c30b to disappear
Jan 19 00:07:02.834: INFO: Pod pod-82276a14-6157-46fc-a0c1-fc2a1ff8c30b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 19 00:07:02.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6548" for this suite.

• [SLOW TEST:6.115 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":352,"skipped":6563,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:07:02.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 19 00:07:08.955: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 19 00:07:08.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1885" for this suite.

• [SLOW TEST:6.155 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":353,"skipped":6603,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
S
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 19 00:07:08.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3152582312
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jan 19 00:07:11.098: INFO: running pods: 0 < 3
Jan 19 00:07:13.102: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 19 00:07:15.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7703" for this suite.

• [SLOW TEST:6.131 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":356,"completed":354,"skipped":6604,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}
SSSSSJan 19 00:07:15.129: INFO: Running AfterSuite actions on all nodes
Jan 19 00:07:15.129: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func19.2
Jan 19 00:07:15.129: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan 19 00:07:15.129: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Jan 19 00:07:15.129: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 19 00:07:15.129: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 19 00:07:15.129: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 19 00:07:15.129: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Jan 19 00:07:15.129: INFO: Running AfterSuite actions on node 1
Jan 19 00:07:15.129: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":356,"completed":354,"skipped":6609,"failed":2,"failures":["[sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","[sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]"]}


Summarizing 2 Failures:

[Fail] [sig-network] Services [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance] 
test/e2e/network/service.go:3367

[Fail] [sig-network] Services [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance] 
test/e2e/network/service.go:3367

Ran 356 of 6965 Specs in 6746.409 seconds
FAIL! -- 354 Passed | 2 Failed | 0 Pending | 6609 Skipped
--- FAIL: TestE2E (6748.74s)
FAIL

Ginkgo ran 1 suite in 1h52m28.817980946s
Test Suite Failed
