I0110 06:56:14.677478      21 e2e.go:129] Starting e2e run "9c514c83-43f6-4579-b8b5-74bc55fa4884" on Ginkgo node 1
{"msg":"Test Suite starting","total":356,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1673333774 - Will randomize all specs
Will run 356 of 6973 specs

Jan 10 06:56:16.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 06:56:16.901: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 10 06:56:16.923: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 10 06:56:16.949: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 10 06:56:16.949: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jan 10 06:56:16.949: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 10 06:56:16.954: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 10 06:56:16.954: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 10 06:56:16.954: INFO: e2e test version: v1.24.8
Jan 10 06:56:16.955: INFO: kube-apiserver version: v1.24.8
Jan 10 06:56:16.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 06:56:16.960: INFO: Cluster IP family: ipv4
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:56:16.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
Jan 10 06:56:16.983: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
W0110 06:56:16.983599      21 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 06:56:18.015: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 10 06:56:20.031: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 10, 6, 56, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 6, 56, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 6, 56, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 6, 56, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 10 06:56:22.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 10, 6, 56, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 6, 56, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 6, 56, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 6, 56, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 06:56:25.049: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 06:56:25.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3974" for this suite.
STEP: Destroying namespace "webhook-3974-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.249 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":356,"completed":1,"skipped":12,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:56:25.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 10 06:56:25.252: INFO: Waiting up to 5m0s for pod "pod-0a4ee38a-dd21-4c6a-84d5-2fb1ba75c6c8" in namespace "emptydir-4964" to be "Succeeded or Failed"
Jan 10 06:56:25.255: INFO: Pod "pod-0a4ee38a-dd21-4c6a-84d5-2fb1ba75c6c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.632735ms
Jan 10 06:56:27.260: INFO: Pod "pod-0a4ee38a-dd21-4c6a-84d5-2fb1ba75c6c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008377808s
Jan 10 06:56:29.270: INFO: Pod "pod-0a4ee38a-dd21-4c6a-84d5-2fb1ba75c6c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018519932s
Jan 10 06:56:31.279: INFO: Pod "pod-0a4ee38a-dd21-4c6a-84d5-2fb1ba75c6c8": Phase="Running", Reason="", readiness=true. Elapsed: 6.026770395s
Jan 10 06:56:33.288: INFO: Pod "pod-0a4ee38a-dd21-4c6a-84d5-2fb1ba75c6c8": Phase="Running", Reason="", readiness=true. Elapsed: 8.03672267s
Jan 10 06:56:35.295: INFO: Pod "pod-0a4ee38a-dd21-4c6a-84d5-2fb1ba75c6c8": Phase="Running", Reason="", readiness=true. Elapsed: 10.043106723s
Jan 10 06:56:37.302: INFO: Pod "pod-0a4ee38a-dd21-4c6a-84d5-2fb1ba75c6c8": Phase="Running", Reason="", readiness=false. Elapsed: 12.050716381s
Jan 10 06:56:39.312: INFO: Pod "pod-0a4ee38a-dd21-4c6a-84d5-2fb1ba75c6c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.060642776s
STEP: Saw pod success
Jan 10 06:56:39.313: INFO: Pod "pod-0a4ee38a-dd21-4c6a-84d5-2fb1ba75c6c8" satisfied condition "Succeeded or Failed"
Jan 10 06:56:39.316: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-0a4ee38a-dd21-4c6a-84d5-2fb1ba75c6c8 container test-container: <nil>
STEP: delete the pod
Jan 10 06:56:39.344: INFO: Waiting for pod pod-0a4ee38a-dd21-4c6a-84d5-2fb1ba75c6c8 to disappear
Jan 10 06:56:39.346: INFO: Pod pod-0a4ee38a-dd21-4c6a-84d5-2fb1ba75c6c8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 10 06:56:39.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4964" for this suite.

• [SLOW TEST:14.140 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":2,"skipped":45,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:56:39.358: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 10 06:56:39.412: INFO: Waiting up to 5m0s for pod "pod-e6690b8f-d1ff-40c0-942c-55d373d88db2" in namespace "emptydir-4979" to be "Succeeded or Failed"
Jan 10 06:56:39.421: INFO: Pod "pod-e6690b8f-d1ff-40c0-942c-55d373d88db2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.471718ms
Jan 10 06:56:41.429: INFO: Pod "pod-e6690b8f-d1ff-40c0-942c-55d373d88db2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017180838s
Jan 10 06:56:43.438: INFO: Pod "pod-e6690b8f-d1ff-40c0-942c-55d373d88db2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025691539s
STEP: Saw pod success
Jan 10 06:56:43.438: INFO: Pod "pod-e6690b8f-d1ff-40c0-942c-55d373d88db2" satisfied condition "Succeeded or Failed"
Jan 10 06:56:43.441: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-e6690b8f-d1ff-40c0-942c-55d373d88db2 container test-container: <nil>
STEP: delete the pod
Jan 10 06:56:43.459: INFO: Waiting for pod pod-e6690b8f-d1ff-40c0-942c-55d373d88db2 to disappear
Jan 10 06:56:43.464: INFO: Pod pod-e6690b8f-d1ff-40c0-942c-55d373d88db2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 10 06:56:43.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4979" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":3,"skipped":66,"failed":0}
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:56:43.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-m8jt7 in namespace proxy-683
I0110 06:56:43.532464      21 runners.go:193] Created replication controller with name: proxy-service-m8jt7, namespace: proxy-683, replica count: 1
I0110 06:56:44.583847      21 runners.go:193] proxy-service-m8jt7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0110 06:56:45.584121      21 runners.go:193] proxy-service-m8jt7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 10 06:56:45.590: INFO: setup took 2.08957102s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 10 06:56:45.615: INFO: (0) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 22.1186ms)
Jan 10 06:56:45.615: INFO: (0) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 22.59602ms)
Jan 10 06:56:45.615: INFO: (0) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 22.501377ms)
Jan 10 06:56:45.616: INFO: (0) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 22.281843ms)
Jan 10 06:56:45.616: INFO: (0) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 24.074584ms)
Jan 10 06:56:45.616: INFO: (0) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 23.901788ms)
Jan 10 06:56:45.616: INFO: (0) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 24.6648ms)
Jan 10 06:56:45.616: INFO: (0) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 22.477433ms)
Jan 10 06:56:45.616: INFO: (0) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 23.621839ms)
Jan 10 06:56:45.617: INFO: (0) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 23.731218ms)
Jan 10 06:56:45.617: INFO: (0) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 25.927422ms)
Jan 10 06:56:45.617: INFO: (0) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 25.07844ms)
Jan 10 06:56:45.617: INFO: (0) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 25.592622ms)
Jan 10 06:56:45.617: INFO: (0) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 24.837977ms)
Jan 10 06:56:45.618: INFO: (0) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 24.867389ms)
Jan 10 06:56:45.618: INFO: (0) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 25.457276ms)
Jan 10 06:56:45.626: INFO: (1) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 6.899644ms)
Jan 10 06:56:45.626: INFO: (1) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 6.967598ms)
Jan 10 06:56:45.626: INFO: (1) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 7.449748ms)
Jan 10 06:56:45.633: INFO: (1) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 13.854352ms)
Jan 10 06:56:45.634: INFO: (1) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 14.982613ms)
Jan 10 06:56:45.634: INFO: (1) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 14.853138ms)
Jan 10 06:56:45.635: INFO: (1) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 16.01979ms)
Jan 10 06:56:45.635: INFO: (1) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 15.775199ms)
Jan 10 06:56:45.635: INFO: (1) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 15.933312ms)
Jan 10 06:56:45.636: INFO: (1) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 16.768438ms)
Jan 10 06:56:45.636: INFO: (1) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 16.870329ms)
Jan 10 06:56:45.636: INFO: (1) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 17.518463ms)
Jan 10 06:56:45.636: INFO: (1) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 17.366574ms)
Jan 10 06:56:45.637: INFO: (1) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 17.474605ms)
Jan 10 06:56:45.637: INFO: (1) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 17.913221ms)
Jan 10 06:56:45.637: INFO: (1) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 18.078647ms)
Jan 10 06:56:45.649: INFO: (2) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 11.091468ms)
Jan 10 06:56:45.650: INFO: (2) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 11.964788ms)
Jan 10 06:56:45.650: INFO: (2) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 11.834919ms)
Jan 10 06:56:45.650: INFO: (2) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 12.447475ms)
Jan 10 06:56:45.650: INFO: (2) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 12.087663ms)
Jan 10 06:56:45.650: INFO: (2) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 12.57923ms)
Jan 10 06:56:45.650: INFO: (2) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 12.644381ms)
Jan 10 06:56:45.651: INFO: (2) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 13.08137ms)
Jan 10 06:56:45.651: INFO: (2) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 13.11503ms)
Jan 10 06:56:45.651: INFO: (2) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 13.646043ms)
Jan 10 06:56:45.653: INFO: (2) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 15.601149ms)
Jan 10 06:56:45.653: INFO: (2) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 15.41766ms)
Jan 10 06:56:45.653: INFO: (2) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 15.508059ms)
Jan 10 06:56:45.654: INFO: (2) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 15.869315ms)
Jan 10 06:56:45.654: INFO: (2) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 16.013162ms)
Jan 10 06:56:45.654: INFO: (2) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 16.211522ms)
Jan 10 06:56:45.660: INFO: (3) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 5.813019ms)
Jan 10 06:56:45.661: INFO: (3) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 6.591731ms)
Jan 10 06:56:45.661: INFO: (3) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 6.686623ms)
Jan 10 06:56:45.661: INFO: (3) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 6.846152ms)
Jan 10 06:56:45.662: INFO: (3) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 7.662108ms)
Jan 10 06:56:45.665: INFO: (3) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 9.95475ms)
Jan 10 06:56:45.665: INFO: (3) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 10.200332ms)
Jan 10 06:56:45.665: INFO: (3) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 10.526098ms)
Jan 10 06:56:45.665: INFO: (3) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 10.694149ms)
Jan 10 06:56:45.666: INFO: (3) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 10.96509ms)
Jan 10 06:56:45.666: INFO: (3) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 11.882378ms)
Jan 10 06:56:45.668: INFO: (3) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 13.631077ms)
Jan 10 06:56:45.668: INFO: (3) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 13.159169ms)
Jan 10 06:56:45.668: INFO: (3) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 13.83119ms)
Jan 10 06:56:45.668: INFO: (3) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 13.58963ms)
Jan 10 06:56:45.668: INFO: (3) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 13.751006ms)
Jan 10 06:56:45.678: INFO: (4) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 9.088164ms)
Jan 10 06:56:45.678: INFO: (4) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 9.252872ms)
Jan 10 06:56:45.678: INFO: (4) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 9.903581ms)
Jan 10 06:56:45.679: INFO: (4) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 9.747143ms)
Jan 10 06:56:45.679: INFO: (4) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 9.93009ms)
Jan 10 06:56:45.679: INFO: (4) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 9.726228ms)
Jan 10 06:56:45.679: INFO: (4) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 9.722726ms)
Jan 10 06:56:45.679: INFO: (4) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 9.882169ms)
Jan 10 06:56:45.679: INFO: (4) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 9.945552ms)
Jan 10 06:56:45.680: INFO: (4) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 11.333672ms)
Jan 10 06:56:45.680: INFO: (4) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 11.513284ms)
Jan 10 06:56:45.681: INFO: (4) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 11.773519ms)
Jan 10 06:56:45.681: INFO: (4) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 12.07066ms)
Jan 10 06:56:45.682: INFO: (4) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 13.291001ms)
Jan 10 06:56:45.682: INFO: (4) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 13.264561ms)
Jan 10 06:56:45.682: INFO: (4) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 13.454601ms)
Jan 10 06:56:45.690: INFO: (5) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 7.231708ms)
Jan 10 06:56:45.697: INFO: (5) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 13.835812ms)
Jan 10 06:56:45.697: INFO: (5) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 14.144824ms)
Jan 10 06:56:45.697: INFO: (5) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 14.58556ms)
Jan 10 06:56:45.697: INFO: (5) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 14.414574ms)
Jan 10 06:56:45.697: INFO: (5) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 14.497804ms)
Jan 10 06:56:45.697: INFO: (5) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 14.275783ms)
Jan 10 06:56:45.700: INFO: (5) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 17.771191ms)
Jan 10 06:56:45.701: INFO: (5) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 18.204981ms)
Jan 10 06:56:45.701: INFO: (5) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 18.499883ms)
Jan 10 06:56:45.701: INFO: (5) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 18.600302ms)
Jan 10 06:56:45.701: INFO: (5) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 18.513315ms)
Jan 10 06:56:45.701: INFO: (5) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 18.972197ms)
Jan 10 06:56:45.701: INFO: (5) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 18.62577ms)
Jan 10 06:56:45.702: INFO: (5) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 19.062056ms)
Jan 10 06:56:45.701: INFO: (5) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 18.571033ms)
Jan 10 06:56:45.710: INFO: (6) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 8.084613ms)
Jan 10 06:56:45.711: INFO: (6) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 8.45422ms)
Jan 10 06:56:45.711: INFO: (6) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 8.766108ms)
Jan 10 06:56:45.711: INFO: (6) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 8.519774ms)
Jan 10 06:56:45.711: INFO: (6) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 8.790776ms)
Jan 10 06:56:45.711: INFO: (6) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 8.968746ms)
Jan 10 06:56:45.711: INFO: (6) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 9.185341ms)
Jan 10 06:56:45.711: INFO: (6) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 8.69875ms)
Jan 10 06:56:45.712: INFO: (6) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 9.472186ms)
Jan 10 06:56:45.712: INFO: (6) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 9.591664ms)
Jan 10 06:56:45.713: INFO: (6) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 10.378034ms)
Jan 10 06:56:45.714: INFO: (6) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 11.69976ms)
Jan 10 06:56:45.715: INFO: (6) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 13.584275ms)
Jan 10 06:56:45.716: INFO: (6) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 13.376838ms)
Jan 10 06:56:45.716: INFO: (6) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 13.481891ms)
Jan 10 06:56:45.716: INFO: (6) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 13.773185ms)
Jan 10 06:56:45.766: INFO: (7) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 49.40884ms)
Jan 10 06:56:45.766: INFO: (7) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 49.572918ms)
Jan 10 06:56:45.767: INFO: (7) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 49.701106ms)
Jan 10 06:56:45.767: INFO: (7) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 49.863638ms)
Jan 10 06:56:45.767: INFO: (7) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 50.241148ms)
Jan 10 06:56:45.767: INFO: (7) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 50.851459ms)
Jan 10 06:56:45.768: INFO: (7) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 50.911528ms)
Jan 10 06:56:45.768: INFO: (7) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 51.541318ms)
Jan 10 06:56:45.770: INFO: (7) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 52.906834ms)
Jan 10 06:56:45.770: INFO: (7) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 53.151462ms)
Jan 10 06:56:45.770: INFO: (7) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 53.702432ms)
Jan 10 06:56:45.771: INFO: (7) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 54.221116ms)
Jan 10 06:56:45.771: INFO: (7) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 54.205675ms)
Jan 10 06:56:45.771: INFO: (7) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 54.428579ms)
Jan 10 06:56:45.771: INFO: (7) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 54.69693ms)
Jan 10 06:56:45.772: INFO: (7) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 55.17168ms)
Jan 10 06:56:45.808: INFO: (8) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 35.948658ms)
Jan 10 06:56:45.808: INFO: (8) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 36.210709ms)
Jan 10 06:56:45.809: INFO: (8) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 35.92241ms)
Jan 10 06:56:45.808: INFO: (8) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 35.657185ms)
Jan 10 06:56:45.809: INFO: (8) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 36.127013ms)
Jan 10 06:56:45.809: INFO: (8) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 36.623952ms)
Jan 10 06:56:45.809: INFO: (8) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 37.06684ms)
Jan 10 06:56:45.809: INFO: (8) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 36.84715ms)
Jan 10 06:56:45.809: INFO: (8) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 36.791027ms)
Jan 10 06:56:45.810: INFO: (8) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 37.293285ms)
Jan 10 06:56:45.810: INFO: (8) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 37.135015ms)
Jan 10 06:56:45.810: INFO: (8) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 37.090567ms)
Jan 10 06:56:45.812: INFO: (8) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 39.315543ms)
Jan 10 06:56:45.812: INFO: (8) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 39.307231ms)
Jan 10 06:56:45.812: INFO: (8) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 39.566258ms)
Jan 10 06:56:45.812: INFO: (8) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 40.092926ms)
Jan 10 06:56:45.837: INFO: (9) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 24.133391ms)
Jan 10 06:56:45.837: INFO: (9) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 24.205445ms)
Jan 10 06:56:45.837: INFO: (9) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 24.527354ms)
Jan 10 06:56:45.838: INFO: (9) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 24.461227ms)
Jan 10 06:56:45.838: INFO: (9) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 24.983871ms)
Jan 10 06:56:45.838: INFO: (9) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 25.395263ms)
Jan 10 06:56:45.838: INFO: (9) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 25.400666ms)
Jan 10 06:56:45.838: INFO: (9) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 25.339972ms)
Jan 10 06:56:45.838: INFO: (9) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 25.572255ms)
Jan 10 06:56:45.839: INFO: (9) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 25.618507ms)
Jan 10 06:56:45.839: INFO: (9) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 26.122913ms)
Jan 10 06:56:45.839: INFO: (9) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 26.514173ms)
Jan 10 06:56:45.839: INFO: (9) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 26.319206ms)
Jan 10 06:56:45.839: INFO: (9) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 26.437821ms)
Jan 10 06:56:45.840: INFO: (9) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 26.995299ms)
Jan 10 06:56:45.840: INFO: (9) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 27.846452ms)
Jan 10 06:56:45.852: INFO: (10) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 11.030185ms)
Jan 10 06:56:45.863: INFO: (10) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 21.846549ms)
Jan 10 06:56:45.863: INFO: (10) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 22.217023ms)
Jan 10 06:56:45.863: INFO: (10) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 22.190993ms)
Jan 10 06:56:45.863: INFO: (10) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 22.35942ms)
Jan 10 06:56:45.863: INFO: (10) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 22.57421ms)
Jan 10 06:56:45.864: INFO: (10) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 22.72976ms)
Jan 10 06:56:45.864: INFO: (10) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 22.993097ms)
Jan 10 06:56:45.864: INFO: (10) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 23.175595ms)
Jan 10 06:56:45.864: INFO: (10) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 23.253828ms)
Jan 10 06:56:45.864: INFO: (10) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 23.643931ms)
Jan 10 06:56:45.864: INFO: (10) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 23.527229ms)
Jan 10 06:56:45.864: INFO: (10) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 23.615125ms)
Jan 10 06:56:45.864: INFO: (10) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 23.57072ms)
Jan 10 06:56:45.865: INFO: (10) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 23.590639ms)
Jan 10 06:56:45.865: INFO: (10) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 23.662053ms)
Jan 10 06:56:45.874: INFO: (11) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 8.723459ms)
Jan 10 06:56:45.874: INFO: (11) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 8.779823ms)
Jan 10 06:56:45.874: INFO: (11) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 8.607513ms)
Jan 10 06:56:45.878: INFO: (11) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 12.527967ms)
Jan 10 06:56:45.878: INFO: (11) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 12.749037ms)
Jan 10 06:56:45.878: INFO: (11) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 12.728679ms)
Jan 10 06:56:45.878: INFO: (11) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 12.557862ms)
Jan 10 06:56:45.878: INFO: (11) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 12.64473ms)
Jan 10 06:56:45.878: INFO: (11) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 12.857155ms)
Jan 10 06:56:45.878: INFO: (11) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 12.389658ms)
Jan 10 06:56:45.879: INFO: (11) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 13.850919ms)
Jan 10 06:56:45.879: INFO: (11) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 14.099469ms)
Jan 10 06:56:45.879: INFO: (11) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 14.232107ms)
Jan 10 06:56:45.879: INFO: (11) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 14.335123ms)
Jan 10 06:56:45.879: INFO: (11) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 14.552921ms)
Jan 10 06:56:45.880: INFO: (11) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 14.475394ms)
Jan 10 06:56:45.890: INFO: (12) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 9.72842ms)
Jan 10 06:56:45.893: INFO: (12) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 13.710581ms)
Jan 10 06:56:45.895: INFO: (12) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 15.358374ms)
Jan 10 06:56:45.896: INFO: (12) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 15.807602ms)
Jan 10 06:56:45.896: INFO: (12) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 15.761454ms)
Jan 10 06:56:45.896: INFO: (12) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 16.096773ms)
Jan 10 06:56:45.896: INFO: (12) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 16.31391ms)
Jan 10 06:56:45.897: INFO: (12) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 16.453637ms)
Jan 10 06:56:45.897: INFO: (12) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 16.652847ms)
Jan 10 06:56:45.897: INFO: (12) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 16.828384ms)
Jan 10 06:56:45.897: INFO: (12) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 16.97844ms)
Jan 10 06:56:45.897: INFO: (12) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 16.819566ms)
Jan 10 06:56:45.897: INFO: (12) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 17.087503ms)
Jan 10 06:56:45.897: INFO: (12) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 17.261043ms)
Jan 10 06:56:45.897: INFO: (12) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 17.278308ms)
Jan 10 06:56:45.898: INFO: (12) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 17.717024ms)
Jan 10 06:56:45.905: INFO: (13) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 7.015781ms)
Jan 10 06:56:45.906: INFO: (13) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 7.724508ms)
Jan 10 06:56:45.907: INFO: (13) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 8.434819ms)
Jan 10 06:56:45.907: INFO: (13) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 9.016315ms)
Jan 10 06:56:45.907: INFO: (13) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 8.29614ms)
Jan 10 06:56:45.907: INFO: (13) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 8.516305ms)
Jan 10 06:56:45.907: INFO: (13) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 8.614133ms)
Jan 10 06:56:45.907: INFO: (13) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 8.844916ms)
Jan 10 06:56:45.907: INFO: (13) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 8.865031ms)
Jan 10 06:56:45.907: INFO: (13) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 8.810491ms)
Jan 10 06:56:45.909: INFO: (13) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 10.36957ms)
Jan 10 06:56:45.909: INFO: (13) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 10.402965ms)
Jan 10 06:56:45.909: INFO: (13) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 10.942263ms)
Jan 10 06:56:45.909: INFO: (13) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 11.016437ms)
Jan 10 06:56:45.909: INFO: (13) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 10.643999ms)
Jan 10 06:56:45.909: INFO: (13) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 10.740619ms)
Jan 10 06:56:45.918: INFO: (14) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 7.799339ms)
Jan 10 06:56:45.920: INFO: (14) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 10.081022ms)
Jan 10 06:56:45.921: INFO: (14) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 10.82069ms)
Jan 10 06:56:45.921: INFO: (14) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 10.258436ms)
Jan 10 06:56:45.921: INFO: (14) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 10.987755ms)
Jan 10 06:56:45.921: INFO: (14) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 10.453565ms)
Jan 10 06:56:45.921: INFO: (14) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 10.836431ms)
Jan 10 06:56:45.921: INFO: (14) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 10.888479ms)
Jan 10 06:56:45.921: INFO: (14) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 11.054754ms)
Jan 10 06:56:45.921: INFO: (14) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 10.477112ms)
Jan 10 06:56:45.921: INFO: (14) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 10.823696ms)
Jan 10 06:56:45.921: INFO: (14) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 10.470287ms)
Jan 10 06:56:45.921: INFO: (14) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 10.904464ms)
Jan 10 06:56:45.921: INFO: (14) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 10.632075ms)
Jan 10 06:56:45.921: INFO: (14) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 10.697085ms)
Jan 10 06:56:45.921: INFO: (14) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 10.814717ms)
Jan 10 06:56:45.931: INFO: (15) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 6.859291ms)
Jan 10 06:56:45.931: INFO: (15) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 7.57276ms)
Jan 10 06:56:45.931: INFO: (15) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 7.954135ms)
Jan 10 06:56:45.935: INFO: (15) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 11.766748ms)
Jan 10 06:56:45.937: INFO: (15) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 13.29138ms)
Jan 10 06:56:45.937: INFO: (15) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 13.55819ms)
Jan 10 06:56:45.937: INFO: (15) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 13.705399ms)
Jan 10 06:56:45.937: INFO: (15) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 14.030882ms)
Jan 10 06:56:45.938: INFO: (15) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 13.837876ms)
Jan 10 06:56:45.939: INFO: (15) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 14.919908ms)
Jan 10 06:56:45.939: INFO: (15) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 15.648923ms)
Jan 10 06:56:45.939: INFO: (15) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 15.873953ms)
Jan 10 06:56:45.940: INFO: (15) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 16.799819ms)
Jan 10 06:56:45.940: INFO: (15) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 16.670515ms)
Jan 10 06:56:45.941: INFO: (15) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 16.661786ms)
Jan 10 06:56:45.941: INFO: (15) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 17.078627ms)
Jan 10 06:56:45.951: INFO: (16) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 9.886773ms)
Jan 10 06:56:45.952: INFO: (16) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 10.372482ms)
Jan 10 06:56:45.952: INFO: (16) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 9.096198ms)
Jan 10 06:56:45.952: INFO: (16) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 9.600415ms)
Jan 10 06:56:45.952: INFO: (16) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 9.925953ms)
Jan 10 06:56:45.952: INFO: (16) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 10.435491ms)
Jan 10 06:56:45.954: INFO: (16) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 11.188331ms)
Jan 10 06:56:45.954: INFO: (16) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 11.785918ms)
Jan 10 06:56:45.954: INFO: (16) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 11.054258ms)
Jan 10 06:56:45.955: INFO: (16) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 13.384708ms)
Jan 10 06:56:45.955: INFO: (16) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 12.158995ms)
Jan 10 06:56:45.955: INFO: (16) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 12.581691ms)
Jan 10 06:56:45.955: INFO: (16) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 14.375576ms)
Jan 10 06:56:45.956: INFO: (16) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 14.136591ms)
Jan 10 06:56:45.956: INFO: (16) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 13.618439ms)
Jan 10 06:56:45.956: INFO: (16) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 14.386543ms)
Jan 10 06:56:45.962: INFO: (17) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 5.71028ms)
Jan 10 06:56:45.970: INFO: (17) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 12.849116ms)
Jan 10 06:56:45.970: INFO: (17) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 13.86659ms)
Jan 10 06:56:45.970: INFO: (17) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 13.569759ms)
Jan 10 06:56:45.971: INFO: (17) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 13.624687ms)
Jan 10 06:56:45.971: INFO: (17) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 14.657381ms)
Jan 10 06:56:45.971: INFO: (17) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 14.46277ms)
Jan 10 06:56:45.972: INFO: (17) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 14.915196ms)
Jan 10 06:56:45.972: INFO: (17) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 14.750181ms)
Jan 10 06:56:45.972: INFO: (17) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 14.901673ms)
Jan 10 06:56:45.972: INFO: (17) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 14.81098ms)
Jan 10 06:56:45.972: INFO: (17) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 15.300846ms)
Jan 10 06:56:45.972: INFO: (17) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 15.430229ms)
Jan 10 06:56:45.972: INFO: (17) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 15.827711ms)
Jan 10 06:56:45.972: INFO: (17) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 15.884134ms)
Jan 10 06:56:45.972: INFO: (17) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 16.036353ms)
Jan 10 06:56:45.980: INFO: (18) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 6.970762ms)
Jan 10 06:56:45.981: INFO: (18) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 8.005764ms)
Jan 10 06:56:45.981: INFO: (18) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 8.495535ms)
Jan 10 06:56:45.981: INFO: (18) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 8.295873ms)
Jan 10 06:56:45.983: INFO: (18) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 9.774655ms)
Jan 10 06:56:45.983: INFO: (18) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 10.16093ms)
Jan 10 06:56:45.983: INFO: (18) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 9.889153ms)
Jan 10 06:56:45.984: INFO: (18) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 11.315508ms)
Jan 10 06:56:45.985: INFO: (18) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 12.218403ms)
Jan 10 06:56:45.985: INFO: (18) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 11.844558ms)
Jan 10 06:56:45.986: INFO: (18) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 12.512909ms)
Jan 10 06:56:45.986: INFO: (18) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 12.978484ms)
Jan 10 06:56:45.986: INFO: (18) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 13.11285ms)
Jan 10 06:56:45.987: INFO: (18) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 13.993006ms)
Jan 10 06:56:45.987: INFO: (18) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 14.354341ms)
Jan 10 06:56:45.988: INFO: (18) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 14.53484ms)
Jan 10 06:56:45.993: INFO: (19) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 4.568822ms)
Jan 10 06:56:45.994: INFO: (19) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn/proxy/rewriteme">test</a> (200; 5.389975ms)
Jan 10 06:56:45.996: INFO: (19) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:460/proxy/: tls baz (200; 7.823906ms)
Jan 10 06:56:45.997: INFO: (19) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">test</... (200; 7.593806ms)
Jan 10 06:56:45.999: INFO: (19) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname1/proxy/: tls baz (200; 11.478958ms)
Jan 10 06:56:46.000: INFO: (19) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname2/proxy/: bar (200; 11.706135ms)
Jan 10 06:56:46.000: INFO: (19) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:462/proxy/: tls qux (200; 10.755255ms)
Jan 10 06:56:46.000: INFO: (19) /api/v1/namespaces/proxy-683/services/proxy-service-m8jt7:portname1/proxy/: foo (200; 12.129119ms)
Jan 10 06:56:46.000: INFO: (19) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:1080/proxy/rewriteme">t... (200; 10.247763ms)
Jan 10 06:56:46.000: INFO: (19) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:162/proxy/: bar (200; 11.002755ms)
Jan 10 06:56:46.000: INFO: (19) /api/v1/namespaces/proxy-683/pods/proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 11.601466ms)
Jan 10 06:56:46.000: INFO: (19) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname2/proxy/: bar (200; 12.225287ms)
Jan 10 06:56:46.000: INFO: (19) /api/v1/namespaces/proxy-683/pods/http:proxy-service-m8jt7-z25rn:160/proxy/: foo (200; 11.589185ms)
Jan 10 06:56:46.001: INFO: (19) /api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/: <a href="/api/v1/namespaces/proxy-683/pods/https:proxy-service-m8jt7-z25rn:443/proxy/tlsrewriteme... (200; 10.865041ms)
Jan 10 06:56:46.001: INFO: (19) /api/v1/namespaces/proxy-683/services/http:proxy-service-m8jt7:portname1/proxy/: foo (200; 11.690829ms)
Jan 10 06:56:46.001: INFO: (19) /api/v1/namespaces/proxy-683/services/https:proxy-service-m8jt7:tlsportname2/proxy/: tls qux (200; 11.745412ms)
STEP: deleting ReplicationController proxy-service-m8jt7 in namespace proxy-683, will wait for the garbage collector to delete the pods
Jan 10 06:56:46.063: INFO: Deleting ReplicationController proxy-service-m8jt7 took: 7.196609ms
Jan 10 06:56:46.164: INFO: Terminating ReplicationController proxy-service-m8jt7 pods took: 100.741766ms
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jan 10 06:56:49.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-683" for this suite.

• [SLOW TEST:5.705 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":356,"completed":4,"skipped":68,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:56:49.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-0c05f4be-27d5-4ad4-ac51-44b9ba399881
STEP: Creating a pod to test consume configMaps
Jan 10 06:56:49.219: INFO: Waiting up to 5m0s for pod "pod-configmaps-a5e35cf7-e44e-4019-a0ca-bb434acc83cd" in namespace "configmap-5540" to be "Succeeded or Failed"
Jan 10 06:56:49.229: INFO: Pod "pod-configmaps-a5e35cf7-e44e-4019-a0ca-bb434acc83cd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.801867ms
Jan 10 06:56:51.236: INFO: Pod "pod-configmaps-a5e35cf7-e44e-4019-a0ca-bb434acc83cd": Phase="Running", Reason="", readiness=false. Elapsed: 2.016541275s
Jan 10 06:56:53.243: INFO: Pod "pod-configmaps-a5e35cf7-e44e-4019-a0ca-bb434acc83cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023512699s
STEP: Saw pod success
Jan 10 06:56:53.243: INFO: Pod "pod-configmaps-a5e35cf7-e44e-4019-a0ca-bb434acc83cd" satisfied condition "Succeeded or Failed"
Jan 10 06:56:53.246: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-configmaps-a5e35cf7-e44e-4019-a0ca-bb434acc83cd container agnhost-container: <nil>
STEP: delete the pod
Jan 10 06:56:53.264: INFO: Waiting for pod pod-configmaps-a5e35cf7-e44e-4019-a0ca-bb434acc83cd to disappear
Jan 10 06:56:53.266: INFO: Pod pod-configmaps-a5e35cf7-e44e-4019-a0ca-bb434acc83cd no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 10 06:56:53.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5540" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":5,"skipped":101,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:56:53.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jan 10 06:56:53.320: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 10 06:56:55.329: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 10 06:56:57.330: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 10 06:56:58.356: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 10 06:56:58.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-794" for this suite.

• [SLOW TEST:5.151 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":356,"completed":6,"skipped":130,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:56:58.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 06:56:58.919: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 06:57:01.942: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 06:57:02.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4223" for this suite.
STEP: Destroying namespace "webhook-4223-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":356,"completed":7,"skipped":156,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:57:02.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-8599
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 10 06:57:02.145: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 10 06:57:02.207: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 06:57:04.218: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 06:57:06.215: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 06:57:08.213: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 06:57:10.215: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 06:57:12.212: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 06:57:14.217: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 06:57:16.216: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 06:57:18.214: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 06:57:20.213: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 06:57:22.215: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 06:57:24.213: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 10 06:57:24.217: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 10 06:57:26.256: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 10 06:57:26.256: INFO: Going to poll 10.233.82.5 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 10 06:57:26.259: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.82.5:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8599 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 06:57:26.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 06:57:26.260: INFO: ExecWithOptions: Clientset creation
Jan 10 06:57:26.260: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8599/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.82.5%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 10 06:57:26.345: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 10 06:57:26.345: INFO: Going to poll 10.233.107.73 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 10 06:57:26.348: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.107.73:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8599 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 06:57:26.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 06:57:26.349: INFO: ExecWithOptions: Clientset creation
Jan 10 06:57:26.349: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8599/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.107.73%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 10 06:57:26.420: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 10 06:57:26.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8599" for this suite.

• [SLOW TEST:24.369 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":8,"skipped":158,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:57:26.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Service
STEP: watching for the Service to be added
Jan 10 06:57:26.508: INFO: Found Service test-service-czvfx in namespace services-320 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 10 06:57:26.508: INFO: Service test-service-czvfx created
STEP: Getting /status
Jan 10 06:57:26.513: INFO: Service test-service-czvfx has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jan 10 06:57:26.524: INFO: observed Service test-service-czvfx in namespace services-320 with annotations: map[] & LoadBalancer: {[]}
Jan 10 06:57:26.525: INFO: Found Service test-service-czvfx in namespace services-320 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 10 06:57:26.525: INFO: Service test-service-czvfx has service status patched
STEP: updating the ServiceStatus
Jan 10 06:57:26.534: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jan 10 06:57:26.537: INFO: Observed Service test-service-czvfx in namespace services-320 with annotations: map[] & Conditions: {[]}
Jan 10 06:57:26.537: INFO: Observed event: &Service{ObjectMeta:{test-service-czvfx  services-320  55ceddf3-248a-4af4-9236-c8f98b18812c 12126 0 2023-01-10 06:57:26 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2023-01-10 06:57:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-10 06:57:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.36.9,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.36.9],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 10 06:57:26.538: INFO: Found Service test-service-czvfx in namespace services-320 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 10 06:57:26.538: INFO: Service test-service-czvfx has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jan 10 06:57:26.552: INFO: observed Service test-service-czvfx in namespace services-320 with labels: map[test-service-static:true]
Jan 10 06:57:26.552: INFO: observed Service test-service-czvfx in namespace services-320 with labels: map[test-service-static:true]
Jan 10 06:57:26.553: INFO: observed Service test-service-czvfx in namespace services-320 with labels: map[test-service-static:true]
Jan 10 06:57:26.553: INFO: Found Service test-service-czvfx in namespace services-320 with labels: map[test-service:patched test-service-static:true]
Jan 10 06:57:26.553: INFO: Service test-service-czvfx patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jan 10 06:57:26.569: INFO: Observed event: ADDED
Jan 10 06:57:26.569: INFO: Observed event: MODIFIED
Jan 10 06:57:26.569: INFO: Observed event: MODIFIED
Jan 10 06:57:26.569: INFO: Observed event: MODIFIED
Jan 10 06:57:26.570: INFO: Found Service test-service-czvfx in namespace services-320 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 10 06:57:26.570: INFO: Service test-service-czvfx deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 06:57:26.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-320" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":356,"completed":9,"skipped":165,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:57:26.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jan 10 06:57:26.616: INFO: Waiting up to 5m0s for pod "security-context-4b6b80c0-4772-4826-8549-59c11417fe52" in namespace "security-context-3939" to be "Succeeded or Failed"
Jan 10 06:57:26.620: INFO: Pod "security-context-4b6b80c0-4772-4826-8549-59c11417fe52": Phase="Pending", Reason="", readiness=false. Elapsed: 3.446658ms
Jan 10 06:57:28.628: INFO: Pod "security-context-4b6b80c0-4772-4826-8549-59c11417fe52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011259196s
Jan 10 06:57:30.634: INFO: Pod "security-context-4b6b80c0-4772-4826-8549-59c11417fe52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017415722s
Jan 10 06:57:32.641: INFO: Pod "security-context-4b6b80c0-4772-4826-8549-59c11417fe52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024593119s
STEP: Saw pod success
Jan 10 06:57:32.641: INFO: Pod "security-context-4b6b80c0-4772-4826-8549-59c11417fe52" satisfied condition "Succeeded or Failed"
Jan 10 06:57:32.644: INFO: Trying to get logs from node kk-instance-vgmmg pod security-context-4b6b80c0-4772-4826-8549-59c11417fe52 container test-container: <nil>
STEP: delete the pod
Jan 10 06:57:32.662: INFO: Waiting for pod security-context-4b6b80c0-4772-4826-8549-59c11417fe52 to disappear
Jan 10 06:57:32.665: INFO: Pod security-context-4b6b80c0-4772-4826-8549-59c11417fe52 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 10 06:57:32.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3939" for this suite.

• [SLOW TEST:6.092 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":10,"skipped":180,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:57:32.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating pod
Jan 10 06:57:32.718: INFO: The status of Pod pod-hostip-567789d6-253c-47a7-9f79-9bfebdf405cf is Pending, waiting for it to be Running (with Ready = true)
Jan 10 06:57:34.723: INFO: The status of Pod pod-hostip-567789d6-253c-47a7-9f79-9bfebdf405cf is Pending, waiting for it to be Running (with Ready = true)
Jan 10 06:57:36.728: INFO: The status of Pod pod-hostip-567789d6-253c-47a7-9f79-9bfebdf405cf is Running (Ready = true)
Jan 10 06:57:36.732: INFO: Pod pod-hostip-567789d6-253c-47a7-9f79-9bfebdf405cf has hostIP: 192.168.0.5
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 10 06:57:36.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7747" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":356,"completed":11,"skipped":184,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:57:36.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-4700
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 10 06:57:36.772: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 10 06:57:36.819: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 06:57:38.826: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 06:57:40.828: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 06:57:42.826: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 06:57:44.825: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 06:57:46.828: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 06:57:48.825: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 10 06:57:48.828: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 10 06:57:50.860: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 10 06:57:50.860: INFO: Breadth first check of 10.233.82.6 on host 192.168.0.4...
Jan 10 06:57:50.862: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.107.78:9080/dial?request=hostname&protocol=http&host=10.233.82.6&port=8083&tries=1'] Namespace:pod-network-test-4700 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 06:57:50.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 06:57:50.863: INFO: ExecWithOptions: Clientset creation
Jan 10 06:57:50.863: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4700/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.107.78%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.82.6%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 10 06:57:50.944: INFO: Waiting for responses: map[]
Jan 10 06:57:50.944: INFO: reached 10.233.82.6 after 0/1 tries
Jan 10 06:57:50.944: INFO: Breadth first check of 10.233.107.77 on host 192.168.0.5...
Jan 10 06:57:50.948: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.107.78:9080/dial?request=hostname&protocol=http&host=10.233.107.77&port=8083&tries=1'] Namespace:pod-network-test-4700 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 06:57:50.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 06:57:50.949: INFO: ExecWithOptions: Clientset creation
Jan 10 06:57:50.949: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4700/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.107.78%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.107.77%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 10 06:57:51.024: INFO: Waiting for responses: map[]
Jan 10 06:57:51.024: INFO: reached 10.233.107.77 after 0/1 tries
Jan 10 06:57:51.024: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 10 06:57:51.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4700" for this suite.

• [SLOW TEST:14.292 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":356,"completed":12,"skipped":198,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:57:51.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test service account token: 
Jan 10 06:57:51.069: INFO: Waiting up to 5m0s for pod "test-pod-512a5779-2f7b-4564-8fd7-2d45e2a7f2f5" in namespace "svcaccounts-8269" to be "Succeeded or Failed"
Jan 10 06:57:51.072: INFO: Pod "test-pod-512a5779-2f7b-4564-8fd7-2d45e2a7f2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.673866ms
Jan 10 06:57:53.078: INFO: Pod "test-pod-512a5779-2f7b-4564-8fd7-2d45e2a7f2f5": Phase="Running", Reason="", readiness=false. Elapsed: 2.009139393s
Jan 10 06:57:55.084: INFO: Pod "test-pod-512a5779-2f7b-4564-8fd7-2d45e2a7f2f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014600395s
STEP: Saw pod success
Jan 10 06:57:55.084: INFO: Pod "test-pod-512a5779-2f7b-4564-8fd7-2d45e2a7f2f5" satisfied condition "Succeeded or Failed"
Jan 10 06:57:55.088: INFO: Trying to get logs from node kk-instance-r65pm pod test-pod-512a5779-2f7b-4564-8fd7-2d45e2a7f2f5 container agnhost-container: <nil>
STEP: delete the pod
Jan 10 06:57:55.119: INFO: Waiting for pod test-pod-512a5779-2f7b-4564-8fd7-2d45e2a7f2f5 to disappear
Jan 10 06:57:55.122: INFO: Pod test-pod-512a5779-2f7b-4564-8fd7-2d45e2a7f2f5 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 10 06:57:55.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8269" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":356,"completed":13,"skipped":270,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:57:55.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 10 06:58:00.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9556" for this suite.

• [SLOW TEST:5.411 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":356,"completed":14,"skipped":288,"failed":0}
SSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:58:00.546: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 10 06:58:02.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1188" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","total":356,"completed":15,"skipped":294,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:58:02.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 10 06:58:02.746: INFO: The status of Pod labelsupdate9c9dd01e-194b-4839-8feb-89e441ce7764 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 06:58:04.752: INFO: The status of Pod labelsupdate9c9dd01e-194b-4839-8feb-89e441ce7764 is Running (Ready = true)
Jan 10 06:58:05.288: INFO: Successfully updated pod "labelsupdate9c9dd01e-194b-4839-8feb-89e441ce7764"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 10 06:58:09.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9041" for this suite.

• [SLOW TEST:6.657 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":16,"skipped":327,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:58:09.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 06:58:09.370: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 10 06:58:14.380: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jan 10 06:58:14.390: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jan 10 06:58:14.416: INFO: observed ReplicaSet test-rs in namespace replicaset-4632 with ReadyReplicas 1, AvailableReplicas 1
Jan 10 06:58:14.426: INFO: observed ReplicaSet test-rs in namespace replicaset-4632 with ReadyReplicas 1, AvailableReplicas 1
Jan 10 06:58:14.468: INFO: observed ReplicaSet test-rs in namespace replicaset-4632 with ReadyReplicas 1, AvailableReplicas 1
Jan 10 06:58:14.490: INFO: observed ReplicaSet test-rs in namespace replicaset-4632 with ReadyReplicas 1, AvailableReplicas 1
Jan 10 06:58:15.509: INFO: observed ReplicaSet test-rs in namespace replicaset-4632 with ReadyReplicas 2, AvailableReplicas 2
Jan 10 06:58:17.077: INFO: observed Replicaset test-rs in namespace replicaset-4632 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 10 06:58:17.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4632" for this suite.

• [SLOW TEST:7.762 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":356,"completed":17,"skipped":330,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:58:17.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a pod template
STEP: Replace a pod template
Jan 10 06:58:17.138: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jan 10 06:58:17.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4944" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","total":356,"completed":18,"skipped":352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:58:17.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-projected-all-test-volume-1b20b414-d015-4ed8-835d-94fb13dced88
STEP: Creating secret with name secret-projected-all-test-volume-aebb4ed4-63bd-4501-8795-d500ca49bb3d
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 10 06:58:17.192: INFO: Waiting up to 5m0s for pod "projected-volume-ed337282-816d-498d-a99f-8e3b5b63e911" in namespace "projected-8530" to be "Succeeded or Failed"
Jan 10 06:58:17.197: INFO: Pod "projected-volume-ed337282-816d-498d-a99f-8e3b5b63e911": Phase="Pending", Reason="", readiness=false. Elapsed: 5.511842ms
Jan 10 06:58:19.204: INFO: Pod "projected-volume-ed337282-816d-498d-a99f-8e3b5b63e911": Phase="Running", Reason="", readiness=true. Elapsed: 2.012643095s
Jan 10 06:58:21.211: INFO: Pod "projected-volume-ed337282-816d-498d-a99f-8e3b5b63e911": Phase="Running", Reason="", readiness=false. Elapsed: 4.019566761s
Jan 10 06:58:23.218: INFO: Pod "projected-volume-ed337282-816d-498d-a99f-8e3b5b63e911": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026442624s
STEP: Saw pod success
Jan 10 06:58:23.218: INFO: Pod "projected-volume-ed337282-816d-498d-a99f-8e3b5b63e911" satisfied condition "Succeeded or Failed"
Jan 10 06:58:23.223: INFO: Trying to get logs from node kk-instance-r65pm pod projected-volume-ed337282-816d-498d-a99f-8e3b5b63e911 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 10 06:58:23.247: INFO: Waiting for pod projected-volume-ed337282-816d-498d-a99f-8e3b5b63e911 to disappear
Jan 10 06:58:23.252: INFO: Pod projected-volume-ed337282-816d-498d-a99f-8e3b5b63e911 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:188
Jan 10 06:58:23.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8530" for this suite.

• [SLOW TEST:6.111 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":356,"completed":19,"skipped":381,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:58:23.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Jan 10 06:58:23.355: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 10 06:58:28.370: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Jan 10 06:58:28.376: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 10 06:58:28.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1995" for this suite.

• [SLOW TEST:5.155 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":356,"completed":20,"skipped":382,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:58:28.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-7210
Jan 10 06:58:28.477: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 10 06:58:30.484: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 10 06:58:30.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7210 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 10 06:58:30.829: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 10 06:58:30.829: INFO: stdout: "iptables"
Jan 10 06:58:30.829: INFO: proxyMode: iptables
Jan 10 06:58:30.840: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 10 06:58:30.851: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-7210
STEP: creating replication controller affinity-clusterip-timeout in namespace services-7210
I0110 06:58:30.872527      21 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-7210, replica count: 3
I0110 06:58:33.923654      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 10 06:58:33.932: INFO: Creating new exec pod
Jan 10 06:58:36.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7210 exec execpod-affinity9jst7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 10 06:58:37.091: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 10 06:58:37.091: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 06:58:37.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7210 exec execpod-affinity9jst7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.60.167 80'
Jan 10 06:58:37.247: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.60.167 80\nConnection to 10.233.60.167 80 port [tcp/http] succeeded!\n"
Jan 10 06:58:37.247: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 06:58:37.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7210 exec execpod-affinity9jst7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.60.167:80/ ; done'
Jan 10 06:58:37.489: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n"
Jan 10 06:58:37.489: INFO: stdout: "\naffinity-clusterip-timeout-pp89b\naffinity-clusterip-timeout-pp89b\naffinity-clusterip-timeout-pp89b\naffinity-clusterip-timeout-pp89b\naffinity-clusterip-timeout-pp89b\naffinity-clusterip-timeout-pp89b\naffinity-clusterip-timeout-pp89b\naffinity-clusterip-timeout-pp89b\naffinity-clusterip-timeout-pp89b\naffinity-clusterip-timeout-pp89b\naffinity-clusterip-timeout-pp89b\naffinity-clusterip-timeout-pp89b\naffinity-clusterip-timeout-pp89b\naffinity-clusterip-timeout-pp89b\naffinity-clusterip-timeout-pp89b\naffinity-clusterip-timeout-pp89b"
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Received response from host: affinity-clusterip-timeout-pp89b
Jan 10 06:58:37.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7210 exec execpod-affinity9jst7 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.60.167:80/'
Jan 10 06:58:37.647: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n"
Jan 10 06:58:37.647: INFO: stdout: "affinity-clusterip-timeout-pp89b"
Jan 10 06:58:57.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7210 exec execpod-affinity9jst7 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.60.167:80/'
Jan 10 06:58:57.800: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.60.167:80/\n"
Jan 10 06:58:57.800: INFO: stdout: "affinity-clusterip-timeout-bfv2n"
Jan 10 06:58:57.800: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-7210, will wait for the garbage collector to delete the pods
Jan 10 06:58:57.886: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.548393ms
Jan 10 06:58:57.986: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.79082ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 06:59:00.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7210" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:31.903 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":21,"skipped":402,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:59:00.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-rdk6
STEP: Creating a pod to test atomic-volume-subpath
Jan 10 06:59:00.372: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rdk6" in namespace "subpath-1473" to be "Succeeded or Failed"
Jan 10 06:59:00.376: INFO: Pod "pod-subpath-test-configmap-rdk6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.414553ms
Jan 10 06:59:02.388: INFO: Pod "pod-subpath-test-configmap-rdk6": Phase="Running", Reason="", readiness=true. Elapsed: 2.016404492s
Jan 10 06:59:04.398: INFO: Pod "pod-subpath-test-configmap-rdk6": Phase="Running", Reason="", readiness=true. Elapsed: 4.026233961s
Jan 10 06:59:06.406: INFO: Pod "pod-subpath-test-configmap-rdk6": Phase="Running", Reason="", readiness=true. Elapsed: 6.03451691s
Jan 10 06:59:08.419: INFO: Pod "pod-subpath-test-configmap-rdk6": Phase="Running", Reason="", readiness=true. Elapsed: 8.046755804s
Jan 10 06:59:10.425: INFO: Pod "pod-subpath-test-configmap-rdk6": Phase="Running", Reason="", readiness=true. Elapsed: 10.05288062s
Jan 10 06:59:12.432: INFO: Pod "pod-subpath-test-configmap-rdk6": Phase="Running", Reason="", readiness=true. Elapsed: 12.06054904s
Jan 10 06:59:14.442: INFO: Pod "pod-subpath-test-configmap-rdk6": Phase="Running", Reason="", readiness=true. Elapsed: 14.069807735s
Jan 10 06:59:16.452: INFO: Pod "pod-subpath-test-configmap-rdk6": Phase="Running", Reason="", readiness=true. Elapsed: 16.080507153s
Jan 10 06:59:18.457: INFO: Pod "pod-subpath-test-configmap-rdk6": Phase="Running", Reason="", readiness=true. Elapsed: 18.085609293s
Jan 10 06:59:20.464: INFO: Pod "pod-subpath-test-configmap-rdk6": Phase="Running", Reason="", readiness=true. Elapsed: 20.091990592s
Jan 10 06:59:22.473: INFO: Pod "pod-subpath-test-configmap-rdk6": Phase="Running", Reason="", readiness=false. Elapsed: 22.101579427s
Jan 10 06:59:24.482: INFO: Pod "pod-subpath-test-configmap-rdk6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.109885852s
STEP: Saw pod success
Jan 10 06:59:24.482: INFO: Pod "pod-subpath-test-configmap-rdk6" satisfied condition "Succeeded or Failed"
Jan 10 06:59:24.484: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-subpath-test-configmap-rdk6 container test-container-subpath-configmap-rdk6: <nil>
STEP: delete the pod
Jan 10 06:59:24.511: INFO: Waiting for pod pod-subpath-test-configmap-rdk6 to disappear
Jan 10 06:59:24.514: INFO: Pod pod-subpath-test-configmap-rdk6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rdk6
Jan 10 06:59:24.514: INFO: Deleting pod "pod-subpath-test-configmap-rdk6" in namespace "subpath-1473"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 10 06:59:24.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1473" for this suite.

• [SLOW TEST:24.192 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","total":356,"completed":22,"skipped":461,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:59:24.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 10 06:59:28.608: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 10 06:59:28.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9532" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":23,"skipped":506,"failed":0}
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:59:28.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jan 10 06:59:28.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1950" for this suite.
STEP: Destroying namespace "nspatchtest-c9894b08-506f-41c8-84e5-f9132b1531cf-7235" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":356,"completed":24,"skipped":509,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:59:28.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1412
STEP: creating an pod
Jan 10 06:59:28.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-1822 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.39 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 10 06:59:28.829: INFO: stderr: ""
Jan 10 06:59:28.829: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for log generator to start.
Jan 10 06:59:28.829: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 10 06:59:28.829: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1822" to be "running and ready, or succeeded"
Jan 10 06:59:28.837: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023257ms
Jan 10 06:59:30.844: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.014736434s
Jan 10 06:59:30.844: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 10 06:59:30.844: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jan 10 06:59:30.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-1822 logs logs-generator logs-generator'
Jan 10 06:59:30.932: INFO: stderr: ""
Jan 10 06:59:30.932: INFO: stdout: "I0110 06:59:29.690327       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/rz4m 544\nI0110 06:59:29.890825       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/52z 535\nI0110 06:59:30.091200       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/76px 290\nI0110 06:59:30.290495       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/8nx 509\nI0110 06:59:30.491039       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/8jt 251\nI0110 06:59:30.691365       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/t4sp 281\nI0110 06:59:30.890717       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/j2z 386\n"
STEP: limiting log lines
Jan 10 06:59:30.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-1822 logs logs-generator logs-generator --tail=1'
Jan 10 06:59:31.009: INFO: stderr: ""
Jan 10 06:59:31.009: INFO: stdout: "I0110 06:59:30.890717       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/j2z 386\n"
Jan 10 06:59:31.009: INFO: got output "I0110 06:59:30.890717       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/j2z 386\n"
STEP: limiting log bytes
Jan 10 06:59:31.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-1822 logs logs-generator logs-generator --limit-bytes=1'
Jan 10 06:59:31.089: INFO: stderr: ""
Jan 10 06:59:31.089: INFO: stdout: "I"
Jan 10 06:59:31.089: INFO: got output "I"
STEP: exposing timestamps
Jan 10 06:59:31.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-1822 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 10 06:59:31.166: INFO: stderr: ""
Jan 10 06:59:31.166: INFO: stdout: "2023-01-10T14:59:31.091172451+08:00 I0110 06:59:31.091044       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/7rj 339\n"
Jan 10 06:59:31.166: INFO: got output "2023-01-10T14:59:31.091172451+08:00 I0110 06:59:31.091044       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/7rj 339\n"
STEP: restricting to a time range
Jan 10 06:59:33.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-1822 logs logs-generator logs-generator --since=1s'
Jan 10 06:59:33.764: INFO: stderr: ""
Jan 10 06:59:33.764: INFO: stdout: "I0110 06:59:32.891239       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/qn7b 403\nI0110 06:59:33.090544       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/544 582\nI0110 06:59:33.290985       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/8h22 336\nI0110 06:59:33.491280       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/nkj 221\nI0110 06:59:33.690643       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/z2kn 343\n"
Jan 10 06:59:33.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-1822 logs logs-generator logs-generator --since=24h'
Jan 10 06:59:33.856: INFO: stderr: ""
Jan 10 06:59:33.856: INFO: stdout: "I0110 06:59:29.690327       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/rz4m 544\nI0110 06:59:29.890825       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/52z 535\nI0110 06:59:30.091200       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/76px 290\nI0110 06:59:30.290495       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/8nx 509\nI0110 06:59:30.491039       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/8jt 251\nI0110 06:59:30.691365       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/t4sp 281\nI0110 06:59:30.890717       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/j2z 386\nI0110 06:59:31.091044       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/7rj 339\nI0110 06:59:31.291426       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/j7j 343\nI0110 06:59:31.490785       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/kzz7 579\nI0110 06:59:31.691129       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/t4gd 265\nI0110 06:59:31.890460       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/hxdr 221\nI0110 06:59:32.090818       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/gn52 347\nI0110 06:59:32.291208       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/nbh 256\nI0110 06:59:32.490488       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/sxcz 391\nI0110 06:59:32.690847       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/whrv 439\nI0110 06:59:32.891239       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/qn7b 403\nI0110 06:59:33.090544       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/544 582\nI0110 06:59:33.290985       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/8h22 336\nI0110 06:59:33.491280       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/nkj 221\nI0110 06:59:33.690643       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/z2kn 343\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1417
Jan 10 06:59:33.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-1822 delete pod logs-generator'
Jan 10 06:59:34.643: INFO: stderr: ""
Jan 10 06:59:34.643: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 06:59:34.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1822" for this suite.

• [SLOW TEST:5.954 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1409
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":356,"completed":25,"skipped":545,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:59:34.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 06:59:35.265: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 06:59:38.296: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 06:59:38.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 06:59:41.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5056" for this suite.
STEP: Destroying namespace "webhook-5056-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.832 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":356,"completed":26,"skipped":552,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 06:59:41.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod test-webserver-d62d4c65-8f40-419a-b539-e0ca2caa334b in namespace container-probe-7853
Jan 10 06:59:43.587: INFO: Started pod test-webserver-d62d4c65-8f40-419a-b539-e0ca2caa334b in namespace container-probe-7853
STEP: checking the pod's current state and verifying that restartCount is present
Jan 10 06:59:43.590: INFO: Initial restart count of pod test-webserver-d62d4c65-8f40-419a-b539-e0ca2caa334b is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 10 07:03:44.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7853" for this suite.

• [SLOW TEST:243.192 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":27,"skipped":567,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:03:44.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 10 07:03:44.721: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 10 07:03:49.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8748" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":356,"completed":28,"skipped":611,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:03:49.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:03:49.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 10 07:03:49.500: INFO: The status of Pod pod-logs-websocket-d444c1f7-370b-4a90-8b01-1401cf5559b5 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:03:51.510: INFO: The status of Pod pod-logs-websocket-d444c1f7-370b-4a90-8b01-1401cf5559b5 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 10 07:03:51.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4614" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":356,"completed":29,"skipped":618,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:03:51.546: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 10 07:03:51.586: INFO: The status of Pod annotationupdate7b2a9e4d-5fd1-4dd4-aaee-3bd25197382b is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:03:53.597: INFO: The status of Pod annotationupdate7b2a9e4d-5fd1-4dd4-aaee-3bd25197382b is Running (Ready = true)
Jan 10 07:03:54.137: INFO: Successfully updated pod "annotationupdate7b2a9e4d-5fd1-4dd4-aaee-3bd25197382b"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 10 07:03:58.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5052" for this suite.

• [SLOW TEST:6.625 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":30,"skipped":628,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:03:58.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 10 07:03:58.210: INFO: Waiting up to 5m0s for pod "pod-57440771-8558-4b65-b319-90ee89df8dc0" in namespace "emptydir-8589" to be "Succeeded or Failed"
Jan 10 07:03:58.213: INFO: Pod "pod-57440771-8558-4b65-b319-90ee89df8dc0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.422501ms
Jan 10 07:04:00.219: INFO: Pod "pod-57440771-8558-4b65-b319-90ee89df8dc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008934046s
Jan 10 07:04:02.234: INFO: Pod "pod-57440771-8558-4b65-b319-90ee89df8dc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024042357s
STEP: Saw pod success
Jan 10 07:04:02.234: INFO: Pod "pod-57440771-8558-4b65-b319-90ee89df8dc0" satisfied condition "Succeeded or Failed"
Jan 10 07:04:02.236: INFO: Trying to get logs from node kk-instance-r65pm pod pod-57440771-8558-4b65-b319-90ee89df8dc0 container test-container: <nil>
STEP: delete the pod
Jan 10 07:04:02.254: INFO: Waiting for pod pod-57440771-8558-4b65-b319-90ee89df8dc0 to disappear
Jan 10 07:04:02.256: INFO: Pod pod-57440771-8558-4b65-b319-90ee89df8dc0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 10 07:04:02.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8589" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":31,"skipped":659,"failed":0}
SSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:04:02.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan 10 07:04:02.292: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 10 07:05:02.325: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:05:02.329: INFO: Starting informer...
STEP: Starting pod...
Jan 10 07:05:02.552: INFO: Pod is running on kk-instance-vgmmg. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jan 10 07:05:02.588: INFO: Pod wasn't evicted. Proceeding
Jan 10 07:05:02.588: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jan 10 07:06:17.636: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:188
Jan 10 07:06:17.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-8773" for this suite.

• [SLOW TEST:135.386 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":356,"completed":32,"skipped":662,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:06:17.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 10 07:06:17.705: INFO: The status of Pod pod-update-activedeadlineseconds-6beb18ee-cfc6-4152-8fe5-ed18e5da1398 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:06:19.713: INFO: The status of Pod pod-update-activedeadlineseconds-6beb18ee-cfc6-4152-8fe5-ed18e5da1398 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 10 07:06:20.236: INFO: Successfully updated pod "pod-update-activedeadlineseconds-6beb18ee-cfc6-4152-8fe5-ed18e5da1398"
Jan 10 07:06:20.236: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6beb18ee-cfc6-4152-8fe5-ed18e5da1398" in namespace "pods-3778" to be "terminated due to deadline exceeded"
Jan 10 07:06:20.240: INFO: Pod "pod-update-activedeadlineseconds-6beb18ee-cfc6-4152-8fe5-ed18e5da1398": Phase="Running", Reason="", readiness=true. Elapsed: 3.690126ms
Jan 10 07:06:22.250: INFO: Pod "pod-update-activedeadlineseconds-6beb18ee-cfc6-4152-8fe5-ed18e5da1398": Phase="Running", Reason="", readiness=true. Elapsed: 2.014043938s
Jan 10 07:06:24.259: INFO: Pod "pod-update-activedeadlineseconds-6beb18ee-cfc6-4152-8fe5-ed18e5da1398": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.022587944s
Jan 10 07:06:24.259: INFO: Pod "pod-update-activedeadlineseconds-6beb18ee-cfc6-4152-8fe5-ed18e5da1398" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 10 07:06:24.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3778" for this suite.

• [SLOW TEST:6.617 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":356,"completed":33,"skipped":668,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:06:24.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1574
[It] should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 10 07:06:24.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8844 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 10 07:06:24.389: INFO: stderr: ""
Jan 10 07:06:24.389: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jan 10 07:06:29.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8844 get pod e2e-test-httpd-pod -o json'
Jan 10 07:06:29.526: INFO: stderr: ""
Jan 10 07:06:29.526: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"9790ff039edd7d8f6bf79c3077d1405582203221db45eb089b58e1c47d7abc5f\",\n            \"cni.projectcalico.org/podIP\": \"10.233.107.92/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.107.92/32\"\n        },\n        \"creationTimestamp\": \"2023-01-10T07:06:24Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8844\",\n        \"resourceVersion\": \"15162\",\n        \"uid\": \"d7eedb41-3671-445d-ae4c-f02daa30d748\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-nh5zt\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kk-instance-vgmmg\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-nh5zt\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-10T07:06:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-10T07:06:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-10T07:06:25Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-10T07:06:24Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://e5ba57b8efc088dcb95f6f3f5e41b3c73a6562ef51ca3287b0c16edba273857f\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-10T07:06:25Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.0.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.107.92\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.107.92\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-10T07:06:24Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 10 07:06:29.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8844 replace -f -'
Jan 10 07:06:30.666: INFO: stderr: ""
Jan 10 07:06:30.666: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1578
Jan 10 07:06:30.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8844 delete pods e2e-test-httpd-pod'
Jan 10 07:06:32.849: INFO: stderr: ""
Jan 10 07:06:32.849: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 07:06:32.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8844" for this suite.

• [SLOW TEST:8.596 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1571
    should update a single-container pod's image  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":356,"completed":34,"skipped":674,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:06:32.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 07:06:32.908: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d31a876c-6930-412f-afef-3f8b8872bfaf" in namespace "downward-api-3900" to be "Succeeded or Failed"
Jan 10 07:06:32.911: INFO: Pod "downwardapi-volume-d31a876c-6930-412f-afef-3f8b8872bfaf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.368458ms
Jan 10 07:06:34.917: INFO: Pod "downwardapi-volume-d31a876c-6930-412f-afef-3f8b8872bfaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009271738s
Jan 10 07:06:36.925: INFO: Pod "downwardapi-volume-d31a876c-6930-412f-afef-3f8b8872bfaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01740702s
STEP: Saw pod success
Jan 10 07:06:36.925: INFO: Pod "downwardapi-volume-d31a876c-6930-412f-afef-3f8b8872bfaf" satisfied condition "Succeeded or Failed"
Jan 10 07:06:36.928: INFO: Trying to get logs from node kk-instance-r65pm pod downwardapi-volume-d31a876c-6930-412f-afef-3f8b8872bfaf container client-container: <nil>
STEP: delete the pod
Jan 10 07:06:36.952: INFO: Waiting for pod downwardapi-volume-d31a876c-6930-412f-afef-3f8b8872bfaf to disappear
Jan 10 07:06:36.962: INFO: Pod downwardapi-volume-d31a876c-6930-412f-afef-3f8b8872bfaf no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 10 07:06:36.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3900" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":35,"skipped":679,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:06:36.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5982
STEP: creating service affinity-nodeport-transition in namespace services-5982
STEP: creating replication controller affinity-nodeport-transition in namespace services-5982
I0110 07:06:37.032510      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5982, replica count: 3
I0110 07:06:40.085116      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 10 07:06:40.097: INFO: Creating new exec pod
Jan 10 07:06:43.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5982 exec execpod-affinityvcqhz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 10 07:06:43.293: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 10 07:06:43.293: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:06:43.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5982 exec execpod-affinityvcqhz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.31.209 80'
Jan 10 07:06:43.431: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.31.209 80\nConnection to 10.233.31.209 80 port [tcp/http] succeeded!\n"
Jan 10 07:06:43.431: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:06:43.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5982 exec execpod-affinityvcqhz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.4 32341'
Jan 10 07:06:43.581: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.4 32341\nConnection to 192.168.0.4 32341 port [tcp/*] succeeded!\n"
Jan 10 07:06:43.581: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:06:43.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5982 exec execpod-affinityvcqhz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.5 32341'
Jan 10 07:06:43.726: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.5 32341\nConnection to 192.168.0.5 32341 port [tcp/*] succeeded!\n"
Jan 10 07:06:43.726: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:06:43.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5982 exec execpod-affinityvcqhz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.4:32341/ ; done'
Jan 10 07:06:43.994: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n"
Jan 10 07:06:43.994: INFO: stdout: "\naffinity-nodeport-transition-4m4hz\naffinity-nodeport-transition-d8cfw\naffinity-nodeport-transition-d8cfw\naffinity-nodeport-transition-d8cfw\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-4m4hz\naffinity-nodeport-transition-4m4hz\naffinity-nodeport-transition-d8cfw\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-4m4hz\naffinity-nodeport-transition-d8cfw\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-4m4hz\naffinity-nodeport-transition-d8cfw"
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-4m4hz
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-d8cfw
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-d8cfw
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-d8cfw
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-4m4hz
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-4m4hz
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-d8cfw
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-4m4hz
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-d8cfw
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-4m4hz
Jan 10 07:06:43.994: INFO: Received response from host: affinity-nodeport-transition-d8cfw
Jan 10 07:06:44.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5982 exec execpod-affinityvcqhz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.4:32341/ ; done'
Jan 10 07:06:44.266: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:32341/\n"
Jan 10 07:06:44.266: INFO: stdout: "\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp\naffinity-nodeport-transition-vdfzp"
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Received response from host: affinity-nodeport-transition-vdfzp
Jan 10 07:06:44.266: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5982, will wait for the garbage collector to delete the pods
Jan 10 07:06:44.357: INFO: Deleting ReplicationController affinity-nodeport-transition took: 13.741791ms
Jan 10 07:06:44.459: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.096863ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 07:06:46.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5982" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:10.031 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":36,"skipped":684,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:06:47.005: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 10 07:06:47.974: INFO: starting watch
STEP: patching
STEP: updating
Jan 10 07:06:47.992: INFO: waiting for watch events with expected annotations
Jan 10 07:06:47.992: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:06:48.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-878" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":356,"completed":37,"skipped":693,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:06:48.086: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 10 07:06:48.116: INFO: Waiting up to 5m0s for pod "pod-ed522f45-5296-4cd7-9746-23c6ec15c8c3" in namespace "emptydir-3145" to be "Succeeded or Failed"
Jan 10 07:06:48.124: INFO: Pod "pod-ed522f45-5296-4cd7-9746-23c6ec15c8c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.843065ms
Jan 10 07:06:50.129: INFO: Pod "pod-ed522f45-5296-4cd7-9746-23c6ec15c8c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013370751s
Jan 10 07:06:52.141: INFO: Pod "pod-ed522f45-5296-4cd7-9746-23c6ec15c8c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02544874s
STEP: Saw pod success
Jan 10 07:06:52.141: INFO: Pod "pod-ed522f45-5296-4cd7-9746-23c6ec15c8c3" satisfied condition "Succeeded or Failed"
Jan 10 07:06:52.145: INFO: Trying to get logs from node kk-instance-r65pm pod pod-ed522f45-5296-4cd7-9746-23c6ec15c8c3 container test-container: <nil>
STEP: delete the pod
Jan 10 07:06:52.169: INFO: Waiting for pod pod-ed522f45-5296-4cd7-9746-23c6ec15c8c3 to disappear
Jan 10 07:06:52.171: INFO: Pod pod-ed522f45-5296-4cd7-9746-23c6ec15c8c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 10 07:06:52.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3145" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":38,"skipped":718,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:06:52.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replication controller my-hostname-basic-9ea00d32-e968-42ec-977f-073e853411ce
Jan 10 07:06:52.239: INFO: Pod name my-hostname-basic-9ea00d32-e968-42ec-977f-073e853411ce: Found 0 pods out of 1
Jan 10 07:06:57.245: INFO: Pod name my-hostname-basic-9ea00d32-e968-42ec-977f-073e853411ce: Found 1 pods out of 1
Jan 10 07:06:57.245: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9ea00d32-e968-42ec-977f-073e853411ce" are running
Jan 10 07:06:57.248: INFO: Pod "my-hostname-basic-9ea00d32-e968-42ec-977f-073e853411ce-lgqhv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-10 07:06:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-10 07:06:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-10 07:06:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-10 07:06:52 +0000 UTC Reason: Message:}])
Jan 10 07:06:57.248: INFO: Trying to dial the pod
Jan 10 07:07:02.268: INFO: Controller my-hostname-basic-9ea00d32-e968-42ec-977f-073e853411ce: Got expected result from replica 1 [my-hostname-basic-9ea00d32-e968-42ec-977f-073e853411ce-lgqhv]: "my-hostname-basic-9ea00d32-e968-42ec-977f-073e853411ce-lgqhv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 10 07:07:02.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1692" for this suite.

• [SLOW TEST:10.096 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":39,"skipped":738,"failed":0}
SSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:07:02.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  test/e2e/framework/framework.go:652
STEP: Request ServerVersion
STEP: Confirm major version
Jan 10 07:07:02.312: INFO: Major version: 1
STEP: Confirm minor version
Jan 10 07:07:02.312: INFO: cleanMinorVersion: 24
Jan 10 07:07:02.312: INFO: Minor version: 24
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:188
Jan 10 07:07:02.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-3719" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":356,"completed":40,"skipped":741,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:07:02.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:07:02.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7292" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":356,"completed":41,"skipped":750,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:07:02.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-31dac83d-e338-499e-8f64-a26c87ae01e4
STEP: Creating a pod to test consume configMaps
Jan 10 07:07:02.418: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5703b8e-4a16-4cb1-a202-c40a011e6c2f" in namespace "configmap-5011" to be "Succeeded or Failed"
Jan 10 07:07:02.422: INFO: Pod "pod-configmaps-e5703b8e-4a16-4cb1-a202-c40a011e6c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.291141ms
Jan 10 07:07:04.431: INFO: Pod "pod-configmaps-e5703b8e-4a16-4cb1-a202-c40a011e6c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012757843s
Jan 10 07:07:06.438: INFO: Pod "pod-configmaps-e5703b8e-4a16-4cb1-a202-c40a011e6c2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019131644s
STEP: Saw pod success
Jan 10 07:07:06.438: INFO: Pod "pod-configmaps-e5703b8e-4a16-4cb1-a202-c40a011e6c2f" satisfied condition "Succeeded or Failed"
Jan 10 07:07:06.442: INFO: Trying to get logs from node kk-instance-r65pm pod pod-configmaps-e5703b8e-4a16-4cb1-a202-c40a011e6c2f container agnhost-container: <nil>
STEP: delete the pod
Jan 10 07:07:06.469: INFO: Waiting for pod pod-configmaps-e5703b8e-4a16-4cb1-a202-c40a011e6c2f to disappear
Jan 10 07:07:06.472: INFO: Pod pod-configmaps-e5703b8e-4a16-4cb1-a202-c40a011e6c2f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 10 07:07:06.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5011" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":42,"skipped":766,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:07:06.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating all guestbook components
Jan 10 07:07:06.553: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 10 07:07:06.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6516 create -f -'
Jan 10 07:07:06.781: INFO: stderr: ""
Jan 10 07:07:06.781: INFO: stdout: "service/agnhost-replica created\n"
Jan 10 07:07:06.781: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 10 07:07:06.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6516 create -f -'
Jan 10 07:07:07.061: INFO: stderr: ""
Jan 10 07:07:07.061: INFO: stdout: "service/agnhost-primary created\n"
Jan 10 07:07:07.061: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 10 07:07:07.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6516 create -f -'
Jan 10 07:07:07.302: INFO: stderr: ""
Jan 10 07:07:07.302: INFO: stdout: "service/frontend created\n"
Jan 10 07:07:07.302: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 10 07:07:07.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6516 create -f -'
Jan 10 07:07:07.589: INFO: stderr: ""
Jan 10 07:07:07.589: INFO: stdout: "deployment.apps/frontend created\n"
Jan 10 07:07:07.589: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 10 07:07:07.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6516 create -f -'
Jan 10 07:07:07.830: INFO: stderr: ""
Jan 10 07:07:07.830: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 10 07:07:07.830: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 10 07:07:07.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6516 create -f -'
Jan 10 07:07:08.155: INFO: stderr: ""
Jan 10 07:07:08.155: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jan 10 07:07:08.155: INFO: Waiting for all frontend pods to be Running.
Jan 10 07:07:13.206: INFO: Waiting for frontend to serve content.
Jan 10 07:07:13.217: INFO: Trying to add a new entry to the guestbook.
Jan 10 07:07:13.228: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan 10 07:07:13.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6516 delete --grace-period=0 --force -f -'
Jan 10 07:07:13.335: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 10 07:07:13.335: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jan 10 07:07:13.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6516 delete --grace-period=0 --force -f -'
Jan 10 07:07:13.449: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 10 07:07:13.449: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 10 07:07:13.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6516 delete --grace-period=0 --force -f -'
Jan 10 07:07:13.551: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 10 07:07:13.551: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 10 07:07:13.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6516 delete --grace-period=0 --force -f -'
Jan 10 07:07:13.637: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 10 07:07:13.637: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 10 07:07:13.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6516 delete --grace-period=0 --force -f -'
Jan 10 07:07:13.759: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 10 07:07:13.759: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 10 07:07:13.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6516 delete --grace-period=0 --force -f -'
Jan 10 07:07:13.875: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 10 07:07:13.875: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 07:07:13.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6516" for this suite.

• [SLOW TEST:7.410 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:340
    should create and stop a working application  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":356,"completed":43,"skipped":796,"failed":0}
[sig-node] Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:07:13.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override all
Jan 10 07:07:13.948: INFO: Waiting up to 5m0s for pod "client-containers-1bd1199e-eaa6-4ede-be1d-41018f93eef5" in namespace "containers-3759" to be "Succeeded or Failed"
Jan 10 07:07:13.953: INFO: Pod "client-containers-1bd1199e-eaa6-4ede-be1d-41018f93eef5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.701747ms
Jan 10 07:07:15.961: INFO: Pod "client-containers-1bd1199e-eaa6-4ede-be1d-41018f93eef5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012628972s
Jan 10 07:07:17.970: INFO: Pod "client-containers-1bd1199e-eaa6-4ede-be1d-41018f93eef5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021962102s
STEP: Saw pod success
Jan 10 07:07:17.970: INFO: Pod "client-containers-1bd1199e-eaa6-4ede-be1d-41018f93eef5" satisfied condition "Succeeded or Failed"
Jan 10 07:07:17.974: INFO: Trying to get logs from node kk-instance-r65pm pod client-containers-1bd1199e-eaa6-4ede-be1d-41018f93eef5 container agnhost-container: <nil>
STEP: delete the pod
Jan 10 07:07:17.999: INFO: Waiting for pod client-containers-1bd1199e-eaa6-4ede-be1d-41018f93eef5 to disappear
Jan 10 07:07:18.009: INFO: Pod client-containers-1bd1199e-eaa6-4ede-be1d-41018f93eef5 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 10 07:07:18.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3759" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":356,"completed":44,"skipped":796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:07:18.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 10 07:07:18.062: INFO: Waiting up to 5m0s for pod "pod-573e9be0-c902-4d16-8c58-0d8d8fc9cae8" in namespace "emptydir-6470" to be "Succeeded or Failed"
Jan 10 07:07:18.068: INFO: Pod "pod-573e9be0-c902-4d16-8c58-0d8d8fc9cae8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.231098ms
Jan 10 07:07:20.073: INFO: Pod "pod-573e9be0-c902-4d16-8c58-0d8d8fc9cae8": Phase="Running", Reason="", readiness=true. Elapsed: 2.010233969s
Jan 10 07:07:22.086: INFO: Pod "pod-573e9be0-c902-4d16-8c58-0d8d8fc9cae8": Phase="Running", Reason="", readiness=false. Elapsed: 4.023334635s
Jan 10 07:07:24.096: INFO: Pod "pod-573e9be0-c902-4d16-8c58-0d8d8fc9cae8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033197522s
STEP: Saw pod success
Jan 10 07:07:24.096: INFO: Pod "pod-573e9be0-c902-4d16-8c58-0d8d8fc9cae8" satisfied condition "Succeeded or Failed"
Jan 10 07:07:24.098: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-573e9be0-c902-4d16-8c58-0d8d8fc9cae8 container test-container: <nil>
STEP: delete the pod
Jan 10 07:07:24.124: INFO: Waiting for pod pod-573e9be0-c902-4d16-8c58-0d8d8fc9cae8 to disappear
Jan 10 07:07:24.126: INFO: Pod pod-573e9be0-c902-4d16-8c58-0d8d8fc9cae8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 10 07:07:24.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6470" for this suite.

• [SLOW TEST:6.110 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":45,"skipped":820,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:07:24.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jan 10 07:07:26.215: INFO: running pods: 0 < 1
Jan 10 07:07:28.226: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 10 07:07:30.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9728" for this suite.

• [SLOW TEST:6.119 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":356,"completed":46,"skipped":843,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:07:30.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override command
Jan 10 07:07:30.297: INFO: Waiting up to 5m0s for pod "client-containers-5ccf2060-be52-4c5a-b163-bdb7956922be" in namespace "containers-5512" to be "Succeeded or Failed"
Jan 10 07:07:30.302: INFO: Pod "client-containers-5ccf2060-be52-4c5a-b163-bdb7956922be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.829454ms
Jan 10 07:07:32.310: INFO: Pod "client-containers-5ccf2060-be52-4c5a-b163-bdb7956922be": Phase="Running", Reason="", readiness=false. Elapsed: 2.013200165s
Jan 10 07:07:34.321: INFO: Pod "client-containers-5ccf2060-be52-4c5a-b163-bdb7956922be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024104794s
STEP: Saw pod success
Jan 10 07:07:34.321: INFO: Pod "client-containers-5ccf2060-be52-4c5a-b163-bdb7956922be" satisfied condition "Succeeded or Failed"
Jan 10 07:07:34.324: INFO: Trying to get logs from node kk-instance-vgmmg pod client-containers-5ccf2060-be52-4c5a-b163-bdb7956922be container agnhost-container: <nil>
STEP: delete the pod
Jan 10 07:07:34.341: INFO: Waiting for pod client-containers-5ccf2060-be52-4c5a-b163-bdb7956922be to disappear
Jan 10 07:07:34.343: INFO: Pod client-containers-5ccf2060-be52-4c5a-b163-bdb7956922be no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 10 07:07:34.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5512" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","total":356,"completed":47,"skipped":852,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:07:34.352: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Indexed job
STEP: Ensuring job reaches completions
STEP: Ensuring pods with index for job exist
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 10 07:07:46.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6431" for this suite.

• [SLOW TEST:12.050 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","total":356,"completed":48,"skipped":882,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:07:46.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 10 07:07:50.484: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 10 07:07:50.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9923" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":49,"skipped":920,"failed":0}
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:07:50.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:188
Jan 10 07:07:50.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2429" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":356,"completed":50,"skipped":921,"failed":0}

------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:07:50.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:07:50.584: INFO: Creating deployment "webserver-deployment"
Jan 10 07:07:50.588: INFO: Waiting for observed generation 1
Jan 10 07:07:52.632: INFO: Waiting for all required pods to come up
Jan 10 07:07:52.661: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 10 07:07:56.684: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 10 07:07:56.692: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 10 07:07:56.701: INFO: Updating deployment webserver-deployment
Jan 10 07:07:56.701: INFO: Waiting for observed generation 2
Jan 10 07:07:58.714: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 10 07:07:58.717: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 10 07:07:58.720: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 10 07:07:58.732: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 10 07:07:58.732: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 10 07:07:58.736: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 10 07:07:58.742: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 10 07:07:58.742: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 10 07:07:58.755: INFO: Updating deployment webserver-deployment
Jan 10 07:07:58.755: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 10 07:07:58.766: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 10 07:07:58.773: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 10 07:07:58.796: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3519  a4c52964-f31a-41c9-a58f-976dda676bab 16617 3 2023-01-10 07:07:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-01-10 07:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00239ecd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-57ccb67bb8" is progressing.,LastUpdateTime:2023-01-10 07:07:56 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-10 07:07:58 +0000 UTC,LastTransitionTime:2023-01-10 07:07:58 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 10 07:07:58.810: INFO: New ReplicaSet "webserver-deployment-57ccb67bb8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-57ccb67bb8  deployment-3519  c6ddecf3-edba-4f31-a0b2-764a52f2c6b6 16614 3 2023-01-10 07:07:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment a4c52964-f31a-41c9-a58f-976dda676bab 0xc00239f1d7 0xc00239f1d8}] []  [{kube-controller-manager Update apps/v1 2023-01-10 07:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4c52964-f31a-41c9-a58f-976dda676bab\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:07:56 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 57ccb67bb8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00239f278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 10 07:07:58.810: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 10 07:07:58.811: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-55df494869  deployment-3519  e00157f0-cba5-431a-9704-d52d8eafa755 16611 3 2023-01-10 07:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment a4c52964-f31a-41c9-a58f-976dda676bab 0xc00239f0d7 0xc00239f0d8}] []  [{kube-controller-manager Update apps/v1 2023-01-10 07:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4c52964-f31a-41c9-a58f-976dda676bab\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:07:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00239f178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 10 07:07:58.836: INFO: Pod "webserver-deployment-55df494869-7ppdt" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-7ppdt webserver-deployment-55df494869- deployment-3519  c1253702-5b3c-4ec4-b49b-0aa9e07157b9 16627 0 2023-01-10 07:07:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 e00157f0-cba5-431a-9704-d52d8eafa755 0xc0024aaf57 0xc0024aaf58}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e00157f0-cba5-431a-9704-d52d8eafa755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5fkh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5fkh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-vgmmg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.837: INFO: Pod "webserver-deployment-55df494869-8lcjb" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-8lcjb webserver-deployment-55df494869- deployment-3519  3ec0a5dd-3c50-483c-8f2b-3f7b93efa139 16463 0 2023-01-10 07:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:94ab360fbb601efa800d564f087f58cef5a8263bbf4484f5e5c5f0cfc64db384 cni.projectcalico.org/podIP:10.233.82.33/32 cni.projectcalico.org/podIPs:10.233.82.33/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 e00157f0-cba5-431a-9704-d52d8eafa755 0xc0024ab0d0 0xc0024ab0d1}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e00157f0-cba5-431a-9704-d52d8eafa755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-10 07:07:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-10 07:07:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.82.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xfj4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xfj4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-r65pm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.233.82.33,StartTime:2023-01-10 07:07:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:07:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4aa9cf212a38598d513b9d1abf4850d74836ab47ec03751215fbe72e8792e61d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.82.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.837: INFO: Pod "webserver-deployment-55df494869-8xt5j" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-8xt5j webserver-deployment-55df494869- deployment-3519  eca2146c-00cf-4751-87bd-ee2a9dfc5fbf 16488 0 2023-01-10 07:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:f8bc14e8fa1faf1efeff78170866d7287b28a0b287928d5f28fb81231b14577e cni.projectcalico.org/podIP:10.233.82.36/32 cni.projectcalico.org/podIPs:10.233.82.36/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 e00157f0-cba5-431a-9704-d52d8eafa755 0xc0024ab2e7 0xc0024ab2e8}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e00157f0-cba5-431a-9704-d52d8eafa755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-10 07:07:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-10 07:07:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.82.36\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gst6v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gst6v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-r65pm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.233.82.36,StartTime:2023-01-10 07:07:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:07:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://df4f1b7bae39fabb96e8cd1065c415a7fa83f61061d9542f7830d6c2f42b3df2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.82.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.837: INFO: Pod "webserver-deployment-55df494869-d4nls" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-d4nls webserver-deployment-55df494869- deployment-3519  bb07709f-e044-408f-9bec-52a08f2a3ce2 16505 0 2023-01-10 07:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:31c82419d803a0c5ff3bc1ecde8977babf347ff9487b64a662401122dcafb2d0 cni.projectcalico.org/podIP:10.233.107.107/32 cni.projectcalico.org/podIPs:10.233.107.107/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 e00157f0-cba5-431a-9704-d52d8eafa755 0xc0024ab4e7 0xc0024ab4e8}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e00157f0-cba5-431a-9704-d52d8eafa755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-10 07:07:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-10 07:07:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.107.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6fxbb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6fxbb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-vgmmg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.233.107.107,StartTime:2023-01-10 07:07:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:07:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://10c66f40fd5395d686b270adea3ce12977c46eef0109e10ff79d6a8c5e8a4bd8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.107.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.838: INFO: Pod "webserver-deployment-55df494869-nbgxw" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-nbgxw webserver-deployment-55df494869- deployment-3519  d71aac13-8eec-4d35-87b1-521ec17950a8 16476 0 2023-01-10 07:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:e88ed63abb0a581781fb33d8a52f5cda6cb876522146d97719f45a74b0726709 cni.projectcalico.org/podIP:10.233.107.103/32 cni.projectcalico.org/podIPs:10.233.107.103/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 e00157f0-cba5-431a-9704-d52d8eafa755 0xc0024ab6e7 0xc0024ab6e8}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e00157f0-cba5-431a-9704-d52d8eafa755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-10 07:07:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-10 07:07:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.107.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-svlhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-svlhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-vgmmg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.233.107.103,StartTime:2023-01-10 07:07:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:07:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://79556c7f673b0de0a4f3bec2bf096121a960514cbccd1b37f669134c345554a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.107.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.838: INFO: Pod "webserver-deployment-55df494869-r4vvv" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-r4vvv webserver-deployment-55df494869- deployment-3519  b95c0b25-ab25-4200-8ccf-941b63f94410 16491 0 2023-01-10 07:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:ce7214915b8dbd4b60dc14fbcd95f54c501a457f4d9861a3aa7736695df1b47c cni.projectcalico.org/podIP:10.233.82.37/32 cni.projectcalico.org/podIPs:10.233.82.37/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 e00157f0-cba5-431a-9704-d52d8eafa755 0xc0024ab907 0xc0024ab908}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e00157f0-cba5-431a-9704-d52d8eafa755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-10 07:07:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-10 07:07:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.82.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bmhrs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bmhrs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-r65pm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.233.82.37,StartTime:2023-01-10 07:07:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:07:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7e56e6a6557ade60011f950c8e7a68a0e33b5bd1cd8ad4a06a0d9e585d5fdbcb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.82.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.839: INFO: Pod "webserver-deployment-55df494869-t7vs8" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-t7vs8 webserver-deployment-55df494869- deployment-3519  75541053-6c25-4dd6-8edb-f567b9dacf50 16454 0 2023-01-10 07:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:46cec3b031d0fbc7055a0b2d8797a622aa181f02c6229d4471551269b219a351 cni.projectcalico.org/podIP:10.233.82.34/32 cni.projectcalico.org/podIPs:10.233.82.34/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 e00157f0-cba5-431a-9704-d52d8eafa755 0xc0024abb27 0xc0024abb28}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e00157f0-cba5-431a-9704-d52d8eafa755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-10 07:07:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-10 07:07:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.82.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-86jzn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-86jzn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-r65pm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.233.82.34,StartTime:2023-01-10 07:07:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:07:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4df5f0591bb39f999cc0ac3ab5aa0923b374757a43a2cb8ef4491e65dacb5868,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.82.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.839: INFO: Pod "webserver-deployment-55df494869-vjncw" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-vjncw webserver-deployment-55df494869- deployment-3519  6fb577e2-127a-4dbe-9f9a-c103d2c365f7 16500 0 2023-01-10 07:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:981b0e73b6a96130e0fd8f12b9abe5202e968b5da01bd803cd39281a20afd2c4 cni.projectcalico.org/podIP:10.233.107.104/32 cni.projectcalico.org/podIPs:10.233.107.104/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 e00157f0-cba5-431a-9704-d52d8eafa755 0xc0024abd47 0xc0024abd48}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e00157f0-cba5-431a-9704-d52d8eafa755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-10 07:07:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-10 07:07:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.107.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q4k65,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q4k65,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-vgmmg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.233.107.104,StartTime:2023-01-10 07:07:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:07:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ebf753f60c5fc49a587a5af7a203762b2105546be39bc4451156d2a057b9caeb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.107.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.840: INFO: Pod "webserver-deployment-55df494869-vmrzr" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-vmrzr webserver-deployment-55df494869- deployment-3519  81873c9a-3a7c-46eb-a710-62570ab7df0e 16485 0 2023-01-10 07:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:05f6f0e2f04597654d25cc704896448a7c29af1bd6065fd5d50df95823ab1a2e cni.projectcalico.org/podIP:10.233.82.35/32 cni.projectcalico.org/podIPs:10.233.82.35/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 e00157f0-cba5-431a-9704-d52d8eafa755 0xc0024abf77 0xc0024abf78}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e00157f0-cba5-431a-9704-d52d8eafa755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-10 07:07:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-10 07:07:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.82.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfbx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfbx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-r65pm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.233.82.35,StartTime:2023-01-10 07:07:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:07:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f99910e81d471ff3f33a7beeff6b5869bb0a8d9070add3b32ed266f32368e4e1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.82.35,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.840: INFO: Pod "webserver-deployment-55df494869-zcfq5" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-zcfq5 webserver-deployment-55df494869- deployment-3519  a6819065-220d-4b50-ae18-72ee3efb6f22 16619 0 2023-01-10 07:07:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 e00157f0-cba5-431a-9704-d52d8eafa755 0xc0023fc177 0xc0023fc178}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e00157f0-cba5-431a-9704-d52d8eafa755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q4k9q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q4k9q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-vgmmg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.841: INFO: Pod "webserver-deployment-55df494869-zf7kg" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-zf7kg webserver-deployment-55df494869- deployment-3519  1329999a-f491-4f72-9d97-97cb583cd8c9 16628 0 2023-01-10 07:07:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 e00157f0-cba5-431a-9704-d52d8eafa755 0xc0023fc2e0 0xc0023fc2e1}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e00157f0-cba5-431a-9704-d52d8eafa755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j75bc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j75bc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-r65pm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.841: INFO: Pod "webserver-deployment-57ccb67bb8-67hcl" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-67hcl webserver-deployment-57ccb67bb8- deployment-3519  c58d4629-2db4-45e4-b7f9-8e51a5712f47 16620 0 2023-01-10 07:07:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c6ddecf3-edba-4f31-a0b2-764a52f2c6b6 0xc0023fc450 0xc0023fc451}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6ddecf3-edba-4f31-a0b2-764a52f2c6b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94bw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94bw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-r65pm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.841: INFO: Pod "webserver-deployment-57ccb67bb8-gjknb" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-gjknb webserver-deployment-57ccb67bb8- deployment-3519  cb16cb41-5935-4de4-b503-3686bef50095 16624 0 2023-01-10 07:07:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c6ddecf3-edba-4f31-a0b2-764a52f2c6b6 0xc0023fc5c0 0xc0023fc5c1}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6ddecf3-edba-4f31-a0b2-764a52f2c6b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kgzgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kgzgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.841: INFO: Pod "webserver-deployment-57ccb67bb8-mfbbn" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-mfbbn webserver-deployment-57ccb67bb8- deployment-3519  924d0419-484e-4bbe-b948-ad24548d9b30 16585 0 2023-01-10 07:07:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:ecd4ef9e7f9104113a992fd7c86bf007f049fd36356aa85e3038e3ab969f432c cni.projectcalico.org/podIP:10.233.82.38/32 cni.projectcalico.org/podIPs:10.233.82.38/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c6ddecf3-edba-4f31-a0b2-764a52f2c6b6 0xc0023fc727 0xc0023fc728}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6ddecf3-edba-4f31-a0b2-764a52f2c6b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-10 07:07:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-10 07:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4f4rh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4f4rh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-r65pm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:,StartTime:2023-01-10 07:07:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.841: INFO: Pod "webserver-deployment-57ccb67bb8-p4whf" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-p4whf webserver-deployment-57ccb67bb8- deployment-3519  870000ee-6640-4e66-8fcd-038b9290ff57 16626 0 2023-01-10 07:07:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c6ddecf3-edba-4f31-a0b2-764a52f2c6b6 0xc0023fc920 0xc0023fc921}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6ddecf3-edba-4f31-a0b2-764a52f2c6b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v9grm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9grm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.842: INFO: Pod "webserver-deployment-57ccb67bb8-r22p2" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-r22p2 webserver-deployment-57ccb67bb8- deployment-3519  cc992be4-0e52-41d4-8d07-dbdb13068110 16599 0 2023-01-10 07:07:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:65adb619e13773c03ad9f372cb0e8d0b6f0d15171ab3639d8ca0aa80d23393cf cni.projectcalico.org/podIP:10.233.107.110/32 cni.projectcalico.org/podIPs:10.233.107.110/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c6ddecf3-edba-4f31-a0b2-764a52f2c6b6 0xc0023fca67 0xc0023fca68}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6ddecf3-edba-4f31-a0b2-764a52f2c6b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-10 07:07:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-10 07:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6kbkz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6kbkz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-vgmmg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:,StartTime:2023-01-10 07:07:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.842: INFO: Pod "webserver-deployment-57ccb67bb8-wsk7v" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-wsk7v webserver-deployment-57ccb67bb8- deployment-3519  9aef52d0-35bd-41b9-b364-dfae15decb08 16589 0 2023-01-10 07:07:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:8e471f2011b3e0db07ac16eaa53d7ffccc3648abd3a50717133fd3f6d01082ed cni.projectcalico.org/podIP:10.233.82.39/32 cni.projectcalico.org/podIPs:10.233.82.39/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c6ddecf3-edba-4f31-a0b2-764a52f2c6b6 0xc0023fcc80 0xc0023fcc81}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6ddecf3-edba-4f31-a0b2-764a52f2c6b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-10 07:07:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-10 07:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hbfqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hbfqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-r65pm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:,StartTime:2023-01-10 07:07:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.842: INFO: Pod "webserver-deployment-57ccb67bb8-x4sjk" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-x4sjk webserver-deployment-57ccb67bb8- deployment-3519  1120e6c7-e1c3-4df4-9e90-cc85a67443a7 16594 0 2023-01-10 07:07:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:00a81d3c90a1d6792dbec8a6049bd30884b4f092d8605ce6c1255b0418f978b8 cni.projectcalico.org/podIP:10.233.107.109/32 cni.projectcalico.org/podIPs:10.233.107.109/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c6ddecf3-edba-4f31-a0b2-764a52f2c6b6 0xc0023fce70 0xc0023fce71}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6ddecf3-edba-4f31-a0b2-764a52f2c6b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-10 07:07:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-10 07:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4d6v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4d6v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-vgmmg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:,StartTime:2023-01-10 07:07:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:07:58.843: INFO: Pod "webserver-deployment-57ccb67bb8-xlrj5" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-xlrj5 webserver-deployment-57ccb67bb8- deployment-3519  ce30bca3-c26e-4345-9ad0-f2b8cafc9c32 16584 0 2023-01-10 07:07:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:86e23a6d23ff97364fc64f6d832ed36569b79aa8ee9dd4971ae5753350fcd2f3 cni.projectcalico.org/podIP:10.233.107.108/32 cni.projectcalico.org/podIPs:10.233.107.108/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c6ddecf3-edba-4f31-a0b2-764a52f2c6b6 0xc0023fd060 0xc0023fd061}] []  [{kube-controller-manager Update v1 2023-01-10 07:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6ddecf3-edba-4f31-a0b2-764a52f2c6b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-10 07:07:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-10 07:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jm2h7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jm2h7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-vgmmg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:07:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:,StartTime:2023-01-10 07:07:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 10 07:07:58.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3519" for this suite.

• [SLOW TEST:8.348 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":356,"completed":51,"skipped":921,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:07:58.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:07:59.037: INFO: The status of Pod busybox-host-aliases215e9037-1535-4e66-96b0-cb66511802da is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:08:01.045: INFO: The status of Pod busybox-host-aliases215e9037-1535-4e66-96b0-cb66511802da is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:08:03.047: INFO: The status of Pod busybox-host-aliases215e9037-1535-4e66-96b0-cb66511802da is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:08:05.041: INFO: The status of Pod busybox-host-aliases215e9037-1535-4e66-96b0-cb66511802da is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:08:07.042: INFO: The status of Pod busybox-host-aliases215e9037-1535-4e66-96b0-cb66511802da is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:08:09.042: INFO: The status of Pod busybox-host-aliases215e9037-1535-4e66-96b0-cb66511802da is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 10 07:08:09.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1394" for this suite.

• [SLOW TEST:10.149 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox Pod with hostAliases
  test/e2e/common/node/kubelet.go:139
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":52,"skipped":952,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:08:09.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 07:08:09.102: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53cfe7e8-4d78-4233-8399-9473c70c66ab" in namespace "projected-825" to be "Succeeded or Failed"
Jan 10 07:08:09.115: INFO: Pod "downwardapi-volume-53cfe7e8-4d78-4233-8399-9473c70c66ab": Phase="Pending", Reason="", readiness=false. Elapsed: 12.979157ms
Jan 10 07:08:11.123: INFO: Pod "downwardapi-volume-53cfe7e8-4d78-4233-8399-9473c70c66ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021586707s
Jan 10 07:08:13.131: INFO: Pod "downwardapi-volume-53cfe7e8-4d78-4233-8399-9473c70c66ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029728064s
STEP: Saw pod success
Jan 10 07:08:13.131: INFO: Pod "downwardapi-volume-53cfe7e8-4d78-4233-8399-9473c70c66ab" satisfied condition "Succeeded or Failed"
Jan 10 07:08:13.134: INFO: Trying to get logs from node kk-instance-r65pm pod downwardapi-volume-53cfe7e8-4d78-4233-8399-9473c70c66ab container client-container: <nil>
STEP: delete the pod
Jan 10 07:08:13.152: INFO: Waiting for pod downwardapi-volume-53cfe7e8-4d78-4233-8399-9473c70c66ab to disappear
Jan 10 07:08:13.156: INFO: Pod downwardapi-volume-53cfe7e8-4d78-4233-8399-9473c70c66ab no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 10 07:08:13.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-825" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":53,"skipped":986,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:08:13.168: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-0854c8a2-b25d-48c8-a3c5-f3e867ebf81b
STEP: Creating a pod to test consume configMaps
Jan 10 07:08:13.263: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-558f9853-177d-429f-ac1d-085c9fd1eeb2" in namespace "projected-4621" to be "Succeeded or Failed"
Jan 10 07:08:13.267: INFO: Pod "pod-projected-configmaps-558f9853-177d-429f-ac1d-085c9fd1eeb2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.863987ms
Jan 10 07:08:15.273: INFO: Pod "pod-projected-configmaps-558f9853-177d-429f-ac1d-085c9fd1eeb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010707621s
Jan 10 07:08:17.279: INFO: Pod "pod-projected-configmaps-558f9853-177d-429f-ac1d-085c9fd1eeb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016749804s
STEP: Saw pod success
Jan 10 07:08:17.280: INFO: Pod "pod-projected-configmaps-558f9853-177d-429f-ac1d-085c9fd1eeb2" satisfied condition "Succeeded or Failed"
Jan 10 07:08:17.282: INFO: Trying to get logs from node kk-instance-r65pm pod pod-projected-configmaps-558f9853-177d-429f-ac1d-085c9fd1eeb2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 10 07:08:17.299: INFO: Waiting for pod pod-projected-configmaps-558f9853-177d-429f-ac1d-085c9fd1eeb2 to disappear
Jan 10 07:08:17.301: INFO: Pod pod-projected-configmaps-558f9853-177d-429f-ac1d-085c9fd1eeb2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 10 07:08:17.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4621" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":54,"skipped":1001,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:08:17.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:08:17.339: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 10 07:08:22.348: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 10 07:08:22.349: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 10 07:08:24.393: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3339  795a3761-3727-4e9f-8179-296489f957a9 17110 1 2023-01-10 07:08:22 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2023-01-10 07:08:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:08:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a4b088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-10 07:08:22 +0000 UTC,LastTransitionTime:2023-01-10 07:08:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-6755c7b765" has successfully progressed.,LastUpdateTime:2023-01-10 07:08:23 +0000 UTC,LastTransitionTime:2023-01-10 07:08:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 10 07:08:24.396: INFO: New ReplicaSet "test-cleanup-deployment-6755c7b765" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-6755c7b765  deployment-3339  fd7cb208-20d6-4d2d-aa72-d9a6b8cb39a4 17100 1 2023-01-10 07:08:22 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:6755c7b765] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 795a3761-3727-4e9f-8179-296489f957a9 0xc002a4b457 0xc002a4b458}] []  [{kube-controller-manager Update apps/v1 2023-01-10 07:08:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"795a3761-3727-4e9f-8179-296489f957a9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:08:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 6755c7b765,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:6755c7b765] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a4b508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 10 07:08:24.399: INFO: Pod "test-cleanup-deployment-6755c7b765-h5697" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-6755c7b765-h5697 test-cleanup-deployment-6755c7b765- deployment-3339  9e64b425-eb73-4f71-a433-a3b732a3c102 17099 0 2023-01-10 07:08:22 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:6755c7b765] map[cni.projectcalico.org/containerID:18e4764eeff9d07ac59d04cf063d7ca8575f68fb9a5a0bbc949046427b6a31fe cni.projectcalico.org/podIP:10.233.82.43/32 cni.projectcalico.org/podIPs:10.233.82.43/32] [{apps/v1 ReplicaSet test-cleanup-deployment-6755c7b765 fd7cb208-20d6-4d2d-aa72-d9a6b8cb39a4 0xc0009bae17 0xc0009bae18}] []  [{calico Update v1 2023-01-10 07:08:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-10 07:08:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd7cb208-20d6-4d2d-aa72-d9a6b8cb39a4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-10 07:08:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.82.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4m45w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4m45w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-r65pm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:08:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:08:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:08:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:08:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.233.82.43,StartTime:2023-01-10 07:08:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:08:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://472a1cf996933ec3ed9c2f0c902b2231b25ee425ba77038ec76fbd197ad331e2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.82.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 10 07:08:24.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3339" for this suite.

• [SLOW TEST:7.102 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":356,"completed":55,"skipped":1085,"failed":0}
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:08:24.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 10 07:08:24.436: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 10 07:08:24.443: INFO: Waiting for terminating namespaces to be deleted...
Jan 10 07:08:24.447: INFO: 
Logging pods the apiserver thinks is on node kk-instance-r65pm before test
Jan 10 07:08:24.453: INFO: test-cleanup-deployment-6755c7b765-h5697 from deployment-3339 started at 2023-01-10 07:08:22 +0000 UTC (1 container statuses recorded)
Jan 10 07:08:24.453: INFO: 	Container agnhost ready: true, restart count 0
Jan 10 07:08:24.453: INFO: calico-kube-controllers-6799f5f4b4-g6h9q from kube-system started at 2023-01-10 06:28:45 +0000 UTC (1 container statuses recorded)
Jan 10 07:08:24.453: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 10 07:08:24.453: INFO: calico-node-khhj9 from kube-system started at 2023-01-10 06:28:32 +0000 UTC (1 container statuses recorded)
Jan 10 07:08:24.453: INFO: 	Container calico-node ready: true, restart count 0
Jan 10 07:08:24.453: INFO: coredns-6d4b75cb6d-4wczh from kube-system started at 2023-01-10 06:28:45 +0000 UTC (1 container statuses recorded)
Jan 10 07:08:24.453: INFO: 	Container coredns ready: true, restart count 0
Jan 10 07:08:24.453: INFO: coredns-6d4b75cb6d-f7d8c from kube-system started at 2023-01-10 06:28:45 +0000 UTC (1 container statuses recorded)
Jan 10 07:08:24.453: INFO: 	Container coredns ready: true, restart count 0
Jan 10 07:08:24.453: INFO: kube-proxy-2v5cp from kube-system started at 2023-01-10 06:04:35 +0000 UTC (1 container statuses recorded)
Jan 10 07:08:24.453: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 10 07:08:24.453: INFO: sonobuoy-systemd-logs-daemon-set-5167b5d313ce4ad1-6zjp9 from sonobuoy started at 2023-01-10 06:56:07 +0000 UTC (2 container statuses recorded)
Jan 10 07:08:24.453: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 10 07:08:24.453: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 10 07:08:24.453: INFO: 
Logging pods the apiserver thinks is on node kk-instance-vgmmg before test
Jan 10 07:08:24.460: INFO: calico-node-wf8tr from kube-system started at 2023-01-10 06:42:17 +0000 UTC (1 container statuses recorded)
Jan 10 07:08:24.461: INFO: 	Container calico-node ready: true, restart count 0
Jan 10 07:08:24.461: INFO: kube-proxy-vrc4r from kube-system started at 2023-01-10 06:42:17 +0000 UTC (1 container statuses recorded)
Jan 10 07:08:24.461: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 10 07:08:24.461: INFO: busybox-host-aliases215e9037-1535-4e66-96b0-cb66511802da from kubelet-test-1394 started at 2023-01-10 07:07:59 +0000 UTC (1 container statuses recorded)
Jan 10 07:08:24.461: INFO: 	Container busybox-host-aliases215e9037-1535-4e66-96b0-cb66511802da ready: true, restart count 0
Jan 10 07:08:24.461: INFO: pod-qos-class-c64e09e1-13cf-4b27-b028-083c09e01b73 from pods-2429 started at 2023-01-10 07:07:50 +0000 UTC (1 container statuses recorded)
Jan 10 07:08:24.461: INFO: 	Container agnhost ready: false, restart count 0
Jan 10 07:08:24.461: INFO: sonobuoy from sonobuoy started at 2023-01-10 06:56:06 +0000 UTC (1 container statuses recorded)
Jan 10 07:08:24.461: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 10 07:08:24.461: INFO: sonobuoy-e2e-job-86252adb0f534a01 from sonobuoy started at 2023-01-10 06:56:07 +0000 UTC (2 container statuses recorded)
Jan 10 07:08:24.461: INFO: 	Container e2e ready: true, restart count 0
Jan 10 07:08:24.461: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 10 07:08:24.461: INFO: sonobuoy-systemd-logs-daemon-set-5167b5d313ce4ad1-smm8d from sonobuoy started at 2023-01-10 06:56:07 +0000 UTC (2 container statuses recorded)
Jan 10 07:08:24.461: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 10 07:08:24.461: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1738e0ddbacdfe31], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 10 07:08:25.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8281" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":356,"completed":56,"skipped":1085,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:08:25.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:08:25.544: INFO: Creating pod...
Jan 10 07:08:27.561: INFO: Creating service...
Jan 10 07:08:27.583: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4138/pods/agnhost/proxy?method=DELETE
Jan 10 07:08:27.594: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 10 07:08:27.594: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4138/pods/agnhost/proxy?method=OPTIONS
Jan 10 07:08:27.601: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 10 07:08:27.601: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4138/pods/agnhost/proxy?method=PATCH
Jan 10 07:08:27.606: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 10 07:08:27.606: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4138/pods/agnhost/proxy?method=POST
Jan 10 07:08:27.610: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 10 07:08:27.610: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4138/pods/agnhost/proxy?method=PUT
Jan 10 07:08:27.614: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 10 07:08:27.614: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4138/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 10 07:08:27.618: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 10 07:08:27.618: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4138/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 10 07:08:27.621: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 10 07:08:27.621: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4138/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 10 07:08:27.627: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 10 07:08:27.627: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4138/services/e2e-proxy-test-service/proxy?method=POST
Jan 10 07:08:27.631: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 10 07:08:27.631: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4138/services/e2e-proxy-test-service/proxy?method=PUT
Jan 10 07:08:27.635: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 10 07:08:27.635: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4138/pods/agnhost/proxy?method=GET
Jan 10 07:08:27.639: INFO: http.Client request:GET StatusCode:301
Jan 10 07:08:27.639: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4138/services/e2e-proxy-test-service/proxy?method=GET
Jan 10 07:08:27.642: INFO: http.Client request:GET StatusCode:301
Jan 10 07:08:27.642: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4138/pods/agnhost/proxy?method=HEAD
Jan 10 07:08:27.644: INFO: http.Client request:HEAD StatusCode:301
Jan 10 07:08:27.644: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4138/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 10 07:08:27.647: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jan 10 07:08:27.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4138" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","total":356,"completed":57,"skipped":1102,"failed":0}
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:08:27.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:08:27.704: INFO: The status of Pod busybox-readonly-fs94251547-1442-4bf7-80d7-3e2a45ff8d20 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:08:29.712: INFO: The status of Pod busybox-readonly-fs94251547-1442-4bf7-80d7-3e2a45ff8d20 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 10 07:08:29.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6466" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":58,"skipped":1109,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:08:29.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jan 10 07:08:30.394: INFO: The status of Pod kube-controller-manager-kk-instance-rx4ck is Running (Ready = true)
Jan 10 07:08:30.467: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 10 07:08:30.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1595" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":356,"completed":59,"skipped":1118,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:08:30.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:08:30.982: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 10 07:08:30.983: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 10 07:08:30.983: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 10 07:08:30.983: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 10 07:08:30.983: INFO: Checking APIGroup: apps
Jan 10 07:08:30.984: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 10 07:08:30.984: INFO: Versions found [{apps/v1 v1}]
Jan 10 07:08:30.984: INFO: apps/v1 matches apps/v1
Jan 10 07:08:30.984: INFO: Checking APIGroup: events.k8s.io
Jan 10 07:08:30.985: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 10 07:08:30.985: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jan 10 07:08:30.986: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 10 07:08:30.986: INFO: Checking APIGroup: authentication.k8s.io
Jan 10 07:08:30.987: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 10 07:08:30.987: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 10 07:08:30.987: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 10 07:08:30.987: INFO: Checking APIGroup: authorization.k8s.io
Jan 10 07:08:30.994: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 10 07:08:30.994: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 10 07:08:30.994: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 10 07:08:30.994: INFO: Checking APIGroup: autoscaling
Jan 10 07:08:30.995: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 10 07:08:30.995: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jan 10 07:08:30.995: INFO: autoscaling/v2 matches autoscaling/v2
Jan 10 07:08:30.995: INFO: Checking APIGroup: batch
Jan 10 07:08:30.996: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 10 07:08:30.996: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jan 10 07:08:30.996: INFO: batch/v1 matches batch/v1
Jan 10 07:08:30.996: INFO: Checking APIGroup: certificates.k8s.io
Jan 10 07:08:30.997: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 10 07:08:30.997: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 10 07:08:30.997: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 10 07:08:30.997: INFO: Checking APIGroup: networking.k8s.io
Jan 10 07:08:30.998: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 10 07:08:30.998: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 10 07:08:30.998: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 10 07:08:30.998: INFO: Checking APIGroup: policy
Jan 10 07:08:30.999: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 10 07:08:30.999: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jan 10 07:08:30.999: INFO: policy/v1 matches policy/v1
Jan 10 07:08:30.999: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 10 07:08:31.000: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 10 07:08:31.000: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 10 07:08:31.000: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 10 07:08:31.000: INFO: Checking APIGroup: storage.k8s.io
Jan 10 07:08:31.001: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 10 07:08:31.001: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 10 07:08:31.001: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 10 07:08:31.001: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 10 07:08:31.002: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 10 07:08:31.003: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 10 07:08:31.003: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 10 07:08:31.003: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 10 07:08:31.004: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 10 07:08:31.004: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 10 07:08:31.004: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 10 07:08:31.004: INFO: Checking APIGroup: scheduling.k8s.io
Jan 10 07:08:31.005: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 10 07:08:31.005: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 10 07:08:31.005: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 10 07:08:31.006: INFO: Checking APIGroup: coordination.k8s.io
Jan 10 07:08:31.007: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 10 07:08:31.007: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 10 07:08:31.007: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 10 07:08:31.007: INFO: Checking APIGroup: node.k8s.io
Jan 10 07:08:31.008: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 10 07:08:31.008: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jan 10 07:08:31.009: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 10 07:08:31.009: INFO: Checking APIGroup: discovery.k8s.io
Jan 10 07:08:31.010: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 10 07:08:31.010: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jan 10 07:08:31.010: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 10 07:08:31.010: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 10 07:08:31.012: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 10 07:08:31.012: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 10 07:08:31.012: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 10 07:08:31.012: INFO: Checking APIGroup: crd.projectcalico.org
Jan 10 07:08:31.013: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 10 07:08:31.013: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 10 07:08:31.013: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:188
Jan 10 07:08:31.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-2516" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":356,"completed":60,"skipped":1136,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:08:31.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 07:08:31.067: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b326065-144d-451e-82a1-36cc0a7116c8" in namespace "downward-api-2037" to be "Succeeded or Failed"
Jan 10 07:08:31.070: INFO: Pod "downwardapi-volume-3b326065-144d-451e-82a1-36cc0a7116c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.371238ms
Jan 10 07:08:33.079: INFO: Pod "downwardapi-volume-3b326065-144d-451e-82a1-36cc0a7116c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011978797s
Jan 10 07:08:35.084: INFO: Pod "downwardapi-volume-3b326065-144d-451e-82a1-36cc0a7116c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017508606s
STEP: Saw pod success
Jan 10 07:08:35.084: INFO: Pod "downwardapi-volume-3b326065-144d-451e-82a1-36cc0a7116c8" satisfied condition "Succeeded or Failed"
Jan 10 07:08:35.087: INFO: Trying to get logs from node kk-instance-r65pm pod downwardapi-volume-3b326065-144d-451e-82a1-36cc0a7116c8 container client-container: <nil>
STEP: delete the pod
Jan 10 07:08:35.106: INFO: Waiting for pod downwardapi-volume-3b326065-144d-451e-82a1-36cc0a7116c8 to disappear
Jan 10 07:08:35.110: INFO: Pod downwardapi-volume-3b326065-144d-451e-82a1-36cc0a7116c8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 10 07:08:35.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2037" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":61,"skipped":1170,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:08:35.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 07:08:35.821: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 07:08:38.844: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:08:50.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3397" for this suite.
STEP: Destroying namespace "webhook-3397-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:15.944 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":356,"completed":62,"skipped":1189,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:08:51.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 10 07:08:51.119: INFO: Waiting up to 5m0s for pod "downward-api-41058113-6b5a-40c9-82e8-93174ef6ee38" in namespace "downward-api-7396" to be "Succeeded or Failed"
Jan 10 07:08:51.122: INFO: Pod "downward-api-41058113-6b5a-40c9-82e8-93174ef6ee38": Phase="Pending", Reason="", readiness=false. Elapsed: 3.004771ms
Jan 10 07:08:53.132: INFO: Pod "downward-api-41058113-6b5a-40c9-82e8-93174ef6ee38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013117974s
Jan 10 07:08:55.137: INFO: Pod "downward-api-41058113-6b5a-40c9-82e8-93174ef6ee38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018195721s
STEP: Saw pod success
Jan 10 07:08:55.137: INFO: Pod "downward-api-41058113-6b5a-40c9-82e8-93174ef6ee38" satisfied condition "Succeeded or Failed"
Jan 10 07:08:55.139: INFO: Trying to get logs from node kk-instance-r65pm pod downward-api-41058113-6b5a-40c9-82e8-93174ef6ee38 container dapi-container: <nil>
STEP: delete the pod
Jan 10 07:08:55.160: INFO: Waiting for pod downward-api-41058113-6b5a-40c9-82e8-93174ef6ee38 to disappear
Jan 10 07:08:55.166: INFO: Pod downward-api-41058113-6b5a-40c9-82e8-93174ef6ee38 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 10 07:08:55.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7396" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":356,"completed":63,"skipped":1239,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:08:55.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:08:55.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-8443
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:188
Jan 10 07:08:59.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-9578" for this suite.
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 10 07:08:59.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8443" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":356,"completed":64,"skipped":1290,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:08:59.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 10 07:08:59.357: INFO: Waiting up to 5m0s for pod "pod-4ef34701-7f19-4b0d-8d88-63bfb449eb4c" in namespace "emptydir-3375" to be "Succeeded or Failed"
Jan 10 07:08:59.363: INFO: Pod "pod-4ef34701-7f19-4b0d-8d88-63bfb449eb4c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.908946ms
Jan 10 07:09:01.372: INFO: Pod "pod-4ef34701-7f19-4b0d-8d88-63bfb449eb4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014854867s
Jan 10 07:09:03.381: INFO: Pod "pod-4ef34701-7f19-4b0d-8d88-63bfb449eb4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024213687s
STEP: Saw pod success
Jan 10 07:09:03.382: INFO: Pod "pod-4ef34701-7f19-4b0d-8d88-63bfb449eb4c" satisfied condition "Succeeded or Failed"
Jan 10 07:09:03.384: INFO: Trying to get logs from node kk-instance-r65pm pod pod-4ef34701-7f19-4b0d-8d88-63bfb449eb4c container test-container: <nil>
STEP: delete the pod
Jan 10 07:09:03.401: INFO: Waiting for pod pod-4ef34701-7f19-4b0d-8d88-63bfb449eb4c to disappear
Jan 10 07:09:03.404: INFO: Pod pod-4ef34701-7f19-4b0d-8d88-63bfb449eb4c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 10 07:09:03.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3375" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":65,"skipped":1304,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:09:03.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
Jan 10 07:09:03.445: INFO: Creating simple deployment test-deployment-98fqp
Jan 10 07:09:03.462: INFO: deployment "test-deployment-98fqp" doesn't have the required revision set
STEP: Getting /status
Jan 10 07:09:05.483: INFO: Deployment test-deployment-98fqp has Conditions: [{Available True 2023-01-10 07:09:05 +0000 UTC 2023-01-10 07:09:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-10 07:09:05 +0000 UTC 2023-01-10 07:09:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-98fqp-688c4d6789" has successfully progressed.}]
STEP: updating Deployment Status
Jan 10 07:09:05.502: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 9, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 9, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 9, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 9, 3, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-98fqp-688c4d6789\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Jan 10 07:09:05.507: INFO: Observed &Deployment event: ADDED
Jan 10 07:09:05.508: INFO: Observed Deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-10 07:09:03 +0000 UTC 2023-01-10 07:09:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-98fqp-688c4d6789"}
Jan 10 07:09:05.508: INFO: Observed &Deployment event: MODIFIED
Jan 10 07:09:05.508: INFO: Observed Deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-10 07:09:03 +0000 UTC 2023-01-10 07:09:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-98fqp-688c4d6789"}
Jan 10 07:09:05.508: INFO: Observed Deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-10 07:09:03 +0000 UTC 2023-01-10 07:09:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 10 07:09:05.508: INFO: Observed &Deployment event: MODIFIED
Jan 10 07:09:05.508: INFO: Observed Deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-10 07:09:03 +0000 UTC 2023-01-10 07:09:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 10 07:09:05.508: INFO: Observed Deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-10 07:09:03 +0000 UTC 2023-01-10 07:09:03 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-98fqp-688c4d6789" is progressing.}
Jan 10 07:09:05.508: INFO: Observed &Deployment event: MODIFIED
Jan 10 07:09:05.508: INFO: Observed Deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-10 07:09:05 +0000 UTC 2023-01-10 07:09:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 10 07:09:05.508: INFO: Observed Deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-10 07:09:05 +0000 UTC 2023-01-10 07:09:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-98fqp-688c4d6789" has successfully progressed.}
Jan 10 07:09:05.509: INFO: Observed &Deployment event: MODIFIED
Jan 10 07:09:05.509: INFO: Observed Deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-10 07:09:05 +0000 UTC 2023-01-10 07:09:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 10 07:09:05.509: INFO: Observed Deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-10 07:09:05 +0000 UTC 2023-01-10 07:09:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-98fqp-688c4d6789" has successfully progressed.}
Jan 10 07:09:05.509: INFO: Found Deployment test-deployment-98fqp in namespace deployment-3935 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 10 07:09:05.509: INFO: Deployment test-deployment-98fqp has an updated status
STEP: patching the Statefulset Status
Jan 10 07:09:05.509: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 10 07:09:05.519: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Jan 10 07:09:05.524: INFO: Observed &Deployment event: ADDED
Jan 10 07:09:05.524: INFO: Observed deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-10 07:09:03 +0000 UTC 2023-01-10 07:09:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-98fqp-688c4d6789"}
Jan 10 07:09:05.525: INFO: Observed &Deployment event: MODIFIED
Jan 10 07:09:05.525: INFO: Observed deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-10 07:09:03 +0000 UTC 2023-01-10 07:09:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-98fqp-688c4d6789"}
Jan 10 07:09:05.525: INFO: Observed deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-10 07:09:03 +0000 UTC 2023-01-10 07:09:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 10 07:09:05.525: INFO: Observed &Deployment event: MODIFIED
Jan 10 07:09:05.525: INFO: Observed deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-10 07:09:03 +0000 UTC 2023-01-10 07:09:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 10 07:09:05.525: INFO: Observed deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-10 07:09:03 +0000 UTC 2023-01-10 07:09:03 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-98fqp-688c4d6789" is progressing.}
Jan 10 07:09:05.525: INFO: Observed &Deployment event: MODIFIED
Jan 10 07:09:05.526: INFO: Observed deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-10 07:09:05 +0000 UTC 2023-01-10 07:09:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 10 07:09:05.526: INFO: Observed deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-10 07:09:05 +0000 UTC 2023-01-10 07:09:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-98fqp-688c4d6789" has successfully progressed.}
Jan 10 07:09:05.526: INFO: Observed &Deployment event: MODIFIED
Jan 10 07:09:05.526: INFO: Observed deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-10 07:09:05 +0000 UTC 2023-01-10 07:09:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 10 07:09:05.526: INFO: Observed deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-10 07:09:05 +0000 UTC 2023-01-10 07:09:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-98fqp-688c4d6789" has successfully progressed.}
Jan 10 07:09:05.526: INFO: Observed deployment test-deployment-98fqp in namespace deployment-3935 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 10 07:09:05.526: INFO: Observed &Deployment event: MODIFIED
Jan 10 07:09:05.526: INFO: Found deployment test-deployment-98fqp in namespace deployment-3935 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 10 07:09:05.526: INFO: Deployment test-deployment-98fqp has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 10 07:09:05.536: INFO: Deployment "test-deployment-98fqp":
&Deployment{ObjectMeta:{test-deployment-98fqp  deployment-3935  4f5129a0-2729-4dd6-8e46-5fb98497f19c 17655 1 2023-01-10 07:09:03 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2023-01-10 07:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-10 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-10 07:09:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0006b1158 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-98fqp-688c4d6789",LastUpdateTime:2023-01-10 07:09:05 +0000 UTC,LastTransitionTime:2023-01-10 07:09:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 10 07:09:05.552: INFO: New ReplicaSet "test-deployment-98fqp-688c4d6789" of Deployment "test-deployment-98fqp":
&ReplicaSet{ObjectMeta:{test-deployment-98fqp-688c4d6789  deployment-3935  61ccf0ab-6759-4898-93a8-d2e79a5acc24 17643 1 2023-01-10 07:09:03 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-98fqp 4f5129a0-2729-4dd6-8e46-5fb98497f19c 0xc0012ba6d0 0xc0012ba6d1}] []  [{kube-controller-manager Update apps/v1 2023-01-10 07:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f5129a0-2729-4dd6-8e46-5fb98497f19c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 688c4d6789,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0012ba778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 10 07:09:05.561: INFO: Pod "test-deployment-98fqp-688c4d6789-rx576" is available:
&Pod{ObjectMeta:{test-deployment-98fqp-688c4d6789-rx576 test-deployment-98fqp-688c4d6789- deployment-3935  eb932824-09ba-46c4-8d66-566bf44cea3b 17642 0 2023-01-10 07:09:03 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[cni.projectcalico.org/containerID:98ba9940c8c2030dde309a9dd5bfd030ab93e82663d0da37702c2cbc85ab7fa5 cni.projectcalico.org/podIP:10.233.82.49/32 cni.projectcalico.org/podIPs:10.233.82.49/32] [{apps/v1 ReplicaSet test-deployment-98fqp-688c4d6789 61ccf0ab-6759-4898-93a8-d2e79a5acc24 0xc0012bab50 0xc0012bab51}] []  [{kube-controller-manager Update v1 2023-01-10 07:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61ccf0ab-6759-4898-93a8-d2e79a5acc24\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-10 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-10 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.82.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q2rsx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q2rsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-r65pm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:09:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:09:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.233.82.49,StartTime:2023-01-10 07:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:09:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://647c439668f81f681ce016e54b872713908a13e4069e6355404252dea5d274ce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.82.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 10 07:09:05.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3935" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":356,"completed":66,"skipped":1326,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:09:05.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 10 07:09:05.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4584" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":356,"completed":67,"skipped":1340,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:09:05.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 10 07:09:05.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5555" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":356,"completed":68,"skipped":1381,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:09:05.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jan 10 07:09:05.788: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Jan 10 07:09:05.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5927" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":356,"completed":69,"skipped":1386,"failed":0}
SSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:09:05.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:09:07.886: INFO: Deleting pod "var-expansion-18dbebcb-1fa3-43e0-8209-0f02f6fe09e1" in namespace "var-expansion-1274"
Jan 10 07:09:07.901: INFO: Wait up to 5m0s for pod "var-expansion-18dbebcb-1fa3-43e0-8209-0f02f6fe09e1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 10 07:09:11.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1274" for this suite.

• [SLOW TEST:6.094 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":356,"completed":70,"skipped":1389,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:09:11.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-b4aa9c58-79c6-458d-b727-38a3e783bf86
STEP: Creating a pod to test consume configMaps
Jan 10 07:09:11.993: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a7d671d-ce75-45f9-9074-e49ca9ef5044" in namespace "configmap-6877" to be "Succeeded or Failed"
Jan 10 07:09:11.999: INFO: Pod "pod-configmaps-5a7d671d-ce75-45f9-9074-e49ca9ef5044": Phase="Pending", Reason="", readiness=false. Elapsed: 5.440538ms
Jan 10 07:09:14.008: INFO: Pod "pod-configmaps-5a7d671d-ce75-45f9-9074-e49ca9ef5044": Phase="Running", Reason="", readiness=true. Elapsed: 2.014683668s
Jan 10 07:09:16.016: INFO: Pod "pod-configmaps-5a7d671d-ce75-45f9-9074-e49ca9ef5044": Phase="Running", Reason="", readiness=false. Elapsed: 4.022886079s
Jan 10 07:09:18.025: INFO: Pod "pod-configmaps-5a7d671d-ce75-45f9-9074-e49ca9ef5044": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031647508s
STEP: Saw pod success
Jan 10 07:09:18.025: INFO: Pod "pod-configmaps-5a7d671d-ce75-45f9-9074-e49ca9ef5044" satisfied condition "Succeeded or Failed"
Jan 10 07:09:18.028: INFO: Trying to get logs from node kk-instance-r65pm pod pod-configmaps-5a7d671d-ce75-45f9-9074-e49ca9ef5044 container agnhost-container: <nil>
STEP: delete the pod
Jan 10 07:09:18.045: INFO: Waiting for pod pod-configmaps-5a7d671d-ce75-45f9-9074-e49ca9ef5044 to disappear
Jan 10 07:09:18.058: INFO: Pod pod-configmaps-5a7d671d-ce75-45f9-9074-e49ca9ef5044 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 10 07:09:18.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6877" for this suite.

• [SLOW TEST:6.135 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":71,"skipped":1492,"failed":0}
SS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:09:18.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 07:09:18.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6125" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":356,"completed":72,"skipped":1494,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:09:18.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name projected-secret-test-5b539961-778f-474b-8914-9a796b6dfc5e
STEP: Creating a pod to test consume secrets
Jan 10 07:09:18.198: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-88397b08-683c-48c8-8fdd-262b9f853566" in namespace "projected-1096" to be "Succeeded or Failed"
Jan 10 07:09:18.202: INFO: Pod "pod-projected-secrets-88397b08-683c-48c8-8fdd-262b9f853566": Phase="Pending", Reason="", readiness=false. Elapsed: 4.330959ms
Jan 10 07:09:20.210: INFO: Pod "pod-projected-secrets-88397b08-683c-48c8-8fdd-262b9f853566": Phase="Running", Reason="", readiness=false. Elapsed: 2.011556528s
Jan 10 07:09:22.219: INFO: Pod "pod-projected-secrets-88397b08-683c-48c8-8fdd-262b9f853566": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021207526s
STEP: Saw pod success
Jan 10 07:09:22.219: INFO: Pod "pod-projected-secrets-88397b08-683c-48c8-8fdd-262b9f853566" satisfied condition "Succeeded or Failed"
Jan 10 07:09:22.222: INFO: Trying to get logs from node kk-instance-r65pm pod pod-projected-secrets-88397b08-683c-48c8-8fdd-262b9f853566 container secret-volume-test: <nil>
STEP: delete the pod
Jan 10 07:09:22.239: INFO: Waiting for pod pod-projected-secrets-88397b08-683c-48c8-8fdd-262b9f853566 to disappear
Jan 10 07:09:22.242: INFO: Pod pod-projected-secrets-88397b08-683c-48c8-8fdd-262b9f853566 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 10 07:09:22.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1096" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":73,"skipped":1494,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:09:22.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-5091
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 10 07:09:22.285: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 10 07:09:22.324: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:09:24.335: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 07:09:26.332: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 07:09:28.333: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 07:09:30.330: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 07:09:32.329: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 07:09:34.333: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 07:09:36.332: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 07:09:38.373: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 07:09:40.331: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 07:09:42.332: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 07:09:44.333: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 10 07:09:44.337: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 10 07:09:46.373: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 10 07:09:46.373: INFO: Going to poll 10.233.82.53 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 10 07:09:46.376: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.82.53 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5091 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:09:46.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:09:46.377: INFO: ExecWithOptions: Clientset creation
Jan 10 07:09:46.377: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5091/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.82.53+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 10 07:09:47.456: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 10 07:09:47.456: INFO: Going to poll 10.233.107.113 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 10 07:09:47.461: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.107.113 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5091 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:09:47.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:09:47.462: INFO: ExecWithOptions: Clientset creation
Jan 10 07:09:47.462: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5091/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.107.113+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 10 07:09:48.540: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 10 07:09:48.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5091" for this suite.

• [SLOW TEST:26.300 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":74,"skipped":1560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:09:48.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:09:48.607: INFO: The status of Pod busybox-scheduling-6f1de709-fc79-46d1-951b-d342c8e72f92 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:09:50.616: INFO: The status of Pod busybox-scheduling-6f1de709-fc79-46d1-951b-d342c8e72f92 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 10 07:09:50.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9507" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":356,"completed":75,"skipped":1607,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:09:50.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 10 07:09:50.683: INFO: Waiting up to 5m0s for pod "pod-2c96e4e6-7a9c-4359-8083-27b7801f26c8" in namespace "emptydir-4301" to be "Succeeded or Failed"
Jan 10 07:09:50.688: INFO: Pod "pod-2c96e4e6-7a9c-4359-8083-27b7801f26c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.515284ms
Jan 10 07:09:52.698: INFO: Pod "pod-2c96e4e6-7a9c-4359-8083-27b7801f26c8": Phase="Running", Reason="", readiness=false. Elapsed: 2.014227299s
Jan 10 07:09:54.709: INFO: Pod "pod-2c96e4e6-7a9c-4359-8083-27b7801f26c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025214556s
STEP: Saw pod success
Jan 10 07:09:54.709: INFO: Pod "pod-2c96e4e6-7a9c-4359-8083-27b7801f26c8" satisfied condition "Succeeded or Failed"
Jan 10 07:09:54.716: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-2c96e4e6-7a9c-4359-8083-27b7801f26c8 container test-container: <nil>
STEP: delete the pod
Jan 10 07:09:54.731: INFO: Waiting for pod pod-2c96e4e6-7a9c-4359-8083-27b7801f26c8 to disappear
Jan 10 07:09:54.734: INFO: Pod pod-2c96e4e6-7a9c-4359-8083-27b7801f26c8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 10 07:09:54.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4301" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":76,"skipped":1641,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:09:54.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 10 07:10:06.823: INFO: DNS probes using dns-5197/dns-test-0974fd21-be4d-4c56-aab7-fe8134646dd1 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 10 07:10:06.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5197" for this suite.

• [SLOW TEST:12.099 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":356,"completed":77,"skipped":1655,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:10:06.845: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 07:10:08.043: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 07:10:11.073: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:10:11.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2067-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:10:14.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4088" for this suite.
STEP: Destroying namespace "webhook-4088-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.505 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":356,"completed":78,"skipped":1719,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:10:14.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 10 07:10:14.414: INFO: The status of Pod pod-update-3b61e018-2d13-46d2-88f6-344fb491604d is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:10:16.422: INFO: The status of Pod pod-update-3b61e018-2d13-46d2-88f6-344fb491604d is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 10 07:10:16.942: INFO: Successfully updated pod "pod-update-3b61e018-2d13-46d2-88f6-344fb491604d"
STEP: verifying the updated pod is in kubernetes
Jan 10 07:10:16.948: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 10 07:10:16.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4046" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":356,"completed":79,"skipped":1721,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:10:16.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:10:17.012: INFO: created pod
Jan 10 07:10:17.012: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6900" to be "Succeeded or Failed"
Jan 10 07:10:17.015: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.397523ms
Jan 10 07:10:19.022: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009567605s
Jan 10 07:10:21.030: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017942178s
STEP: Saw pod success
Jan 10 07:10:21.030: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 10 07:10:51.031: INFO: polling logs
Jan 10 07:10:51.038: INFO: Pod logs: 
I0110 07:10:17.885267       1 log.go:195] OK: Got token
I0110 07:10:17.885319       1 log.go:195] validating with in-cluster discovery
I0110 07:10:17.885757       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0110 07:10:17.885797       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6900:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673335217, NotBefore:1673334617, IssuedAt:1673334617, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6900", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"92f28a30-a0aa-4157-a5fc-82052fde8ca6"}}}
I0110 07:10:17.903930       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0110 07:10:17.912610       1 log.go:195] OK: Validated signature on JWT
I0110 07:10:17.912801       1 log.go:195] OK: Got valid claims from token!
I0110 07:10:17.912829       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6900:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673335217, NotBefore:1673334617, IssuedAt:1673334617, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6900", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"92f28a30-a0aa-4157-a5fc-82052fde8ca6"}}}

Jan 10 07:10:51.038: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 10 07:10:51.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6900" for this suite.

• [SLOW TEST:34.097 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":356,"completed":80,"skipped":1725,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:10:51.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 10 07:10:51.547: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 07:10:54.569: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:10:54.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:10:57.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4388" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:6.832 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":356,"completed":81,"skipped":1747,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:10:57.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-3968
STEP: creating service affinity-clusterip in namespace services-3968
STEP: creating replication controller affinity-clusterip in namespace services-3968
I0110 07:10:57.958381      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3968, replica count: 3
I0110 07:11:01.010294      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 10 07:11:01.020: INFO: Creating new exec pod
Jan 10 07:11:04.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-3968 exec execpod-affinityxf4t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 10 07:11:04.209: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 10 07:11:04.209: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:11:04.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-3968 exec execpod-affinityxf4t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.20.6 80'
Jan 10 07:11:04.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.20.6 80\nConnection to 10.233.20.6 80 port [tcp/http] succeeded!\n"
Jan 10 07:11:04.364: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:11:04.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-3968 exec execpod-affinityxf4t9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.20.6:80/ ; done'
Jan 10 07:11:04.625: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.20.6:80/\n"
Jan 10 07:11:04.625: INFO: stdout: "\naffinity-clusterip-dh5df\naffinity-clusterip-dh5df\naffinity-clusterip-dh5df\naffinity-clusterip-dh5df\naffinity-clusterip-dh5df\naffinity-clusterip-dh5df\naffinity-clusterip-dh5df\naffinity-clusterip-dh5df\naffinity-clusterip-dh5df\naffinity-clusterip-dh5df\naffinity-clusterip-dh5df\naffinity-clusterip-dh5df\naffinity-clusterip-dh5df\naffinity-clusterip-dh5df\naffinity-clusterip-dh5df\naffinity-clusterip-dh5df"
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Received response from host: affinity-clusterip-dh5df
Jan 10 07:11:04.626: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3968, will wait for the garbage collector to delete the pods
Jan 10 07:11:04.717: INFO: Deleting ReplicationController affinity-clusterip took: 6.416255ms
Jan 10 07:11:04.818: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.287944ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 07:11:07.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3968" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.161 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":82,"skipped":1805,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:11:07.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 10 07:11:18.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9778" for this suite.

• [SLOW TEST:11.094 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":356,"completed":83,"skipped":1809,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:11:18.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:11:18.196: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 10 07:11:18.205: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:11:18.205: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Jan 10 07:11:18.233: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:11:18.233: INFO: Node kk-instance-vgmmg is running 0 daemon pod, expected 1
Jan 10 07:11:19.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:11:19.239: INFO: Node kk-instance-vgmmg is running 0 daemon pod, expected 1
Jan 10 07:11:20.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 10 07:11:20.240: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 10 07:11:20.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 10 07:11:20.267: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 10 07:11:21.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:11:21.276: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 10 07:11:21.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:11:21.291: INFO: Node kk-instance-vgmmg is running 0 daemon pod, expected 1
Jan 10 07:11:22.298: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:11:22.298: INFO: Node kk-instance-vgmmg is running 0 daemon pod, expected 1
Jan 10 07:11:23.300: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:11:23.300: INFO: Node kk-instance-vgmmg is running 0 daemon pod, expected 1
Jan 10 07:11:24.298: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 10 07:11:24.298: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9925, will wait for the garbage collector to delete the pods
Jan 10 07:11:24.363: INFO: Deleting DaemonSet.extensions daemon-set took: 6.935135ms
Jan 10 07:11:24.464: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.896837ms
Jan 10 07:11:27.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:11:27.077: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 10 07:11:27.080: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18969"},"items":null}

Jan 10 07:11:27.083: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18969"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 10 07:11:27.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9925" for this suite.

• [SLOW TEST:8.969 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":356,"completed":84,"skipped":1844,"failed":0}
SSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:11:27.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jan 10 07:11:27.167: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jan 10 07:11:27.173: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 10 07:11:27.173: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jan 10 07:11:27.185: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 10 07:11:27.185: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jan 10 07:11:27.200: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 10 07:11:27.200: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jan 10 07:11:34.254: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:188
Jan 10 07:11:34.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8159" for this suite.

• [SLOW TEST:7.151 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":356,"completed":85,"skipped":1849,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:11:34.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4176
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-4176
Jan 10 07:11:34.347: INFO: Found 0 stateful pods, waiting for 1
Jan 10 07:11:44.364: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Jan 10 07:11:44.390: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Jan 10 07:11:44.401: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Jan 10 07:11:44.405: INFO: Observed &StatefulSet event: ADDED
Jan 10 07:11:44.405: INFO: Found Statefulset ss in namespace statefulset-4176 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 10 07:11:44.405: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Jan 10 07:11:44.405: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 10 07:11:44.413: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Jan 10 07:11:44.415: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 10 07:11:44.415: INFO: Deleting all statefulset in ns statefulset-4176
Jan 10 07:11:44.417: INFO: Scaling statefulset ss to 0
Jan 10 07:11:54.452: INFO: Waiting for statefulset status.replicas updated to 0
Jan 10 07:11:54.455: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 10 07:11:54.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4176" for this suite.

• [SLOW TEST:20.200 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":356,"completed":86,"skipped":1932,"failed":0}
SSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:11:54.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 10 07:11:56.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8339" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":356,"completed":87,"skipped":1938,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:11:56.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-ff32f5ba-e9ea-4038-8cd5-c00aa68c29b8
STEP: Creating secret with name s-test-opt-upd-7671ea1e-655e-4316-b7ae-a6e87e1d93e2
STEP: Creating the pod
Jan 10 07:11:56.643: INFO: The status of Pod pod-secrets-17ed154b-3804-4fd1-bb95-c7bead663591 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:11:58.648: INFO: The status of Pod pod-secrets-17ed154b-3804-4fd1-bb95-c7bead663591 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-ff32f5ba-e9ea-4038-8cd5-c00aa68c29b8
STEP: Updating secret s-test-opt-upd-7671ea1e-655e-4316-b7ae-a6e87e1d93e2
STEP: Creating secret with name s-test-opt-create-40d854af-1373-4cc7-bd67-72476b4d01a5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 10 07:12:00.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9562" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":88,"skipped":1990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:12:00.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 07:12:01.382: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 07:12:04.408: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:12:04.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2984" for this suite.
STEP: Destroying namespace "webhook-2984-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":356,"completed":89,"skipped":2033,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:12:04.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-0111a158-f05e-425f-8c26-cc0af76ab1ef
STEP: Creating a pod to test consume secrets
Jan 10 07:12:04.787: INFO: Waiting up to 5m0s for pod "pod-secrets-63f89595-6a57-45e0-808e-18244b4b28ec" in namespace "secrets-8243" to be "Succeeded or Failed"
Jan 10 07:12:04.798: INFO: Pod "pod-secrets-63f89595-6a57-45e0-808e-18244b4b28ec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.809858ms
Jan 10 07:12:06.805: INFO: Pod "pod-secrets-63f89595-6a57-45e0-808e-18244b4b28ec": Phase="Running", Reason="", readiness=true. Elapsed: 2.017804421s
Jan 10 07:12:08.812: INFO: Pod "pod-secrets-63f89595-6a57-45e0-808e-18244b4b28ec": Phase="Running", Reason="", readiness=false. Elapsed: 4.025414715s
Jan 10 07:12:10.821: INFO: Pod "pod-secrets-63f89595-6a57-45e0-808e-18244b4b28ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033825733s
STEP: Saw pod success
Jan 10 07:12:10.821: INFO: Pod "pod-secrets-63f89595-6a57-45e0-808e-18244b4b28ec" satisfied condition "Succeeded or Failed"
Jan 10 07:12:10.827: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-secrets-63f89595-6a57-45e0-808e-18244b4b28ec container secret-volume-test: <nil>
STEP: delete the pod
Jan 10 07:12:10.844: INFO: Waiting for pod pod-secrets-63f89595-6a57-45e0-808e-18244b4b28ec to disappear
Jan 10 07:12:10.849: INFO: Pod pod-secrets-63f89595-6a57-45e0-808e-18244b4b28ec no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 10 07:12:10.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8243" for this suite.

• [SLOW TEST:6.148 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":90,"skipped":2059,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:12:10.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Starting the proxy
Jan 10 07:12:10.886: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-4818 proxy --unix-socket=/tmp/kubectl-proxy-unix3953408795/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 07:12:10.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4818" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":356,"completed":91,"skipped":2060,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:12:10.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 07:12:10.997: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa915a12-6ce9-41f2-9299-a5d761c9b615" in namespace "downward-api-8503" to be "Succeeded or Failed"
Jan 10 07:12:11.004: INFO: Pod "downwardapi-volume-fa915a12-6ce9-41f2-9299-a5d761c9b615": Phase="Pending", Reason="", readiness=false. Elapsed: 6.689817ms
Jan 10 07:12:13.015: INFO: Pod "downwardapi-volume-fa915a12-6ce9-41f2-9299-a5d761c9b615": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018348623s
Jan 10 07:12:15.021: INFO: Pod "downwardapi-volume-fa915a12-6ce9-41f2-9299-a5d761c9b615": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024340479s
STEP: Saw pod success
Jan 10 07:12:15.021: INFO: Pod "downwardapi-volume-fa915a12-6ce9-41f2-9299-a5d761c9b615" satisfied condition "Succeeded or Failed"
Jan 10 07:12:15.024: INFO: Trying to get logs from node kk-instance-r65pm pod downwardapi-volume-fa915a12-6ce9-41f2-9299-a5d761c9b615 container client-container: <nil>
STEP: delete the pod
Jan 10 07:12:15.044: INFO: Waiting for pod downwardapi-volume-fa915a12-6ce9-41f2-9299-a5d761c9b615 to disappear
Jan 10 07:12:15.047: INFO: Pod downwardapi-volume-fa915a12-6ce9-41f2-9299-a5d761c9b615 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 10 07:12:15.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8503" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":356,"completed":92,"skipped":2069,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:12:15.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override arguments
Jan 10 07:12:15.111: INFO: Waiting up to 5m0s for pod "client-containers-1fca0b1f-914e-4b04-9854-2db6064819f8" in namespace "containers-1700" to be "Succeeded or Failed"
Jan 10 07:12:15.121: INFO: Pod "client-containers-1fca0b1f-914e-4b04-9854-2db6064819f8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.551493ms
Jan 10 07:12:17.131: INFO: Pod "client-containers-1fca0b1f-914e-4b04-9854-2db6064819f8": Phase="Running", Reason="", readiness=false. Elapsed: 2.019638993s
Jan 10 07:12:19.147: INFO: Pod "client-containers-1fca0b1f-914e-4b04-9854-2db6064819f8": Phase="Running", Reason="", readiness=false. Elapsed: 4.035121809s
Jan 10 07:12:21.155: INFO: Pod "client-containers-1fca0b1f-914e-4b04-9854-2db6064819f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043462943s
STEP: Saw pod success
Jan 10 07:12:21.155: INFO: Pod "client-containers-1fca0b1f-914e-4b04-9854-2db6064819f8" satisfied condition "Succeeded or Failed"
Jan 10 07:12:21.158: INFO: Trying to get logs from node kk-instance-vgmmg pod client-containers-1fca0b1f-914e-4b04-9854-2db6064819f8 container agnhost-container: <nil>
STEP: delete the pod
Jan 10 07:12:21.176: INFO: Waiting for pod client-containers-1fca0b1f-914e-4b04-9854-2db6064819f8 to disappear
Jan 10 07:12:21.185: INFO: Pod client-containers-1fca0b1f-914e-4b04-9854-2db6064819f8 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 10 07:12:21.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1700" for this suite.

• [SLOW TEST:6.135 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","total":356,"completed":93,"skipped":2087,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:12:21.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-47bp
STEP: Creating a pod to test atomic-volume-subpath
Jan 10 07:12:21.245: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-47bp" in namespace "subpath-2487" to be "Succeeded or Failed"
Jan 10 07:12:21.249: INFO: Pod "pod-subpath-test-configmap-47bp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.384783ms
Jan 10 07:12:23.258: INFO: Pod "pod-subpath-test-configmap-47bp": Phase="Running", Reason="", readiness=true. Elapsed: 2.012313776s
Jan 10 07:12:25.264: INFO: Pod "pod-subpath-test-configmap-47bp": Phase="Running", Reason="", readiness=true. Elapsed: 4.01847273s
Jan 10 07:12:27.273: INFO: Pod "pod-subpath-test-configmap-47bp": Phase="Running", Reason="", readiness=true. Elapsed: 6.027382467s
Jan 10 07:12:29.278: INFO: Pod "pod-subpath-test-configmap-47bp": Phase="Running", Reason="", readiness=true. Elapsed: 8.032615975s
Jan 10 07:12:31.287: INFO: Pod "pod-subpath-test-configmap-47bp": Phase="Running", Reason="", readiness=true. Elapsed: 10.041702488s
Jan 10 07:12:33.300: INFO: Pod "pod-subpath-test-configmap-47bp": Phase="Running", Reason="", readiness=true. Elapsed: 12.054339912s
Jan 10 07:12:35.307: INFO: Pod "pod-subpath-test-configmap-47bp": Phase="Running", Reason="", readiness=true. Elapsed: 14.061180823s
Jan 10 07:12:37.316: INFO: Pod "pod-subpath-test-configmap-47bp": Phase="Running", Reason="", readiness=true. Elapsed: 16.070549246s
Jan 10 07:12:39.322: INFO: Pod "pod-subpath-test-configmap-47bp": Phase="Running", Reason="", readiness=true. Elapsed: 18.07662002s
Jan 10 07:12:41.331: INFO: Pod "pod-subpath-test-configmap-47bp": Phase="Running", Reason="", readiness=true. Elapsed: 20.085262807s
Jan 10 07:12:43.341: INFO: Pod "pod-subpath-test-configmap-47bp": Phase="Running", Reason="", readiness=false. Elapsed: 22.095576095s
Jan 10 07:12:45.348: INFO: Pod "pod-subpath-test-configmap-47bp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.101951109s
STEP: Saw pod success
Jan 10 07:12:45.348: INFO: Pod "pod-subpath-test-configmap-47bp" satisfied condition "Succeeded or Failed"
Jan 10 07:12:45.350: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-subpath-test-configmap-47bp container test-container-subpath-configmap-47bp: <nil>
STEP: delete the pod
Jan 10 07:12:45.368: INFO: Waiting for pod pod-subpath-test-configmap-47bp to disappear
Jan 10 07:12:45.371: INFO: Pod pod-subpath-test-configmap-47bp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-47bp
Jan 10 07:12:45.371: INFO: Deleting pod "pod-subpath-test-configmap-47bp" in namespace "subpath-2487"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 10 07:12:45.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2487" for this suite.

• [SLOW TEST:24.182 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","total":356,"completed":94,"skipped":2087,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:12:45.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-projected-dhk5
STEP: Creating a pod to test atomic-volume-subpath
Jan 10 07:12:45.429: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-dhk5" in namespace "subpath-2807" to be "Succeeded or Failed"
Jan 10 07:12:45.432: INFO: Pod "pod-subpath-test-projected-dhk5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.056809ms
Jan 10 07:12:47.442: INFO: Pod "pod-subpath-test-projected-dhk5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012320978s
Jan 10 07:12:49.447: INFO: Pod "pod-subpath-test-projected-dhk5": Phase="Running", Reason="", readiness=true. Elapsed: 4.017921297s
Jan 10 07:12:51.456: INFO: Pod "pod-subpath-test-projected-dhk5": Phase="Running", Reason="", readiness=true. Elapsed: 6.026825954s
Jan 10 07:12:53.462: INFO: Pod "pod-subpath-test-projected-dhk5": Phase="Running", Reason="", readiness=true. Elapsed: 8.032444556s
Jan 10 07:12:55.469: INFO: Pod "pod-subpath-test-projected-dhk5": Phase="Running", Reason="", readiness=true. Elapsed: 10.039769851s
Jan 10 07:12:57.479: INFO: Pod "pod-subpath-test-projected-dhk5": Phase="Running", Reason="", readiness=true. Elapsed: 12.049299376s
Jan 10 07:12:59.484: INFO: Pod "pod-subpath-test-projected-dhk5": Phase="Running", Reason="", readiness=true. Elapsed: 14.054964428s
Jan 10 07:13:01.493: INFO: Pod "pod-subpath-test-projected-dhk5": Phase="Running", Reason="", readiness=true. Elapsed: 16.063397333s
Jan 10 07:13:03.505: INFO: Pod "pod-subpath-test-projected-dhk5": Phase="Running", Reason="", readiness=true. Elapsed: 18.075380668s
Jan 10 07:13:05.512: INFO: Pod "pod-subpath-test-projected-dhk5": Phase="Running", Reason="", readiness=true. Elapsed: 20.082530494s
Jan 10 07:13:07.521: INFO: Pod "pod-subpath-test-projected-dhk5": Phase="Running", Reason="", readiness=false. Elapsed: 22.09213079s
Jan 10 07:13:09.527: INFO: Pod "pod-subpath-test-projected-dhk5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.097812727s
STEP: Saw pod success
Jan 10 07:13:09.528: INFO: Pod "pod-subpath-test-projected-dhk5" satisfied condition "Succeeded or Failed"
Jan 10 07:13:09.531: INFO: Trying to get logs from node kk-instance-r65pm pod pod-subpath-test-projected-dhk5 container test-container-subpath-projected-dhk5: <nil>
STEP: delete the pod
Jan 10 07:13:09.550: INFO: Waiting for pod pod-subpath-test-projected-dhk5 to disappear
Jan 10 07:13:09.553: INFO: Pod pod-subpath-test-projected-dhk5 no longer exists
STEP: Deleting pod pod-subpath-test-projected-dhk5
Jan 10 07:13:09.554: INFO: Deleting pod "pod-subpath-test-projected-dhk5" in namespace "subpath-2807"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 10 07:13:09.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2807" for this suite.

• [SLOW TEST:24.184 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","total":356,"completed":95,"skipped":2088,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:13:09.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 10 07:13:09.603: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 10 07:14:09.636: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:14:09.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:14:09.682: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jan 10 07:14:09.685: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:188
Jan 10 07:14:09.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3025" for this suite.
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 10 07:14:09.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3730" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:60.198 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":356,"completed":96,"skipped":2090,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:14:09.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 10 07:14:09.827: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:14:09.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:14:09.832: INFO: Node kk-instance-r65pm is running 0 daemon pod, expected 1
Jan 10 07:14:10.840: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:14:10.844: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:14:10.844: INFO: Node kk-instance-r65pm is running 0 daemon pod, expected 1
Jan 10 07:14:11.838: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:14:11.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 10 07:14:11.843: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 10 07:14:11.859: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:14:11.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 10 07:14:11.868: INFO: Node kk-instance-vgmmg is running 0 daemon pod, expected 1
Jan 10 07:14:12.877: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:14:12.887: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 10 07:14:12.887: INFO: Node kk-instance-vgmmg is running 0 daemon pod, expected 1
Jan 10 07:14:13.877: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:14:13.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 10 07:14:13.881: INFO: Node kk-instance-vgmmg is running 0 daemon pod, expected 1
Jan 10 07:14:14.873: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:14:14.876: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 10 07:14:14.876: INFO: Node kk-instance-vgmmg is running 0 daemon pod, expected 1
Jan 10 07:14:15.877: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:14:15.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 10 07:14:15.881: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6, will wait for the garbage collector to delete the pods
Jan 10 07:14:15.946: INFO: Deleting DaemonSet.extensions daemon-set took: 7.003593ms
Jan 10 07:14:16.047: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.323239ms
Jan 10 07:14:18.557: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:14:18.558: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 10 07:14:18.560: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20138"},"items":null}

Jan 10 07:14:18.563: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20138"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 10 07:14:18.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6" for this suite.

• [SLOW TEST:8.814 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":356,"completed":97,"skipped":2095,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:14:18.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:14:18.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jan 10 07:14:21.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-4334 --namespace=crd-publish-openapi-4334 create -f -'
Jan 10 07:14:22.225: INFO: stderr: ""
Jan 10 07:14:22.225: INFO: stdout: "e2e-test-crd-publish-openapi-2452-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 10 07:14:22.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-4334 --namespace=crd-publish-openapi-4334 delete e2e-test-crd-publish-openapi-2452-crds test-cr'
Jan 10 07:14:22.302: INFO: stderr: ""
Jan 10 07:14:22.302: INFO: stdout: "e2e-test-crd-publish-openapi-2452-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 10 07:14:22.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-4334 --namespace=crd-publish-openapi-4334 apply -f -'
Jan 10 07:14:22.540: INFO: stderr: ""
Jan 10 07:14:22.540: INFO: stdout: "e2e-test-crd-publish-openapi-2452-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 10 07:14:22.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-4334 --namespace=crd-publish-openapi-4334 delete e2e-test-crd-publish-openapi-2452-crds test-cr'
Jan 10 07:14:22.628: INFO: stderr: ""
Jan 10 07:14:22.628: INFO: stdout: "e2e-test-crd-publish-openapi-2452-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 10 07:14:22.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-4334 explain e2e-test-crd-publish-openapi-2452-crds'
Jan 10 07:14:22.842: INFO: stderr: ""
Jan 10 07:14:22.842: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2452-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:14:25.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4334" for this suite.

• [SLOW TEST:7.063 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":356,"completed":98,"skipped":2096,"failed":0}
SSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:14:25.644: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
STEP: mirroring a new custom Endpoint
Jan 10 07:14:25.693: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Jan 10 07:14:27.716: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Jan 10 07:14:29.730: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:188
Jan 10 07:14:31.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-4501" for this suite.

• [SLOW TEST:6.106 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":356,"completed":99,"skipped":2100,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:14:31.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 10 07:14:31.794: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-180  9215acff-4e0f-49c1-a65e-677fe81c25f2 20242 0 2023-01-10 07:14:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-10 07:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 10 07:14:31.794: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-180  9215acff-4e0f-49c1-a65e-677fe81c25f2 20243 0 2023-01-10 07:14:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-10 07:14:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 10 07:14:31.806: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-180  9215acff-4e0f-49c1-a65e-677fe81c25f2 20244 0 2023-01-10 07:14:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-10 07:14:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 10 07:14:31.807: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-180  9215acff-4e0f-49c1-a65e-677fe81c25f2 20245 0 2023-01-10 07:14:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-10 07:14:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 10 07:14:31.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-180" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":356,"completed":100,"skipped":2104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:14:31.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1052
[It] should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-1052
Jan 10 07:14:31.852: INFO: Found 0 stateful pods, waiting for 1
Jan 10 07:14:41.862: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 10 07:14:41.891: INFO: Deleting all statefulset in ns statefulset-1052
Jan 10 07:14:41.898: INFO: Scaling statefulset ss to 0
Jan 10 07:14:51.930: INFO: Waiting for statefulset status.replicas updated to 0
Jan 10 07:14:51.933: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 10 07:14:51.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1052" for this suite.

• [SLOW TEST:20.143 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":356,"completed":101,"skipped":2131,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:14:51.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-1b626b23-84e1-468d-b261-defe979ce81c
STEP: Creating a pod to test consume secrets
Jan 10 07:14:52.005: INFO: Waiting up to 5m0s for pod "pod-secrets-a59e4042-008b-4b2c-9816-5dc78ac9dcdf" in namespace "secrets-5366" to be "Succeeded or Failed"
Jan 10 07:14:52.011: INFO: Pod "pod-secrets-a59e4042-008b-4b2c-9816-5dc78ac9dcdf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.965828ms
Jan 10 07:14:54.018: INFO: Pod "pod-secrets-a59e4042-008b-4b2c-9816-5dc78ac9dcdf": Phase="Running", Reason="", readiness=true. Elapsed: 2.012607118s
Jan 10 07:14:56.026: INFO: Pod "pod-secrets-a59e4042-008b-4b2c-9816-5dc78ac9dcdf": Phase="Running", Reason="", readiness=false. Elapsed: 4.020569815s
Jan 10 07:14:58.036: INFO: Pod "pod-secrets-a59e4042-008b-4b2c-9816-5dc78ac9dcdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03104988s
STEP: Saw pod success
Jan 10 07:14:58.036: INFO: Pod "pod-secrets-a59e4042-008b-4b2c-9816-5dc78ac9dcdf" satisfied condition "Succeeded or Failed"
Jan 10 07:14:58.039: INFO: Trying to get logs from node kk-instance-r65pm pod pod-secrets-a59e4042-008b-4b2c-9816-5dc78ac9dcdf container secret-volume-test: <nil>
STEP: delete the pod
Jan 10 07:14:58.079: INFO: Waiting for pod pod-secrets-a59e4042-008b-4b2c-9816-5dc78ac9dcdf to disappear
Jan 10 07:14:58.081: INFO: Pod pod-secrets-a59e4042-008b-4b2c-9816-5dc78ac9dcdf no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 10 07:14:58.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5366" for this suite.

• [SLOW TEST:6.127 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":102,"skipped":2137,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:14:58.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 07:14:58.119: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd06318c-8390-46f7-aaac-ddfd2e976ecd" in namespace "projected-1364" to be "Succeeded or Failed"
Jan 10 07:14:58.123: INFO: Pod "downwardapi-volume-cd06318c-8390-46f7-aaac-ddfd2e976ecd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.253911ms
Jan 10 07:15:00.129: INFO: Pod "downwardapi-volume-cd06318c-8390-46f7-aaac-ddfd2e976ecd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009772742s
Jan 10 07:15:02.149: INFO: Pod "downwardapi-volume-cd06318c-8390-46f7-aaac-ddfd2e976ecd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029621844s
STEP: Saw pod success
Jan 10 07:15:02.149: INFO: Pod "downwardapi-volume-cd06318c-8390-46f7-aaac-ddfd2e976ecd" satisfied condition "Succeeded or Failed"
Jan 10 07:15:02.153: INFO: Trying to get logs from node kk-instance-vgmmg pod downwardapi-volume-cd06318c-8390-46f7-aaac-ddfd2e976ecd container client-container: <nil>
STEP: delete the pod
Jan 10 07:15:02.204: INFO: Waiting for pod downwardapi-volume-cd06318c-8390-46f7-aaac-ddfd2e976ecd to disappear
Jan 10 07:15:02.210: INFO: Pod downwardapi-volume-cd06318c-8390-46f7-aaac-ddfd2e976ecd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 10 07:15:02.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1364" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":103,"skipped":2138,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:15:02.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-edeeb3b8-ddfd-451e-af1a-08d82335ec48
STEP: Creating a pod to test consume configMaps
Jan 10 07:15:02.283: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-396dd247-e40e-4428-a305-a47f65dfbc9c" in namespace "projected-6777" to be "Succeeded or Failed"
Jan 10 07:15:02.286: INFO: Pod "pod-projected-configmaps-396dd247-e40e-4428-a305-a47f65dfbc9c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.229194ms
Jan 10 07:15:04.297: INFO: Pod "pod-projected-configmaps-396dd247-e40e-4428-a305-a47f65dfbc9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013852719s
Jan 10 07:15:06.306: INFO: Pod "pod-projected-configmaps-396dd247-e40e-4428-a305-a47f65dfbc9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023279309s
STEP: Saw pod success
Jan 10 07:15:06.306: INFO: Pod "pod-projected-configmaps-396dd247-e40e-4428-a305-a47f65dfbc9c" satisfied condition "Succeeded or Failed"
Jan 10 07:15:06.310: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-projected-configmaps-396dd247-e40e-4428-a305-a47f65dfbc9c container agnhost-container: <nil>
STEP: delete the pod
Jan 10 07:15:06.336: INFO: Waiting for pod pod-projected-configmaps-396dd247-e40e-4428-a305-a47f65dfbc9c to disappear
Jan 10 07:15:06.340: INFO: Pod pod-projected-configmaps-396dd247-e40e-4428-a305-a47f65dfbc9c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 10 07:15:06.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6777" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":104,"skipped":2152,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:15:06.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jan 10 07:15:46.490: INFO: The status of Pod kube-controller-manager-kk-instance-rx4ck is Running (Ready = true)
Jan 10 07:15:46.597: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 10 07:15:46.597: INFO: Deleting pod "simpletest.rc-2dgtz" in namespace "gc-4991"
Jan 10 07:15:46.612: INFO: Deleting pod "simpletest.rc-2dkmt" in namespace "gc-4991"
Jan 10 07:15:46.638: INFO: Deleting pod "simpletest.rc-2h6ms" in namespace "gc-4991"
Jan 10 07:15:46.651: INFO: Deleting pod "simpletest.rc-2l5rm" in namespace "gc-4991"
Jan 10 07:15:46.669: INFO: Deleting pod "simpletest.rc-2x49x" in namespace "gc-4991"
Jan 10 07:15:46.683: INFO: Deleting pod "simpletest.rc-4ns7v" in namespace "gc-4991"
Jan 10 07:15:46.702: INFO: Deleting pod "simpletest.rc-4xd78" in namespace "gc-4991"
Jan 10 07:15:46.724: INFO: Deleting pod "simpletest.rc-5477v" in namespace "gc-4991"
Jan 10 07:15:46.755: INFO: Deleting pod "simpletest.rc-54mdn" in namespace "gc-4991"
Jan 10 07:15:46.776: INFO: Deleting pod "simpletest.rc-5bwvs" in namespace "gc-4991"
Jan 10 07:15:46.790: INFO: Deleting pod "simpletest.rc-5fnnx" in namespace "gc-4991"
Jan 10 07:15:46.805: INFO: Deleting pod "simpletest.rc-5hf8f" in namespace "gc-4991"
Jan 10 07:15:46.842: INFO: Deleting pod "simpletest.rc-5htc2" in namespace "gc-4991"
Jan 10 07:15:46.884: INFO: Deleting pod "simpletest.rc-5t2b9" in namespace "gc-4991"
Jan 10 07:15:46.912: INFO: Deleting pod "simpletest.rc-66v2h" in namespace "gc-4991"
Jan 10 07:15:46.944: INFO: Deleting pod "simpletest.rc-6cb96" in namespace "gc-4991"
Jan 10 07:15:46.973: INFO: Deleting pod "simpletest.rc-6knhq" in namespace "gc-4991"
Jan 10 07:15:46.984: INFO: Deleting pod "simpletest.rc-6lkng" in namespace "gc-4991"
Jan 10 07:15:47.004: INFO: Deleting pod "simpletest.rc-6t6fz" in namespace "gc-4991"
Jan 10 07:15:47.021: INFO: Deleting pod "simpletest.rc-74cr2" in namespace "gc-4991"
Jan 10 07:15:47.030: INFO: Deleting pod "simpletest.rc-76qbp" in namespace "gc-4991"
Jan 10 07:15:47.056: INFO: Deleting pod "simpletest.rc-87p5z" in namespace "gc-4991"
Jan 10 07:15:47.085: INFO: Deleting pod "simpletest.rc-8d55n" in namespace "gc-4991"
Jan 10 07:15:47.128: INFO: Deleting pod "simpletest.rc-8h64w" in namespace "gc-4991"
Jan 10 07:15:47.146: INFO: Deleting pod "simpletest.rc-8hqhg" in namespace "gc-4991"
Jan 10 07:15:47.212: INFO: Deleting pod "simpletest.rc-8rdzc" in namespace "gc-4991"
Jan 10 07:15:47.238: INFO: Deleting pod "simpletest.rc-92jpf" in namespace "gc-4991"
Jan 10 07:15:47.256: INFO: Deleting pod "simpletest.rc-9dmgv" in namespace "gc-4991"
Jan 10 07:15:47.268: INFO: Deleting pod "simpletest.rc-9w74g" in namespace "gc-4991"
Jan 10 07:15:47.287: INFO: Deleting pod "simpletest.rc-b5hfb" in namespace "gc-4991"
Jan 10 07:15:47.324: INFO: Deleting pod "simpletest.rc-b5vqg" in namespace "gc-4991"
Jan 10 07:15:47.339: INFO: Deleting pod "simpletest.rc-bgwqn" in namespace "gc-4991"
Jan 10 07:15:47.359: INFO: Deleting pod "simpletest.rc-bhc8q" in namespace "gc-4991"
Jan 10 07:15:47.373: INFO: Deleting pod "simpletest.rc-c28tg" in namespace "gc-4991"
Jan 10 07:15:47.410: INFO: Deleting pod "simpletest.rc-c8kxv" in namespace "gc-4991"
Jan 10 07:15:47.465: INFO: Deleting pod "simpletest.rc-ckfsw" in namespace "gc-4991"
Jan 10 07:15:47.482: INFO: Deleting pod "simpletest.rc-cn654" in namespace "gc-4991"
Jan 10 07:15:47.498: INFO: Deleting pod "simpletest.rc-cqtsp" in namespace "gc-4991"
Jan 10 07:15:47.511: INFO: Deleting pod "simpletest.rc-ctcgt" in namespace "gc-4991"
Jan 10 07:15:47.527: INFO: Deleting pod "simpletest.rc-czgxh" in namespace "gc-4991"
Jan 10 07:15:47.540: INFO: Deleting pod "simpletest.rc-d2zj5" in namespace "gc-4991"
Jan 10 07:15:47.555: INFO: Deleting pod "simpletest.rc-dw7gr" in namespace "gc-4991"
Jan 10 07:15:47.568: INFO: Deleting pod "simpletest.rc-fkvdm" in namespace "gc-4991"
Jan 10 07:15:47.581: INFO: Deleting pod "simpletest.rc-ftdzw" in namespace "gc-4991"
Jan 10 07:15:47.596: INFO: Deleting pod "simpletest.rc-fwlkl" in namespace "gc-4991"
Jan 10 07:15:47.616: INFO: Deleting pod "simpletest.rc-g6dvz" in namespace "gc-4991"
Jan 10 07:15:47.632: INFO: Deleting pod "simpletest.rc-gl27g" in namespace "gc-4991"
Jan 10 07:15:47.645: INFO: Deleting pod "simpletest.rc-glcqk" in namespace "gc-4991"
Jan 10 07:15:47.660: INFO: Deleting pod "simpletest.rc-h7xlg" in namespace "gc-4991"
Jan 10 07:15:47.681: INFO: Deleting pod "simpletest.rc-jfmgw" in namespace "gc-4991"
Jan 10 07:15:47.694: INFO: Deleting pod "simpletest.rc-jq9xp" in namespace "gc-4991"
Jan 10 07:15:47.720: INFO: Deleting pod "simpletest.rc-jtdcw" in namespace "gc-4991"
Jan 10 07:15:47.741: INFO: Deleting pod "simpletest.rc-kr9rg" in namespace "gc-4991"
Jan 10 07:15:47.761: INFO: Deleting pod "simpletest.rc-lcz5s" in namespace "gc-4991"
Jan 10 07:15:47.788: INFO: Deleting pod "simpletest.rc-lvqfb" in namespace "gc-4991"
Jan 10 07:15:47.799: INFO: Deleting pod "simpletest.rc-lzb8q" in namespace "gc-4991"
Jan 10 07:15:47.824: INFO: Deleting pod "simpletest.rc-ml89z" in namespace "gc-4991"
Jan 10 07:15:47.854: INFO: Deleting pod "simpletest.rc-mlsvg" in namespace "gc-4991"
Jan 10 07:15:47.863: INFO: Deleting pod "simpletest.rc-n4vgw" in namespace "gc-4991"
Jan 10 07:15:47.888: INFO: Deleting pod "simpletest.rc-n4z2q" in namespace "gc-4991"
Jan 10 07:15:47.911: INFO: Deleting pod "simpletest.rc-nbkf6" in namespace "gc-4991"
Jan 10 07:15:47.921: INFO: Deleting pod "simpletest.rc-nd5h9" in namespace "gc-4991"
Jan 10 07:15:47.928: INFO: Deleting pod "simpletest.rc-nfr98" in namespace "gc-4991"
Jan 10 07:15:47.938: INFO: Deleting pod "simpletest.rc-nks7t" in namespace "gc-4991"
Jan 10 07:15:47.947: INFO: Deleting pod "simpletest.rc-p5ncc" in namespace "gc-4991"
Jan 10 07:15:47.955: INFO: Deleting pod "simpletest.rc-p9sxh" in namespace "gc-4991"
Jan 10 07:15:47.962: INFO: Deleting pod "simpletest.rc-pcxf6" in namespace "gc-4991"
Jan 10 07:15:47.977: INFO: Deleting pod "simpletest.rc-ppqnj" in namespace "gc-4991"
Jan 10 07:15:47.995: INFO: Deleting pod "simpletest.rc-qbg8d" in namespace "gc-4991"
Jan 10 07:15:48.005: INFO: Deleting pod "simpletest.rc-r2hzk" in namespace "gc-4991"
Jan 10 07:15:48.017: INFO: Deleting pod "simpletest.rc-r4m2d" in namespace "gc-4991"
Jan 10 07:15:48.026: INFO: Deleting pod "simpletest.rc-rpbfm" in namespace "gc-4991"
Jan 10 07:15:48.045: INFO: Deleting pod "simpletest.rc-rplpp" in namespace "gc-4991"
Jan 10 07:15:48.060: INFO: Deleting pod "simpletest.rc-rpq24" in namespace "gc-4991"
Jan 10 07:15:48.084: INFO: Deleting pod "simpletest.rc-s2klc" in namespace "gc-4991"
Jan 10 07:15:48.101: INFO: Deleting pod "simpletest.rc-s9tdh" in namespace "gc-4991"
Jan 10 07:15:48.118: INFO: Deleting pod "simpletest.rc-skxl4" in namespace "gc-4991"
Jan 10 07:15:48.130: INFO: Deleting pod "simpletest.rc-sn6fg" in namespace "gc-4991"
Jan 10 07:15:48.139: INFO: Deleting pod "simpletest.rc-str4l" in namespace "gc-4991"
Jan 10 07:15:48.150: INFO: Deleting pod "simpletest.rc-sz6qq" in namespace "gc-4991"
Jan 10 07:15:48.160: INFO: Deleting pod "simpletest.rc-t4j56" in namespace "gc-4991"
Jan 10 07:15:48.203: INFO: Deleting pod "simpletest.rc-tcjjg" in namespace "gc-4991"
Jan 10 07:15:48.255: INFO: Deleting pod "simpletest.rc-td9ft" in namespace "gc-4991"
Jan 10 07:15:48.304: INFO: Deleting pod "simpletest.rc-tvggb" in namespace "gc-4991"
Jan 10 07:15:48.355: INFO: Deleting pod "simpletest.rc-vcm4h" in namespace "gc-4991"
Jan 10 07:15:48.411: INFO: Deleting pod "simpletest.rc-vqlnx" in namespace "gc-4991"
Jan 10 07:15:48.458: INFO: Deleting pod "simpletest.rc-vsqkk" in namespace "gc-4991"
Jan 10 07:15:48.531: INFO: Deleting pod "simpletest.rc-vszh7" in namespace "gc-4991"
Jan 10 07:15:48.560: INFO: Deleting pod "simpletest.rc-vxv2l" in namespace "gc-4991"
Jan 10 07:15:48.608: INFO: Deleting pod "simpletest.rc-vxx8p" in namespace "gc-4991"
Jan 10 07:15:48.652: INFO: Deleting pod "simpletest.rc-vz5gx" in namespace "gc-4991"
Jan 10 07:15:48.709: INFO: Deleting pod "simpletest.rc-wbdkf" in namespace "gc-4991"
Jan 10 07:15:48.755: INFO: Deleting pod "simpletest.rc-wz9tp" in namespace "gc-4991"
Jan 10 07:15:48.813: INFO: Deleting pod "simpletest.rc-xk74v" in namespace "gc-4991"
Jan 10 07:15:48.854: INFO: Deleting pod "simpletest.rc-xkd7d" in namespace "gc-4991"
Jan 10 07:15:48.905: INFO: Deleting pod "simpletest.rc-z5ss5" in namespace "gc-4991"
Jan 10 07:15:48.965: INFO: Deleting pod "simpletest.rc-z6zl9" in namespace "gc-4991"
Jan 10 07:15:49.002: INFO: Deleting pod "simpletest.rc-z9rjz" in namespace "gc-4991"
Jan 10 07:15:49.067: INFO: Deleting pod "simpletest.rc-zdsp5" in namespace "gc-4991"
Jan 10 07:15:49.107: INFO: Deleting pod "simpletest.rc-zj5bq" in namespace "gc-4991"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 10 07:15:49.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4991" for this suite.

• [SLOW TEST:42.898 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":356,"completed":105,"skipped":2182,"failed":0}
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:15:49.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 10 07:15:49.291: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 10 07:16:02.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5888" for this suite.

• [SLOW TEST:13.606 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":356,"completed":106,"skipped":2187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:16:02.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:16:02.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jan 10 07:16:05.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-2772 --namespace=crd-publish-openapi-2772 create -f -'
Jan 10 07:16:06.779: INFO: stderr: ""
Jan 10 07:16:06.779: INFO: stdout: "e2e-test-crd-publish-openapi-4743-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 10 07:16:06.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-2772 --namespace=crd-publish-openapi-2772 delete e2e-test-crd-publish-openapi-4743-crds test-cr'
Jan 10 07:16:06.877: INFO: stderr: ""
Jan 10 07:16:06.877: INFO: stdout: "e2e-test-crd-publish-openapi-4743-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 10 07:16:06.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-2772 --namespace=crd-publish-openapi-2772 apply -f -'
Jan 10 07:16:07.122: INFO: stderr: ""
Jan 10 07:16:07.122: INFO: stdout: "e2e-test-crd-publish-openapi-4743-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 10 07:16:07.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-2772 --namespace=crd-publish-openapi-2772 delete e2e-test-crd-publish-openapi-4743-crds test-cr'
Jan 10 07:16:07.206: INFO: stderr: ""
Jan 10 07:16:07.206: INFO: stdout: "e2e-test-crd-publish-openapi-4743-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 10 07:16:07.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-2772 explain e2e-test-crd-publish-openapi-4743-crds'
Jan 10 07:16:07.445: INFO: stderr: ""
Jan 10 07:16:07.445: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4743-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:16:10.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2772" for this suite.

• [SLOW TEST:7.412 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":356,"completed":107,"skipped":2219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:16:10.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 10 07:16:10.317: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 10 07:16:10.321: INFO: starting watch
STEP: patching
STEP: updating
Jan 10 07:16:10.339: INFO: waiting for watch events with expected annotations
Jan 10 07:16:10.339: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 10 07:16:10.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4659" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":356,"completed":108,"skipped":2245,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:16:10.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 10 07:16:14.466: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 10 07:16:14.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4938" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":356,"completed":109,"skipped":2259,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:16:14.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 10 07:16:27.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5234" for this suite.

• [SLOW TEST:13.151 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":356,"completed":110,"skipped":2262,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:16:27.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 10 07:16:27.685: INFO: Waiting up to 5m0s for pod "pod-cfcb8b01-8f3b-4be2-a034-1998e32a0340" in namespace "emptydir-4420" to be "Succeeded or Failed"
Jan 10 07:16:27.688: INFO: Pod "pod-cfcb8b01-8f3b-4be2-a034-1998e32a0340": Phase="Pending", Reason="", readiness=false. Elapsed: 2.711472ms
Jan 10 07:16:29.695: INFO: Pod "pod-cfcb8b01-8f3b-4be2-a034-1998e32a0340": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009865423s
Jan 10 07:16:31.704: INFO: Pod "pod-cfcb8b01-8f3b-4be2-a034-1998e32a0340": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018801359s
STEP: Saw pod success
Jan 10 07:16:31.704: INFO: Pod "pod-cfcb8b01-8f3b-4be2-a034-1998e32a0340" satisfied condition "Succeeded or Failed"
Jan 10 07:16:31.708: INFO: Trying to get logs from node kk-instance-r65pm pod pod-cfcb8b01-8f3b-4be2-a034-1998e32a0340 container test-container: <nil>
STEP: delete the pod
Jan 10 07:16:31.735: INFO: Waiting for pod pod-cfcb8b01-8f3b-4be2-a034-1998e32a0340 to disappear
Jan 10 07:16:31.738: INFO: Pod pod-cfcb8b01-8f3b-4be2-a034-1998e32a0340 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 10 07:16:31.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4420" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":111,"skipped":2276,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:16:31.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:16:31.788: INFO: Got root ca configmap in namespace "svcaccounts-3612"
Jan 10 07:16:31.794: INFO: Deleted root ca configmap in namespace "svcaccounts-3612"
STEP: waiting for a new root ca configmap created
Jan 10 07:16:32.301: INFO: Recreated root ca configmap in namespace "svcaccounts-3612"
Jan 10 07:16:32.312: INFO: Updated root ca configmap in namespace "svcaccounts-3612"
STEP: waiting for the root ca configmap reconciled
Jan 10 07:16:32.822: INFO: Reconciled root ca configmap in namespace "svcaccounts-3612"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 10 07:16:32.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3612" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":356,"completed":112,"skipped":2277,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:16:32.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:16:32.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:16:36.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-981" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":356,"completed":113,"skipped":2300,"failed":0}
SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:16:36.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service multi-endpoint-test in namespace services-7508
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7508 to expose endpoints map[]
Jan 10 07:16:36.099: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 10 07:16:37.117: INFO: successfully validated that service multi-endpoint-test in namespace services-7508 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7508
Jan 10 07:16:37.130: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:16:39.140: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7508 to expose endpoints map[pod1:[100]]
Jan 10 07:16:39.154: INFO: successfully validated that service multi-endpoint-test in namespace services-7508 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7508
Jan 10 07:16:39.164: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:16:41.174: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7508 to expose endpoints map[pod1:[100] pod2:[101]]
Jan 10 07:16:41.191: INFO: successfully validated that service multi-endpoint-test in namespace services-7508 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Jan 10 07:16:41.191: INFO: Creating new exec pod
Jan 10 07:16:44.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7508 exec execpoddbqqx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 10 07:16:44.398: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 10 07:16:44.398: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:16:44.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7508 exec execpoddbqqx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.49.249 80'
Jan 10 07:16:44.566: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.49.249 80\nConnection to 10.233.49.249 80 port [tcp/http] succeeded!\n"
Jan 10 07:16:44.566: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:16:44.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7508 exec execpoddbqqx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 10 07:16:44.740: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 10 07:16:44.740: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:16:44.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7508 exec execpoddbqqx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.49.249 81'
Jan 10 07:16:44.938: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.49.249 81\nConnection to 10.233.49.249 81 port [tcp/*] succeeded!\n"
Jan 10 07:16:44.938: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7508
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7508 to expose endpoints map[pod2:[101]]
Jan 10 07:16:44.986: INFO: successfully validated that service multi-endpoint-test in namespace services-7508 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7508
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7508 to expose endpoints map[]
Jan 10 07:16:45.037: INFO: successfully validated that service multi-endpoint-test in namespace services-7508 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 07:16:45.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7508" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.028 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":356,"completed":114,"skipped":2302,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:16:45.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5506
STEP: creating service affinity-nodeport in namespace services-5506
STEP: creating replication controller affinity-nodeport in namespace services-5506
I0110 07:16:45.140844      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5506, replica count: 3
I0110 07:16:48.192479      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 10 07:16:48.211: INFO: Creating new exec pod
Jan 10 07:16:51.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5506 exec execpod-affinityhrpgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 10 07:16:51.384: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 10 07:16:51.384: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:16:51.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5506 exec execpod-affinityhrpgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.3.222 80'
Jan 10 07:16:51.545: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.3.222 80\nConnection to 10.233.3.222 80 port [tcp/http] succeeded!\n"
Jan 10 07:16:51.545: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:16:51.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5506 exec execpod-affinityhrpgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.4 30228'
Jan 10 07:16:51.699: INFO: stderr: "+ nc -v -t -w 2 192.168.0.4 30228\nConnection to 192.168.0.4 30228 port [tcp/*] succeeded!\n+ echo hostName\n"
Jan 10 07:16:51.699: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:16:51.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5506 exec execpod-affinityhrpgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.5 30228'
Jan 10 07:16:51.853: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.5 30228\nConnection to 192.168.0.5 30228 port [tcp/*] succeeded!\n"
Jan 10 07:16:51.853: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:16:51.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5506 exec execpod-affinityhrpgg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.4:30228/ ; done'
Jan 10 07:16:52.074: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30228/\n"
Jan 10 07:16:52.074: INFO: stdout: "\naffinity-nodeport-l8zzq\naffinity-nodeport-l8zzq\naffinity-nodeport-l8zzq\naffinity-nodeport-l8zzq\naffinity-nodeport-l8zzq\naffinity-nodeport-l8zzq\naffinity-nodeport-l8zzq\naffinity-nodeport-l8zzq\naffinity-nodeport-l8zzq\naffinity-nodeport-l8zzq\naffinity-nodeport-l8zzq\naffinity-nodeport-l8zzq\naffinity-nodeport-l8zzq\naffinity-nodeport-l8zzq\naffinity-nodeport-l8zzq\naffinity-nodeport-l8zzq"
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.074: INFO: Received response from host: affinity-nodeport-l8zzq
Jan 10 07:16:52.075: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5506, will wait for the garbage collector to delete the pods
Jan 10 07:16:52.152: INFO: Deleting ReplicationController affinity-nodeport took: 7.687767ms
Jan 10 07:16:52.253: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.888181ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 07:16:54.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5506" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.224 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":115,"skipped":2315,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:16:54.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating secret secrets-4076/secret-test-43f5a5f5-74ef-45b4-bbab-3634eacaba57
STEP: Creating a pod to test consume secrets
Jan 10 07:16:54.352: INFO: Waiting up to 5m0s for pod "pod-configmaps-ed52ab8e-e2c6-4ef4-a12c-4069aaf659d7" in namespace "secrets-4076" to be "Succeeded or Failed"
Jan 10 07:16:54.357: INFO: Pod "pod-configmaps-ed52ab8e-e2c6-4ef4-a12c-4069aaf659d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.231676ms
Jan 10 07:16:56.366: INFO: Pod "pod-configmaps-ed52ab8e-e2c6-4ef4-a12c-4069aaf659d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013376253s
Jan 10 07:16:58.379: INFO: Pod "pod-configmaps-ed52ab8e-e2c6-4ef4-a12c-4069aaf659d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026539251s
STEP: Saw pod success
Jan 10 07:16:58.379: INFO: Pod "pod-configmaps-ed52ab8e-e2c6-4ef4-a12c-4069aaf659d7" satisfied condition "Succeeded or Failed"
Jan 10 07:16:58.382: INFO: Trying to get logs from node kk-instance-r65pm pod pod-configmaps-ed52ab8e-e2c6-4ef4-a12c-4069aaf659d7 container env-test: <nil>
STEP: delete the pod
Jan 10 07:16:58.400: INFO: Waiting for pod pod-configmaps-ed52ab8e-e2c6-4ef4-a12c-4069aaf659d7 to disappear
Jan 10 07:16:58.410: INFO: Pod pod-configmaps-ed52ab8e-e2c6-4ef4-a12c-4069aaf659d7 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 10 07:16:58.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4076" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":116,"skipped":2345,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:16:58.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 10 07:17:02.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-6408" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":117,"skipped":2358,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:17:02.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 10 07:17:06.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3502" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":356,"completed":118,"skipped":2377,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:17:06.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:17:06.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-9881 version'
Jan 10 07:17:06.766: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 10 07:17:06.766: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.8\", GitCommit:\"fdc77503e954d1ee641c0e350481f7528e8d068b\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:38:19Z\", GoVersion:\"go1.18.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.4\nServer Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.8\", GitCommit:\"fdc77503e954d1ee641c0e350481f7528e8d068b\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:31:40Z\", GoVersion:\"go1.18.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 07:17:06.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9881" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":356,"completed":119,"skipped":2397,"failed":0}
S
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:17:06.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 10 07:17:31.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6086" for this suite.

• [SLOW TEST:24.359 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":356,"completed":120,"skipped":2398,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:17:31.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:188
Jan 10 07:17:31.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-806" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":356,"completed":121,"skipped":2407,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:17:31.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 10 07:17:31.255: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:17:33.261: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 10 07:17:33.277: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:17:35.285: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 10 07:17:35.298: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 10 07:17:35.302: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 10 07:17:37.303: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 10 07:17:37.314: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 10 07:17:39.302: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 10 07:17:39.309: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 10 07:17:39.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7741" for this suite.

• [SLOW TEST:8.104 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":356,"completed":122,"skipped":2425,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:17:39.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 10 07:17:39.370: INFO: Waiting up to 5m0s for pod "pod-d728aad9-9c48-4eaa-b76a-919e6e8ab821" in namespace "emptydir-6330" to be "Succeeded or Failed"
Jan 10 07:17:39.373: INFO: Pod "pod-d728aad9-9c48-4eaa-b76a-919e6e8ab821": Phase="Pending", Reason="", readiness=false. Elapsed: 2.673799ms
Jan 10 07:17:41.381: INFO: Pod "pod-d728aad9-9c48-4eaa-b76a-919e6e8ab821": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010981183s
Jan 10 07:17:43.391: INFO: Pod "pod-d728aad9-9c48-4eaa-b76a-919e6e8ab821": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020343825s
STEP: Saw pod success
Jan 10 07:17:43.391: INFO: Pod "pod-d728aad9-9c48-4eaa-b76a-919e6e8ab821" satisfied condition "Succeeded or Failed"
Jan 10 07:17:43.393: INFO: Trying to get logs from node kk-instance-r65pm pod pod-d728aad9-9c48-4eaa-b76a-919e6e8ab821 container test-container: <nil>
STEP: delete the pod
Jan 10 07:17:43.413: INFO: Waiting for pod pod-d728aad9-9c48-4eaa-b76a-919e6e8ab821 to disappear
Jan 10 07:17:43.416: INFO: Pod pod-d728aad9-9c48-4eaa-b76a-919e6e8ab821 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 10 07:17:43.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6330" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":123,"skipped":2434,"failed":0}
SSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:17:43.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 10 07:23:01.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6129" for this suite.

• [SLOW TEST:318.090 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":356,"completed":124,"skipped":2440,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:23:01.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 10 07:23:01.586: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:23:01.589: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:23:01.589: INFO: Node kk-instance-r65pm is running 0 daemon pod, expected 1
Jan 10 07:23:02.597: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:23:02.608: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:23:02.608: INFO: Node kk-instance-r65pm is running 0 daemon pod, expected 1
Jan 10 07:23:03.599: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:23:03.602: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 10 07:23:03.602: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 10 07:23:03.633: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:23:03.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 10 07:23:03.641: INFO: Node kk-instance-r65pm is running 0 daemon pod, expected 1
Jan 10 07:23:04.650: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:23:04.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 10 07:23:04.653: INFO: Node kk-instance-r65pm is running 0 daemon pod, expected 1
Jan 10 07:23:05.650: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:23:05.654: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 10 07:23:05.654: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2742, will wait for the garbage collector to delete the pods
Jan 10 07:23:05.718: INFO: Deleting DaemonSet.extensions daemon-set took: 6.226808ms
Jan 10 07:23:05.819: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.997243ms
Jan 10 07:23:07.930: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:23:07.930: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 10 07:23:07.933: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25289"},"items":null}

Jan 10 07:23:07.936: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25289"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 10 07:23:07.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2742" for this suite.

• [SLOW TEST:6.425 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":356,"completed":125,"skipped":2495,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:23:07.954: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 07:23:07.996: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4fe6ffbf-9e8c-494b-84ec-95f6526a1d93" in namespace "projected-255" to be "Succeeded or Failed"
Jan 10 07:23:08.000: INFO: Pod "downwardapi-volume-4fe6ffbf-9e8c-494b-84ec-95f6526a1d93": Phase="Pending", Reason="", readiness=false. Elapsed: 3.808463ms
Jan 10 07:23:10.006: INFO: Pod "downwardapi-volume-4fe6ffbf-9e8c-494b-84ec-95f6526a1d93": Phase="Running", Reason="", readiness=false. Elapsed: 2.009964724s
Jan 10 07:23:12.015: INFO: Pod "downwardapi-volume-4fe6ffbf-9e8c-494b-84ec-95f6526a1d93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019329467s
STEP: Saw pod success
Jan 10 07:23:12.016: INFO: Pod "downwardapi-volume-4fe6ffbf-9e8c-494b-84ec-95f6526a1d93" satisfied condition "Succeeded or Failed"
Jan 10 07:23:12.018: INFO: Trying to get logs from node kk-instance-r65pm pod downwardapi-volume-4fe6ffbf-9e8c-494b-84ec-95f6526a1d93 container client-container: <nil>
STEP: delete the pod
Jan 10 07:23:12.043: INFO: Waiting for pod downwardapi-volume-4fe6ffbf-9e8c-494b-84ec-95f6526a1d93 to disappear
Jan 10 07:23:12.046: INFO: Pod downwardapi-volume-4fe6ffbf-9e8c-494b-84ec-95f6526a1d93 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 10 07:23:12.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-255" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":126,"skipped":2502,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:23:12.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:23:12.087: INFO: Creating ReplicaSet my-hostname-basic-bae88c93-cb90-4372-965c-6d189db33734
Jan 10 07:23:12.095: INFO: Pod name my-hostname-basic-bae88c93-cb90-4372-965c-6d189db33734: Found 0 pods out of 1
Jan 10 07:23:17.109: INFO: Pod name my-hostname-basic-bae88c93-cb90-4372-965c-6d189db33734: Found 1 pods out of 1
Jan 10 07:23:17.109: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-bae88c93-cb90-4372-965c-6d189db33734" is running
Jan 10 07:23:17.116: INFO: Pod "my-hostname-basic-bae88c93-cb90-4372-965c-6d189db33734-4mkql" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-10 07:23:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-10 07:23:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-10 07:23:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-10 07:23:12 +0000 UTC Reason: Message:}])
Jan 10 07:23:17.117: INFO: Trying to dial the pod
Jan 10 07:23:22.134: INFO: Controller my-hostname-basic-bae88c93-cb90-4372-965c-6d189db33734: Got expected result from replica 1 [my-hostname-basic-bae88c93-cb90-4372-965c-6d189db33734-4mkql]: "my-hostname-basic-bae88c93-cb90-4372-965c-6d189db33734-4mkql", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 10 07:23:22.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9053" for this suite.

• [SLOW TEST:10.096 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":127,"skipped":2509,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:23:22.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:23:22.192: INFO: Creating pod...
Jan 10 07:23:24.211: INFO: Creating service...
Jan 10 07:23:24.223: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-884/pods/agnhost/proxy/some/path/with/DELETE
Jan 10 07:23:24.238: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 10 07:23:24.238: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-884/pods/agnhost/proxy/some/path/with/GET
Jan 10 07:23:24.243: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 10 07:23:24.243: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-884/pods/agnhost/proxy/some/path/with/HEAD
Jan 10 07:23:24.246: INFO: http.Client request:HEAD | StatusCode:200
Jan 10 07:23:24.246: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-884/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 10 07:23:24.249: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 10 07:23:24.249: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-884/pods/agnhost/proxy/some/path/with/PATCH
Jan 10 07:23:24.252: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 10 07:23:24.252: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-884/pods/agnhost/proxy/some/path/with/POST
Jan 10 07:23:24.255: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 10 07:23:24.255: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-884/pods/agnhost/proxy/some/path/with/PUT
Jan 10 07:23:24.257: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 10 07:23:24.257: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-884/services/test-service/proxy/some/path/with/DELETE
Jan 10 07:23:24.261: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 10 07:23:24.261: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-884/services/test-service/proxy/some/path/with/GET
Jan 10 07:23:24.264: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 10 07:23:24.264: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-884/services/test-service/proxy/some/path/with/HEAD
Jan 10 07:23:24.268: INFO: http.Client request:HEAD | StatusCode:200
Jan 10 07:23:24.268: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-884/services/test-service/proxy/some/path/with/OPTIONS
Jan 10 07:23:24.270: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 10 07:23:24.270: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-884/services/test-service/proxy/some/path/with/PATCH
Jan 10 07:23:24.274: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 10 07:23:24.274: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-884/services/test-service/proxy/some/path/with/POST
Jan 10 07:23:24.277: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 10 07:23:24.278: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-884/services/test-service/proxy/some/path/with/PUT
Jan 10 07:23:24.282: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jan 10 07:23:24.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-884" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":356,"completed":128,"skipped":2517,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:23:24.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:23:24.329: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jan 10 07:23:26.384: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 10 07:23:27.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2058" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":356,"completed":129,"skipped":2540,"failed":0}
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:23:27.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-2202/configmap-test-c55c79b5-0e7f-4b60-9381-c913f9c2fe9e
STEP: Creating a pod to test consume configMaps
Jan 10 07:23:27.444: INFO: Waiting up to 5m0s for pod "pod-configmaps-7479cfc1-f3ea-4d19-b083-ff9854ce2f5a" in namespace "configmap-2202" to be "Succeeded or Failed"
Jan 10 07:23:27.451: INFO: Pod "pod-configmaps-7479cfc1-f3ea-4d19-b083-ff9854ce2f5a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.504672ms
Jan 10 07:23:29.467: INFO: Pod "pod-configmaps-7479cfc1-f3ea-4d19-b083-ff9854ce2f5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023017377s
Jan 10 07:23:31.472: INFO: Pod "pod-configmaps-7479cfc1-f3ea-4d19-b083-ff9854ce2f5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027667564s
STEP: Saw pod success
Jan 10 07:23:31.472: INFO: Pod "pod-configmaps-7479cfc1-f3ea-4d19-b083-ff9854ce2f5a" satisfied condition "Succeeded or Failed"
Jan 10 07:23:31.474: INFO: Trying to get logs from node kk-instance-r65pm pod pod-configmaps-7479cfc1-f3ea-4d19-b083-ff9854ce2f5a container env-test: <nil>
STEP: delete the pod
Jan 10 07:23:31.490: INFO: Waiting for pod pod-configmaps-7479cfc1-f3ea-4d19-b083-ff9854ce2f5a to disappear
Jan 10 07:23:31.493: INFO: Pod pod-configmaps-7479cfc1-f3ea-4d19-b083-ff9854ce2f5a no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 10 07:23:31.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2202" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":130,"skipped":2546,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:23:31.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 10 07:23:31.536: INFO: PodSpec: initContainers in spec.initContainers
Jan 10 07:24:12.418: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-76d4a72b-8c1e-45cc-b753-d3543a44d1df", GenerateName:"", Namespace:"init-container-7775", SelfLink:"", UID:"2fb4577a-2a3c-493f-8f3b-0e8fb369d4b8", ResourceVersion:"25797", Generation:0, CreationTimestamp:time.Date(2023, time.January, 10, 7, 23, 31, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"536442466"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"791fdd45ba9e5e3ae3d9c89e97bf2990cc2e0a2107be1639dedce861caedacd2", "cni.projectcalico.org/podIP":"10.233.107.103/32", "cni.projectcalico.org/podIPs":"10.233.107.103/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 10, 7, 23, 31, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003142708), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 10, 7, 23, 32, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003142738), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 10, 7, 23, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003142768), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-svgws", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc002b9aec0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-svgws", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-svgws", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.7", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-svgws", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00396d058), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kk-instance-vgmmg", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001ac0fc0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00396d0d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00396d0f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00396d0f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00396d0fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000ed9a30), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 10, 7, 23, 31, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 10, 7, 23, 31, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 10, 7, 23, 31, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 10, 7, 23, 31, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.5", PodIP:"10.233.107.103", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.107.103"}}, StartTime:time.Date(2023, time.January, 10, 7, 23, 31, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001ac10a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001ac1180)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://5338052d17d97b8820ee5d70fecbd1e5f685bcfffbc1e34a2e6881a74b40119e", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002b9af80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002b9af20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.7", ImageID:"", ContainerID:"", Started:(*bool)(0xc00396d17f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 10 07:24:12.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7775" for this suite.

• [SLOW TEST:40.930 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":356,"completed":131,"skipped":2566,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:24:12.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 10 07:24:12.878: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 07:24:15.902: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:24:15.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:24:19.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5306" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:6.688 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":356,"completed":132,"skipped":2568,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:24:19.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 10 07:24:19.173: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6297  80264a33-33cc-491c-bb51-ea7aa7e8a10e 25899 0 2023-01-10 07:24:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-10 07:24:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 10 07:24:19.174: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6297  80264a33-33cc-491c-bb51-ea7aa7e8a10e 25900 0 2023-01-10 07:24:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-10 07:24:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 10 07:24:19.174: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6297  80264a33-33cc-491c-bb51-ea7aa7e8a10e 25902 0 2023-01-10 07:24:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-10 07:24:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 10 07:24:29.212: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6297  80264a33-33cc-491c-bb51-ea7aa7e8a10e 25953 0 2023-01-10 07:24:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-10 07:24:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 10 07:24:29.212: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6297  80264a33-33cc-491c-bb51-ea7aa7e8a10e 25954 0 2023-01-10 07:24:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-10 07:24:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 10 07:24:29.212: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6297  80264a33-33cc-491c-bb51-ea7aa7e8a10e 25955 0 2023-01-10 07:24:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-10 07:24:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 10 07:24:29.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6297" for this suite.

• [SLOW TEST:10.099 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":356,"completed":133,"skipped":2605,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:24:29.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap that has name configmap-test-emptyKey-d232572d-826b-46fb-9700-1578dbab441f
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 10 07:24:29.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4206" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":356,"completed":134,"skipped":2609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:24:29.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 10 07:24:31.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-791" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","total":356,"completed":135,"skipped":2638,"failed":0}
SS
------------------------------
[sig-storage] CSIStorageCapacity 
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:24:31.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename csistoragecapacity
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/storage.k8s.io
STEP: getting /apis/storage.k8s.io/v1
STEP: creating
STEP: watching
Jan 10 07:24:31.378: INFO: starting watch
STEP: getting
STEP: listing in namespace
STEP: listing across namespaces
STEP: patching
STEP: updating
Jan 10 07:24:31.403: INFO: waiting for watch events with expected annotations in namespace
Jan 10 07:24:31.403: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:188
Jan 10 07:24:31.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-9357" for this suite.
•{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","total":356,"completed":136,"skipped":2640,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:24:31.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-7506a73b-af12-4e19-8f3c-8ca80e0fe429
STEP: Creating a pod to test consume configMaps
Jan 10 07:24:31.474: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e556f731-5f4a-4560-919a-56fb4dcca525" in namespace "projected-5291" to be "Succeeded or Failed"
Jan 10 07:24:31.478: INFO: Pod "pod-projected-configmaps-e556f731-5f4a-4560-919a-56fb4dcca525": Phase="Pending", Reason="", readiness=false. Elapsed: 3.99583ms
Jan 10 07:24:33.484: INFO: Pod "pod-projected-configmaps-e556f731-5f4a-4560-919a-56fb4dcca525": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010631241s
Jan 10 07:24:35.492: INFO: Pod "pod-projected-configmaps-e556f731-5f4a-4560-919a-56fb4dcca525": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017789238s
STEP: Saw pod success
Jan 10 07:24:35.492: INFO: Pod "pod-projected-configmaps-e556f731-5f4a-4560-919a-56fb4dcca525" satisfied condition "Succeeded or Failed"
Jan 10 07:24:35.494: INFO: Trying to get logs from node kk-instance-r65pm pod pod-projected-configmaps-e556f731-5f4a-4560-919a-56fb4dcca525 container agnhost-container: <nil>
STEP: delete the pod
Jan 10 07:24:35.512: INFO: Waiting for pod pod-projected-configmaps-e556f731-5f4a-4560-919a-56fb4dcca525 to disappear
Jan 10 07:24:35.516: INFO: Pod pod-projected-configmaps-e556f731-5f4a-4560-919a-56fb4dcca525 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 10 07:24:35.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5291" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":137,"skipped":2643,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:24:35.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:24:35.577: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 10 07:24:35.590: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:24:35.594: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:24:35.594: INFO: Node kk-instance-r65pm is running 0 daemon pod, expected 1
Jan 10 07:24:36.602: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:24:36.605: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:24:36.606: INFO: Node kk-instance-r65pm is running 0 daemon pod, expected 1
Jan 10 07:24:37.608: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:24:37.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 10 07:24:37.637: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 10 07:24:37.685: INFO: Wrong image for pod: daemon-set-csgrc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 10 07:24:37.685: INFO: Wrong image for pod: daemon-set-wc5sg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 10 07:24:37.697: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:24:38.704: INFO: Wrong image for pod: daemon-set-wc5sg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 10 07:24:38.708: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:24:39.703: INFO: Wrong image for pod: daemon-set-wc5sg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 10 07:24:39.707: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:24:40.707: INFO: Pod daemon-set-pbbvs is not available
Jan 10 07:24:40.707: INFO: Wrong image for pod: daemon-set-wc5sg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 10 07:24:40.711: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:24:41.707: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:24:42.708: INFO: Pod daemon-set-tmmqq is not available
Jan 10 07:24:42.712: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 10 07:24:42.715: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:24:42.718: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 10 07:24:42.718: INFO: Node kk-instance-vgmmg is running 0 daemon pod, expected 1
Jan 10 07:24:43.725: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:24:43.728: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 10 07:24:43.728: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5511, will wait for the garbage collector to delete the pods
Jan 10 07:24:43.799: INFO: Deleting DaemonSet.extensions daemon-set took: 6.041927ms
Jan 10 07:24:43.900: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.022571ms
Jan 10 07:24:46.608: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:24:46.608: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 10 07:24:46.611: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26205"},"items":null}

Jan 10 07:24:46.613: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26205"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 10 07:24:46.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5511" for this suite.

• [SLOW TEST:11.105 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":356,"completed":138,"skipped":2653,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:24:46.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Jan 10 07:24:46.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8560 create -f -'
Jan 10 07:24:47.820: INFO: stderr: ""
Jan 10 07:24:47.820: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 10 07:24:47.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 10 07:24:47.928: INFO: stderr: ""
Jan 10 07:24:47.928: INFO: stdout: "update-demo-nautilus-774ps update-demo-nautilus-g2g6h "
Jan 10 07:24:47.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8560 get pods update-demo-nautilus-774ps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 10 07:24:48.031: INFO: stderr: ""
Jan 10 07:24:48.031: INFO: stdout: ""
Jan 10 07:24:48.031: INFO: update-demo-nautilus-774ps is created but not running
Jan 10 07:24:53.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 10 07:24:53.130: INFO: stderr: ""
Jan 10 07:24:53.130: INFO: stdout: "update-demo-nautilus-774ps update-demo-nautilus-g2g6h "
Jan 10 07:24:53.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8560 get pods update-demo-nautilus-774ps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 10 07:24:53.219: INFO: stderr: ""
Jan 10 07:24:53.219: INFO: stdout: ""
Jan 10 07:24:53.219: INFO: update-demo-nautilus-774ps is created but not running
Jan 10 07:24:58.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 10 07:24:58.364: INFO: stderr: ""
Jan 10 07:24:58.365: INFO: stdout: "update-demo-nautilus-774ps update-demo-nautilus-g2g6h "
Jan 10 07:24:58.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8560 get pods update-demo-nautilus-774ps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 10 07:24:58.459: INFO: stderr: ""
Jan 10 07:24:58.459: INFO: stdout: "true"
Jan 10 07:24:58.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8560 get pods update-demo-nautilus-774ps -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 10 07:24:58.534: INFO: stderr: ""
Jan 10 07:24:58.534: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 10 07:24:58.534: INFO: validating pod update-demo-nautilus-774ps
Jan 10 07:24:58.540: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 10 07:24:58.540: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 10 07:24:58.540: INFO: update-demo-nautilus-774ps is verified up and running
Jan 10 07:24:58.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8560 get pods update-demo-nautilus-g2g6h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 10 07:24:58.610: INFO: stderr: ""
Jan 10 07:24:58.610: INFO: stdout: "true"
Jan 10 07:24:58.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8560 get pods update-demo-nautilus-g2g6h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 10 07:24:58.684: INFO: stderr: ""
Jan 10 07:24:58.685: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 10 07:24:58.685: INFO: validating pod update-demo-nautilus-g2g6h
Jan 10 07:24:58.693: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 10 07:24:58.693: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 10 07:24:58.693: INFO: update-demo-nautilus-g2g6h is verified up and running
STEP: using delete to clean up resources
Jan 10 07:24:58.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8560 delete --grace-period=0 --force -f -'
Jan 10 07:24:58.773: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 10 07:24:58.773: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 10 07:24:58.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8560 get rc,svc -l name=update-demo --no-headers'
Jan 10 07:24:58.870: INFO: stderr: "No resources found in kubectl-8560 namespace.\n"
Jan 10 07:24:58.870: INFO: stdout: ""
Jan 10 07:24:58.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8560 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 10 07:24:58.966: INFO: stderr: ""
Jan 10 07:24:58.966: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 07:24:58.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8560" for this suite.

• [SLOW TEST:12.337 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should create and stop a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":356,"completed":139,"skipped":2667,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:24:58.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 10 07:24:59.006: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 10 07:24:59.012: INFO: Waiting for terminating namespaces to be deleted...
Jan 10 07:24:59.015: INFO: 
Logging pods the apiserver thinks is on node kk-instance-r65pm before test
Jan 10 07:24:59.022: INFO: calico-kube-controllers-6799f5f4b4-g6h9q from kube-system started at 2023-01-10 06:28:45 +0000 UTC (1 container statuses recorded)
Jan 10 07:24:59.022: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 10 07:24:59.022: INFO: calico-node-khhj9 from kube-system started at 2023-01-10 06:28:32 +0000 UTC (1 container statuses recorded)
Jan 10 07:24:59.022: INFO: 	Container calico-node ready: true, restart count 0
Jan 10 07:24:59.022: INFO: coredns-6d4b75cb6d-4wczh from kube-system started at 2023-01-10 06:28:45 +0000 UTC (1 container statuses recorded)
Jan 10 07:24:59.022: INFO: 	Container coredns ready: true, restart count 0
Jan 10 07:24:59.022: INFO: coredns-6d4b75cb6d-f7d8c from kube-system started at 2023-01-10 06:28:45 +0000 UTC (1 container statuses recorded)
Jan 10 07:24:59.022: INFO: 	Container coredns ready: true, restart count 0
Jan 10 07:24:59.022: INFO: kube-proxy-2v5cp from kube-system started at 2023-01-10 06:04:35 +0000 UTC (1 container statuses recorded)
Jan 10 07:24:59.022: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 10 07:24:59.022: INFO: update-demo-nautilus-g2g6h from kubectl-8560 started at 2023-01-10 07:24:47 +0000 UTC (1 container statuses recorded)
Jan 10 07:24:59.022: INFO: 	Container update-demo ready: true, restart count 0
Jan 10 07:24:59.022: INFO: sonobuoy-systemd-logs-daemon-set-5167b5d313ce4ad1-6zjp9 from sonobuoy started at 2023-01-10 06:56:07 +0000 UTC (2 container statuses recorded)
Jan 10 07:24:59.022: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 10 07:24:59.022: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 10 07:24:59.022: INFO: 
Logging pods the apiserver thinks is on node kk-instance-vgmmg before test
Jan 10 07:24:59.029: INFO: calico-node-wf8tr from kube-system started at 2023-01-10 06:42:17 +0000 UTC (1 container statuses recorded)
Jan 10 07:24:59.029: INFO: 	Container calico-node ready: true, restart count 0
Jan 10 07:24:59.029: INFO: kube-proxy-vrc4r from kube-system started at 2023-01-10 06:42:17 +0000 UTC (1 container statuses recorded)
Jan 10 07:24:59.029: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 10 07:24:59.030: INFO: update-demo-nautilus-774ps from kubectl-8560 started at 2023-01-10 07:24:47 +0000 UTC (1 container statuses recorded)
Jan 10 07:24:59.030: INFO: 	Container update-demo ready: true, restart count 0
Jan 10 07:24:59.030: INFO: sonobuoy from sonobuoy started at 2023-01-10 06:56:06 +0000 UTC (1 container statuses recorded)
Jan 10 07:24:59.030: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 10 07:24:59.030: INFO: sonobuoy-e2e-job-86252adb0f534a01 from sonobuoy started at 2023-01-10 06:56:07 +0000 UTC (2 container statuses recorded)
Jan 10 07:24:59.030: INFO: 	Container e2e ready: true, restart count 0
Jan 10 07:24:59.030: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 10 07:24:59.030: INFO: sonobuoy-systemd-logs-daemon-set-5167b5d313ce4ad1-smm8d from sonobuoy started at 2023-01-10 06:56:07 +0000 UTC (2 container statuses recorded)
Jan 10 07:24:59.030: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 10 07:24:59.030: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-245cc3ec-9d26-4fdf-9a11-18180aebd925 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-245cc3ec-9d26-4fdf-9a11-18180aebd925 off the node kk-instance-r65pm
STEP: verifying the node doesn't have the label kubernetes.io/e2e-245cc3ec-9d26-4fdf-9a11-18180aebd925
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 10 07:25:05.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4773" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:6.180 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":356,"completed":140,"skipped":2672,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:25:05.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 10 07:25:05.212: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:25:07.224: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 10 07:25:07.237: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:25:09.249: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 10 07:25:09.287: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 10 07:25:09.296: INFO: Pod pod-with-poststart-http-hook still exists
Jan 10 07:25:11.296: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 10 07:25:11.308: INFO: Pod pod-with-poststart-http-hook still exists
Jan 10 07:25:13.296: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 10 07:25:13.305: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 10 07:25:13.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-139" for this suite.

• [SLOW TEST:8.161 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":356,"completed":141,"skipped":2691,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:25:13.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-downwardapi-78xc
STEP: Creating a pod to test atomic-volume-subpath
Jan 10 07:25:13.370: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-78xc" in namespace "subpath-9570" to be "Succeeded or Failed"
Jan 10 07:25:13.376: INFO: Pod "pod-subpath-test-downwardapi-78xc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.134007ms
Jan 10 07:25:15.383: INFO: Pod "pod-subpath-test-downwardapi-78xc": Phase="Running", Reason="", readiness=true. Elapsed: 2.012212262s
Jan 10 07:25:17.389: INFO: Pod "pod-subpath-test-downwardapi-78xc": Phase="Running", Reason="", readiness=true. Elapsed: 4.018108469s
Jan 10 07:25:19.401: INFO: Pod "pod-subpath-test-downwardapi-78xc": Phase="Running", Reason="", readiness=true. Elapsed: 6.030806867s
Jan 10 07:25:21.411: INFO: Pod "pod-subpath-test-downwardapi-78xc": Phase="Running", Reason="", readiness=true. Elapsed: 8.040943191s
Jan 10 07:25:23.422: INFO: Pod "pod-subpath-test-downwardapi-78xc": Phase="Running", Reason="", readiness=true. Elapsed: 10.051692792s
Jan 10 07:25:25.429: INFO: Pod "pod-subpath-test-downwardapi-78xc": Phase="Running", Reason="", readiness=true. Elapsed: 12.058124628s
Jan 10 07:25:27.442: INFO: Pod "pod-subpath-test-downwardapi-78xc": Phase="Running", Reason="", readiness=true. Elapsed: 14.071492571s
Jan 10 07:25:29.458: INFO: Pod "pod-subpath-test-downwardapi-78xc": Phase="Running", Reason="", readiness=true. Elapsed: 16.087195443s
Jan 10 07:25:31.469: INFO: Pod "pod-subpath-test-downwardapi-78xc": Phase="Running", Reason="", readiness=true. Elapsed: 18.098597592s
Jan 10 07:25:33.482: INFO: Pod "pod-subpath-test-downwardapi-78xc": Phase="Running", Reason="", readiness=true. Elapsed: 20.111576415s
Jan 10 07:25:35.489: INFO: Pod "pod-subpath-test-downwardapi-78xc": Phase="Running", Reason="", readiness=false. Elapsed: 22.118427113s
Jan 10 07:25:37.500: INFO: Pod "pod-subpath-test-downwardapi-78xc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.129569995s
STEP: Saw pod success
Jan 10 07:25:37.500: INFO: Pod "pod-subpath-test-downwardapi-78xc" satisfied condition "Succeeded or Failed"
Jan 10 07:25:37.503: INFO: Trying to get logs from node kk-instance-r65pm pod pod-subpath-test-downwardapi-78xc container test-container-subpath-downwardapi-78xc: <nil>
STEP: delete the pod
Jan 10 07:25:37.525: INFO: Waiting for pod pod-subpath-test-downwardapi-78xc to disappear
Jan 10 07:25:37.530: INFO: Pod pod-subpath-test-downwardapi-78xc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-78xc
Jan 10 07:25:37.530: INFO: Deleting pod "pod-subpath-test-downwardapi-78xc" in namespace "subpath-9570"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 10 07:25:37.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9570" for this suite.

• [SLOW TEST:24.224 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","total":356,"completed":142,"skipped":2768,"failed":0}
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:25:37.546: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 10 07:25:37.590: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:25:39.598: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 10 07:25:39.610: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:25:41.619: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 10 07:25:41.633: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 10 07:25:41.636: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 10 07:25:43.636: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 10 07:25:43.643: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 10 07:25:45.636: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 10 07:25:45.644: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 10 07:25:45.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8970" for this suite.

• [SLOW TEST:8.116 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":356,"completed":143,"skipped":2771,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:25:45.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7474
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating stateful set ss in namespace statefulset-7474
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7474
Jan 10 07:25:45.700: INFO: Found 0 stateful pods, waiting for 1
Jan 10 07:25:55.708: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 10 07:25:55.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-7474 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 10 07:25:55.882: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 10 07:25:55.882: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 10 07:25:55.882: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 10 07:25:55.886: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 10 07:26:05.902: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 10 07:26:05.902: INFO: Waiting for statefulset status.replicas updated to 0
Jan 10 07:26:05.920: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Jan 10 07:26:05.920: INFO: ss-0  kk-instance-r65pm  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:25:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:25:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:25:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:25:45 +0000 UTC  }]
Jan 10 07:26:05.920: INFO: 
Jan 10 07:26:05.920: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 10 07:26:06.928: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994501748s
Jan 10 07:26:07.937: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985962418s
Jan 10 07:26:08.945: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977199482s
Jan 10 07:26:09.952: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969239981s
Jan 10 07:26:10.959: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962974345s
Jan 10 07:26:11.967: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.955619122s
Jan 10 07:26:12.975: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.94740774s
Jan 10 07:26:13.981: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.939165048s
Jan 10 07:26:14.989: INFO: Verifying statefulset ss doesn't scale past 3 for another 933.849996ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7474
Jan 10 07:26:15.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-7474 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 10 07:26:16.140: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 10 07:26:16.140: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 10 07:26:16.140: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 10 07:26:16.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-7474 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 10 07:26:16.315: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 10 07:26:16.315: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 10 07:26:16.315: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 10 07:26:16.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-7474 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 10 07:26:16.459: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 10 07:26:16.459: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 10 07:26:16.459: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 10 07:26:16.463: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 10 07:26:26.473: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 10 07:26:26.473: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 10 07:26:26.473: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 10 07:26:26.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-7474 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 10 07:26:26.622: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 10 07:26:26.622: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 10 07:26:26.622: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 10 07:26:26.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-7474 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 10 07:26:26.762: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 10 07:26:26.762: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 10 07:26:26.762: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 10 07:26:26.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-7474 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 10 07:26:26.920: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 10 07:26:26.920: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 10 07:26:26.920: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 10 07:26:26.920: INFO: Waiting for statefulset status.replicas updated to 0
Jan 10 07:26:26.923: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 10 07:26:36.936: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 10 07:26:36.936: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 10 07:26:36.936: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 10 07:26:36.951: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Jan 10 07:26:36.952: INFO: ss-0  kk-instance-r65pm  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:25:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:25:45 +0000 UTC  }]
Jan 10 07:26:36.952: INFO: ss-1  kk-instance-vgmmg  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:05 +0000 UTC  }]
Jan 10 07:26:36.952: INFO: ss-2  kk-instance-r65pm  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:05 +0000 UTC  }]
Jan 10 07:26:36.952: INFO: 
Jan 10 07:26:36.952: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 10 07:26:37.959: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Jan 10 07:26:37.959: INFO: ss-0  kk-instance-r65pm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:25:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:25:45 +0000 UTC  }]
Jan 10 07:26:37.959: INFO: ss-2  kk-instance-r65pm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:26:05 +0000 UTC  }]
Jan 10 07:26:37.959: INFO: 
Jan 10 07:26:37.959: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 10 07:26:38.967: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.98436674s
Jan 10 07:26:39.973: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.975686412s
Jan 10 07:26:40.980: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.970477702s
Jan 10 07:26:41.988: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.963269392s
Jan 10 07:26:42.995: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.95585329s
Jan 10 07:26:44.001: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.948652351s
Jan 10 07:26:45.008: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.941460564s
Jan 10 07:26:46.016: INFO: Verifying statefulset ss doesn't scale past 0 for another 935.855737ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7474
Jan 10 07:26:47.022: INFO: Scaling statefulset ss to 0
Jan 10 07:26:47.031: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 10 07:26:47.033: INFO: Deleting all statefulset in ns statefulset-7474
Jan 10 07:26:47.036: INFO: Scaling statefulset ss to 0
Jan 10 07:26:47.043: INFO: Waiting for statefulset status.replicas updated to 0
Jan 10 07:26:47.045: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 10 07:26:47.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7474" for this suite.

• [SLOW TEST:61.412 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":356,"completed":144,"skipped":2776,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:26:47.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 10 07:26:47.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3905" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":145,"skipped":2820,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:26:47.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 07:26:47.573: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 07:26:50.601: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:26:50.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5548-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:26:53.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5174" for this suite.
STEP: Destroying namespace "webhook-5174-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.810 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":356,"completed":146,"skipped":2823,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:26:53.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7726 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7726;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7726 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7726;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7726.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7726.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7726.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7726.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7726.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7726.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7726.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7726.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7726.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7726.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7726.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7726.svc;check="$$(dig +notcp +noall +answer +search 38.45.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.45.38_udp@PTR;check="$$(dig +tcp +noall +answer +search 38.45.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.45.38_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7726 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7726;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7726 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7726;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7726.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7726.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7726.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7726.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7726.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7726.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7726.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7726.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7726.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7726.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7726.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7726.svc;check="$$(dig +notcp +noall +answer +search 38.45.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.45.38_udp@PTR;check="$$(dig +tcp +noall +answer +search 38.45.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.45.38_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 10 07:27:06.076: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.082: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.087: INFO: Unable to read wheezy_udp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.091: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.095: INFO: Unable to read wheezy_udp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.101: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.107: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.118: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.138: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.142: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.147: INFO: Unable to read jessie_udp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.151: INFO: Unable to read jessie_tcp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.155: INFO: Unable to read jessie_udp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.158: INFO: Unable to read jessie_tcp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.161: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.165: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:06.181: INFO: Lookups using dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7726 wheezy_tcp@dns-test-service.dns-7726 wheezy_udp@dns-test-service.dns-7726.svc wheezy_tcp@dns-test-service.dns-7726.svc wheezy_udp@_http._tcp.dns-test-service.dns-7726.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7726.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7726 jessie_tcp@dns-test-service.dns-7726 jessie_udp@dns-test-service.dns-7726.svc jessie_tcp@dns-test-service.dns-7726.svc jessie_udp@_http._tcp.dns-test-service.dns-7726.svc jessie_tcp@_http._tcp.dns-test-service.dns-7726.svc]

Jan 10 07:27:11.187: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.191: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.195: INFO: Unable to read wheezy_udp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.198: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.201: INFO: Unable to read wheezy_udp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.205: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.209: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.212: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.230: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.234: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.237: INFO: Unable to read jessie_udp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.241: INFO: Unable to read jessie_tcp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.244: INFO: Unable to read jessie_udp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.247: INFO: Unable to read jessie_tcp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.250: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.253: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:11.266: INFO: Lookups using dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7726 wheezy_tcp@dns-test-service.dns-7726 wheezy_udp@dns-test-service.dns-7726.svc wheezy_tcp@dns-test-service.dns-7726.svc wheezy_udp@_http._tcp.dns-test-service.dns-7726.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7726.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7726 jessie_tcp@dns-test-service.dns-7726 jessie_udp@dns-test-service.dns-7726.svc jessie_tcp@dns-test-service.dns-7726.svc jessie_udp@_http._tcp.dns-test-service.dns-7726.svc jessie_tcp@_http._tcp.dns-test-service.dns-7726.svc]

Jan 10 07:27:16.187: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.189: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.192: INFO: Unable to read wheezy_udp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.195: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.198: INFO: Unable to read wheezy_udp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.202: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.205: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.207: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.219: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.222: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.224: INFO: Unable to read jessie_udp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.227: INFO: Unable to read jessie_tcp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.230: INFO: Unable to read jessie_udp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.232: INFO: Unable to read jessie_tcp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.235: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.238: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:16.252: INFO: Lookups using dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7726 wheezy_tcp@dns-test-service.dns-7726 wheezy_udp@dns-test-service.dns-7726.svc wheezy_tcp@dns-test-service.dns-7726.svc wheezy_udp@_http._tcp.dns-test-service.dns-7726.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7726.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7726 jessie_tcp@dns-test-service.dns-7726 jessie_udp@dns-test-service.dns-7726.svc jessie_tcp@dns-test-service.dns-7726.svc jessie_udp@_http._tcp.dns-test-service.dns-7726.svc jessie_tcp@_http._tcp.dns-test-service.dns-7726.svc]

Jan 10 07:27:21.187: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.190: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.193: INFO: Unable to read wheezy_udp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.195: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.197: INFO: Unable to read wheezy_udp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.200: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.203: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.206: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.221: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.224: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.227: INFO: Unable to read jessie_udp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.229: INFO: Unable to read jessie_tcp@dns-test-service.dns-7726 from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.232: INFO: Unable to read jessie_udp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.235: INFO: Unable to read jessie_tcp@dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.238: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.241: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7726.svc from pod dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0: the server could not find the requested resource (get pods dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0)
Jan 10 07:27:21.251: INFO: Lookups using dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7726 wheezy_tcp@dns-test-service.dns-7726 wheezy_udp@dns-test-service.dns-7726.svc wheezy_tcp@dns-test-service.dns-7726.svc wheezy_udp@_http._tcp.dns-test-service.dns-7726.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7726.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7726 jessie_tcp@dns-test-service.dns-7726 jessie_udp@dns-test-service.dns-7726.svc jessie_tcp@dns-test-service.dns-7726.svc jessie_udp@_http._tcp.dns-test-service.dns-7726.svc jessie_tcp@_http._tcp.dns-test-service.dns-7726.svc]

Jan 10 07:27:26.259: INFO: DNS probes using dns-7726/dns-test-02f205df-fe5d-4771-af4b-793c1c7efcb0 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 10 07:27:26.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7726" for this suite.

• [SLOW TEST:32.419 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":356,"completed":147,"skipped":2828,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:27:26.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:27:26.435: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"4677e648-eab4-4ddb-a1e9-5880baa4d5fc", Controller:(*bool)(0xc0036fb756), BlockOwnerDeletion:(*bool)(0xc0036fb757)}}
Jan 10 07:27:26.451: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"db3721c9-b11e-47ef-8987-7180dab2aa90", Controller:(*bool)(0xc000b3f1a6), BlockOwnerDeletion:(*bool)(0xc000b3f1a7)}}
Jan 10 07:27:26.460: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a26ea468-db1b-438e-a2a4-4f8a2cdae21c", Controller:(*bool)(0xc000b3f92e), BlockOwnerDeletion:(*bool)(0xc000b3f92f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 10 07:27:31.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9499" for this suite.

• [SLOW TEST:5.126 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":356,"completed":148,"skipped":2831,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:27:31.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:27:31.542: INFO: Create a RollingUpdate DaemonSet
Jan 10 07:27:31.549: INFO: Check that daemon pods launch on every node of the cluster
Jan 10 07:27:31.556: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:27:31.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:27:31.561: INFO: Node kk-instance-r65pm is running 0 daemon pod, expected 1
Jan 10 07:27:32.574: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:27:32.577: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:27:32.577: INFO: Node kk-instance-r65pm is running 0 daemon pod, expected 1
Jan 10 07:27:33.568: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:27:33.571: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 10 07:27:33.571: INFO: Node kk-instance-vgmmg is running 0 daemon pod, expected 1
Jan 10 07:27:34.568: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:27:34.573: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 10 07:27:34.573: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jan 10 07:27:34.573: INFO: Update the DaemonSet to trigger a rollout
Jan 10 07:27:34.584: INFO: Updating DaemonSet daemon-set
Jan 10 07:27:37.602: INFO: Roll back the DaemonSet before rollout is complete
Jan 10 07:27:37.612: INFO: Updating DaemonSet daemon-set
Jan 10 07:27:37.612: INFO: Make sure DaemonSet rollback is complete
Jan 10 07:27:37.618: INFO: Wrong image for pod: daemon-set-4l9pl. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 10 07:27:37.618: INFO: Pod daemon-set-4l9pl is not available
Jan 10 07:27:37.628: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:27:38.638: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:27:39.639: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:27:40.642: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:27:41.639: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 07:27:42.635: INFO: Pod daemon-set-98xk7 is not available
Jan 10 07:27:42.646: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5438, will wait for the garbage collector to delete the pods
Jan 10 07:27:42.716: INFO: Deleting DaemonSet.extensions daemon-set took: 8.910379ms
Jan 10 07:27:42.816: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.419547ms
Jan 10 07:27:45.823: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 07:27:45.823: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 10 07:27:45.826: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27653"},"items":null}

Jan 10 07:27:45.829: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27653"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 10 07:27:45.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5438" for this suite.

• [SLOW TEST:14.354 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":356,"completed":149,"skipped":2839,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:27:45.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Jan 10 07:27:45.885: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 10 07:27:50.899: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Jan 10 07:27:50.903: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Jan 10 07:27:50.914: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Jan 10 07:27:50.916: INFO: Observed &ReplicaSet event: ADDED
Jan 10 07:27:50.916: INFO: Observed &ReplicaSet event: MODIFIED
Jan 10 07:27:50.916: INFO: Observed &ReplicaSet event: MODIFIED
Jan 10 07:27:50.916: INFO: Observed &ReplicaSet event: MODIFIED
Jan 10 07:27:50.916: INFO: Found replicaset test-rs in namespace replicaset-9486 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 10 07:27:50.916: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Jan 10 07:27:50.916: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 10 07:27:50.925: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Jan 10 07:27:50.928: INFO: Observed &ReplicaSet event: ADDED
Jan 10 07:27:50.929: INFO: Observed &ReplicaSet event: MODIFIED
Jan 10 07:27:50.929: INFO: Observed &ReplicaSet event: MODIFIED
Jan 10 07:27:50.930: INFO: Observed &ReplicaSet event: MODIFIED
Jan 10 07:27:50.930: INFO: Observed replicaset test-rs in namespace replicaset-9486 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 10 07:27:50.930: INFO: Observed &ReplicaSet event: MODIFIED
Jan 10 07:27:50.930: INFO: Found replicaset test-rs in namespace replicaset-9486 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 10 07:27:50.930: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 10 07:27:50.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9486" for this suite.

• [SLOW TEST:5.092 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":356,"completed":150,"skipped":2846,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:27:50.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:27:50.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:27:51.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6649" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":356,"completed":151,"skipped":2904,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:27:51.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:27:51.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Creating first CR 
Jan 10 07:27:54.223: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-10T07:27:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-10T07:27:54Z]] name:name1 resourceVersion:27761 uid:bd066430-e7d4-4fdc-a941-f99afba49375] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jan 10 07:28:04.252: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-10T07:28:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-10T07:28:04Z]] name:name2 resourceVersion:27816 uid:a2b50e4f-15f1-4aa5-8234-2c12a5d7cfa4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jan 10 07:28:14.274: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-10T07:27:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-10T07:28:14Z]] name:name1 resourceVersion:27851 uid:bd066430-e7d4-4fdc-a941-f99afba49375] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jan 10 07:28:24.295: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-10T07:28:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-10T07:28:24Z]] name:name2 resourceVersion:27886 uid:a2b50e4f-15f1-4aa5-8234-2c12a5d7cfa4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jan 10 07:28:34.316: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-10T07:27:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-10T07:28:14Z]] name:name1 resourceVersion:27921 uid:bd066430-e7d4-4fdc-a941-f99afba49375] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jan 10 07:28:44.339: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-10T07:28:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-10T07:28:24Z]] name:name2 resourceVersion:27956 uid:a2b50e4f-15f1-4aa5-8234-2c12a5d7cfa4] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:28:54.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6208" for this suite.

• [SLOW TEST:63.351 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":356,"completed":152,"skipped":2907,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:28:54.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-398eb364-b72f-4f34-aedd-f1ffe277a53b
STEP: Creating a pod to test consume secrets
Jan 10 07:28:54.939: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ed16b7d8-79c9-44b4-a334-9b6e9222899e" in namespace "projected-5372" to be "Succeeded or Failed"
Jan 10 07:28:54.945: INFO: Pod "pod-projected-secrets-ed16b7d8-79c9-44b4-a334-9b6e9222899e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.951486ms
Jan 10 07:28:56.955: INFO: Pod "pod-projected-secrets-ed16b7d8-79c9-44b4-a334-9b6e9222899e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016161516s
Jan 10 07:28:58.964: INFO: Pod "pod-projected-secrets-ed16b7d8-79c9-44b4-a334-9b6e9222899e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024909891s
STEP: Saw pod success
Jan 10 07:28:58.964: INFO: Pod "pod-projected-secrets-ed16b7d8-79c9-44b4-a334-9b6e9222899e" satisfied condition "Succeeded or Failed"
Jan 10 07:28:58.966: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-projected-secrets-ed16b7d8-79c9-44b4-a334-9b6e9222899e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 10 07:28:58.996: INFO: Waiting for pod pod-projected-secrets-ed16b7d8-79c9-44b4-a334-9b6e9222899e to disappear
Jan 10 07:28:58.998: INFO: Pod pod-projected-secrets-ed16b7d8-79c9-44b4-a334-9b6e9222899e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 10 07:28:58.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5372" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":153,"skipped":2927,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:28:59.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-ee25398d-e2f5-40c3-ba0b-f6faaae16a6d
STEP: Creating a pod to test consume configMaps
Jan 10 07:28:59.038: INFO: Waiting up to 5m0s for pod "pod-configmaps-5fc9a10d-c1d3-4c2b-bca7-a51aa0f52867" in namespace "configmap-9595" to be "Succeeded or Failed"
Jan 10 07:28:59.043: INFO: Pod "pod-configmaps-5fc9a10d-c1d3-4c2b-bca7-a51aa0f52867": Phase="Pending", Reason="", readiness=false. Elapsed: 4.832524ms
Jan 10 07:29:01.050: INFO: Pod "pod-configmaps-5fc9a10d-c1d3-4c2b-bca7-a51aa0f52867": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011922514s
Jan 10 07:29:03.061: INFO: Pod "pod-configmaps-5fc9a10d-c1d3-4c2b-bca7-a51aa0f52867": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022786069s
STEP: Saw pod success
Jan 10 07:29:03.061: INFO: Pod "pod-configmaps-5fc9a10d-c1d3-4c2b-bca7-a51aa0f52867" satisfied condition "Succeeded or Failed"
Jan 10 07:29:03.064: INFO: Trying to get logs from node kk-instance-r65pm pod pod-configmaps-5fc9a10d-c1d3-4c2b-bca7-a51aa0f52867 container agnhost-container: <nil>
STEP: delete the pod
Jan 10 07:29:03.093: INFO: Waiting for pod pod-configmaps-5fc9a10d-c1d3-4c2b-bca7-a51aa0f52867 to disappear
Jan 10 07:29:03.097: INFO: Pod pod-configmaps-5fc9a10d-c1d3-4c2b-bca7-a51aa0f52867 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 10 07:29:03.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9595" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":154,"skipped":2948,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:29:03.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-9142ae4f-0cc7-4c91-9c5c-a27cf27b0a6f
STEP: Creating a pod to test consume secrets
Jan 10 07:29:03.151: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4c8c79c8-3bb4-4ab3-92a2-319eda9ed53b" in namespace "projected-8591" to be "Succeeded or Failed"
Jan 10 07:29:03.154: INFO: Pod "pod-projected-secrets-4c8c79c8-3bb4-4ab3-92a2-319eda9ed53b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.726387ms
Jan 10 07:29:05.160: INFO: Pod "pod-projected-secrets-4c8c79c8-3bb4-4ab3-92a2-319eda9ed53b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008569425s
Jan 10 07:29:07.167: INFO: Pod "pod-projected-secrets-4c8c79c8-3bb4-4ab3-92a2-319eda9ed53b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015874674s
STEP: Saw pod success
Jan 10 07:29:07.167: INFO: Pod "pod-projected-secrets-4c8c79c8-3bb4-4ab3-92a2-319eda9ed53b" satisfied condition "Succeeded or Failed"
Jan 10 07:29:07.170: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-projected-secrets-4c8c79c8-3bb4-4ab3-92a2-319eda9ed53b container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 10 07:29:07.189: INFO: Waiting for pod pod-projected-secrets-4c8c79c8-3bb4-4ab3-92a2-319eda9ed53b to disappear
Jan 10 07:29:07.192: INFO: Pod pod-projected-secrets-4c8c79c8-3bb4-4ab3-92a2-319eda9ed53b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 10 07:29:07.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8591" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":155,"skipped":2962,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:29:07.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5530
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Jan 10 07:29:07.268: INFO: Found 0 stateful pods, waiting for 3
Jan 10 07:29:17.281: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 10 07:29:17.281: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 10 07:29:17.281: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jan 10 07:29:17.309: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 10 07:29:27.366: INFO: Updating stateful set ss2
Jan 10 07:29:27.379: INFO: Waiting for Pod statefulset-5530/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Jan 10 07:29:37.469: INFO: Found 2 stateful pods, waiting for 3
Jan 10 07:29:47.488: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 10 07:29:47.488: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 10 07:29:47.488: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 10 07:29:47.522: INFO: Updating stateful set ss2
Jan 10 07:29:47.528: INFO: Waiting for Pod statefulset-5530/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Jan 10 07:29:57.562: INFO: Updating stateful set ss2
Jan 10 07:29:57.572: INFO: Waiting for StatefulSet statefulset-5530/ss2 to complete update
Jan 10 07:29:57.572: INFO: Waiting for Pod statefulset-5530/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 10 07:30:07.591: INFO: Deleting all statefulset in ns statefulset-5530
Jan 10 07:30:07.595: INFO: Scaling statefulset ss2 to 0
Jan 10 07:30:17.633: INFO: Waiting for statefulset status.replicas updated to 0
Jan 10 07:30:17.635: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 10 07:30:17.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5530" for this suite.

• [SLOW TEST:70.482 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":356,"completed":156,"skipped":2966,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:30:17.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-6901113b-0e1a-48c2-a752-0f23a60f4a20
STEP: Creating the pod
Jan 10 07:30:17.739: INFO: The status of Pod pod-configmaps-f0f6dcd8-54f4-4dce-a8a7-a85ed26324dc is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:30:19.751: INFO: The status of Pod pod-configmaps-f0f6dcd8-54f4-4dce-a8a7-a85ed26324dc is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-6901113b-0e1a-48c2-a752-0f23a60f4a20
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 10 07:30:21.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1115" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":157,"skipped":2979,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:30:21.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 10 07:30:21.858: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 10 07:30:21.862: INFO: starting watch
STEP: patching
STEP: updating
Jan 10 07:30:21.878: INFO: waiting for watch events with expected annotations
Jan 10 07:30:21.878: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:188
Jan 10 07:30:21.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5532" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":356,"completed":158,"skipped":3040,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:30:21.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 10 07:30:22.306: INFO: Pod name wrapped-volume-race-8071568f-62ec-4056-9c29-1a5b7fca96cf: Found 3 pods out of 5
Jan 10 07:30:27.319: INFO: Pod name wrapped-volume-race-8071568f-62ec-4056-9c29-1a5b7fca96cf: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8071568f-62ec-4056-9c29-1a5b7fca96cf in namespace emptydir-wrapper-89, will wait for the garbage collector to delete the pods
Jan 10 07:30:37.410: INFO: Deleting ReplicationController wrapped-volume-race-8071568f-62ec-4056-9c29-1a5b7fca96cf took: 10.128629ms
Jan 10 07:30:37.611: INFO: Terminating ReplicationController wrapped-volume-race-8071568f-62ec-4056-9c29-1a5b7fca96cf pods took: 200.967497ms
STEP: Creating RC which spawns configmap-volume pods
Jan 10 07:30:41.865: INFO: Pod name wrapped-volume-race-3cb7e42f-b436-48fc-8ca5-47d5e1028b42: Found 0 pods out of 5
Jan 10 07:30:46.881: INFO: Pod name wrapped-volume-race-3cb7e42f-b436-48fc-8ca5-47d5e1028b42: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3cb7e42f-b436-48fc-8ca5-47d5e1028b42 in namespace emptydir-wrapper-89, will wait for the garbage collector to delete the pods
Jan 10 07:30:56.968: INFO: Deleting ReplicationController wrapped-volume-race-3cb7e42f-b436-48fc-8ca5-47d5e1028b42 took: 7.168715ms
Jan 10 07:30:57.069: INFO: Terminating ReplicationController wrapped-volume-race-3cb7e42f-b436-48fc-8ca5-47d5e1028b42 pods took: 100.858304ms
STEP: Creating RC which spawns configmap-volume pods
Jan 10 07:31:01.999: INFO: Pod name wrapped-volume-race-3b57ff0f-0788-42fd-a5ab-200a1d732826: Found 0 pods out of 5
Jan 10 07:31:07.014: INFO: Pod name wrapped-volume-race-3b57ff0f-0788-42fd-a5ab-200a1d732826: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3b57ff0f-0788-42fd-a5ab-200a1d732826 in namespace emptydir-wrapper-89, will wait for the garbage collector to delete the pods
Jan 10 07:31:17.098: INFO: Deleting ReplicationController wrapped-volume-race-3b57ff0f-0788-42fd-a5ab-200a1d732826 took: 6.383423ms
Jan 10 07:31:17.199: INFO: Terminating ReplicationController wrapped-volume-race-3b57ff0f-0788-42fd-a5ab-200a1d732826 pods took: 100.7895ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Jan 10 07:31:21.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-89" for this suite.

• [SLOW TEST:59.721 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":356,"completed":159,"skipped":3051,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:31:21.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jan 10 07:31:21.676: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5846  5039592c-0678-4276-8df4-2211ae7e8aef 29748 0 2023-01-10 07:31:21 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2023-01-10 07:31:21 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zl45q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zl45q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 07:31:21.681: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:31:23.690: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:31:25.688: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jan 10 07:31:25.688: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5846 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:31:25.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:31:25.689: INFO: ExecWithOptions: Clientset creation
Jan 10 07:31:25.689: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-5846/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod...
Jan 10 07:31:25.795: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5846 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:31:25.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:31:25.796: INFO: ExecWithOptions: Clientset creation
Jan 10 07:31:25.796: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-5846/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 10 07:31:25.884: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 10 07:31:25.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5846" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":356,"completed":160,"skipped":3060,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:31:25.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-6c220f6a-30b6-4532-95b9-5225226acc6e
STEP: Creating a pod to test consume configMaps
Jan 10 07:31:25.990: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-641e9cbd-9c9e-483c-8844-3244b8af6e4c" in namespace "projected-5170" to be "Succeeded or Failed"
Jan 10 07:31:25.993: INFO: Pod "pod-projected-configmaps-641e9cbd-9c9e-483c-8844-3244b8af6e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.834718ms
Jan 10 07:31:28.000: INFO: Pod "pod-projected-configmaps-641e9cbd-9c9e-483c-8844-3244b8af6e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009904752s
Jan 10 07:31:30.006: INFO: Pod "pod-projected-configmaps-641e9cbd-9c9e-483c-8844-3244b8af6e4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015539526s
STEP: Saw pod success
Jan 10 07:31:30.006: INFO: Pod "pod-projected-configmaps-641e9cbd-9c9e-483c-8844-3244b8af6e4c" satisfied condition "Succeeded or Failed"
Jan 10 07:31:30.009: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-projected-configmaps-641e9cbd-9c9e-483c-8844-3244b8af6e4c container agnhost-container: <nil>
STEP: delete the pod
Jan 10 07:31:30.034: INFO: Waiting for pod pod-projected-configmaps-641e9cbd-9c9e-483c-8844-3244b8af6e4c to disappear
Jan 10 07:31:30.037: INFO: Pod pod-projected-configmaps-641e9cbd-9c9e-483c-8844-3244b8af6e4c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 10 07:31:30.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5170" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":161,"skipped":3079,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:31:30.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jan 10 07:31:36.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-81" for this suite.
STEP: Destroying namespace "nsdeletetest-491" for this suite.
Jan 10 07:31:36.208: INFO: Namespace nsdeletetest-491 was already deleted
STEP: Destroying namespace "nsdeletetest-9226" for this suite.

• [SLOW TEST:6.166 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":356,"completed":162,"skipped":3105,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:31:36.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's command
Jan 10 07:31:36.257: INFO: Waiting up to 5m0s for pod "var-expansion-296339f5-8e7e-4fe5-9bef-657cb520d576" in namespace "var-expansion-4110" to be "Succeeded or Failed"
Jan 10 07:31:36.265: INFO: Pod "var-expansion-296339f5-8e7e-4fe5-9bef-657cb520d576": Phase="Pending", Reason="", readiness=false. Elapsed: 7.471498ms
Jan 10 07:31:38.275: INFO: Pod "var-expansion-296339f5-8e7e-4fe5-9bef-657cb520d576": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017709077s
Jan 10 07:31:40.280: INFO: Pod "var-expansion-296339f5-8e7e-4fe5-9bef-657cb520d576": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022722616s
STEP: Saw pod success
Jan 10 07:31:40.280: INFO: Pod "var-expansion-296339f5-8e7e-4fe5-9bef-657cb520d576" satisfied condition "Succeeded or Failed"
Jan 10 07:31:40.283: INFO: Trying to get logs from node kk-instance-vgmmg pod var-expansion-296339f5-8e7e-4fe5-9bef-657cb520d576 container dapi-container: <nil>
STEP: delete the pod
Jan 10 07:31:40.297: INFO: Waiting for pod var-expansion-296339f5-8e7e-4fe5-9bef-657cb520d576 to disappear
Jan 10 07:31:40.300: INFO: Pod var-expansion-296339f5-8e7e-4fe5-9bef-657cb520d576 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 10 07:31:40.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4110" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":356,"completed":163,"skipped":3122,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:31:40.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:31:40.346: INFO: The status of Pod server-envvars-12f276d2-7c56-4ab8-8e18-fffbe6c5b34d is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:31:42.358: INFO: The status of Pod server-envvars-12f276d2-7c56-4ab8-8e18-fffbe6c5b34d is Running (Ready = true)
Jan 10 07:31:42.386: INFO: Waiting up to 5m0s for pod "client-envvars-8e9efd55-5b18-4e22-ae16-40e4276b1901" in namespace "pods-4036" to be "Succeeded or Failed"
Jan 10 07:31:42.399: INFO: Pod "client-envvars-8e9efd55-5b18-4e22-ae16-40e4276b1901": Phase="Pending", Reason="", readiness=false. Elapsed: 13.012901ms
Jan 10 07:31:44.409: INFO: Pod "client-envvars-8e9efd55-5b18-4e22-ae16-40e4276b1901": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023448423s
Jan 10 07:31:46.417: INFO: Pod "client-envvars-8e9efd55-5b18-4e22-ae16-40e4276b1901": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03093905s
STEP: Saw pod success
Jan 10 07:31:46.417: INFO: Pod "client-envvars-8e9efd55-5b18-4e22-ae16-40e4276b1901" satisfied condition "Succeeded or Failed"
Jan 10 07:31:46.420: INFO: Trying to get logs from node kk-instance-vgmmg pod client-envvars-8e9efd55-5b18-4e22-ae16-40e4276b1901 container env3cont: <nil>
STEP: delete the pod
Jan 10 07:31:46.442: INFO: Waiting for pod client-envvars-8e9efd55-5b18-4e22-ae16-40e4276b1901 to disappear
Jan 10 07:31:46.445: INFO: Pod client-envvars-8e9efd55-5b18-4e22-ae16-40e4276b1901 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 10 07:31:46.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4036" for this suite.

• [SLOW TEST:6.147 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":356,"completed":164,"skipped":3136,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:31:46.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 10 07:31:46.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5881" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":356,"completed":165,"skipped":3157,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:31:46.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jan 10 07:31:46.576: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jan 10 07:31:46.597: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 10 07:31:46.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4819" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":356,"completed":166,"skipped":3175,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:31:46.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5161
Jan 10 07:31:46.658: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:31:48.672: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 10 07:31:48.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5161 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 10 07:31:48.846: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 10 07:31:48.846: INFO: stdout: "iptables"
Jan 10 07:31:48.846: INFO: proxyMode: iptables
Jan 10 07:31:48.857: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 10 07:31:48.859: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-5161
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5161
I0110 07:31:48.884146      21 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5161, replica count: 3
I0110 07:31:51.934955      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 10 07:31:51.950: INFO: Creating new exec pod
Jan 10 07:31:54.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5161 exec execpod-affinitybbrrc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 10 07:31:55.152: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-timeout 80\n+ echo hostName\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 10 07:31:55.152: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:31:55.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5161 exec execpod-affinitybbrrc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.8.170 80'
Jan 10 07:31:55.305: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.8.170 80\nConnection to 10.233.8.170 80 port [tcp/http] succeeded!\n"
Jan 10 07:31:55.305: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:31:55.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5161 exec execpod-affinitybbrrc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.4 30748'
Jan 10 07:31:55.443: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.4 30748\nConnection to 192.168.0.4 30748 port [tcp/*] succeeded!\n"
Jan 10 07:31:55.443: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:31:55.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5161 exec execpod-affinitybbrrc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.5 30748'
Jan 10 07:31:55.602: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.5 30748\nConnection to 192.168.0.5 30748 port [tcp/*] succeeded!\n"
Jan 10 07:31:55.602: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:31:55.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5161 exec execpod-affinitybbrrc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.4:30748/ ; done'
Jan 10 07:31:55.900: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n"
Jan 10 07:31:55.900: INFO: stdout: "\naffinity-nodeport-timeout-5wlqc\naffinity-nodeport-timeout-5wlqc\naffinity-nodeport-timeout-5wlqc\naffinity-nodeport-timeout-5wlqc\naffinity-nodeport-timeout-5wlqc\naffinity-nodeport-timeout-5wlqc\naffinity-nodeport-timeout-5wlqc\naffinity-nodeport-timeout-5wlqc\naffinity-nodeport-timeout-5wlqc\naffinity-nodeport-timeout-5wlqc\naffinity-nodeport-timeout-5wlqc\naffinity-nodeport-timeout-5wlqc\naffinity-nodeport-timeout-5wlqc\naffinity-nodeport-timeout-5wlqc\naffinity-nodeport-timeout-5wlqc\naffinity-nodeport-timeout-5wlqc"
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Received response from host: affinity-nodeport-timeout-5wlqc
Jan 10 07:31:55.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5161 exec execpod-affinitybbrrc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.4:30748/'
Jan 10 07:31:56.056: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n"
Jan 10 07:31:56.056: INFO: stdout: "affinity-nodeport-timeout-5wlqc"
Jan 10 07:32:16.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-5161 exec execpod-affinitybbrrc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.4:30748/'
Jan 10 07:32:16.236: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.4:30748/\n"
Jan 10 07:32:16.236: INFO: stdout: "affinity-nodeport-timeout-xxrrf"
Jan 10 07:32:16.236: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5161, will wait for the garbage collector to delete the pods
Jan 10 07:32:16.316: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.090394ms
Jan 10 07:32:16.417: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.036889ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 07:32:18.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5161" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:32.126 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":167,"skipped":3188,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:32:18.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
STEP: create deployment with httpd image
Jan 10 07:32:18.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-1647 create -f -'
Jan 10 07:32:20.008: INFO: stderr: ""
Jan 10 07:32:20.008: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jan 10 07:32:20.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-1647 diff -f -'
Jan 10 07:32:20.255: INFO: rc: 1
Jan 10 07:32:20.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-1647 delete -f -'
Jan 10 07:32:20.338: INFO: stderr: ""
Jan 10 07:32:20.338: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 07:32:20.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1647" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":356,"completed":168,"skipped":3205,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:32:20.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5477.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5477.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5477.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5477.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 10 07:32:22.434: INFO: DNS probes using dns-test-9db95059-bb8a-4f26-9cce-68a80b22537a succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5477.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5477.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5477.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5477.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 10 07:32:24.488: INFO: File wheezy_udp@dns-test-service-3.dns-5477.svc.cluster.local from pod  dns-5477/dns-test-dd02de1a-bfe7-4d33-b0be-a71563f9b640 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 10 07:32:24.491: INFO: Lookups using dns-5477/dns-test-dd02de1a-bfe7-4d33-b0be-a71563f9b640 failed for: [wheezy_udp@dns-test-service-3.dns-5477.svc.cluster.local]

Jan 10 07:32:29.498: INFO: File wheezy_udp@dns-test-service-3.dns-5477.svc.cluster.local from pod  dns-5477/dns-test-dd02de1a-bfe7-4d33-b0be-a71563f9b640 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 10 07:32:29.501: INFO: File jessie_udp@dns-test-service-3.dns-5477.svc.cluster.local from pod  dns-5477/dns-test-dd02de1a-bfe7-4d33-b0be-a71563f9b640 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 10 07:32:29.501: INFO: Lookups using dns-5477/dns-test-dd02de1a-bfe7-4d33-b0be-a71563f9b640 failed for: [wheezy_udp@dns-test-service-3.dns-5477.svc.cluster.local jessie_udp@dns-test-service-3.dns-5477.svc.cluster.local]

Jan 10 07:32:34.498: INFO: File wheezy_udp@dns-test-service-3.dns-5477.svc.cluster.local from pod  dns-5477/dns-test-dd02de1a-bfe7-4d33-b0be-a71563f9b640 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 10 07:32:34.503: INFO: Lookups using dns-5477/dns-test-dd02de1a-bfe7-4d33-b0be-a71563f9b640 failed for: [wheezy_udp@dns-test-service-3.dns-5477.svc.cluster.local]

Jan 10 07:32:39.497: INFO: File wheezy_udp@dns-test-service-3.dns-5477.svc.cluster.local from pod  dns-5477/dns-test-dd02de1a-bfe7-4d33-b0be-a71563f9b640 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 10 07:32:39.501: INFO: File jessie_udp@dns-test-service-3.dns-5477.svc.cluster.local from pod  dns-5477/dns-test-dd02de1a-bfe7-4d33-b0be-a71563f9b640 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 10 07:32:39.501: INFO: Lookups using dns-5477/dns-test-dd02de1a-bfe7-4d33-b0be-a71563f9b640 failed for: [wheezy_udp@dns-test-service-3.dns-5477.svc.cluster.local jessie_udp@dns-test-service-3.dns-5477.svc.cluster.local]

Jan 10 07:32:44.501: INFO: DNS probes using dns-test-dd02de1a-bfe7-4d33-b0be-a71563f9b640 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5477.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5477.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5477.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5477.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 10 07:32:48.588: INFO: DNS probes using dns-test-8052638a-6c26-4a2b-a91c-be65db02b9ff succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 10 07:32:48.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5477" for this suite.

• [SLOW TEST:28.285 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":356,"completed":169,"skipped":3209,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:32:48.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating server pod server in namespace prestop-2177
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2177
STEP: Deleting pre-stop pod
Jan 10 07:32:57.720: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:188
Jan 10 07:32:57.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2177" for this suite.

• [SLOW TEST:9.131 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":356,"completed":170,"skipped":3225,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:32:57.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Jan 10 07:32:57.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6999" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":171,"skipped":3249,"failed":0}
SSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:32:57.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 10 07:37:57.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-931" for this suite.

• [SLOW TEST:300.057 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":356,"completed":172,"skipped":3252,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:37:57.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jan 10 07:37:58.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:38:02.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:38:15.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-509" for this suite.

• [SLOW TEST:17.858 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":356,"completed":173,"skipped":3264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:38:15.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 07:38:16.356: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 07:38:19.385: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:38:19.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3568" for this suite.
STEP: Destroying namespace "webhook-3568-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":356,"completed":174,"skipped":3300,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:38:19.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 07:38:19.586: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bfdddb08-d5da-4553-afff-c43235202fdc" in namespace "projected-2036" to be "Succeeded or Failed"
Jan 10 07:38:19.589: INFO: Pod "downwardapi-volume-bfdddb08-d5da-4553-afff-c43235202fdc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.251069ms
Jan 10 07:38:21.597: INFO: Pod "downwardapi-volume-bfdddb08-d5da-4553-afff-c43235202fdc": Phase="Running", Reason="", readiness=false. Elapsed: 2.011332511s
Jan 10 07:38:23.606: INFO: Pod "downwardapi-volume-bfdddb08-d5da-4553-afff-c43235202fdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019810641s
STEP: Saw pod success
Jan 10 07:38:23.606: INFO: Pod "downwardapi-volume-bfdddb08-d5da-4553-afff-c43235202fdc" satisfied condition "Succeeded or Failed"
Jan 10 07:38:23.608: INFO: Trying to get logs from node kk-instance-r65pm pod downwardapi-volume-bfdddb08-d5da-4553-afff-c43235202fdc container client-container: <nil>
STEP: delete the pod
Jan 10 07:38:23.637: INFO: Waiting for pod downwardapi-volume-bfdddb08-d5da-4553-afff-c43235202fdc to disappear
Jan 10 07:38:23.640: INFO: Pod downwardapi-volume-bfdddb08-d5da-4553-afff-c43235202fdc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 10 07:38:23.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2036" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":175,"skipped":3350,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:38:23.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 07:38:23.683: INFO: Waiting up to 5m0s for pod "downwardapi-volume-beb3cbb5-5892-404b-bdba-84943783fd13" in namespace "downward-api-7080" to be "Succeeded or Failed"
Jan 10 07:38:23.687: INFO: Pod "downwardapi-volume-beb3cbb5-5892-404b-bdba-84943783fd13": Phase="Pending", Reason="", readiness=false. Elapsed: 3.289466ms
Jan 10 07:38:25.694: INFO: Pod "downwardapi-volume-beb3cbb5-5892-404b-bdba-84943783fd13": Phase="Running", Reason="", readiness=false. Elapsed: 2.010722783s
Jan 10 07:38:27.704: INFO: Pod "downwardapi-volume-beb3cbb5-5892-404b-bdba-84943783fd13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020884937s
STEP: Saw pod success
Jan 10 07:38:27.704: INFO: Pod "downwardapi-volume-beb3cbb5-5892-404b-bdba-84943783fd13" satisfied condition "Succeeded or Failed"
Jan 10 07:38:27.707: INFO: Trying to get logs from node kk-instance-r65pm pod downwardapi-volume-beb3cbb5-5892-404b-bdba-84943783fd13 container client-container: <nil>
STEP: delete the pod
Jan 10 07:38:27.724: INFO: Waiting for pod downwardapi-volume-beb3cbb5-5892-404b-bdba-84943783fd13 to disappear
Jan 10 07:38:27.727: INFO: Pod downwardapi-volume-beb3cbb5-5892-404b-bdba-84943783fd13 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 10 07:38:27.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7080" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":176,"skipped":3361,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:38:27.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jan 10 07:38:27.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-543" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":356,"completed":177,"skipped":3411,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:38:27.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 10 07:38:43.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1253" for this suite.

• [SLOW TEST:16.180 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":356,"completed":178,"skipped":3414,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:38:43.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-81
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 10 07:38:44.011: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 10 07:38:44.037: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:38:46.044: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 07:38:48.047: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 07:38:50.045: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 07:38:52.051: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 07:38:54.046: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 10 07:38:56.046: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 10 07:38:56.052: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 10 07:38:58.083: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 10 07:38:58.083: INFO: Breadth first check of 10.233.82.19 on host 192.168.0.4...
Jan 10 07:38:58.085: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.82.10:9080/dial?request=hostname&protocol=udp&host=10.233.82.19&port=8081&tries=1'] Namespace:pod-network-test-81 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:38:58.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:38:58.086: INFO: ExecWithOptions: Clientset creation
Jan 10 07:38:58.086: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-81/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.82.10%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.82.19%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 10 07:38:58.169: INFO: Waiting for responses: map[]
Jan 10 07:38:58.169: INFO: reached 10.233.82.19 after 0/1 tries
Jan 10 07:38:58.170: INFO: Breadth first check of 10.233.107.76 on host 192.168.0.5...
Jan 10 07:38:58.174: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.82.10:9080/dial?request=hostname&protocol=udp&host=10.233.107.76&port=8081&tries=1'] Namespace:pod-network-test-81 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:38:58.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:38:58.175: INFO: ExecWithOptions: Clientset creation
Jan 10 07:38:58.176: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-81/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.82.10%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.107.76%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 10 07:38:58.249: INFO: Waiting for responses: map[]
Jan 10 07:38:58.249: INFO: reached 10.233.107.76 after 0/1 tries
Jan 10 07:38:58.249: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 10 07:38:58.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-81" for this suite.

• [SLOW TEST:14.269 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":356,"completed":179,"skipped":3434,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:38:58.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 10 07:38:58.347: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 10 07:39:58.415: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Jan 10 07:39:58.473: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 10 07:39:58.483: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 10 07:39:58.507: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 10 07:39:58.512: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 10 07:40:14.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2608" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:76.373 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":356,"completed":180,"skipped":3442,"failed":0}
SSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:40:14.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pod templates
Jan 10 07:40:14.670: INFO: created test-podtemplate-1
Jan 10 07:40:14.673: INFO: created test-podtemplate-2
Jan 10 07:40:14.677: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jan 10 07:40:14.680: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jan 10 07:40:14.692: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jan 10 07:40:14.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2684" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":356,"completed":181,"skipped":3449,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:40:14.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 10 07:40:14.728: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 10 07:40:17.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6877" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":356,"completed":182,"skipped":3490,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:40:17.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Jan 10 07:40:24.044: INFO: 80 pods remaining
Jan 10 07:40:24.044: INFO: 80 pods has nil DeletionTimestamp
Jan 10 07:40:24.044: INFO: 
Jan 10 07:40:25.088: INFO: 70 pods remaining
Jan 10 07:40:25.088: INFO: 70 pods has nil DeletionTimestamp
Jan 10 07:40:25.088: INFO: 
Jan 10 07:40:26.063: INFO: 60 pods remaining
Jan 10 07:40:26.063: INFO: 60 pods has nil DeletionTimestamp
Jan 10 07:40:26.063: INFO: 
Jan 10 07:40:27.042: INFO: 40 pods remaining
Jan 10 07:40:27.042: INFO: 40 pods has nil DeletionTimestamp
Jan 10 07:40:27.042: INFO: 
Jan 10 07:40:28.085: INFO: 30 pods remaining
Jan 10 07:40:28.085: INFO: 30 pods has nil DeletionTimestamp
Jan 10 07:40:28.085: INFO: 
Jan 10 07:40:29.039: INFO: 20 pods remaining
Jan 10 07:40:29.039: INFO: 20 pods has nil DeletionTimestamp
Jan 10 07:40:29.039: INFO: 
STEP: Gathering metrics
Jan 10 07:40:30.078: INFO: The status of Pod kube-controller-manager-kk-instance-rx4ck is Running (Ready = true)
Jan 10 07:40:30.195: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 10 07:40:30.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1088" for this suite.

• [SLOW TEST:12.248 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":356,"completed":183,"skipped":3493,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:40:30.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 10 07:40:30.263: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 10 07:40:35.269: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 10 07:40:36.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4840" for this suite.

• [SLOW TEST:6.113 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":356,"completed":184,"skipped":3515,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:40:36.320: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1299.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1299.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1299.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1299.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 10 07:40:50.412: INFO: DNS probes using dns-1299/dns-test-b951a934-6d47-49ba-9a6d-2e703477c6ab succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 10 07:40:50.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1299" for this suite.

• [SLOW TEST:14.151 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","total":356,"completed":185,"skipped":3545,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:40:50.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 07:40:51.136: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 07:40:54.165: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:40:54.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-904" for this suite.
STEP: Destroying namespace "webhook-904-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":356,"completed":186,"skipped":3548,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:40:54.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:40:54.329: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 10 07:40:54.340: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 10 07:40:59.358: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 10 07:40:59.358: INFO: Creating deployment "test-rolling-update-deployment"
Jan 10 07:40:59.362: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 10 07:40:59.379: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 10 07:41:01.390: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 10 07:41:01.400: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 10 07:41:01.411: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-170  a6657d43-e781-46a0-a303-e7335fb1d053 34548 1 2023-01-10 07:40:59 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2023-01-10 07:40:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0021ff9b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-10 07:40:59 +0000 UTC,LastTransitionTime:2023-01-10 07:40:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67c8f74c6c" has successfully progressed.,LastUpdateTime:2023-01-10 07:41:00 +0000 UTC,LastTransitionTime:2023-01-10 07:40:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 10 07:41:01.414: INFO: New ReplicaSet "test-rolling-update-deployment-67c8f74c6c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c  deployment-170  d34ddfb2-85bc-4d6d-b95e-66f281a0c757 34539 1 2023-01-10 07:40:59 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment a6657d43-e781-46a0-a303-e7335fb1d053 0xc0021ffec7 0xc0021ffec8}] []  [{kube-controller-manager Update apps/v1 2023-01-10 07:40:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6657d43-e781-46a0-a303-e7335fb1d053\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:41:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67c8f74c6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0021fff78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 10 07:41:01.414: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 10 07:41:01.414: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-170  d462d78a-51f4-4a82-83af-ad3938df16bf 34547 2 2023-01-10 07:40:54 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment a6657d43-e781-46a0-a303-e7335fb1d053 0xc0021ffd97 0xc0021ffd98}] []  [{e2e.test Update apps/v1 2023-01-10 07:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6657d43-e781-46a0-a303-e7335fb1d053\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:41:00 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0021ffe58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 10 07:41:01.418: INFO: Pod "test-rolling-update-deployment-67c8f74c6c-jjpfr" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c-jjpfr test-rolling-update-deployment-67c8f74c6c- deployment-170  a4ccf923-179e-4aff-b7c2-a4c27ebe8dff 34538 0 2023-01-10 07:40:59 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[cni.projectcalico.org/containerID:75521c631f1b31d325b03b57b2ee0c5a5dc28109002023409d8700d30a1a0bd5 cni.projectcalico.org/podIP:10.233.82.48/32 cni.projectcalico.org/podIPs:10.233.82.48/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-67c8f74c6c d34ddfb2-85bc-4d6d-b95e-66f281a0c757 0xc0021923f7 0xc0021923f8}] []  [{calico Update v1 2023-01-10 07:40:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-10 07:40:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d34ddfb2-85bc-4d6d-b95e-66f281a0c757\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-10 07:41:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.82.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lnhdw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lnhdw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-r65pm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:40:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:41:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:41:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:40:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.233.82.48,StartTime:2023-01-10 07:40:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:41:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://7273864e3899011986eb2407068ca26c8257db5f9989b3c92af4dde9812f75f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.82.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 10 07:41:01.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-170" for this suite.

• [SLOW TEST:7.129 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":187,"skipped":3589,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:41:01.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 10 07:41:01.501: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9442  298b9dcf-0d73-4c54-a99b-43bed15233e0 34560 0 2023-01-10 07:41:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2023-01-10 07:41:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 10 07:41:01.502: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9442  298b9dcf-0d73-4c54-a99b-43bed15233e0 34561 0 2023-01-10 07:41:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2023-01-10 07:41:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 10 07:41:01.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9442" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":356,"completed":188,"skipped":3603,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:41:01.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 10 07:41:01.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-953" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":356,"completed":189,"skipped":3643,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:41:01.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service endpoint-test2 in namespace services-7600
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7600 to expose endpoints map[]
Jan 10 07:41:01.607: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 10 07:41:02.624: INFO: successfully validated that service endpoint-test2 in namespace services-7600 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7600
Jan 10 07:41:02.646: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:41:04.654: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7600 to expose endpoints map[pod1:[80]]
Jan 10 07:41:04.664: INFO: successfully validated that service endpoint-test2 in namespace services-7600 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Jan 10 07:41:04.664: INFO: Creating new exec pod
Jan 10 07:41:07.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7600 exec execpodn2bjm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 10 07:41:07.881: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 10 07:41:07.881: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:41:07.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7600 exec execpodn2bjm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.50.200 80'
Jan 10 07:41:08.049: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.50.200 80\nConnection to 10.233.50.200 80 port [tcp/http] succeeded!\n"
Jan 10 07:41:08.049: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-7600
Jan 10 07:41:08.072: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:41:10.078: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7600 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 10 07:41:10.090: INFO: successfully validated that service endpoint-test2 in namespace services-7600 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Jan 10 07:41:11.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7600 exec execpodn2bjm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 10 07:41:11.266: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 10 07:41:11.266: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:41:11.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7600 exec execpodn2bjm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.50.200 80'
Jan 10 07:41:11.426: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.50.200 80\nConnection to 10.233.50.200 80 port [tcp/http] succeeded!\n"
Jan 10 07:41:11.426: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7600
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7600 to expose endpoints map[pod2:[80]]
Jan 10 07:41:11.490: INFO: successfully validated that service endpoint-test2 in namespace services-7600 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Jan 10 07:41:12.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7600 exec execpodn2bjm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 10 07:41:12.689: INFO: stderr: "+ + nc -v -t -w 2 endpoint-test2 80\necho hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 10 07:41:12.689: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 07:41:12.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7600 exec execpodn2bjm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.50.200 80'
Jan 10 07:41:12.842: INFO: stderr: "+ nc -v -t -w 2 10.233.50.200 80\n+ echo hostName\nConnection to 10.233.50.200 80 port [tcp/http] succeeded!\n"
Jan 10 07:41:12.842: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-7600
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7600 to expose endpoints map[]
Jan 10 07:41:12.908: INFO: successfully validated that service endpoint-test2 in namespace services-7600 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 07:41:12.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7600" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:11.406 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":356,"completed":190,"skipped":3656,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:41:12.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 07:41:13.053: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dca00001-7d41-48b2-92b8-4f35afaf90ed" in namespace "downward-api-7060" to be "Succeeded or Failed"
Jan 10 07:41:13.057: INFO: Pod "downwardapi-volume-dca00001-7d41-48b2-92b8-4f35afaf90ed": Phase="Pending", Reason="", readiness=false. Elapsed: 3.968429ms
Jan 10 07:41:15.063: INFO: Pod "downwardapi-volume-dca00001-7d41-48b2-92b8-4f35afaf90ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010652063s
Jan 10 07:41:17.076: INFO: Pod "downwardapi-volume-dca00001-7d41-48b2-92b8-4f35afaf90ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02300705s
STEP: Saw pod success
Jan 10 07:41:17.076: INFO: Pod "downwardapi-volume-dca00001-7d41-48b2-92b8-4f35afaf90ed" satisfied condition "Succeeded or Failed"
Jan 10 07:41:17.078: INFO: Trying to get logs from node kk-instance-vgmmg pod downwardapi-volume-dca00001-7d41-48b2-92b8-4f35afaf90ed container client-container: <nil>
STEP: delete the pod
Jan 10 07:41:17.106: INFO: Waiting for pod downwardapi-volume-dca00001-7d41-48b2-92b8-4f35afaf90ed to disappear
Jan 10 07:41:17.108: INFO: Pod downwardapi-volume-dca00001-7d41-48b2-92b8-4f35afaf90ed no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 10 07:41:17.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7060" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":191,"skipped":3679,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:41:17.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jan 10 07:41:27.196: INFO: The status of Pod kube-controller-manager-kk-instance-rx4ck is Running (Ready = true)
Jan 10 07:41:27.262: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 10 07:41:27.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5342" for this suite.

• [SLOW TEST:10.150 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":356,"completed":192,"skipped":3704,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:41:27.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-b77b7390-f163-4e72-a82a-c9c2d8dc68d1
STEP: Creating a pod to test consume secrets
Jan 10 07:41:27.305: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f866d161-9796-409e-abbc-93810fa67036" in namespace "projected-6945" to be "Succeeded or Failed"
Jan 10 07:41:27.309: INFO: Pod "pod-projected-secrets-f866d161-9796-409e-abbc-93810fa67036": Phase="Pending", Reason="", readiness=false. Elapsed: 3.530015ms
Jan 10 07:41:29.318: INFO: Pod "pod-projected-secrets-f866d161-9796-409e-abbc-93810fa67036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013169007s
Jan 10 07:41:31.325: INFO: Pod "pod-projected-secrets-f866d161-9796-409e-abbc-93810fa67036": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020020234s
STEP: Saw pod success
Jan 10 07:41:31.325: INFO: Pod "pod-projected-secrets-f866d161-9796-409e-abbc-93810fa67036" satisfied condition "Succeeded or Failed"
Jan 10 07:41:31.328: INFO: Trying to get logs from node kk-instance-r65pm pod pod-projected-secrets-f866d161-9796-409e-abbc-93810fa67036 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 10 07:41:31.353: INFO: Waiting for pod pod-projected-secrets-f866d161-9796-409e-abbc-93810fa67036 to disappear
Jan 10 07:41:31.357: INFO: Pod pod-projected-secrets-f866d161-9796-409e-abbc-93810fa67036 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 10 07:41:31.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6945" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":193,"skipped":3752,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:41:31.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 07:41:31.418: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2cf516f-ee15-4609-95dc-24897da17469" in namespace "downward-api-4874" to be "Succeeded or Failed"
Jan 10 07:41:31.425: INFO: Pod "downwardapi-volume-c2cf516f-ee15-4609-95dc-24897da17469": Phase="Pending", Reason="", readiness=false. Elapsed: 6.582811ms
Jan 10 07:41:33.434: INFO: Pod "downwardapi-volume-c2cf516f-ee15-4609-95dc-24897da17469": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016110926s
Jan 10 07:41:35.443: INFO: Pod "downwardapi-volume-c2cf516f-ee15-4609-95dc-24897da17469": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02503868s
STEP: Saw pod success
Jan 10 07:41:35.443: INFO: Pod "downwardapi-volume-c2cf516f-ee15-4609-95dc-24897da17469" satisfied condition "Succeeded or Failed"
Jan 10 07:41:35.446: INFO: Trying to get logs from node kk-instance-r65pm pod downwardapi-volume-c2cf516f-ee15-4609-95dc-24897da17469 container client-container: <nil>
STEP: delete the pod
Jan 10 07:41:35.474: INFO: Waiting for pod downwardapi-volume-c2cf516f-ee15-4609-95dc-24897da17469 to disappear
Jan 10 07:41:35.477: INFO: Pod downwardapi-volume-c2cf516f-ee15-4609-95dc-24897da17469 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 10 07:41:35.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4874" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":194,"skipped":3805,"failed":0}
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:41:35.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jan 10 07:41:35.548: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:41:37.559: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jan 10 07:41:37.573: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:41:39.584: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 10 07:41:39.587: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8399 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:41:39.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:41:39.588: INFO: ExecWithOptions: Clientset creation
Jan 10 07:41:39.588: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8399/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 10 07:41:39.665: INFO: Exec stderr: ""
Jan 10 07:41:39.665: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8399 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:41:39.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:41:39.666: INFO: ExecWithOptions: Clientset creation
Jan 10 07:41:39.666: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8399/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 10 07:41:39.746: INFO: Exec stderr: ""
Jan 10 07:41:39.746: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8399 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:41:39.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:41:39.746: INFO: ExecWithOptions: Clientset creation
Jan 10 07:41:39.746: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8399/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 10 07:41:39.826: INFO: Exec stderr: ""
Jan 10 07:41:39.827: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8399 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:41:39.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:41:39.828: INFO: ExecWithOptions: Clientset creation
Jan 10 07:41:39.828: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8399/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 10 07:41:39.896: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 10 07:41:39.896: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8399 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:41:39.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:41:39.897: INFO: ExecWithOptions: Clientset creation
Jan 10 07:41:39.898: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8399/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 10 07:41:39.977: INFO: Exec stderr: ""
Jan 10 07:41:39.977: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8399 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:41:39.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:41:39.978: INFO: ExecWithOptions: Clientset creation
Jan 10 07:41:39.978: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8399/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 10 07:41:40.055: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 10 07:41:40.056: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8399 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:41:40.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:41:40.057: INFO: ExecWithOptions: Clientset creation
Jan 10 07:41:40.057: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8399/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 10 07:41:40.137: INFO: Exec stderr: ""
Jan 10 07:41:40.137: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8399 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:41:40.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:41:40.138: INFO: ExecWithOptions: Clientset creation
Jan 10 07:41:40.138: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8399/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 10 07:41:40.231: INFO: Exec stderr: ""
Jan 10 07:41:40.231: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8399 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:41:40.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:41:40.232: INFO: ExecWithOptions: Clientset creation
Jan 10 07:41:40.232: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8399/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 10 07:41:40.315: INFO: Exec stderr: ""
Jan 10 07:41:40.316: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8399 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:41:40.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:41:40.316: INFO: ExecWithOptions: Clientset creation
Jan 10 07:41:40.317: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8399/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 10 07:41:40.388: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:188
Jan 10 07:41:40.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8399" for this suite.
•{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":195,"skipped":3809,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:41:40.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-0bfe214a-d96e-4131-b6b3-b46545b06f1d
STEP: Creating a pod to test consume secrets
Jan 10 07:41:40.451: INFO: Waiting up to 5m0s for pod "pod-secrets-61f476a5-1dca-418c-bcdc-f1a06b77450f" in namespace "secrets-3144" to be "Succeeded or Failed"
Jan 10 07:41:40.456: INFO: Pod "pod-secrets-61f476a5-1dca-418c-bcdc-f1a06b77450f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.742504ms
Jan 10 07:41:42.468: INFO: Pod "pod-secrets-61f476a5-1dca-418c-bcdc-f1a06b77450f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016601358s
Jan 10 07:41:44.479: INFO: Pod "pod-secrets-61f476a5-1dca-418c-bcdc-f1a06b77450f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027303967s
STEP: Saw pod success
Jan 10 07:41:44.479: INFO: Pod "pod-secrets-61f476a5-1dca-418c-bcdc-f1a06b77450f" satisfied condition "Succeeded or Failed"
Jan 10 07:41:44.482: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-secrets-61f476a5-1dca-418c-bcdc-f1a06b77450f container secret-volume-test: <nil>
STEP: delete the pod
Jan 10 07:41:44.499: INFO: Waiting for pod pod-secrets-61f476a5-1dca-418c-bcdc-f1a06b77450f to disappear
Jan 10 07:41:44.502: INFO: Pod pod-secrets-61f476a5-1dca-418c-bcdc-f1a06b77450f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 10 07:41:44.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3144" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":196,"skipped":3826,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:41:44.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-3535771d-dae7-4b2e-9dfc-c21d419ae976
STEP: Creating secret with name s-test-opt-upd-38e875b7-29cb-4d3b-b9b0-c2284ae86e14
STEP: Creating the pod
Jan 10 07:41:44.561: INFO: The status of Pod pod-projected-secrets-373f63d7-5878-44df-9154-b6e9f82549a1 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:41:46.570: INFO: The status of Pod pod-projected-secrets-373f63d7-5878-44df-9154-b6e9f82549a1 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-3535771d-dae7-4b2e-9dfc-c21d419ae976
STEP: Updating secret s-test-opt-upd-38e875b7-29cb-4d3b-b9b0-c2284ae86e14
STEP: Creating secret with name s-test-opt-create-837718a5-f418-4ebc-b27e-9c7764c13407
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 10 07:41:48.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3877" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":197,"skipped":3855,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:41:48.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pods
Jan 10 07:41:48.678: INFO: created test-pod-1
Jan 10 07:41:48.688: INFO: created test-pod-2
Jan 10 07:41:48.694: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running
Jan 10 07:41:48.694: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5310' to be running and ready
Jan 10 07:41:48.716: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 10 07:41:48.716: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 10 07:41:48.716: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 10 07:41:48.716: INFO: 0 / 3 pods in namespace 'pods-5310' are running and ready (0 seconds elapsed)
Jan 10 07:41:48.716: INFO: expected 0 pod replicas in namespace 'pods-5310', 0 are Running and Ready.
Jan 10 07:41:48.716: INFO: POD         NODE               PHASE    GRACE  CONDITIONS
Jan 10 07:41:48.716: INFO: test-pod-1  kk-instance-r65pm  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:41:48 +0000 UTC  }]
Jan 10 07:41:48.716: INFO: test-pod-2  kk-instance-r65pm  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:41:48 +0000 UTC  }]
Jan 10 07:41:48.716: INFO: test-pod-3  kk-instance-r65pm  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:41:48 +0000 UTC  }]
Jan 10 07:41:48.716: INFO: 
Jan 10 07:41:50.727: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 10 07:41:50.727: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 10 07:41:50.727: INFO: 1 / 3 pods in namespace 'pods-5310' are running and ready (2 seconds elapsed)
Jan 10 07:41:50.727: INFO: expected 0 pod replicas in namespace 'pods-5310', 0 are Running and Ready.
Jan 10 07:41:50.728: INFO: POD         NODE               PHASE    GRACE  CONDITIONS
Jan 10 07:41:50.728: INFO: test-pod-2  kk-instance-r65pm  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:41:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:41:48 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:41:48 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:41:48 +0000 UTC  }]
Jan 10 07:41:50.728: INFO: test-pod-3  kk-instance-r65pm  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:41:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:41:48 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:41:48 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 07:41:48 +0000 UTC  }]
Jan 10 07:41:50.728: INFO: 
Jan 10 07:41:52.734: INFO: 3 / 3 pods in namespace 'pods-5310' are running and ready (4 seconds elapsed)
Jan 10 07:41:52.735: INFO: expected 0 pod replicas in namespace 'pods-5310', 0 are Running and Ready.
STEP: waiting for all pods to be deleted
Jan 10 07:41:52.765: INFO: Pod quantity 3 is different from expected quantity 0
Jan 10 07:41:53.772: INFO: Pod quantity 3 is different from expected quantity 0
Jan 10 07:41:54.771: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 10 07:41:55.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5310" for this suite.

• [SLOW TEST:7.148 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":356,"completed":198,"skipped":3857,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:41:55.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jan 10 07:41:57.832: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9733 PodName:pod-sharedvolume-1d924021-510d-4ce1-8a1a-b76c5a1a755d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 07:41:57.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 07:41:57.834: INFO: ExecWithOptions: Clientset creation
Jan 10 07:41:57.834: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-9733/pods/pod-sharedvolume-1d924021-510d-4ce1-8a1a-b76c5a1a755d/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 10 07:41:57.904: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 10 07:41:57.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9733" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":356,"completed":199,"skipped":3860,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:41:57.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 10 07:42:25.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3821" for this suite.

• [SLOW TEST:28.092 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":356,"completed":200,"skipped":3867,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:42:26.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test env composition
Jan 10 07:42:26.049: INFO: Waiting up to 5m0s for pod "var-expansion-147891cd-69d9-45ab-8863-bba125d42afe" in namespace "var-expansion-8022" to be "Succeeded or Failed"
Jan 10 07:42:26.052: INFO: Pod "var-expansion-147891cd-69d9-45ab-8863-bba125d42afe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.431877ms
Jan 10 07:42:28.064: INFO: Pod "var-expansion-147891cd-69d9-45ab-8863-bba125d42afe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015328326s
Jan 10 07:42:30.070: INFO: Pod "var-expansion-147891cd-69d9-45ab-8863-bba125d42afe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021510484s
STEP: Saw pod success
Jan 10 07:42:30.071: INFO: Pod "var-expansion-147891cd-69d9-45ab-8863-bba125d42afe" satisfied condition "Succeeded or Failed"
Jan 10 07:42:30.074: INFO: Trying to get logs from node kk-instance-r65pm pod var-expansion-147891cd-69d9-45ab-8863-bba125d42afe container dapi-container: <nil>
STEP: delete the pod
Jan 10 07:42:30.099: INFO: Waiting for pod var-expansion-147891cd-69d9-45ab-8863-bba125d42afe to disappear
Jan 10 07:42:30.102: INFO: Pod var-expansion-147891cd-69d9-45ab-8863-bba125d42afe no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 10 07:42:30.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8022" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":356,"completed":201,"skipped":3871,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:42:30.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting the proxy server
Jan 10 07:42:30.151: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6567 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 07:42:30.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6567" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":356,"completed":202,"skipped":3975,"failed":0}
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:42:30.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 07:42:30.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6819" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":356,"completed":203,"skipped":3976,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:42:30.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching services
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 07:42:30.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9834" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":356,"completed":204,"skipped":3999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:42:30.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:42:30.373: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 10 07:42:35.381: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 10 07:42:35.381: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 10 07:42:37.391: INFO: Creating deployment "test-rollover-deployment"
Jan 10 07:42:37.406: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 10 07:42:39.418: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 10 07:42:39.433: INFO: Ensure that both replica sets have 1 created replica
Jan 10 07:42:39.443: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 10 07:42:39.465: INFO: Updating deployment test-rollover-deployment
Jan 10 07:42:39.465: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 10 07:42:41.478: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 10 07:42:41.484: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 10 07:42:41.488: INFO: all replica sets need to contain the pod-template-hash label
Jan 10 07:42:41.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 42, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 42, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 42, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 42, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 10 07:42:43.498: INFO: all replica sets need to contain the pod-template-hash label
Jan 10 07:42:43.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 42, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 42, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 42, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 42, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 10 07:42:45.498: INFO: all replica sets need to contain the pod-template-hash label
Jan 10 07:42:45.499: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 42, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 42, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 42, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 42, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 10 07:42:47.504: INFO: all replica sets need to contain the pod-template-hash label
Jan 10 07:42:47.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 42, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 42, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 42, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 42, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 10 07:42:49.502: INFO: all replica sets need to contain the pod-template-hash label
Jan 10 07:42:49.502: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 42, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 42, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 42, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 42, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 10 07:42:51.499: INFO: 
Jan 10 07:42:51.499: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 10 07:42:51.505: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5765  a57a89fc-9996-4b7f-ada0-a4588e4219bf 35631 2 2023-01-10 07:42:37 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-01-10 07:42:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:42:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0024b7e68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-10 07:42:37 +0000 UTC,LastTransitionTime:2023-01-10 07:42:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-779c67f4f8" has successfully progressed.,LastUpdateTime:2023-01-10 07:42:50 +0000 UTC,LastTransitionTime:2023-01-10 07:42:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 10 07:42:51.507: INFO: New ReplicaSet "test-rollover-deployment-779c67f4f8" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-779c67f4f8  deployment-5765  38701ff9-88dc-4f2f-83e1-b57d59abc276 35621 2 2023-01-10 07:42:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment a57a89fc-9996-4b7f-ada0-a4588e4219bf 0xc002262c37 0xc002262c38}] []  [{kube-controller-manager Update apps/v1 2023-01-10 07:42:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a57a89fc-9996-4b7f-ada0-a4588e4219bf\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:42:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 779c67f4f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002262ce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 10 07:42:51.507: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 10 07:42:51.507: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5765  4ee69534-f977-4651-94c9-ac0f36d5a291 35630 2 2023-01-10 07:42:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment a57a89fc-9996-4b7f-ada0-a4588e4219bf 0xc002262b07 0xc002262b08}] []  [{e2e.test Update apps/v1 2023-01-10 07:42:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:42:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a57a89fc-9996-4b7f-ada0-a4588e4219bf\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:42:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002262bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 10 07:42:51.508: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-87f8f6dcf  deployment-5765  36d46a8b-e8c7-4cde-8c20-79595e874d8b 35567 2 2023-01-10 07:42:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment a57a89fc-9996-4b7f-ada0-a4588e4219bf 0xc002262d50 0xc002262d51}] []  [{kube-controller-manager Update apps/v1 2023-01-10 07:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a57a89fc-9996-4b7f-ada0-a4588e4219bf\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:42:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 87f8f6dcf,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002262df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 10 07:42:51.510: INFO: Pod "test-rollover-deployment-779c67f4f8-hdplf" is available:
&Pod{ObjectMeta:{test-rollover-deployment-779c67f4f8-hdplf test-rollover-deployment-779c67f4f8- deployment-5765  a4900c9a-085c-4a34-8be5-21f776f5d063 35581 0 2023-01-10 07:42:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[cni.projectcalico.org/containerID:27ff5f33cda660d3706d4c669c0c544275be7a58bf828a237a7ac19e62acbc32 cni.projectcalico.org/podIP:10.233.107.74/32 cni.projectcalico.org/podIPs:10.233.107.74/32] [{apps/v1 ReplicaSet test-rollover-deployment-779c67f4f8 38701ff9-88dc-4f2f-83e1-b57d59abc276 0xc0037d0237 0xc0037d0238}] []  [{kube-controller-manager Update v1 2023-01-10 07:42:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"38701ff9-88dc-4f2f-83e1-b57d59abc276\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-10 07:42:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-10 07:42:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.107.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rskj8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rskj8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-vgmmg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:42:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:42:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:42:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:42:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.233.107.74,StartTime:2023-01-10 07:42:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:42:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://dc891514ace7de7a03238f9c0a47ac2f4c84f0313ce87fa7c09123f3985c8c80,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.107.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 10 07:42:51.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5765" for this suite.

• [SLOW TEST:21.217 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":356,"completed":205,"skipped":4062,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:42:51.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Jan 10 07:42:51.552: INFO: namespace kubectl-6476
Jan 10 07:42:51.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6476 create -f -'
Jan 10 07:42:52.322: INFO: stderr: ""
Jan 10 07:42:52.322: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 10 07:42:53.329: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 10 07:42:53.329: INFO: Found 1 / 1
Jan 10 07:42:53.329: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 10 07:42:53.332: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 10 07:42:53.332: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 10 07:42:53.332: INFO: wait on agnhost-primary startup in kubectl-6476 
Jan 10 07:42:53.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6476 logs agnhost-primary-fvdkb agnhost-primary'
Jan 10 07:42:53.414: INFO: stderr: ""
Jan 10 07:42:53.414: INFO: stdout: "Paused\n"
STEP: exposing RC
Jan 10 07:42:53.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6476 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 10 07:42:53.509: INFO: stderr: ""
Jan 10 07:42:53.509: INFO: stdout: "service/rm2 exposed\n"
Jan 10 07:42:53.516: INFO: Service rm2 in namespace kubectl-6476 found.
STEP: exposing service
Jan 10 07:42:55.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6476 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 10 07:42:55.637: INFO: stderr: ""
Jan 10 07:42:55.638: INFO: stdout: "service/rm3 exposed\n"
Jan 10 07:42:55.646: INFO: Service rm3 in namespace kubectl-6476 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 07:42:57.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6476" for this suite.

• [SLOW TEST:6.149 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1249
    should create services for rc  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":356,"completed":206,"skipped":4076,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:42:57.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 10 07:43:08.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9075" for this suite.

• [SLOW TEST:11.212 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":356,"completed":207,"skipped":4087,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:43:08.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:43:08.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:43:09.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1072" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":356,"completed":208,"skipped":4092,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:43:09.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-469d3649-6050-410b-ac06-9981f128b9ce
STEP: Creating configMap with name cm-test-opt-upd-c39cc4f3-b4b2-4852-966b-ef06d9433661
STEP: Creating the pod
Jan 10 07:43:10.005: INFO: The status of Pod pod-projected-configmaps-a6ac1b82-2b8a-44d2-9d81-1e13816ed2c0 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:43:12.013: INFO: The status of Pod pod-projected-configmaps-a6ac1b82-2b8a-44d2-9d81-1e13816ed2c0 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-469d3649-6050-410b-ac06-9981f128b9ce
STEP: Updating configmap cm-test-opt-upd-c39cc4f3-b4b2-4852-966b-ef06d9433661
STEP: Creating configMap with name cm-test-opt-create-c841c66c-1c8d-469a-ac81-9f92f584d44a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 10 07:43:14.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6495" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":209,"skipped":4145,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:43:14.086: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-10d9d1b2-4c10-4d94-87e8-4b0eefa3f867
STEP: Creating a pod to test consume secrets
Jan 10 07:43:14.128: INFO: Waiting up to 5m0s for pod "pod-secrets-c50d674c-903e-4d36-917a-3b7770571ce1" in namespace "secrets-4288" to be "Succeeded or Failed"
Jan 10 07:43:14.134: INFO: Pod "pod-secrets-c50d674c-903e-4d36-917a-3b7770571ce1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.226602ms
Jan 10 07:43:16.143: INFO: Pod "pod-secrets-c50d674c-903e-4d36-917a-3b7770571ce1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014558785s
Jan 10 07:43:18.153: INFO: Pod "pod-secrets-c50d674c-903e-4d36-917a-3b7770571ce1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02487146s
STEP: Saw pod success
Jan 10 07:43:18.153: INFO: Pod "pod-secrets-c50d674c-903e-4d36-917a-3b7770571ce1" satisfied condition "Succeeded or Failed"
Jan 10 07:43:18.156: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-secrets-c50d674c-903e-4d36-917a-3b7770571ce1 container secret-volume-test: <nil>
STEP: delete the pod
Jan 10 07:43:18.174: INFO: Waiting for pod pod-secrets-c50d674c-903e-4d36-917a-3b7770571ce1 to disappear
Jan 10 07:43:18.177: INFO: Pod pod-secrets-c50d674c-903e-4d36-917a-3b7770571ce1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 10 07:43:18.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4288" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":210,"skipped":4156,"failed":0}
S
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:43:18.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jan 10 07:43:20.299: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 10 07:43:22.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2750" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":356,"completed":211,"skipped":4157,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:43:22.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 10 07:43:22.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5239" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":356,"completed":212,"skipped":4170,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:43:22.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 10 07:43:22.497: INFO: Waiting up to 5m0s for pod "pod-bca8dc9e-b675-4955-8b4e-45e22d010775" in namespace "emptydir-6051" to be "Succeeded or Failed"
Jan 10 07:43:22.504: INFO: Pod "pod-bca8dc9e-b675-4955-8b4e-45e22d010775": Phase="Pending", Reason="", readiness=false. Elapsed: 7.529332ms
Jan 10 07:43:24.513: INFO: Pod "pod-bca8dc9e-b675-4955-8b4e-45e22d010775": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016601597s
Jan 10 07:43:26.526: INFO: Pod "pod-bca8dc9e-b675-4955-8b4e-45e22d010775": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029120819s
STEP: Saw pod success
Jan 10 07:43:26.526: INFO: Pod "pod-bca8dc9e-b675-4955-8b4e-45e22d010775" satisfied condition "Succeeded or Failed"
Jan 10 07:43:26.536: INFO: Trying to get logs from node kk-instance-r65pm pod pod-bca8dc9e-b675-4955-8b4e-45e22d010775 container test-container: <nil>
STEP: delete the pod
Jan 10 07:43:26.561: INFO: Waiting for pod pod-bca8dc9e-b675-4955-8b4e-45e22d010775 to disappear
Jan 10 07:43:26.568: INFO: Pod pod-bca8dc9e-b675-4955-8b4e-45e22d010775 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 10 07:43:26.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6051" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":213,"skipped":4190,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:43:26.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 10 07:43:26.616: INFO: Waiting up to 5m0s for pod "downward-api-6bab9320-0a70-47a5-aa84-cebc3546fa34" in namespace "downward-api-4218" to be "Succeeded or Failed"
Jan 10 07:43:26.619: INFO: Pod "downward-api-6bab9320-0a70-47a5-aa84-cebc3546fa34": Phase="Pending", Reason="", readiness=false. Elapsed: 3.56778ms
Jan 10 07:43:28.626: INFO: Pod "downward-api-6bab9320-0a70-47a5-aa84-cebc3546fa34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009920121s
Jan 10 07:43:30.633: INFO: Pod "downward-api-6bab9320-0a70-47a5-aa84-cebc3546fa34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016800632s
STEP: Saw pod success
Jan 10 07:43:30.633: INFO: Pod "downward-api-6bab9320-0a70-47a5-aa84-cebc3546fa34" satisfied condition "Succeeded or Failed"
Jan 10 07:43:30.636: INFO: Trying to get logs from node kk-instance-r65pm pod downward-api-6bab9320-0a70-47a5-aa84-cebc3546fa34 container dapi-container: <nil>
STEP: delete the pod
Jan 10 07:43:30.655: INFO: Waiting for pod downward-api-6bab9320-0a70-47a5-aa84-cebc3546fa34 to disappear
Jan 10 07:43:30.658: INFO: Pod downward-api-6bab9320-0a70-47a5-aa84-cebc3546fa34 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 10 07:43:30.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4218" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":356,"completed":214,"skipped":4208,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:43:30.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 10 07:43:32.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5434" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":356,"completed":215,"skipped":4228,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:43:32.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-20e04d12-bcd7-4c80-a3d3-92301b60f057
STEP: Creating a pod to test consume configMaps
Jan 10 07:43:32.883: INFO: Waiting up to 5m0s for pod "pod-configmaps-7ee7ac85-9e9a-4ed5-9955-307852d06305" in namespace "configmap-8769" to be "Succeeded or Failed"
Jan 10 07:43:32.886: INFO: Pod "pod-configmaps-7ee7ac85-9e9a-4ed5-9955-307852d06305": Phase="Pending", Reason="", readiness=false. Elapsed: 2.853434ms
Jan 10 07:43:34.893: INFO: Pod "pod-configmaps-7ee7ac85-9e9a-4ed5-9955-307852d06305": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009458444s
Jan 10 07:43:36.900: INFO: Pod "pod-configmaps-7ee7ac85-9e9a-4ed5-9955-307852d06305": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016491858s
STEP: Saw pod success
Jan 10 07:43:36.900: INFO: Pod "pod-configmaps-7ee7ac85-9e9a-4ed5-9955-307852d06305" satisfied condition "Succeeded or Failed"
Jan 10 07:43:36.903: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-configmaps-7ee7ac85-9e9a-4ed5-9955-307852d06305 container agnhost-container: <nil>
STEP: delete the pod
Jan 10 07:43:36.918: INFO: Waiting for pod pod-configmaps-7ee7ac85-9e9a-4ed5-9955-307852d06305 to disappear
Jan 10 07:43:36.920: INFO: Pod pod-configmaps-7ee7ac85-9e9a-4ed5-9955-307852d06305 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 10 07:43:36.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8769" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":216,"skipped":4249,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:43:36.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
Jan 10 07:43:36.957: INFO: created test-event-1
Jan 10 07:43:36.961: INFO: created test-event-2
Jan 10 07:43:36.964: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jan 10 07:43:36.967: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jan 10 07:43:36.978: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Jan 10 07:43:36.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2180" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":356,"completed":217,"skipped":4254,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:43:36.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-e0fad521-c4db-4b5d-a49f-d25da6963e53
STEP: Creating a pod to test consume secrets
Jan 10 07:43:37.023: INFO: Waiting up to 5m0s for pod "pod-secrets-6c867c6f-0716-439e-8348-706174b41323" in namespace "secrets-4655" to be "Succeeded or Failed"
Jan 10 07:43:37.028: INFO: Pod "pod-secrets-6c867c6f-0716-439e-8348-706174b41323": Phase="Pending", Reason="", readiness=false. Elapsed: 4.515669ms
Jan 10 07:43:39.036: INFO: Pod "pod-secrets-6c867c6f-0716-439e-8348-706174b41323": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013278751s
Jan 10 07:43:41.041: INFO: Pod "pod-secrets-6c867c6f-0716-439e-8348-706174b41323": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018308926s
STEP: Saw pod success
Jan 10 07:43:41.042: INFO: Pod "pod-secrets-6c867c6f-0716-439e-8348-706174b41323" satisfied condition "Succeeded or Failed"
Jan 10 07:43:41.044: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-secrets-6c867c6f-0716-439e-8348-706174b41323 container secret-volume-test: <nil>
STEP: delete the pod
Jan 10 07:43:41.061: INFO: Waiting for pod pod-secrets-6c867c6f-0716-439e-8348-706174b41323 to disappear
Jan 10 07:43:41.064: INFO: Pod pod-secrets-6c867c6f-0716-439e-8348-706174b41323 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 10 07:43:41.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4655" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":218,"skipped":4256,"failed":0}

------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:43:41.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:79
Jan 10 07:43:41.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the sample API server.
Jan 10 07:43:42.003: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 10 07:43:44.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-66955c4dd6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 10 07:43:46.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-66955c4dd6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 10 07:43:48.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-66955c4dd6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 10 07:43:50.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-66955c4dd6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 10 07:43:52.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-66955c4dd6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 10 07:43:54.062: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 10, 7, 43, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-66955c4dd6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 10 07:43:56.192: INFO: Waited 126.134364ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jan 10 07:43:56.277: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:69
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:188
Jan 10 07:43:56.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4234" for this suite.

• [SLOW TEST:15.703 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":356,"completed":219,"skipped":4256,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:43:56.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:43:56.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jan 10 07:43:59.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-2329 --namespace=crd-publish-openapi-2329 create -f -'
Jan 10 07:44:00.544: INFO: stderr: ""
Jan 10 07:44:00.544: INFO: stdout: "e2e-test-crd-publish-openapi-9296-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 10 07:44:00.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-2329 --namespace=crd-publish-openapi-2329 delete e2e-test-crd-publish-openapi-9296-crds test-cr'
Jan 10 07:44:00.651: INFO: stderr: ""
Jan 10 07:44:00.651: INFO: stdout: "e2e-test-crd-publish-openapi-9296-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 10 07:44:00.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-2329 --namespace=crd-publish-openapi-2329 apply -f -'
Jan 10 07:44:00.879: INFO: stderr: ""
Jan 10 07:44:00.879: INFO: stdout: "e2e-test-crd-publish-openapi-9296-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 10 07:44:00.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-2329 --namespace=crd-publish-openapi-2329 delete e2e-test-crd-publish-openapi-9296-crds test-cr'
Jan 10 07:44:00.960: INFO: stderr: ""
Jan 10 07:44:00.961: INFO: stdout: "e2e-test-crd-publish-openapi-9296-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jan 10 07:44:00.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-2329 explain e2e-test-crd-publish-openapi-9296-crds'
Jan 10 07:44:01.170: INFO: stderr: ""
Jan 10 07:44:01.170: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9296-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:44:04.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2329" for this suite.

• [SLOW TEST:7.286 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":356,"completed":220,"skipped":4312,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:44:04.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jan 10 07:44:04.116: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 10 07:44:04.116: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 10 07:44:04.122: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 10 07:44:04.122: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 10 07:44:04.144: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 10 07:44:04.145: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 10 07:44:04.198: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 10 07:44:04.199: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 10 07:44:05.440: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 10 07:44:05.440: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 10 07:44:05.722: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jan 10 07:44:05.742: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jan 10 07:44:05.746: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0
Jan 10 07:44:05.746: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0
Jan 10 07:44:05.746: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0
Jan 10 07:44:05.746: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0
Jan 10 07:44:05.746: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0
Jan 10 07:44:05.746: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0
Jan 10 07:44:05.746: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0
Jan 10 07:44:05.746: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 0
Jan 10 07:44:05.747: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1
Jan 10 07:44:05.747: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1
Jan 10 07:44:05.747: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2
Jan 10 07:44:05.747: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2
Jan 10 07:44:05.747: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2
Jan 10 07:44:05.747: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2
Jan 10 07:44:05.752: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2
Jan 10 07:44:05.752: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2
Jan 10 07:44:05.772: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2
Jan 10 07:44:05.794: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2
Jan 10 07:44:05.796: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1
Jan 10 07:44:05.797: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1
Jan 10 07:44:05.808: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1
Jan 10 07:44:05.808: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1
Jan 10 07:44:06.727: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2
Jan 10 07:44:06.727: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2
Jan 10 07:44:06.750: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1
STEP: listing Deployments
Jan 10 07:44:06.757: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jan 10 07:44:06.778: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jan 10 07:44:06.786: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 10 07:44:06.787: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 10 07:44:06.823: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 10 07:44:06.848: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 10 07:44:06.869: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 10 07:44:08.483: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 10 07:44:08.504: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 10 07:44:08.527: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 10 07:44:08.558: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 10 07:44:09.752: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jan 10 07:44:09.816: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1
Jan 10 07:44:09.816: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1
Jan 10 07:44:09.816: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1
Jan 10 07:44:09.816: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1
Jan 10 07:44:09.816: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 1
Jan 10 07:44:09.817: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2
Jan 10 07:44:09.817: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2
Jan 10 07:44:09.817: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2
Jan 10 07:44:09.817: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 2
Jan 10 07:44:09.817: INFO: observed Deployment test-deployment in namespace deployment-6058 with ReadyReplicas 3
STEP: deleting the Deployment
Jan 10 07:44:09.825: INFO: observed event type MODIFIED
Jan 10 07:44:09.825: INFO: observed event type MODIFIED
Jan 10 07:44:09.825: INFO: observed event type MODIFIED
Jan 10 07:44:09.825: INFO: observed event type MODIFIED
Jan 10 07:44:09.826: INFO: observed event type MODIFIED
Jan 10 07:44:09.826: INFO: observed event type MODIFIED
Jan 10 07:44:09.826: INFO: observed event type MODIFIED
Jan 10 07:44:09.826: INFO: observed event type MODIFIED
Jan 10 07:44:09.826: INFO: observed event type MODIFIED
Jan 10 07:44:09.826: INFO: observed event type MODIFIED
Jan 10 07:44:09.826: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 10 07:44:09.828: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 10 07:44:09.831: INFO: ReplicaSet "test-deployment-6b48c869b6":
&ReplicaSet{ObjectMeta:{test-deployment-6b48c869b6  deployment-6058  8fdf966e-f6b1-485c-a269-bb8ac367017f 36562 3 2023-01-10 07:44:04 +0000 UTC <nil> <nil> map[pod-template-hash:6b48c869b6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 18acad51-0223-44ae-8185-29631aadba3f 0xc004e0de27 0xc004e0de28}] []  [{kube-controller-manager Update apps/v1 2023-01-10 07:44:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18acad51-0223-44ae-8185-29631aadba3f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:44:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6b48c869b6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6b48c869b6 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e0dec0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 10 07:44:09.837: INFO: ReplicaSet "test-deployment-74c6dd549b":
&ReplicaSet{ObjectMeta:{test-deployment-74c6dd549b  deployment-6058  00a220e5-4596-41b9-9f54-89fb3f7c801a 36659 2 2023-01-10 07:44:06 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 18acad51-0223-44ae-8185-29631aadba3f 0xc004e0df37 0xc004e0df38}] []  [{kube-controller-manager Update apps/v1 2023-01-10 07:44:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18acad51-0223-44ae-8185-29631aadba3f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:44:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 74c6dd549b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e0dfd0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 10 07:44:09.842: INFO: pod: "test-deployment-74c6dd549b-g4t9l":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-g4t9l test-deployment-74c6dd549b- deployment-6058  fcf9837b-a62c-46cc-bd22-9f644a6cc85e 36617 0 2023-01-10 07:44:06 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[cni.projectcalico.org/containerID:4831e4387201685f894ddfe2841da9ae86a849dc40a72628e4bffc7e70aa053b cni.projectcalico.org/podIP:10.233.82.19/32 cni.projectcalico.org/podIPs:10.233.82.19/32] [{apps/v1 ReplicaSet test-deployment-74c6dd549b 00a220e5-4596-41b9-9f54-89fb3f7c801a 0xc005924487 0xc005924488}] []  [{kube-controller-manager Update v1 2023-01-10 07:44:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00a220e5-4596-41b9-9f54-89fb3f7c801a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-10 07:44:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-10 07:44:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.82.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kcfh2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kcfh2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-r65pm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.233.82.19,StartTime:2023-01-10 07:44:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:44:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e4aef4d00512fda0f339f1c13ca9e104b623d37a24055163287b60146ce2d862,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.82.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 10 07:44:09.842: INFO: pod: "test-deployment-74c6dd549b-pts5w":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-pts5w test-deployment-74c6dd549b- deployment-6058  38e57498-7311-485b-b606-c5349eb8632a 36658 0 2023-01-10 07:44:08 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[cni.projectcalico.org/containerID:4c272a13c8f937647a1adf4db9cd9d35fea69889f9cff1291b06bf772440c080 cni.projectcalico.org/podIP:10.233.107.106/32 cni.projectcalico.org/podIPs:10.233.107.106/32] [{apps/v1 ReplicaSet test-deployment-74c6dd549b 00a220e5-4596-41b9-9f54-89fb3f7c801a 0xc0059246d7 0xc0059246d8}] []  [{kube-controller-manager Update v1 2023-01-10 07:44:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00a220e5-4596-41b9-9f54-89fb3f7c801a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-10 07:44:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-10 07:44:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.107.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7tshx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7tshx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-vgmmg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.233.107.106,StartTime:2023-01-10 07:44:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:44:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://82e68ff3fae65ca750ddc41053206a94cb671b603e066be0229f810534692f99,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.107.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 10 07:44:09.842: INFO: ReplicaSet "test-deployment-84b949bdfc":
&ReplicaSet{ObjectMeta:{test-deployment-84b949bdfc  deployment-6058  76cb4ebe-0f1a-43d4-b7be-a5ae288521cd 36667 4 2023-01-10 07:44:05 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 18acad51-0223-44ae-8185-29631aadba3f 0xc005924047 0xc005924048}] []  [{kube-controller-manager Update apps/v1 2023-01-10 07:44:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18acad51-0223-44ae-8185-29631aadba3f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:44:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 84b949bdfc,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.7 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059240e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 10 07:44:09.850: INFO: pod: "test-deployment-84b949bdfc-bk72l":
&Pod{ObjectMeta:{test-deployment-84b949bdfc-bk72l test-deployment-84b949bdfc- deployment-6058  4367db13-7f52-4d43-ba1e-20b08ae32e2a 36625 0 2023-01-10 07:44:06 +0000 UTC 2023-01-10 07:44:09 +0000 UTC 0xc005925e88 map[pod-template-hash:84b949bdfc test-deployment-static:true] map[cni.projectcalico.org/containerID:5e25fda2e4e9093bfec8d0f2779e6140eef6c9206f71db36f00c8d26f06b8d95 cni.projectcalico.org/podIP:10.233.82.10/32 cni.projectcalico.org/podIPs:10.233.82.10/32] [{apps/v1 ReplicaSet test-deployment-84b949bdfc 76cb4ebe-0f1a-43d4-b7be-a5ae288521cd 0xc005925ee7 0xc005925ee8}] []  [{kube-controller-manager Update v1 2023-01-10 07:44:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76cb4ebe-0f1a-43d4-b7be-a5ae288521cd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-10 07:44:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-10 07:44:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.82.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-22zsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.7,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-22zsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-r65pm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.233.82.10,StartTime:2023-01-10 07:44:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:44:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.7,ImageID:k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c,ContainerID:containerd://ba1e3e2299eb8541da0f0d69ffb8ceeb0e34792d5dc4855b6859170b58b5c556,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.82.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 10 07:44:09.850: INFO: pod: "test-deployment-84b949bdfc-kpx29":
&Pod{ObjectMeta:{test-deployment-84b949bdfc-kpx29 test-deployment-84b949bdfc- deployment-6058  21ca4424-18d2-43d1-8df8-0fd907a7ee59 36663 0 2023-01-10 07:44:05 +0000 UTC 2023-01-10 07:44:10 +0000 UTC 0xc004cfc110 map[pod-template-hash:84b949bdfc test-deployment-static:true] map[cni.projectcalico.org/containerID:6e6fe8bb95904c98367feb6c8d0e30022821369abed3e409d6d939b5d96269b6 cni.projectcalico.org/podIP:10.233.107.117/32 cni.projectcalico.org/podIPs:10.233.107.117/32] [{apps/v1 ReplicaSet test-deployment-84b949bdfc 76cb4ebe-0f1a-43d4-b7be-a5ae288521cd 0xc004cfc147 0xc004cfc148}] []  [{kube-controller-manager Update v1 2023-01-10 07:44:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76cb4ebe-0f1a-43d4-b7be-a5ae288521cd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-10 07:44:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-10 07:44:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.107.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8r8zd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.7,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8r8zd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-vgmmg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:44:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:10.233.107.117,StartTime:2023-01-10 07:44:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 07:44:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.7,ImageID:k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c,ContainerID:containerd://e7e418e820be7885c3094bb7a836ce818010b78cfa04eb44e2a8fec60dd94cb8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.107.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 10 07:44:09.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6058" for this suite.

• [SLOW TEST:5.794 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":356,"completed":221,"skipped":4324,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:44:09.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 10 07:44:09.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2161" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":222,"skipped":4334,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:44:09.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Jan 10 07:44:09.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:44:31.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9532" for this suite.

• [SLOW TEST:21.522 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":356,"completed":223,"skipped":4342,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:44:31.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 07:44:31.982: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 07:44:35.008: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:44:35.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5928" for this suite.
STEP: Destroying namespace "webhook-5928-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":356,"completed":224,"skipped":4353,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:44:35.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 10 07:44:35.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6189 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 10 07:44:35.265: INFO: stderr: ""
Jan 10 07:44:35.265: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jan 10 07:44:35.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6189 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 10 07:44:36.041: INFO: stderr: ""
Jan 10 07:44:36.041: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 10 07:44:36.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6189 delete pods e2e-test-httpd-pod'
Jan 10 07:44:37.548: INFO: stderr: ""
Jan 10 07:44:37.548: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 07:44:37.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6189" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":356,"completed":225,"skipped":4387,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:44:37.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with configMap that has name projected-configmap-test-upd-7d0bdf70-ce48-4c56-8567-94060dbea6cf
STEP: Creating the pod
Jan 10 07:44:37.634: INFO: The status of Pod pod-projected-configmaps-4aa53d60-60aa-434d-ab42-f16d043fd260 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 07:44:39.645: INFO: The status of Pod pod-projected-configmaps-4aa53d60-60aa-434d-ab42-f16d043fd260 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-7d0bdf70-ce48-4c56-8567-94060dbea6cf
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 10 07:44:41.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5671" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":226,"skipped":4407,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:44:41.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-secret-b27g
STEP: Creating a pod to test atomic-volume-subpath
Jan 10 07:44:41.741: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-b27g" in namespace "subpath-1072" to be "Succeeded or Failed"
Jan 10 07:44:41.749: INFO: Pod "pod-subpath-test-secret-b27g": Phase="Pending", Reason="", readiness=false. Elapsed: 8.29879ms
Jan 10 07:44:43.759: INFO: Pod "pod-subpath-test-secret-b27g": Phase="Running", Reason="", readiness=true. Elapsed: 2.017714749s
Jan 10 07:44:45.766: INFO: Pod "pod-subpath-test-secret-b27g": Phase="Running", Reason="", readiness=true. Elapsed: 4.024576769s
Jan 10 07:44:47.772: INFO: Pod "pod-subpath-test-secret-b27g": Phase="Running", Reason="", readiness=true. Elapsed: 6.030798951s
Jan 10 07:44:49.777: INFO: Pod "pod-subpath-test-secret-b27g": Phase="Running", Reason="", readiness=true. Elapsed: 8.03601878s
Jan 10 07:44:51.785: INFO: Pod "pod-subpath-test-secret-b27g": Phase="Running", Reason="", readiness=true. Elapsed: 10.04347941s
Jan 10 07:44:53.791: INFO: Pod "pod-subpath-test-secret-b27g": Phase="Running", Reason="", readiness=true. Elapsed: 12.050012561s
Jan 10 07:44:55.798: INFO: Pod "pod-subpath-test-secret-b27g": Phase="Running", Reason="", readiness=true. Elapsed: 14.057223267s
Jan 10 07:44:57.812: INFO: Pod "pod-subpath-test-secret-b27g": Phase="Running", Reason="", readiness=true. Elapsed: 16.070527882s
Jan 10 07:44:59.817: INFO: Pod "pod-subpath-test-secret-b27g": Phase="Running", Reason="", readiness=true. Elapsed: 18.075885392s
Jan 10 07:45:01.826: INFO: Pod "pod-subpath-test-secret-b27g": Phase="Running", Reason="", readiness=true. Elapsed: 20.085175599s
Jan 10 07:45:03.838: INFO: Pod "pod-subpath-test-secret-b27g": Phase="Running", Reason="", readiness=false. Elapsed: 22.097100965s
Jan 10 07:45:05.846: INFO: Pod "pod-subpath-test-secret-b27g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.10511326s
STEP: Saw pod success
Jan 10 07:45:05.846: INFO: Pod "pod-subpath-test-secret-b27g" satisfied condition "Succeeded or Failed"
Jan 10 07:45:05.849: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-subpath-test-secret-b27g container test-container-subpath-secret-b27g: <nil>
STEP: delete the pod
Jan 10 07:45:05.868: INFO: Waiting for pod pod-subpath-test-secret-b27g to disappear
Jan 10 07:45:05.871: INFO: Pod pod-subpath-test-secret-b27g no longer exists
STEP: Deleting pod pod-subpath-test-secret-b27g
Jan 10 07:45:05.871: INFO: Deleting pod "pod-subpath-test-secret-b27g" in namespace "subpath-1072"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 10 07:45:05.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1072" for this suite.

• [SLOW TEST:24.186 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","total":356,"completed":227,"skipped":4433,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:45:05.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-08716274-334e-49e4-838e-17a55061ee13
STEP: Creating a pod to test consume secrets
Jan 10 07:45:05.936: INFO: Waiting up to 5m0s for pod "pod-secrets-d4fb56c2-79a6-43b3-bfa3-616762292efb" in namespace "secrets-3336" to be "Succeeded or Failed"
Jan 10 07:45:05.941: INFO: Pod "pod-secrets-d4fb56c2-79a6-43b3-bfa3-616762292efb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.509448ms
Jan 10 07:45:07.951: INFO: Pod "pod-secrets-d4fb56c2-79a6-43b3-bfa3-616762292efb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014921142s
Jan 10 07:45:09.956: INFO: Pod "pod-secrets-d4fb56c2-79a6-43b3-bfa3-616762292efb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019798378s
STEP: Saw pod success
Jan 10 07:45:09.956: INFO: Pod "pod-secrets-d4fb56c2-79a6-43b3-bfa3-616762292efb" satisfied condition "Succeeded or Failed"
Jan 10 07:45:09.958: INFO: Trying to get logs from node kk-instance-r65pm pod pod-secrets-d4fb56c2-79a6-43b3-bfa3-616762292efb container secret-volume-test: <nil>
STEP: delete the pod
Jan 10 07:45:09.975: INFO: Waiting for pod pod-secrets-d4fb56c2-79a6-43b3-bfa3-616762292efb to disappear
Jan 10 07:45:09.979: INFO: Pod pod-secrets-d4fb56c2-79a6-43b3-bfa3-616762292efb no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 10 07:45:09.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3336" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":228,"skipped":4460,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:45:09.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jan 10 07:45:14.548: INFO: Successfully updated pod "adopt-release-7npms"
STEP: Checking that the Job readopts the Pod
Jan 10 07:45:14.548: INFO: Waiting up to 15m0s for pod "adopt-release-7npms" in namespace "job-2677" to be "adopted"
Jan 10 07:45:14.559: INFO: Pod "adopt-release-7npms": Phase="Running", Reason="", readiness=true. Elapsed: 10.500434ms
Jan 10 07:45:16.568: INFO: Pod "adopt-release-7npms": Phase="Running", Reason="", readiness=true. Elapsed: 2.019932923s
Jan 10 07:45:16.568: INFO: Pod "adopt-release-7npms" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jan 10 07:45:17.085: INFO: Successfully updated pod "adopt-release-7npms"
STEP: Checking that the Job releases the Pod
Jan 10 07:45:17.088: INFO: Waiting up to 15m0s for pod "adopt-release-7npms" in namespace "job-2677" to be "released"
Jan 10 07:45:17.090: INFO: Pod "adopt-release-7npms": Phase="Running", Reason="", readiness=true. Elapsed: 2.408749ms
Jan 10 07:45:19.101: INFO: Pod "adopt-release-7npms": Phase="Running", Reason="", readiness=true. Elapsed: 2.013384772s
Jan 10 07:45:19.101: INFO: Pod "adopt-release-7npms" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 10 07:45:19.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2677" for this suite.

• [SLOW TEST:9.122 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":356,"completed":229,"skipped":4483,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:45:19.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7531.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7531.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7531.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7531.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7531.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7531.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7531.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7531.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 10 07:45:21.185: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:21.188: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:21.191: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:21.194: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:21.197: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:21.200: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:21.203: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:21.207: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:21.207: INFO: Lookups using dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7531.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7531.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local jessie_udp@dns-test-service-2.dns-7531.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7531.svc.cluster.local]

Jan 10 07:45:26.214: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:26.217: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:26.220: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:26.223: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:26.226: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:26.231: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:26.234: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:26.237: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:26.237: INFO: Lookups using dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7531.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7531.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local jessie_udp@dns-test-service-2.dns-7531.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7531.svc.cluster.local]

Jan 10 07:45:31.212: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:31.216: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:31.218: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:31.221: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:31.224: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:31.228: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:31.231: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:31.234: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:31.234: INFO: Lookups using dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7531.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7531.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local jessie_udp@dns-test-service-2.dns-7531.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7531.svc.cluster.local]

Jan 10 07:45:36.212: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:36.216: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:36.219: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:36.222: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:36.225: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:36.228: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:36.231: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:36.234: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:36.234: INFO: Lookups using dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7531.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7531.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local jessie_udp@dns-test-service-2.dns-7531.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7531.svc.cluster.local]

Jan 10 07:45:41.211: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:41.215: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:41.217: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:41.220: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:41.223: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:41.227: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:41.232: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:41.235: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:41.235: INFO: Lookups using dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7531.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7531.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local jessie_udp@dns-test-service-2.dns-7531.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7531.svc.cluster.local]

Jan 10 07:45:46.213: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:46.216: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:46.219: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:46.222: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:46.225: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:46.227: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:46.232: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:46.235: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7531.svc.cluster.local from pod dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531: the server could not find the requested resource (get pods dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531)
Jan 10 07:45:46.235: INFO: Lookups using dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7531.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7531.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7531.svc.cluster.local jessie_udp@dns-test-service-2.dns-7531.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7531.svc.cluster.local]

Jan 10 07:45:51.233: INFO: DNS probes using dns-7531/dns-test-c2b91e8a-09ce-4b00-a2ec-30ec0bbe7531 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 10 07:45:51.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7531" for this suite.

• [SLOW TEST:32.194 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":356,"completed":230,"skipped":4511,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:45:51.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:188
Jan 10 07:45:51.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-726" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":356,"completed":231,"skipped":4529,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:45:51.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 10 07:45:53.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4759" for this suite.
•{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":356,"completed":232,"skipped":4544,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:45:53.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-112
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-112
STEP: creating replication controller externalsvc in namespace services-112
I0110 07:45:53.502714      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-112, replica count: 2
I0110 07:45:56.554580      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jan 10 07:45:56.581: INFO: Creating new exec pod
Jan 10 07:45:58.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-112 exec execpod6524t -- /bin/sh -x -c nslookup clusterip-service.services-112.svc.cluster.local'
Jan 10 07:45:58.809: INFO: stderr: "+ nslookup clusterip-service.services-112.svc.cluster.local\n"
Jan 10 07:45:58.810: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-112.svc.cluster.local\tcanonical name = externalsvc.services-112.svc.cluster.local.\nName:\texternalsvc.services-112.svc.cluster.local\nAddress: 10.233.20.44\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-112, will wait for the garbage collector to delete the pods
Jan 10 07:45:58.871: INFO: Deleting ReplicationController externalsvc took: 7.993177ms
Jan 10 07:45:58.972: INFO: Terminating ReplicationController externalsvc pods took: 100.17219ms
Jan 10 07:46:01.200: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 07:46:01.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-112" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:7.793 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":356,"completed":233,"skipped":4574,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:46:01.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 07:46:02.057: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 07:46:05.083: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:46:15.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2735" for this suite.
STEP: Destroying namespace "webhook-2735-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:14.071 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":356,"completed":234,"skipped":4602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:46:15.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Jan 10 07:46:15.333: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:46:33.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7840" for this suite.

• [SLOW TEST:18.259 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":356,"completed":235,"skipped":4625,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:46:33.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 10 07:46:33.625: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 10 07:46:33.629: INFO: starting watch
STEP: patching
STEP: updating
Jan 10 07:46:33.646: INFO: waiting for watch events with expected annotations
Jan 10 07:46:33.646: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 10 07:46:33.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9641" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":356,"completed":236,"skipped":4685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:46:33.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-16db6784-8de0-410f-86d2-d954c7f2182e
STEP: Creating a pod to test consume secrets
Jan 10 07:46:33.714: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-112ef980-0c4a-4a88-a45a-783442c007bd" in namespace "projected-4444" to be "Succeeded or Failed"
Jan 10 07:46:33.722: INFO: Pod "pod-projected-secrets-112ef980-0c4a-4a88-a45a-783442c007bd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.291956ms
Jan 10 07:46:35.729: INFO: Pod "pod-projected-secrets-112ef980-0c4a-4a88-a45a-783442c007bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014725311s
Jan 10 07:46:37.743: INFO: Pod "pod-projected-secrets-112ef980-0c4a-4a88-a45a-783442c007bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028530123s
STEP: Saw pod success
Jan 10 07:46:37.743: INFO: Pod "pod-projected-secrets-112ef980-0c4a-4a88-a45a-783442c007bd" satisfied condition "Succeeded or Failed"
Jan 10 07:46:37.748: INFO: Trying to get logs from node kk-instance-r65pm pod pod-projected-secrets-112ef980-0c4a-4a88-a45a-783442c007bd container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 10 07:46:37.766: INFO: Waiting for pod pod-projected-secrets-112ef980-0c4a-4a88-a45a-783442c007bd to disappear
Jan 10 07:46:37.768: INFO: Pod pod-projected-secrets-112ef980-0c4a-4a88-a45a-783442c007bd no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 10 07:46:37.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4444" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":237,"skipped":4714,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:46:37.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 07:46:38.192: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 07:46:41.215: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:46:41.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6887-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:46:44.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6898" for this suite.
STEP: Destroying namespace "webhook-6898-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.637 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":356,"completed":238,"skipped":4715,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:46:44.421: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:188
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 10 07:46:44.530: INFO: starting watch
STEP: patching
STEP: updating
Jan 10 07:46:44.548: INFO: waiting for watch events with expected annotations
Jan 10 07:46:44.549: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:188
Jan 10 07:46:44.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7952" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":356,"completed":239,"skipped":4731,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:46:44.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7896
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-7896
I0110 07:46:44.645065      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7896, replica count: 2
I0110 07:46:47.697650      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 10 07:46:47.697: INFO: Creating new exec pod
Jan 10 07:46:50.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7896 exec execpodvkxgq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 10 07:46:50.884: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 10 07:46:50.884: INFO: stdout: "externalname-service-wr2bs"
Jan 10 07:46:50.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7896 exec execpodvkxgq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.16.91 80'
Jan 10 07:46:51.025: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.16.91 80\nConnection to 10.233.16.91 80 port [tcp/http] succeeded!\n"
Jan 10 07:46:51.025: INFO: stdout: ""
Jan 10 07:46:52.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7896 exec execpodvkxgq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.16.91 80'
Jan 10 07:46:52.184: INFO: stderr: "+ + echonc hostName\n -v -t -w 2 10.233.16.91 80\nConnection to 10.233.16.91 80 port [tcp/http] succeeded!\n"
Jan 10 07:46:52.184: INFO: stdout: ""
Jan 10 07:46:53.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7896 exec execpodvkxgq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.16.91 80'
Jan 10 07:46:53.192: INFO: stderr: "+ nc -v -t -w 2 10.233.16.91 80\n+ echo hostName\nConnection to 10.233.16.91 80 port [tcp/http] succeeded!\n"
Jan 10 07:46:53.192: INFO: stdout: ""
Jan 10 07:46:54.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-7896 exec execpodvkxgq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.16.91 80'
Jan 10 07:46:54.179: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.16.91 80\nConnection to 10.233.16.91 80 port [tcp/http] succeeded!\n"
Jan 10 07:46:54.179: INFO: stdout: "externalname-service-wr2bs"
Jan 10 07:46:54.179: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 07:46:54.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7896" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.648 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":356,"completed":240,"skipped":4732,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:46:54.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8365
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8365
STEP: Waiting until pod test-pod will start running in namespace statefulset-8365
STEP: Creating statefulset with conflicting port in namespace statefulset-8365
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8365
Jan 10 07:46:56.308: INFO: Observed stateful pod in namespace: statefulset-8365, name: ss-0, uid: 56cc3929-83bd-462c-acff-3891edd7eff1, status phase: Pending. Waiting for statefulset controller to delete.
Jan 10 07:46:56.327: INFO: Observed stateful pod in namespace: statefulset-8365, name: ss-0, uid: 56cc3929-83bd-462c-acff-3891edd7eff1, status phase: Failed. Waiting for statefulset controller to delete.
Jan 10 07:46:56.336: INFO: Observed stateful pod in namespace: statefulset-8365, name: ss-0, uid: 56cc3929-83bd-462c-acff-3891edd7eff1, status phase: Failed. Waiting for statefulset controller to delete.
Jan 10 07:46:56.340: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8365
STEP: Removing pod with conflicting port in namespace statefulset-8365
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8365 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 10 07:46:58.382: INFO: Deleting all statefulset in ns statefulset-8365
Jan 10 07:46:58.385: INFO: Scaling statefulset ss to 0
Jan 10 07:47:08.404: INFO: Waiting for statefulset status.replicas updated to 0
Jan 10 07:47:08.406: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 10 07:47:08.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8365" for this suite.

• [SLOW TEST:14.205 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":356,"completed":241,"skipped":4745,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:47:08.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 10 07:47:08.504: INFO: Waiting up to 5m0s for pod "downward-api-76daff5a-2f41-49a1-92e3-e7e08bf1a000" in namespace "downward-api-4363" to be "Succeeded or Failed"
Jan 10 07:47:08.510: INFO: Pod "downward-api-76daff5a-2f41-49a1-92e3-e7e08bf1a000": Phase="Pending", Reason="", readiness=false. Elapsed: 5.305942ms
Jan 10 07:47:10.518: INFO: Pod "downward-api-76daff5a-2f41-49a1-92e3-e7e08bf1a000": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013183452s
Jan 10 07:47:12.525: INFO: Pod "downward-api-76daff5a-2f41-49a1-92e3-e7e08bf1a000": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020872364s
STEP: Saw pod success
Jan 10 07:47:12.526: INFO: Pod "downward-api-76daff5a-2f41-49a1-92e3-e7e08bf1a000" satisfied condition "Succeeded or Failed"
Jan 10 07:47:12.534: INFO: Trying to get logs from node kk-instance-r65pm pod downward-api-76daff5a-2f41-49a1-92e3-e7e08bf1a000 container dapi-container: <nil>
STEP: delete the pod
Jan 10 07:47:12.580: INFO: Waiting for pod downward-api-76daff5a-2f41-49a1-92e3-e7e08bf1a000 to disappear
Jan 10 07:47:12.589: INFO: Pod downward-api-76daff5a-2f41-49a1-92e3-e7e08bf1a000 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 10 07:47:12.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4363" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":356,"completed":242,"skipped":4782,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:47:12.612: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-130
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-130
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-130
Jan 10 07:47:12.682: INFO: Found 0 stateful pods, waiting for 1
Jan 10 07:47:22.688: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 10 07:47:22.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-130 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 10 07:47:22.855: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 10 07:47:22.855: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 10 07:47:22.855: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 10 07:47:22.860: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 10 07:47:32.868: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 10 07:47:32.868: INFO: Waiting for statefulset status.replicas updated to 0
Jan 10 07:47:32.897: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999461s
Jan 10 07:47:33.905: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.984338951s
Jan 10 07:47:34.910: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.97741298s
Jan 10 07:47:35.919: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.972090325s
Jan 10 07:47:36.925: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.963209561s
Jan 10 07:47:37.933: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.956461284s
Jan 10 07:47:38.941: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.949114531s
Jan 10 07:47:39.947: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.940349874s
Jan 10 07:47:40.952: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.935369246s
Jan 10 07:47:41.960: INFO: Verifying statefulset ss doesn't scale past 1 for another 929.440571ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-130
Jan 10 07:47:42.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-130 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 10 07:47:43.143: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 10 07:47:43.143: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 10 07:47:43.143: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 10 07:47:43.148: INFO: Found 1 stateful pods, waiting for 3
Jan 10 07:47:53.154: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 10 07:47:53.154: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 10 07:47:53.154: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 10 07:47:53.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-130 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 10 07:47:53.333: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 10 07:47:53.333: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 10 07:47:53.333: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 10 07:47:53.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-130 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 10 07:47:53.483: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 10 07:47:53.483: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 10 07:47:53.483: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 10 07:47:53.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-130 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 10 07:47:53.634: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 10 07:47:53.634: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 10 07:47:53.634: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 10 07:47:53.634: INFO: Waiting for statefulset status.replicas updated to 0
Jan 10 07:47:53.638: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 10 07:48:03.650: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 10 07:48:03.650: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 10 07:48:03.650: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 10 07:48:03.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999769s
Jan 10 07:48:04.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989031154s
Jan 10 07:48:05.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.976433428s
Jan 10 07:48:06.700: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.96610946s
Jan 10 07:48:07.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.959484889s
Jan 10 07:48:08.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.951290108s
Jan 10 07:48:09.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.941387388s
Jan 10 07:48:10.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.936753334s
Jan 10 07:48:11.740: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.925816468s
Jan 10 07:48:12.746: INFO: Verifying statefulset ss doesn't scale past 3 for another 919.001918ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-130
Jan 10 07:48:13.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-130 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 10 07:48:13.909: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 10 07:48:13.909: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 10 07:48:13.909: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 10 07:48:13.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-130 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 10 07:48:14.055: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 10 07:48:14.055: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 10 07:48:14.055: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 10 07:48:14.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-130 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 10 07:48:14.214: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 10 07:48:14.214: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 10 07:48:14.214: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 10 07:48:14.214: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 10 07:48:24.246: INFO: Deleting all statefulset in ns statefulset-130
Jan 10 07:48:24.248: INFO: Scaling statefulset ss to 0
Jan 10 07:48:24.259: INFO: Waiting for statefulset status.replicas updated to 0
Jan 10 07:48:24.262: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 10 07:48:24.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-130" for this suite.

• [SLOW TEST:71.678 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":356,"completed":243,"skipped":4801,"failed":0}
SSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:48:24.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-a00deaf5-5c37-4033-9a19-09424eaf8f06
STEP: Creating a pod to test consume secrets
Jan 10 07:48:24.332: INFO: Waiting up to 5m0s for pod "pod-secrets-36a6bc6a-2e8e-4d48-8963-21003445bb7c" in namespace "secrets-7810" to be "Succeeded or Failed"
Jan 10 07:48:24.340: INFO: Pod "pod-secrets-36a6bc6a-2e8e-4d48-8963-21003445bb7c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033519ms
Jan 10 07:48:26.347: INFO: Pod "pod-secrets-36a6bc6a-2e8e-4d48-8963-21003445bb7c": Phase="Running", Reason="", readiness=false. Elapsed: 2.014719155s
Jan 10 07:48:28.362: INFO: Pod "pod-secrets-36a6bc6a-2e8e-4d48-8963-21003445bb7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030292372s
STEP: Saw pod success
Jan 10 07:48:28.362: INFO: Pod "pod-secrets-36a6bc6a-2e8e-4d48-8963-21003445bb7c" satisfied condition "Succeeded or Failed"
Jan 10 07:48:28.373: INFO: Trying to get logs from node kk-instance-r65pm pod pod-secrets-36a6bc6a-2e8e-4d48-8963-21003445bb7c container secret-env-test: <nil>
STEP: delete the pod
Jan 10 07:48:28.409: INFO: Waiting for pod pod-secrets-36a6bc6a-2e8e-4d48-8963-21003445bb7c to disappear
Jan 10 07:48:28.412: INFO: Pod pod-secrets-36a6bc6a-2e8e-4d48-8963-21003445bb7c no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 10 07:48:28.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7810" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":356,"completed":244,"skipped":4805,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:48:28.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:48:28.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties
Jan 10 07:48:33.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-6833 --namespace=crd-publish-openapi-6833 create -f -'
Jan 10 07:48:33.980: INFO: stderr: ""
Jan 10 07:48:33.980: INFO: stdout: "e2e-test-crd-publish-openapi-3958-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 10 07:48:33.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-6833 --namespace=crd-publish-openapi-6833 delete e2e-test-crd-publish-openapi-3958-crds test-foo'
Jan 10 07:48:34.054: INFO: stderr: ""
Jan 10 07:48:34.054: INFO: stdout: "e2e-test-crd-publish-openapi-3958-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 10 07:48:34.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-6833 --namespace=crd-publish-openapi-6833 apply -f -'
Jan 10 07:48:34.276: INFO: stderr: ""
Jan 10 07:48:34.276: INFO: stdout: "e2e-test-crd-publish-openapi-3958-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 10 07:48:34.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-6833 --namespace=crd-publish-openapi-6833 delete e2e-test-crd-publish-openapi-3958-crds test-foo'
Jan 10 07:48:34.349: INFO: stderr: ""
Jan 10 07:48:34.349: INFO: stdout: "e2e-test-crd-publish-openapi-3958-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values
Jan 10 07:48:34.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-6833 --namespace=crd-publish-openapi-6833 create -f -'
Jan 10 07:48:34.545: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jan 10 07:48:34.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-6833 --namespace=crd-publish-openapi-6833 create -f -'
Jan 10 07:48:34.737: INFO: rc: 1
Jan 10 07:48:34.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-6833 --namespace=crd-publish-openapi-6833 apply -f -'
Jan 10 07:48:34.934: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties
Jan 10 07:48:34.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-6833 --namespace=crd-publish-openapi-6833 create -f -'
Jan 10 07:48:35.163: INFO: rc: 1
Jan 10 07:48:35.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-6833 --namespace=crd-publish-openapi-6833 apply -f -'
Jan 10 07:48:35.383: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jan 10 07:48:35.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-6833 explain e2e-test-crd-publish-openapi-3958-crds'
Jan 10 07:48:35.582: INFO: stderr: ""
Jan 10 07:48:35.582: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3958-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jan 10 07:48:35.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-6833 explain e2e-test-crd-publish-openapi-3958-crds.metadata'
Jan 10 07:48:35.784: INFO: stderr: ""
Jan 10 07:48:35.784: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3958-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     Deprecated: ClusterName is a legacy field that was always cleared by the\n     system and never used; it will be removed completely in 1.25.\n\n     The name in the go struct is changed to help clients detect accidental use.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 10 07:48:35.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-6833 explain e2e-test-crd-publish-openapi-3958-crds.spec'
Jan 10 07:48:35.986: INFO: stderr: ""
Jan 10 07:48:35.986: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3958-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 10 07:48:35.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-6833 explain e2e-test-crd-publish-openapi-3958-crds.spec.bars'
Jan 10 07:48:36.201: INFO: stderr: ""
Jan 10 07:48:36.201: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3958-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jan 10 07:48:36.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=crd-publish-openapi-6833 explain e2e-test-crd-publish-openapi-3958-crds.spec.bars2'
Jan 10 07:48:36.405: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:48:39.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6833" for this suite.

• [SLOW TEST:10.671 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":356,"completed":245,"skipped":4831,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:48:39.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 07:48:39.712: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 07:48:42.739: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 07:48:42.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3761" for this suite.
STEP: Destroying namespace "webhook-3761-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":356,"completed":246,"skipped":4844,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:48:42.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-3955dda8-646c-49d7-9c70-fbd26c928eed
STEP: Creating a pod to test consume secrets
Jan 10 07:48:43.057: INFO: Waiting up to 5m0s for pod "pod-secrets-9d5997ae-0c09-4f0f-9311-5e292f5ae284" in namespace "secrets-4767" to be "Succeeded or Failed"
Jan 10 07:48:43.062: INFO: Pod "pod-secrets-9d5997ae-0c09-4f0f-9311-5e292f5ae284": Phase="Pending", Reason="", readiness=false. Elapsed: 4.136344ms
Jan 10 07:48:45.067: INFO: Pod "pod-secrets-9d5997ae-0c09-4f0f-9311-5e292f5ae284": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00965456s
Jan 10 07:48:47.077: INFO: Pod "pod-secrets-9d5997ae-0c09-4f0f-9311-5e292f5ae284": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019694463s
STEP: Saw pod success
Jan 10 07:48:47.077: INFO: Pod "pod-secrets-9d5997ae-0c09-4f0f-9311-5e292f5ae284" satisfied condition "Succeeded or Failed"
Jan 10 07:48:47.081: INFO: Trying to get logs from node kk-instance-r65pm pod pod-secrets-9d5997ae-0c09-4f0f-9311-5e292f5ae284 container secret-volume-test: <nil>
STEP: delete the pod
Jan 10 07:48:47.096: INFO: Waiting for pod pod-secrets-9d5997ae-0c09-4f0f-9311-5e292f5ae284 to disappear
Jan 10 07:48:47.098: INFO: Pod pod-secrets-9d5997ae-0c09-4f0f-9311-5e292f5ae284 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 10 07:48:47.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4767" for this suite.
STEP: Destroying namespace "secret-namespace-6382" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":356,"completed":247,"skipped":4850,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:48:47.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-9e928816-78c1-4c82-8a7d-d11f41892d7e
STEP: Creating a pod to test consume secrets
Jan 10 07:48:47.164: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9a5515b9-e1ff-4c69-b296-f9bb8ff4b2e6" in namespace "projected-3716" to be "Succeeded or Failed"
Jan 10 07:48:47.168: INFO: Pod "pod-projected-secrets-9a5515b9-e1ff-4c69-b296-f9bb8ff4b2e6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.433566ms
Jan 10 07:48:49.177: INFO: Pod "pod-projected-secrets-9a5515b9-e1ff-4c69-b296-f9bb8ff4b2e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012503976s
Jan 10 07:48:51.182: INFO: Pod "pod-projected-secrets-9a5515b9-e1ff-4c69-b296-f9bb8ff4b2e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017018813s
STEP: Saw pod success
Jan 10 07:48:51.182: INFO: Pod "pod-projected-secrets-9a5515b9-e1ff-4c69-b296-f9bb8ff4b2e6" satisfied condition "Succeeded or Failed"
Jan 10 07:48:51.185: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-projected-secrets-9a5515b9-e1ff-4c69-b296-f9bb8ff4b2e6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 10 07:48:51.213: INFO: Waiting for pod pod-projected-secrets-9a5515b9-e1ff-4c69-b296-f9bb8ff4b2e6 to disappear
Jan 10 07:48:51.217: INFO: Pod pod-projected-secrets-9a5515b9-e1ff-4c69-b296-f9bb8ff4b2e6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 10 07:48:51.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3716" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":248,"skipped":4866,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:48:51.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod with failed condition
STEP: updating the pod
Jan 10 07:50:51.789: INFO: Successfully updated pod "var-expansion-d372c547-dfb9-46da-9f89-e9c0b6cb5256"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jan 10 07:50:53.801: INFO: Deleting pod "var-expansion-d372c547-dfb9-46da-9f89-e9c0b6cb5256" in namespace "var-expansion-4981"
Jan 10 07:50:53.809: INFO: Wait up to 5m0s for pod "var-expansion-d372c547-dfb9-46da-9f89-e9c0b6cb5256" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 10 07:51:25.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4981" for this suite.

• [SLOW TEST:154.607 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":356,"completed":249,"skipped":4868,"failed":0}
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:51:25.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 10 07:51:28.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7835" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":356,"completed":250,"skipped":4868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:51:28.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 10 07:51:28.780: INFO: Waiting up to 5m0s for pod "pod-9d57a7e5-b29d-4c58-926d-63136ec7c862" in namespace "emptydir-1681" to be "Succeeded or Failed"
Jan 10 07:51:28.784: INFO: Pod "pod-9d57a7e5-b29d-4c58-926d-63136ec7c862": Phase="Pending", Reason="", readiness=false. Elapsed: 3.64233ms
Jan 10 07:51:30.792: INFO: Pod "pod-9d57a7e5-b29d-4c58-926d-63136ec7c862": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012167725s
Jan 10 07:51:32.805: INFO: Pod "pod-9d57a7e5-b29d-4c58-926d-63136ec7c862": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024518923s
STEP: Saw pod success
Jan 10 07:51:32.805: INFO: Pod "pod-9d57a7e5-b29d-4c58-926d-63136ec7c862" satisfied condition "Succeeded or Failed"
Jan 10 07:51:32.808: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-9d57a7e5-b29d-4c58-926d-63136ec7c862 container test-container: <nil>
STEP: delete the pod
Jan 10 07:51:32.834: INFO: Waiting for pod pod-9d57a7e5-b29d-4c58-926d-63136ec7c862 to disappear
Jan 10 07:51:32.837: INFO: Pod pod-9d57a7e5-b29d-4c58-926d-63136ec7c862 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 10 07:51:32.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1681" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":251,"skipped":4903,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:51:32.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-5e8d472b-a5e9-422a-a0dd-a37db3008bc9 in namespace container-probe-2091
Jan 10 07:51:34.886: INFO: Started pod busybox-5e8d472b-a5e9-422a-a0dd-a37db3008bc9 in namespace container-probe-2091
STEP: checking the pod's current state and verifying that restartCount is present
Jan 10 07:51:34.889: INFO: Initial restart count of pod busybox-5e8d472b-a5e9-422a-a0dd-a37db3008bc9 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 10 07:55:35.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2091" for this suite.

• [SLOW TEST:243.142 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":252,"skipped":4909,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:55:35.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 07:55:36.014: INFO: Creating deployment "test-recreate-deployment"
Jan 10 07:55:36.019: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 10 07:55:36.026: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 10 07:55:38.039: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 10 07:55:38.041: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 10 07:55:38.055: INFO: Updating deployment test-recreate-deployment
Jan 10 07:55:38.055: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 10 07:55:38.161: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1167  21ee0391-6c59-44ef-813c-0eea6943c276 40832 2 2023-01-10 07:55:36 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-01-10 07:55:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:55:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f51ea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-10 07:55:38 +0000 UTC,LastTransitionTime:2023-01-10 07:55:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cd8586fc7" is progressing.,LastUpdateTime:2023-01-10 07:55:38 +0000 UTC,LastTransitionTime:2023-01-10 07:55:36 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 10 07:55:38.165: INFO: New ReplicaSet "test-recreate-deployment-cd8586fc7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cd8586fc7  deployment-1167  b6641bd8-fccd-4747-92fa-f1b45683c0ce 40828 1 2023-01-10 07:55:38 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 21ee0391-6c59-44ef-813c-0eea6943c276 0xc0021b7410 0xc0021b7411}] []  [{kube-controller-manager Update apps/v1 2023-01-10 07:55:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"21ee0391-6c59-44ef-813c-0eea6943c276\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:55:38 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cd8586fc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0021b74c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 10 07:55:38.165: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 10 07:55:38.165: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-845d658455  deployment-1167  dd546abf-9a19-48e9-92e7-7dfc34080f3e 40820 2 2023-01-10 07:55:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 21ee0391-6c59-44ef-813c-0eea6943c276 0xc0021b72f7 0xc0021b72f8}] []  [{kube-controller-manager Update apps/v1 2023-01-10 07:55:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"21ee0391-6c59-44ef-813c-0eea6943c276\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 07:55:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 845d658455,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0021b73a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 10 07:55:38.169: INFO: Pod "test-recreate-deployment-cd8586fc7-772xg" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cd8586fc7-772xg test-recreate-deployment-cd8586fc7- deployment-1167  7a944f0b-afdc-4360-b32e-b496c8bc322d 40831 0 2023-01-10 07:55:38 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cd8586fc7 b6641bd8-fccd-4747-92fa-f1b45683c0ce 0xc0028d6280 0xc0028d6281}] []  [{kube-controller-manager Update v1 2023-01-10 07:55:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6641bd8-fccd-4747-92fa-f1b45683c0ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-10 07:55:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k5b8k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k5b8k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-vgmmg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:55:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:55:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:55:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 07:55:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:,StartTime:2023-01-10 07:55:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 10 07:55:38.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1167" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":253,"skipped":4928,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:55:38.191: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-9345, will wait for the garbage collector to delete the pods
Jan 10 07:55:40.289: INFO: Deleting Job.batch foo took: 8.910991ms
Jan 10 07:55:40.390: INFO: Terminating Job.batch foo pods took: 101.076546ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 10 07:56:12.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9345" for this suite.

• [SLOW TEST:34.538 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":356,"completed":254,"skipped":4973,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:56:12.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 10 07:56:12.766: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5864  fcfe0510-98b4-4af5-b48c-28306b0a906d 41043 0 2023-01-10 07:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-10 07:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 10 07:56:12.766: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5864  fcfe0510-98b4-4af5-b48c-28306b0a906d 41043 0 2023-01-10 07:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-10 07:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 10 07:56:12.773: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5864  fcfe0510-98b4-4af5-b48c-28306b0a906d 41044 0 2023-01-10 07:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-10 07:56:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 10 07:56:12.773: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5864  fcfe0510-98b4-4af5-b48c-28306b0a906d 41044 0 2023-01-10 07:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-10 07:56:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 10 07:56:12.782: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5864  fcfe0510-98b4-4af5-b48c-28306b0a906d 41045 0 2023-01-10 07:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-10 07:56:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 10 07:56:12.782: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5864  fcfe0510-98b4-4af5-b48c-28306b0a906d 41045 0 2023-01-10 07:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-10 07:56:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 10 07:56:12.788: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5864  fcfe0510-98b4-4af5-b48c-28306b0a906d 41046 0 2023-01-10 07:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-10 07:56:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 10 07:56:12.788: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5864  fcfe0510-98b4-4af5-b48c-28306b0a906d 41046 0 2023-01-10 07:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-10 07:56:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 10 07:56:12.792: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5864  ccc0ac77-01fe-4985-9a20-f4742c5deb9d 41047 0 2023-01-10 07:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-10 07:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 10 07:56:12.793: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5864  ccc0ac77-01fe-4985-9a20-f4742c5deb9d 41047 0 2023-01-10 07:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-10 07:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 10 07:56:22.809: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5864  ccc0ac77-01fe-4985-9a20-f4742c5deb9d 41095 0 2023-01-10 07:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-10 07:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 10 07:56:22.809: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5864  ccc0ac77-01fe-4985-9a20-f4742c5deb9d 41095 0 2023-01-10 07:56:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-10 07:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 10 07:56:32.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5864" for this suite.

• [SLOW TEST:20.101 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":356,"completed":255,"skipped":5001,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Job 
  should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:56:32.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensure pods equal to paralellism count is attached to the job
STEP: patching /status
STEP: updating /status
STEP: get /status
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 10 07:56:34.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2305" for this suite.
•{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","total":356,"completed":256,"skipped":5009,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:56:34.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 10 07:56:34.967: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 10 07:57:35.004: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Jan 10 07:57:35.025: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 10 07:57:35.039: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 10 07:57:35.073: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 10 07:57:35.086: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 10 07:57:41.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7761" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:66.291 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":356,"completed":257,"skipped":5043,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:57:41.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Jan 10 07:57:41.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-9399 create -f -'
Jan 10 07:57:41.956: INFO: stderr: ""
Jan 10 07:57:41.956: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 10 07:57:42.964: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 10 07:57:42.964: INFO: Found 0 / 1
Jan 10 07:57:43.965: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 10 07:57:43.965: INFO: Found 1 / 1
Jan 10 07:57:43.965: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 10 07:57:43.967: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 10 07:57:43.967: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 10 07:57:43.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-9399 patch pod agnhost-primary-kthzr -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 10 07:57:44.060: INFO: stderr: ""
Jan 10 07:57:44.060: INFO: stdout: "pod/agnhost-primary-kthzr patched\n"
STEP: checking annotations
Jan 10 07:57:44.064: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 10 07:57:44.064: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 07:57:44.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9399" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":356,"completed":258,"skipped":5055,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 07:57:44.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 10 07:57:44.110: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 10 07:57:44.116: INFO: Waiting for terminating namespaces to be deleted...
Jan 10 07:57:44.119: INFO: 
Logging pods the apiserver thinks is on node kk-instance-r65pm before test
Jan 10 07:57:44.124: INFO: calico-kube-controllers-6799f5f4b4-g6h9q from kube-system started at 2023-01-10 06:28:45 +0000 UTC (1 container statuses recorded)
Jan 10 07:57:44.124: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 10 07:57:44.124: INFO: calico-node-khhj9 from kube-system started at 2023-01-10 06:28:32 +0000 UTC (1 container statuses recorded)
Jan 10 07:57:44.124: INFO: 	Container calico-node ready: true, restart count 0
Jan 10 07:57:44.124: INFO: coredns-6d4b75cb6d-4wczh from kube-system started at 2023-01-10 06:28:45 +0000 UTC (1 container statuses recorded)
Jan 10 07:57:44.125: INFO: 	Container coredns ready: true, restart count 0
Jan 10 07:57:44.125: INFO: coredns-6d4b75cb6d-f7d8c from kube-system started at 2023-01-10 06:28:45 +0000 UTC (1 container statuses recorded)
Jan 10 07:57:44.125: INFO: 	Container coredns ready: true, restart count 0
Jan 10 07:57:44.125: INFO: kube-proxy-2v5cp from kube-system started at 2023-01-10 06:04:35 +0000 UTC (1 container statuses recorded)
Jan 10 07:57:44.125: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 10 07:57:44.125: INFO: agnhost-primary-kthzr from kubectl-9399 started at 2023-01-10 07:57:41 +0000 UTC (1 container statuses recorded)
Jan 10 07:57:44.125: INFO: 	Container agnhost-primary ready: true, restart count 0
Jan 10 07:57:44.125: INFO: pod0-1-sched-preemption-medium-priority from sched-preemption-7761 started at 2023-01-10 07:57:35 +0000 UTC (1 container statuses recorded)
Jan 10 07:57:44.125: INFO: 	Container pod0-1-sched-preemption-medium-priority ready: true, restart count 0
Jan 10 07:57:44.125: INFO: preemptor-pod from sched-preemption-7761 started at 2023-01-10 07:57:39 +0000 UTC (1 container statuses recorded)
Jan 10 07:57:44.125: INFO: 	Container preemptor-pod ready: true, restart count 0
Jan 10 07:57:44.125: INFO: sonobuoy-systemd-logs-daemon-set-5167b5d313ce4ad1-6zjp9 from sonobuoy started at 2023-01-10 06:56:07 +0000 UTC (2 container statuses recorded)
Jan 10 07:57:44.125: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 10 07:57:44.125: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 10 07:57:44.125: INFO: 
Logging pods the apiserver thinks is on node kk-instance-vgmmg before test
Jan 10 07:57:44.131: INFO: calico-node-wf8tr from kube-system started at 2023-01-10 06:42:17 +0000 UTC (1 container statuses recorded)
Jan 10 07:57:44.131: INFO: 	Container calico-node ready: true, restart count 0
Jan 10 07:57:44.131: INFO: kube-proxy-vrc4r from kube-system started at 2023-01-10 06:42:17 +0000 UTC (1 container statuses recorded)
Jan 10 07:57:44.131: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 10 07:57:44.132: INFO: pod1-0-sched-preemption-medium-priority from sched-preemption-7761 started at 2023-01-10 07:57:35 +0000 UTC (1 container statuses recorded)
Jan 10 07:57:44.132: INFO: 	Container pod1-0-sched-preemption-medium-priority ready: true, restart count 0
Jan 10 07:57:44.132: INFO: pod1-1-sched-preemption-medium-priority from sched-preemption-7761 started at 2023-01-10 07:57:35 +0000 UTC (1 container statuses recorded)
Jan 10 07:57:44.132: INFO: 	Container pod1-1-sched-preemption-medium-priority ready: true, restart count 0
Jan 10 07:57:44.132: INFO: sonobuoy from sonobuoy started at 2023-01-10 06:56:06 +0000 UTC (1 container statuses recorded)
Jan 10 07:57:44.132: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 10 07:57:44.132: INFO: sonobuoy-e2e-job-86252adb0f534a01 from sonobuoy started at 2023-01-10 06:56:07 +0000 UTC (2 container statuses recorded)
Jan 10 07:57:44.132: INFO: 	Container e2e ready: true, restart count 0
Jan 10 07:57:44.132: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 10 07:57:44.132: INFO: sonobuoy-systemd-logs-daemon-set-5167b5d313ce4ad1-smm8d from sonobuoy started at 2023-01-10 06:56:07 +0000 UTC (2 container statuses recorded)
Jan 10 07:57:44.132: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 10 07:57:44.132: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-df2a8ef9-7441-416a-a3ea-5b334cf7c211 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.0.5 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-df2a8ef9-7441-416a-a3ea-5b334cf7c211 off the node kk-instance-vgmmg
STEP: verifying the node doesn't have the label kubernetes.io/e2e-df2a8ef9-7441-416a-a3ea-5b334cf7c211
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 10 08:02:48.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2576" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:304.236 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":356,"completed":259,"skipped":5101,"failed":0}
SSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:02:48.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in volume subpath
Jan 10 08:02:48.392: INFO: Waiting up to 5m0s for pod "var-expansion-6cdf0576-c00c-47c6-9d3b-2fd994f1cd8c" in namespace "var-expansion-2951" to be "Succeeded or Failed"
Jan 10 08:02:48.398: INFO: Pod "var-expansion-6cdf0576-c00c-47c6-9d3b-2fd994f1cd8c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.888192ms
Jan 10 08:02:50.410: INFO: Pod "var-expansion-6cdf0576-c00c-47c6-9d3b-2fd994f1cd8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018691774s
Jan 10 08:02:52.417: INFO: Pod "var-expansion-6cdf0576-c00c-47c6-9d3b-2fd994f1cd8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025621781s
STEP: Saw pod success
Jan 10 08:02:52.418: INFO: Pod "var-expansion-6cdf0576-c00c-47c6-9d3b-2fd994f1cd8c" satisfied condition "Succeeded or Failed"
Jan 10 08:02:52.421: INFO: Trying to get logs from node kk-instance-r65pm pod var-expansion-6cdf0576-c00c-47c6-9d3b-2fd994f1cd8c container dapi-container: <nil>
STEP: delete the pod
Jan 10 08:02:52.456: INFO: Waiting for pod var-expansion-6cdf0576-c00c-47c6-9d3b-2fd994f1cd8c to disappear
Jan 10 08:02:52.459: INFO: Pod var-expansion-6cdf0576-c00c-47c6-9d3b-2fd994f1cd8c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 10 08:02:52.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2951" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":356,"completed":260,"skipped":5104,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:02:52.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: reading a file in the container
Jan 10 08:02:54.558: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8928 pod-service-account-3a8c3d24-1ef1-47bb-ba6b-23e568934d04 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan 10 08:02:54.712: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8928 pod-service-account-3a8c3d24-1ef1-47bb-ba6b-23e568934d04 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan 10 08:02:54.859: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8928 pod-service-account-3a8c3d24-1ef1-47bb-ba6b-23e568934d04 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 10 08:02:55.007: INFO: Got root ca configmap in namespace "svcaccounts-8928"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 10 08:02:55.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8928" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":356,"completed":261,"skipped":5108,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:02:55.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 08:02:55.814: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 08:02:58.838: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jan 10 08:02:58.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 08:02:58.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9939" for this suite.
STEP: Destroying namespace "webhook-9939-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":356,"completed":262,"skipped":5112,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:02:58.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 08:02:59.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e3165ef-445a-41b6-95ae-bd0117bf9897" in namespace "projected-8605" to be "Succeeded or Failed"
Jan 10 08:02:59.023: INFO: Pod "downwardapi-volume-2e3165ef-445a-41b6-95ae-bd0117bf9897": Phase="Pending", Reason="", readiness=false. Elapsed: 4.481208ms
Jan 10 08:03:01.032: INFO: Pod "downwardapi-volume-2e3165ef-445a-41b6-95ae-bd0117bf9897": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013728301s
Jan 10 08:03:03.040: INFO: Pod "downwardapi-volume-2e3165ef-445a-41b6-95ae-bd0117bf9897": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021688405s
STEP: Saw pod success
Jan 10 08:03:03.040: INFO: Pod "downwardapi-volume-2e3165ef-445a-41b6-95ae-bd0117bf9897" satisfied condition "Succeeded or Failed"
Jan 10 08:03:03.044: INFO: Trying to get logs from node kk-instance-vgmmg pod downwardapi-volume-2e3165ef-445a-41b6-95ae-bd0117bf9897 container client-container: <nil>
STEP: delete the pod
Jan 10 08:03:03.076: INFO: Waiting for pod downwardapi-volume-2e3165ef-445a-41b6-95ae-bd0117bf9897 to disappear
Jan 10 08:03:03.079: INFO: Pod downwardapi-volume-2e3165ef-445a-41b6-95ae-bd0117bf9897 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 10 08:03:03.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8605" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":263,"skipped":5113,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:03:03.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-43126efe-56c7-41d4-8fb3-465bb3df8f57
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 10 08:03:05.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6643" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":264,"skipped":5116,"failed":0}
SSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:03:05.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:03:05.225: INFO: Waiting up to 5m0s for pod "busybox-user-65534-45e4987d-e7cc-4069-8e81-a28d5be8f2e4" in namespace "security-context-test-2881" to be "Succeeded or Failed"
Jan 10 08:03:05.230: INFO: Pod "busybox-user-65534-45e4987d-e7cc-4069-8e81-a28d5be8f2e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.432079ms
Jan 10 08:03:07.237: INFO: Pod "busybox-user-65534-45e4987d-e7cc-4069-8e81-a28d5be8f2e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011022811s
Jan 10 08:03:09.248: INFO: Pod "busybox-user-65534-45e4987d-e7cc-4069-8e81-a28d5be8f2e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02189981s
Jan 10 08:03:09.248: INFO: Pod "busybox-user-65534-45e4987d-e7cc-4069-8e81-a28d5be8f2e4" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 10 08:03:09.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2881" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":265,"skipped":5120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:03:09.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating api versions
Jan 10 08:03:09.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8059 api-versions'
Jan 10 08:03:09.387: INFO: stderr: ""
Jan 10 08:03:09.387: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 08:03:09.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8059" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":356,"completed":266,"skipped":5142,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:03:09.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 08:03:09.436: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7aaf67d7-fcdb-4522-bb72-700fe51c70e9" in namespace "downward-api-3966" to be "Succeeded or Failed"
Jan 10 08:03:09.440: INFO: Pod "downwardapi-volume-7aaf67d7-fcdb-4522-bb72-700fe51c70e9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.979127ms
Jan 10 08:03:11.448: INFO: Pod "downwardapi-volume-7aaf67d7-fcdb-4522-bb72-700fe51c70e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011998688s
Jan 10 08:03:13.458: INFO: Pod "downwardapi-volume-7aaf67d7-fcdb-4522-bb72-700fe51c70e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022442021s
STEP: Saw pod success
Jan 10 08:03:13.458: INFO: Pod "downwardapi-volume-7aaf67d7-fcdb-4522-bb72-700fe51c70e9" satisfied condition "Succeeded or Failed"
Jan 10 08:03:13.462: INFO: Trying to get logs from node kk-instance-vgmmg pod downwardapi-volume-7aaf67d7-fcdb-4522-bb72-700fe51c70e9 container client-container: <nil>
STEP: delete the pod
Jan 10 08:03:13.484: INFO: Waiting for pod downwardapi-volume-7aaf67d7-fcdb-4522-bb72-700fe51c70e9 to disappear
Jan 10 08:03:13.488: INFO: Pod downwardapi-volume-7aaf67d7-fcdb-4522-bb72-700fe51c70e9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 10 08:03:13.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3966" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":267,"skipped":5147,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:03:13.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 08:03:13.540: INFO: Waiting up to 5m0s for pod "downwardapi-volume-583f8354-0fb8-4087-82d6-ddf0aa0b394f" in namespace "downward-api-5720" to be "Succeeded or Failed"
Jan 10 08:03:13.543: INFO: Pod "downwardapi-volume-583f8354-0fb8-4087-82d6-ddf0aa0b394f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.764149ms
Jan 10 08:03:15.552: INFO: Pod "downwardapi-volume-583f8354-0fb8-4087-82d6-ddf0aa0b394f": Phase="Running", Reason="", readiness=false. Elapsed: 2.01152971s
Jan 10 08:03:17.561: INFO: Pod "downwardapi-volume-583f8354-0fb8-4087-82d6-ddf0aa0b394f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02020991s
STEP: Saw pod success
Jan 10 08:03:17.561: INFO: Pod "downwardapi-volume-583f8354-0fb8-4087-82d6-ddf0aa0b394f" satisfied condition "Succeeded or Failed"
Jan 10 08:03:17.563: INFO: Trying to get logs from node kk-instance-r65pm pod downwardapi-volume-583f8354-0fb8-4087-82d6-ddf0aa0b394f container client-container: <nil>
STEP: delete the pod
Jan 10 08:03:17.579: INFO: Waiting for pod downwardapi-volume-583f8354-0fb8-4087-82d6-ddf0aa0b394f to disappear
Jan 10 08:03:17.582: INFO: Pod downwardapi-volume-583f8354-0fb8-4087-82d6-ddf0aa0b394f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 10 08:03:17.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5720" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":268,"skipped":5174,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:03:17.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-4946/configmap-test-346b411e-7067-47db-9f52-bff3a3f517b6
STEP: Creating a pod to test consume configMaps
Jan 10 08:03:17.636: INFO: Waiting up to 5m0s for pod "pod-configmaps-619023bf-9f1b-4905-b0b6-d66ce784407b" in namespace "configmap-4946" to be "Succeeded or Failed"
Jan 10 08:03:17.644: INFO: Pod "pod-configmaps-619023bf-9f1b-4905-b0b6-d66ce784407b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.982897ms
Jan 10 08:03:19.652: INFO: Pod "pod-configmaps-619023bf-9f1b-4905-b0b6-d66ce784407b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016621251s
Jan 10 08:03:21.662: INFO: Pod "pod-configmaps-619023bf-9f1b-4905-b0b6-d66ce784407b": Phase="Running", Reason="", readiness=false. Elapsed: 4.025858902s
Jan 10 08:03:23.672: INFO: Pod "pod-configmaps-619023bf-9f1b-4905-b0b6-d66ce784407b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035866236s
STEP: Saw pod success
Jan 10 08:03:23.672: INFO: Pod "pod-configmaps-619023bf-9f1b-4905-b0b6-d66ce784407b" satisfied condition "Succeeded or Failed"
Jan 10 08:03:23.675: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-configmaps-619023bf-9f1b-4905-b0b6-d66ce784407b container env-test: <nil>
STEP: delete the pod
Jan 10 08:03:23.691: INFO: Waiting for pod pod-configmaps-619023bf-9f1b-4905-b0b6-d66ce784407b to disappear
Jan 10 08:03:23.693: INFO: Pod pod-configmaps-619023bf-9f1b-4905-b0b6-d66ce784407b no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 10 08:03:23.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4946" for this suite.

• [SLOW TEST:6.117 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":356,"completed":269,"skipped":5179,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:03:23.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 10 08:03:39.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9836" for this suite.

• [SLOW TEST:16.106 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":356,"completed":270,"skipped":5180,"failed":0}
SSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:03:39.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:03:41.860: INFO: Deleting pod "var-expansion-91ff6dc5-762d-4e90-ba05-97f69dcf3bf1" in namespace "var-expansion-9330"
Jan 10 08:03:41.868: INFO: Wait up to 5m0s for pod "var-expansion-91ff6dc5-762d-4e90-ba05-97f69dcf3bf1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 10 08:03:43.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9330" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":356,"completed":271,"skipped":5185,"failed":0}
SSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:03:43.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Jan 10 08:03:43.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-213" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":272,"skipped":5188,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:03:43.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1334
STEP: creating the pod
Jan 10 08:03:43.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-2445 create -f -'
Jan 10 08:03:44.199: INFO: stderr: ""
Jan 10 08:03:44.199: INFO: stdout: "pod/pause created\n"
Jan 10 08:03:44.199: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 10 08:03:44.199: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2445" to be "running and ready"
Jan 10 08:03:44.202: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.576481ms
Jan 10 08:03:46.211: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.011999081s
Jan 10 08:03:46.211: INFO: Pod "pause" satisfied condition "running and ready"
Jan 10 08:03:46.211: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 10 08:03:46.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-2445 label pods pause testing-label=testing-label-value'
Jan 10 08:03:46.312: INFO: stderr: ""
Jan 10 08:03:46.312: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 10 08:03:46.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-2445 get pod pause -L testing-label'
Jan 10 08:03:46.385: INFO: stderr: ""
Jan 10 08:03:46.385: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 10 08:03:46.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-2445 label pods pause testing-label-'
Jan 10 08:03:46.480: INFO: stderr: ""
Jan 10 08:03:46.480: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 10 08:03:46.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-2445 get pod pause -L testing-label'
Jan 10 08:03:46.565: INFO: stderr: ""
Jan 10 08:03:46.565: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Jan 10 08:03:46.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-2445 delete --grace-period=0 --force -f -'
Jan 10 08:03:46.659: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 10 08:03:46.659: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 10 08:03:46.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-2445 get rc,svc -l name=pause --no-headers'
Jan 10 08:03:46.737: INFO: stderr: "No resources found in kubectl-2445 namespace.\n"
Jan 10 08:03:46.737: INFO: stdout: ""
Jan 10 08:03:46.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-2445 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 10 08:03:46.818: INFO: stderr: ""
Jan 10 08:03:46.818: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 08:03:46.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2445" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":356,"completed":273,"skipped":5190,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:03:46.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-9861f91a-df5e-4917-b770-a8ba0bdbb11e
STEP: Creating a pod to test consume configMaps
Jan 10 08:03:46.866: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d4154862-fd54-4483-9179-e17a005c5da3" in namespace "projected-6159" to be "Succeeded or Failed"
Jan 10 08:03:46.869: INFO: Pod "pod-projected-configmaps-d4154862-fd54-4483-9179-e17a005c5da3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.074399ms
Jan 10 08:03:48.880: INFO: Pod "pod-projected-configmaps-d4154862-fd54-4483-9179-e17a005c5da3": Phase="Running", Reason="", readiness=false. Elapsed: 2.01375798s
Jan 10 08:03:50.890: INFO: Pod "pod-projected-configmaps-d4154862-fd54-4483-9179-e17a005c5da3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023815775s
STEP: Saw pod success
Jan 10 08:03:50.890: INFO: Pod "pod-projected-configmaps-d4154862-fd54-4483-9179-e17a005c5da3" satisfied condition "Succeeded or Failed"
Jan 10 08:03:50.898: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-projected-configmaps-d4154862-fd54-4483-9179-e17a005c5da3 container agnhost-container: <nil>
STEP: delete the pod
Jan 10 08:03:50.914: INFO: Waiting for pod pod-projected-configmaps-d4154862-fd54-4483-9179-e17a005c5da3 to disappear
Jan 10 08:03:50.916: INFO: Pod pod-projected-configmaps-d4154862-fd54-4483-9179-e17a005c5da3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 10 08:03:50.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6159" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":274,"skipped":5207,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:03:50.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-cfa57f0e-d47e-4d2c-a4df-31175e955b24
STEP: Creating a pod to test consume configMaps
Jan 10 08:03:50.964: INFO: Waiting up to 5m0s for pod "pod-configmaps-496185ab-c507-4a31-bf6d-c0670b0c598f" in namespace "configmap-5649" to be "Succeeded or Failed"
Jan 10 08:03:50.972: INFO: Pod "pod-configmaps-496185ab-c507-4a31-bf6d-c0670b0c598f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083205ms
Jan 10 08:03:52.979: INFO: Pod "pod-configmaps-496185ab-c507-4a31-bf6d-c0670b0c598f": Phase="Running", Reason="", readiness=false. Elapsed: 2.015406527s
Jan 10 08:03:54.984: INFO: Pod "pod-configmaps-496185ab-c507-4a31-bf6d-c0670b0c598f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020470621s
STEP: Saw pod success
Jan 10 08:03:54.984: INFO: Pod "pod-configmaps-496185ab-c507-4a31-bf6d-c0670b0c598f" satisfied condition "Succeeded or Failed"
Jan 10 08:03:54.987: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-configmaps-496185ab-c507-4a31-bf6d-c0670b0c598f container configmap-volume-test: <nil>
STEP: delete the pod
Jan 10 08:03:55.013: INFO: Waiting for pod pod-configmaps-496185ab-c507-4a31-bf6d-c0670b0c598f to disappear
Jan 10 08:03:55.016: INFO: Pod pod-configmaps-496185ab-c507-4a31-bf6d-c0670b0c598f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 10 08:03:55.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5649" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":275,"skipped":5215,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:03:55.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's args
Jan 10 08:03:55.071: INFO: Waiting up to 5m0s for pod "var-expansion-e94bb73b-3762-4b79-8412-7de062238f2f" in namespace "var-expansion-3913" to be "Succeeded or Failed"
Jan 10 08:03:55.076: INFO: Pod "var-expansion-e94bb73b-3762-4b79-8412-7de062238f2f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.876278ms
Jan 10 08:03:57.087: INFO: Pod "var-expansion-e94bb73b-3762-4b79-8412-7de062238f2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016389241s
Jan 10 08:03:59.092: INFO: Pod "var-expansion-e94bb73b-3762-4b79-8412-7de062238f2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021142346s
STEP: Saw pod success
Jan 10 08:03:59.092: INFO: Pod "var-expansion-e94bb73b-3762-4b79-8412-7de062238f2f" satisfied condition "Succeeded or Failed"
Jan 10 08:03:59.094: INFO: Trying to get logs from node kk-instance-r65pm pod var-expansion-e94bb73b-3762-4b79-8412-7de062238f2f container dapi-container: <nil>
STEP: delete the pod
Jan 10 08:03:59.110: INFO: Waiting for pod var-expansion-e94bb73b-3762-4b79-8412-7de062238f2f to disappear
Jan 10 08:03:59.113: INFO: Pod var-expansion-e94bb73b-3762-4b79-8412-7de062238f2f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 10 08:03:59.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3913" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":356,"completed":276,"skipped":5219,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:03:59.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 10 08:03:59.175: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 08:03:59.179: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 08:03:59.179: INFO: Node kk-instance-r65pm is running 0 daemon pod, expected 1
Jan 10 08:04:00.187: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 08:04:00.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 08:04:00.191: INFO: Node kk-instance-r65pm is running 0 daemon pod, expected 1
Jan 10 08:04:01.188: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 08:04:01.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 10 08:04:01.192: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status
Jan 10 08:04:01.198: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Jan 10 08:04:01.208: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Jan 10 08:04:01.211: INFO: Observed &DaemonSet event: ADDED
Jan 10 08:04:01.211: INFO: Observed &DaemonSet event: MODIFIED
Jan 10 08:04:01.212: INFO: Observed &DaemonSet event: MODIFIED
Jan 10 08:04:01.212: INFO: Observed &DaemonSet event: MODIFIED
Jan 10 08:04:01.212: INFO: Observed &DaemonSet event: MODIFIED
Jan 10 08:04:01.212: INFO: Found daemon set daemon-set in namespace daemonsets-7529 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 10 08:04:01.213: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Jan 10 08:04:01.225: INFO: Observed &DaemonSet event: ADDED
Jan 10 08:04:01.225: INFO: Observed &DaemonSet event: MODIFIED
Jan 10 08:04:01.225: INFO: Observed &DaemonSet event: MODIFIED
Jan 10 08:04:01.226: INFO: Observed &DaemonSet event: MODIFIED
Jan 10 08:04:01.226: INFO: Observed &DaemonSet event: MODIFIED
Jan 10 08:04:01.226: INFO: Observed daemon set daemon-set in namespace daemonsets-7529 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 10 08:04:01.226: INFO: Observed &DaemonSet event: MODIFIED
Jan 10 08:04:01.227: INFO: Found daemon set daemon-set in namespace daemonsets-7529 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 10 08:04:01.227: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7529, will wait for the garbage collector to delete the pods
Jan 10 08:04:01.296: INFO: Deleting DaemonSet.extensions daemon-set took: 11.970681ms
Jan 10 08:04:01.396: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.500061ms
Jan 10 08:04:04.004: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 08:04:04.004: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 10 08:04:04.006: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43542"},"items":null}

Jan 10 08:04:04.009: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43542"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 10 08:04:04.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7529" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":356,"completed":277,"skipped":5239,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:04:04.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 10 08:04:08.104: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 10 08:04:08.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1933" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":278,"skipped":5251,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:04:08.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 10 08:06:00.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7069" for this suite.

• [SLOW TEST:112.177 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":356,"completed":279,"skipped":5268,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:06:00.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 08:06:00.767: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 08:06:03.791: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 08:06:03.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3780" for this suite.
STEP: Destroying namespace "webhook-3780-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":356,"completed":280,"skipped":5284,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:06:03.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:06:03.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-5472 create -f -'
Jan 10 08:06:04.152: INFO: stderr: ""
Jan 10 08:06:04.152: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 10 08:06:04.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-5472 create -f -'
Jan 10 08:06:04.394: INFO: stderr: ""
Jan 10 08:06:04.394: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 10 08:06:05.401: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 10 08:06:05.401: INFO: Found 1 / 1
Jan 10 08:06:05.401: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 10 08:06:05.405: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 10 08:06:05.405: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 10 08:06:05.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-5472 describe pod agnhost-primary-krz2m'
Jan 10 08:06:05.576: INFO: stderr: ""
Jan 10 08:06:05.576: INFO: stdout: "Name:         agnhost-primary-krz2m\nNamespace:    kubectl-5472\nPriority:     0\nNode:         kk-instance-vgmmg/192.168.0.5\nStart Time:   Tue, 10 Jan 2023 08:06:04 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: ca586b75d8988c707f6eaef86f30024d3bb3aad6b44ef9485e7ed0398905a26b\n              cni.projectcalico.org/podIP: 10.233.107.70/32\n              cni.projectcalico.org/podIPs: 10.233.107.70/32\nStatus:       Running\nIP:           10.233.107.70\nIPs:\n  IP:           10.233.107.70\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://4b7454d8a650b87fcb382d60ab9bc62f30105167f60718ca0e86bb6b3e811197\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 10 Jan 2023 08:06:04 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rhjqf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-rhjqf:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-5472/agnhost-primary-krz2m to kk-instance-vgmmg\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.39\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Jan 10 08:06:05.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-5472 describe rc agnhost-primary'
Jan 10 08:06:05.679: INFO: stderr: ""
Jan 10 08:06:05.679: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5472\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-krz2m\n"
Jan 10 08:06:05.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-5472 describe service agnhost-primary'
Jan 10 08:06:05.790: INFO: stderr: ""
Jan 10 08:06:05.790: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5472\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.12.88\nIPs:               10.233.12.88\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.107.70:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 10 08:06:05.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-5472 describe node kk-instance-r65pm'
Jan 10 08:06:05.967: INFO: stderr: ""
Jan 10 08:06:05.967: INFO: stdout: "Name:               kk-instance-r65pm\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=kk-instance-r65pm\n                    kubernetes.io/os=linux\nAnnotations:        cluster.x-k8s.io/cluster-name: capi-quickstart\n                    cluster.x-k8s.io/cluster-namespace: default\n                    cluster.x-k8s.io/machine: capi-quickstart-md-0-667c86bc56-vkpcb\n                    cluster.x-k8s.io/owner-kind: MachineSet\n                    cluster.x-k8s.io/owner-name: capi-quickstart-md-0-667c86bc56\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.0.4/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.233.82.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 10 Jan 2023 06:04:26 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  kk-instance-r65pm\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 10 Jan 2023 08:06:05 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 10 Jan 2023 06:28:45 +0000   Tue, 10 Jan 2023 06:28:45 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 10 Jan 2023 08:02:50 +0000   Tue, 10 Jan 2023 06:04:26 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 10 Jan 2023 08:02:50 +0000   Tue, 10 Jan 2023 06:04:26 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 10 Jan 2023 08:02:50 +0000   Tue, 10 Jan 2023 06:04:26 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 10 Jan 2023 08:02:50 +0000   Tue, 10 Jan 2023 06:28:45 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.0.4\n  Hostname:    kk-instance-r65pm\nCapacity:\n  cpu:                2\n  ephemeral-storage:  101444584Ki\n  hugepages-2Mi:      0\n  memory:             4026024Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  93491328460\n  hugepages-2Mi:      0\n  memory:             3923624Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 9444c4490b3c9ff6823b4a0f63abc53a\n  System UUID:                eacf48c5-a39d-399f-a295-eca571efc707\n  Boot ID:                    e0a0b914-7346-486e-804e-d668cb9aa99f\n  Kernel Version:             5.4.0-104-generic\n  OS Image:                   Ubuntu 20.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.4\n  Kubelet Version:            v1.24.8\n  Kube-Proxy Version:         v1.24.8\nPodCIDR:                      10.233.65.0/24\nPodCIDRs:                     10.233.65.0/24\nProviderID:                   kk:///capi-quickstart/kk-instance-r65pm\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  cronjob-7069                concurrent-27888966-t25cl                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         5s\n  kube-system                 calico-kube-controllers-6799f5f4b4-g6h9q                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         97m\n  kube-system                 calico-node-khhj9                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         97m\n  kube-system                 coredns-6d4b75cb6d-4wczh                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     123m\n  kube-system                 coredns-6d4b75cb6d-f7d8c                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     123m\n  kube-system                 kube-proxy-2v5cp                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         121m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-5167b5d313ce4ad1-6zjp9    0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                450m (22%)  0 (0%)\n  memory             140Mi (3%)  340Mi (8%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Jan 10 08:06:05.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-5472 describe namespace kubectl-5472'
Jan 10 08:06:06.069: INFO: stderr: ""
Jan 10 08:06:06.069: INFO: stdout: "Name:         kubectl-5472\nLabels:       e2e-framework=kubectl\n              e2e-run=9c514c83-43f6-4579-b8b5-74bc55fa4884\n              kubernetes.io/metadata.name=kubectl-5472\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 08:06:06.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5472" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":356,"completed":281,"skipped":5295,"failed":0}
S
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:06:06.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan 10 08:06:06.110: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 10 08:07:06.146: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:07:06.151: INFO: Starting informer...
STEP: Starting pods...
Jan 10 08:07:06.381: INFO: Pod1 is running on kk-instance-vgmmg. Tainting Node
Jan 10 08:07:08.612: INFO: Pod2 is running on kk-instance-vgmmg. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jan 10 08:07:14.441: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 10 08:07:34.487: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:188
Jan 10 08:07:34.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8298" for this suite.

• [SLOW TEST:88.438 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":356,"completed":282,"skipped":5296,"failed":0}
SSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:07:34.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Jan 10 08:07:34.588: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 10 08:07:40.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6691" for this suite.

• [SLOW TEST:6.222 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":356,"completed":283,"skipped":5303,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:07:40.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:07:40.796: INFO: The status of Pod test-webserver-41d08476-e2d1-4da7-bfc0-032fff98cddc is Pending, waiting for it to be Running (with Ready = true)
Jan 10 08:07:42.803: INFO: The status of Pod test-webserver-41d08476-e2d1-4da7-bfc0-032fff98cddc is Running (Ready = false)
Jan 10 08:07:44.801: INFO: The status of Pod test-webserver-41d08476-e2d1-4da7-bfc0-032fff98cddc is Running (Ready = false)
Jan 10 08:07:46.805: INFO: The status of Pod test-webserver-41d08476-e2d1-4da7-bfc0-032fff98cddc is Running (Ready = false)
Jan 10 08:07:48.807: INFO: The status of Pod test-webserver-41d08476-e2d1-4da7-bfc0-032fff98cddc is Running (Ready = false)
Jan 10 08:07:50.801: INFO: The status of Pod test-webserver-41d08476-e2d1-4da7-bfc0-032fff98cddc is Running (Ready = false)
Jan 10 08:07:52.803: INFO: The status of Pod test-webserver-41d08476-e2d1-4da7-bfc0-032fff98cddc is Running (Ready = false)
Jan 10 08:07:54.800: INFO: The status of Pod test-webserver-41d08476-e2d1-4da7-bfc0-032fff98cddc is Running (Ready = false)
Jan 10 08:07:56.806: INFO: The status of Pod test-webserver-41d08476-e2d1-4da7-bfc0-032fff98cddc is Running (Ready = false)
Jan 10 08:07:58.801: INFO: The status of Pod test-webserver-41d08476-e2d1-4da7-bfc0-032fff98cddc is Running (Ready = false)
Jan 10 08:08:00.802: INFO: The status of Pod test-webserver-41d08476-e2d1-4da7-bfc0-032fff98cddc is Running (Ready = false)
Jan 10 08:08:02.805: INFO: The status of Pod test-webserver-41d08476-e2d1-4da7-bfc0-032fff98cddc is Running (Ready = true)
Jan 10 08:08:02.808: INFO: Container started at 2023-01-10 08:07:41 +0000 UTC, pod became ready at 2023-01-10 08:08:01 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 10 08:08:02.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3646" for this suite.

• [SLOW TEST:22.066 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":356,"completed":284,"skipped":5335,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:08:02.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jan 10 08:08:02.888: INFO: observed Pod pod-test in namespace pods-3205 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 10 08:08:02.890: INFO: observed Pod pod-test in namespace pods-3205 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 08:08:02 +0000 UTC  }]
Jan 10 08:08:02.915: INFO: observed Pod pod-test in namespace pods-3205 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 08:08:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 08:08:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 08:08:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 08:08:02 +0000 UTC  }]
Jan 10 08:08:03.443: INFO: observed Pod pod-test in namespace pods-3205 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 08:08:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 08:08:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-10 08:08:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 08:08:02 +0000 UTC  }]
Jan 10 08:08:04.580: INFO: Found Pod pod-test in namespace pods-3205 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 08:08:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 08:08:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 08:08:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-10 08:08:02 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jan 10 08:08:04.638: INFO: observed event type MODIFIED
Jan 10 08:08:06.591: INFO: observed event type MODIFIED
Jan 10 08:08:06.969: INFO: observed event type MODIFIED
Jan 10 08:08:07.598: INFO: observed event type MODIFIED
Jan 10 08:08:07.607: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 10 08:08:07.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3205" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":356,"completed":285,"skipped":5350,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:08:07.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 10 08:08:07.680: INFO: The status of Pod labelsupdate50eb9f09-014a-4f09-bbcc-20a1e9068214 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 08:08:09.695: INFO: The status of Pod labelsupdate50eb9f09-014a-4f09-bbcc-20a1e9068214 is Running (Ready = true)
Jan 10 08:08:10.229: INFO: Successfully updated pod "labelsupdate50eb9f09-014a-4f09-bbcc-20a1e9068214"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 10 08:08:14.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4311" for this suite.

• [SLOW TEST:6.631 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":286,"skipped":5369,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:08:14.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-254bd59f-1cce-48b8-93fb-aa3bafefce59 in namespace container-probe-6586
Jan 10 08:08:16.318: INFO: Started pod liveness-254bd59f-1cce-48b8-93fb-aa3bafefce59 in namespace container-probe-6586
STEP: checking the pod's current state and verifying that restartCount is present
Jan 10 08:08:16.321: INFO: Initial restart count of pod liveness-254bd59f-1cce-48b8-93fb-aa3bafefce59 is 0
Jan 10 08:08:36.411: INFO: Restart count of pod container-probe-6586/liveness-254bd59f-1cce-48b8-93fb-aa3bafefce59 is now 1 (20.089882214s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 10 08:08:36.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6586" for this suite.

• [SLOW TEST:22.167 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":287,"skipped":5382,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:08:36.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jan 10 08:08:36.488: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 08:08:38.493: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.0.5 on the node which pod1 resides and expect scheduled
Jan 10 08:08:38.503: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 08:08:40.510: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.0.5 but use UDP protocol on the node which pod2 resides
Jan 10 08:08:40.531: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 08:08:42.541: INFO: The status of Pod pod3 is Running (Ready = true)
Jan 10 08:08:42.567: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 10 08:08:44.575: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jan 10 08:08:44.580: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.0.5 http://127.0.0.1:54323/hostname] Namespace:hostport-4641 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 08:08:44.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 08:08:44.581: INFO: ExecWithOptions: Clientset creation
Jan 10 08:08:44.581: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4641/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.0.5+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.5, port: 54323
Jan 10 08:08:44.668: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.0.5:54323/hostname] Namespace:hostport-4641 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 08:08:44.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 08:08:44.669: INFO: ExecWithOptions: Clientset creation
Jan 10 08:08:44.669: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4641/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.0.5%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.5, port: 54323 UDP
Jan 10 08:08:44.747: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.0.5 54323] Namespace:hostport-4641 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 08:08:44.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 08:08:44.747: INFO: ExecWithOptions: Clientset creation
Jan 10 08:08:44.747: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4641/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+192.168.0.5+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:188
Jan 10 08:08:49.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-4641" for this suite.

• [SLOW TEST:13.394 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":356,"completed":288,"skipped":5405,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:08:49.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 08:08:50.330: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 08:08:53.360: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jan 10 08:08:55.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=webhook-4787 attach --namespace=webhook-4787 to-be-attached-pod -i -c=container1'
Jan 10 08:08:55.554: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 08:08:55.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4787" for this suite.
STEP: Destroying namespace "webhook-4787-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.830 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":356,"completed":289,"skipped":5469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:08:55.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:08:55.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 08:09:02.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3368" for this suite.

• [SLOW TEST:6.443 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":356,"completed":290,"skipped":5528,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:09:02.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jan 10 08:09:02.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jan 10 08:09:18.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 08:09:21.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 08:09:38.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6704" for this suite.

• [SLOW TEST:35.921 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":356,"completed":291,"skipped":5544,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:09:38.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 10 08:09:38.676: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 10 08:09:41.694: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 08:09:41.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2692" for this suite.
STEP: Destroying namespace "webhook-2692-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":356,"completed":292,"skipped":5546,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:09:41.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-eef6eaf5-090d-4e5d-ac81-53c89c8a7786
STEP: Creating a pod to test consume configMaps
Jan 10 08:09:41.844: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-88d17b70-7c32-4da7-a9ca-05f9c99c2760" in namespace "projected-8582" to be "Succeeded or Failed"
Jan 10 08:09:41.849: INFO: Pod "pod-projected-configmaps-88d17b70-7c32-4da7-a9ca-05f9c99c2760": Phase="Pending", Reason="", readiness=false. Elapsed: 5.555873ms
Jan 10 08:09:43.854: INFO: Pod "pod-projected-configmaps-88d17b70-7c32-4da7-a9ca-05f9c99c2760": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010394023s
Jan 10 08:09:45.862: INFO: Pod "pod-projected-configmaps-88d17b70-7c32-4da7-a9ca-05f9c99c2760": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018518426s
STEP: Saw pod success
Jan 10 08:09:45.862: INFO: Pod "pod-projected-configmaps-88d17b70-7c32-4da7-a9ca-05f9c99c2760" satisfied condition "Succeeded or Failed"
Jan 10 08:09:45.866: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-projected-configmaps-88d17b70-7c32-4da7-a9ca-05f9c99c2760 container agnhost-container: <nil>
STEP: delete the pod
Jan 10 08:09:45.892: INFO: Waiting for pod pod-projected-configmaps-88d17b70-7c32-4da7-a9ca-05f9c99c2760 to disappear
Jan 10 08:09:45.899: INFO: Pod pod-projected-configmaps-88d17b70-7c32-4da7-a9ca-05f9c99c2760 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 10 08:09:45.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8582" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":293,"skipped":5562,"failed":0}
SSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:09:45.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jan 10 08:10:06.075: INFO: EndpointSlice for Service endpointslice-8785/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 10 08:10:16.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8785" for this suite.

• [SLOW TEST:30.195 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":356,"completed":294,"skipped":5566,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:10:16.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:10:16.144: INFO: Creating simple deployment test-new-deployment
Jan 10 08:10:16.175: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 10 08:10:18.240: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9390  e91a02a8-7354-4270-8469-8b325bf39ebe 45853 3 2023-01-10 08:10:16 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-10 08:10:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 08:10:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047cd1d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-10 08:10:17 +0000 UTC,LastTransitionTime:2023-01-10 08:10:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-55df494869" has successfully progressed.,LastUpdateTime:2023-01-10 08:10:17 +0000 UTC,LastTransitionTime:2023-01-10 08:10:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 10 08:10:18.250: INFO: New ReplicaSet "test-new-deployment-55df494869" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-55df494869  deployment-9390  78d39c0e-3dae-4821-9940-42e584cae1ef 45860 2 2023-01-10 08:10:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment e91a02a8-7354-4270-8469-8b325bf39ebe 0xc004802337 0xc004802338}] []  [{kube-controller-manager Update apps/v1 2023-01-10 08:10:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e91a02a8-7354-4270-8469-8b325bf39ebe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-10 08:10:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048023d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 10 08:10:18.255: INFO: Pod "test-new-deployment-55df494869-8pr62" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-8pr62 test-new-deployment-55df494869- deployment-9390  2515a872-8182-4f19-aeeb-f66ab7002cea 45861 0 2023-01-10 08:10:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet test-new-deployment-55df494869 78d39c0e-3dae-4821-9940-42e584cae1ef 0xc0048028f7 0xc0048028f8}] []  [{kube-controller-manager Update v1 2023-01-10 08:10:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78d39c0e-3dae-4821-9940-42e584cae1ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-10 08:10:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x9cnx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x9cnx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-vgmmg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 08:10:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 08:10:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 08:10:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 08:10:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:,StartTime:2023-01-10 08:10:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 10 08:10:18.257: INFO: Pod "test-new-deployment-55df494869-jg5vw" is available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-jg5vw test-new-deployment-55df494869- deployment-9390  bd7271b2-db9e-4f52-bc3b-069f880ff6ab 45844 0 2023-01-10 08:10:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:619d6ff444cc0b1ea397bde73e988135290003942accfb58001108a02b4373f7 cni.projectcalico.org/podIP:10.233.82.39/32 cni.projectcalico.org/podIPs:10.233.82.39/32] [{apps/v1 ReplicaSet test-new-deployment-55df494869 78d39c0e-3dae-4821-9940-42e584cae1ef 0xc004802ad7 0xc004802ad8}] []  [{calico Update v1 2023-01-10 08:10:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-10 08:10:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78d39c0e-3dae-4821-9940-42e584cae1ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-10 08:10:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.82.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7jbl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7jbl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kk-instance-r65pm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 08:10:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 08:10:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 08:10:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-10 08:10:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.4,PodIP:10.233.82.39,StartTime:2023-01-10 08:10:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-10 08:10:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2bdd4ad742ab5522c5d3d468021c4cf84919c38cb8380ff045760fa59e9a5deb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.82.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 10 08:10:18.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9390" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":356,"completed":295,"skipped":5580,"failed":0}
SSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:10:18.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan 10 08:10:18.362: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 10 08:10:23.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1503" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":356,"completed":296,"skipped":5585,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:10:23.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jan 10 08:10:23.903: INFO: The status of Pod kube-controller-manager-kk-instance-rx4ck is Running (Ready = true)
Jan 10 08:10:24.028: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 10 08:10:24.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8364" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":356,"completed":297,"skipped":5587,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:10:24.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1540
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 10 08:10:24.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6761 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Jan 10 08:10:24.164: INFO: stderr: ""
Jan 10 08:10:24.164: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1544
Jan 10 08:10:24.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-6761 delete pods e2e-test-httpd-pod'
Jan 10 08:10:26.050: INFO: stderr: ""
Jan 10 08:10:26.050: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 08:10:26.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6761" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":356,"completed":298,"skipped":5591,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:10:26.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-d061b8c2-05bf-43e0-9e52-11ed4ab08f8f
STEP: Creating a pod to test consume configMaps
Jan 10 08:10:26.111: INFO: Waiting up to 5m0s for pod "pod-configmaps-936e8c80-3539-48ef-8760-f4c20b6aa6e4" in namespace "configmap-3001" to be "Succeeded or Failed"
Jan 10 08:10:26.116: INFO: Pod "pod-configmaps-936e8c80-3539-48ef-8760-f4c20b6aa6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.789809ms
Jan 10 08:10:28.123: INFO: Pod "pod-configmaps-936e8c80-3539-48ef-8760-f4c20b6aa6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012018091s
Jan 10 08:10:30.129: INFO: Pod "pod-configmaps-936e8c80-3539-48ef-8760-f4c20b6aa6e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017906146s
STEP: Saw pod success
Jan 10 08:10:30.129: INFO: Pod "pod-configmaps-936e8c80-3539-48ef-8760-f4c20b6aa6e4" satisfied condition "Succeeded or Failed"
Jan 10 08:10:30.134: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-configmaps-936e8c80-3539-48ef-8760-f4c20b6aa6e4 container agnhost-container: <nil>
STEP: delete the pod
Jan 10 08:10:30.153: INFO: Waiting for pod pod-configmaps-936e8c80-3539-48ef-8760-f4c20b6aa6e4 to disappear
Jan 10 08:10:30.156: INFO: Pod pod-configmaps-936e8c80-3539-48ef-8760-f4c20b6aa6e4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 10 08:10:30.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3001" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":299,"skipped":5592,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:10:30.168: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 10 08:10:37.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8300" for this suite.

• [SLOW TEST:7.061 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":356,"completed":300,"skipped":5612,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:10:37.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 08:10:37.265: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8e6c1edf-3a71-4a00-a71a-047ae2a5f4d3" in namespace "projected-3459" to be "Succeeded or Failed"
Jan 10 08:10:37.286: INFO: Pod "downwardapi-volume-8e6c1edf-3a71-4a00-a71a-047ae2a5f4d3": Phase="Pending", Reason="", readiness=false. Elapsed: 21.197461ms
Jan 10 08:10:39.296: INFO: Pod "downwardapi-volume-8e6c1edf-3a71-4a00-a71a-047ae2a5f4d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030949954s
Jan 10 08:10:41.304: INFO: Pod "downwardapi-volume-8e6c1edf-3a71-4a00-a71a-047ae2a5f4d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039854167s
STEP: Saw pod success
Jan 10 08:10:41.305: INFO: Pod "downwardapi-volume-8e6c1edf-3a71-4a00-a71a-047ae2a5f4d3" satisfied condition "Succeeded or Failed"
Jan 10 08:10:41.307: INFO: Trying to get logs from node kk-instance-vgmmg pod downwardapi-volume-8e6c1edf-3a71-4a00-a71a-047ae2a5f4d3 container client-container: <nil>
STEP: delete the pod
Jan 10 08:10:41.321: INFO: Waiting for pod downwardapi-volume-8e6c1edf-3a71-4a00-a71a-047ae2a5f4d3 to disappear
Jan 10 08:10:41.324: INFO: Pod downwardapi-volume-8e6c1edf-3a71-4a00-a71a-047ae2a5f4d3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 10 08:10:41.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3459" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":301,"skipped":5651,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:10:41.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7361.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7361.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7361.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7361.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 10 08:10:43.420: INFO: DNS probes using dns-7361/dns-test-e1c8dac1-30aa-4bc5-8c14-e7d899de3f3a succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 10 08:10:43.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7361" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","total":356,"completed":302,"skipped":5679,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:10:43.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a collection of services
Jan 10 08:10:43.503: INFO: Creating e2e-svc-a-sdrb6
Jan 10 08:10:43.512: INFO: Creating e2e-svc-b-hwg8j
Jan 10 08:10:43.520: INFO: Creating e2e-svc-c-xt45k
STEP: deleting service collection
Jan 10 08:10:43.559: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 08:10:43.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-320" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":356,"completed":303,"skipped":5715,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:10:43.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jan 10 08:10:43.613: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 10 08:10:48.628: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 10 08:10:48.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1331" for this suite.

• [SLOW TEST:5.157 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":356,"completed":304,"skipped":5738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:10:48.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2595
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:10:48.806: INFO: Found 0 stateful pods, waiting for 1
Jan 10 08:10:58.820: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Jan 10 08:10:58.841: INFO: Found 1 stateful pods, waiting for 2
Jan 10 08:11:08.851: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 10 08:11:08.851: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 10 08:11:08.873: INFO: Deleting all statefulset in ns statefulset-2595
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 10 08:11:08.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2595" for this suite.

• [SLOW TEST:20.186 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":356,"completed":305,"skipped":5760,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:11:08.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4936.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4936.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4936.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4936.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4936.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4936.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4936.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4936.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4936.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4936.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 94.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.94_udp@PTR;check="$$(dig +tcp +noall +answer +search 94.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.94_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4936.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4936.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4936.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4936.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4936.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4936.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4936.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4936.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4936.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4936.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 94.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.94_udp@PTR;check="$$(dig +tcp +noall +answer +search 94.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.94_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 10 08:11:11.011: INFO: Unable to read wheezy_udp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:11.014: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:11.017: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:11.020: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:11.033: INFO: Unable to read jessie_udp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:11.036: INFO: Unable to read jessie_tcp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:11.038: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:11.040: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:11.052: INFO: Lookups using dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe failed for: [wheezy_udp@dns-test-service.dns-4936.svc.cluster.local wheezy_tcp@dns-test-service.dns-4936.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local jessie_udp@dns-test-service.dns-4936.svc.cluster.local jessie_tcp@dns-test-service.dns-4936.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local]

Jan 10 08:11:16.058: INFO: Unable to read wheezy_udp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:16.061: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:16.064: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:16.068: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:16.082: INFO: Unable to read jessie_udp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:16.088: INFO: Unable to read jessie_tcp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:16.091: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:16.094: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:16.108: INFO: Lookups using dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe failed for: [wheezy_udp@dns-test-service.dns-4936.svc.cluster.local wheezy_tcp@dns-test-service.dns-4936.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local jessie_udp@dns-test-service.dns-4936.svc.cluster.local jessie_tcp@dns-test-service.dns-4936.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local]

Jan 10 08:11:21.057: INFO: Unable to read wheezy_udp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:21.060: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:21.063: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:21.066: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:21.082: INFO: Unable to read jessie_udp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:21.085: INFO: Unable to read jessie_tcp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:21.088: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:21.091: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:21.100: INFO: Lookups using dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe failed for: [wheezy_udp@dns-test-service.dns-4936.svc.cluster.local wheezy_tcp@dns-test-service.dns-4936.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local jessie_udp@dns-test-service.dns-4936.svc.cluster.local jessie_tcp@dns-test-service.dns-4936.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local]

Jan 10 08:11:26.058: INFO: Unable to read wheezy_udp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:26.063: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:26.068: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:26.072: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:26.093: INFO: Unable to read jessie_udp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:26.096: INFO: Unable to read jessie_tcp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:26.101: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:26.108: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:26.121: INFO: Lookups using dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe failed for: [wheezy_udp@dns-test-service.dns-4936.svc.cluster.local wheezy_tcp@dns-test-service.dns-4936.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local jessie_udp@dns-test-service.dns-4936.svc.cluster.local jessie_tcp@dns-test-service.dns-4936.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local]

Jan 10 08:11:31.058: INFO: Unable to read wheezy_udp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:31.062: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:31.065: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:31.068: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:31.090: INFO: Unable to read jessie_udp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:31.096: INFO: Unable to read jessie_tcp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:31.099: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:31.102: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:31.113: INFO: Lookups using dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe failed for: [wheezy_udp@dns-test-service.dns-4936.svc.cluster.local wheezy_tcp@dns-test-service.dns-4936.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local jessie_udp@dns-test-service.dns-4936.svc.cluster.local jessie_tcp@dns-test-service.dns-4936.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local]

Jan 10 08:11:36.057: INFO: Unable to read wheezy_udp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:36.060: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:36.063: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:36.067: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:36.088: INFO: Unable to read jessie_udp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:36.092: INFO: Unable to read jessie_tcp@dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:36.095: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:36.098: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local from pod dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe: the server could not find the requested resource (get pods dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe)
Jan 10 08:11:36.113: INFO: Lookups using dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe failed for: [wheezy_udp@dns-test-service.dns-4936.svc.cluster.local wheezy_tcp@dns-test-service.dns-4936.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local jessie_udp@dns-test-service.dns-4936.svc.cluster.local jessie_tcp@dns-test-service.dns-4936.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4936.svc.cluster.local]

Jan 10 08:11:41.120: INFO: DNS probes using dns-4936/dns-test-ccc094bc-9a71-4c22-9a70-4ba34ea1a8fe succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 10 08:11:41.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4936" for this suite.

• [SLOW TEST:32.290 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":356,"completed":306,"skipped":5797,"failed":0}
SS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:11:41.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Deleting RuntimeClass runtimeclass-1385-delete-me
STEP: Waiting for the RuntimeClass to disappear
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 10 08:11:41.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1385" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":307,"skipped":5799,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:11:41.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-8851
STEP: creating service affinity-clusterip-transition in namespace services-8851
STEP: creating replication controller affinity-clusterip-transition in namespace services-8851
I0110 08:11:41.305386      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8851, replica count: 3
I0110 08:11:44.356797      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 10 08:11:44.367: INFO: Creating new exec pod
Jan 10 08:11:47.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-8851 exec execpod-affinity5wnhc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 10 08:11:47.573: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 10 08:11:47.573: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 08:11:47.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-8851 exec execpod-affinity5wnhc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.63.116 80'
Jan 10 08:11:47.726: INFO: stderr: "+ nc -v -t -w 2 10.233.63.116 80\nConnection to 10.233.63.116 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jan 10 08:11:47.726: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 10 08:11:47.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-8851 exec execpod-affinity5wnhc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.63.116:80/ ; done'
Jan 10 08:11:47.984: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n"
Jan 10 08:11:47.984: INFO: stdout: "\naffinity-clusterip-transition-snfr5\naffinity-clusterip-transition-snfr5\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-tcmtv\naffinity-clusterip-transition-snfr5\naffinity-clusterip-transition-snfr5\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-snfr5\naffinity-clusterip-transition-snfr5\naffinity-clusterip-transition-snfr5\naffinity-clusterip-transition-tcmtv\naffinity-clusterip-transition-snfr5\naffinity-clusterip-transition-tcmtv\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-tcmtv\naffinity-clusterip-transition-snfr5"
Jan 10 08:11:47.984: INFO: Received response from host: affinity-clusterip-transition-snfr5
Jan 10 08:11:47.984: INFO: Received response from host: affinity-clusterip-transition-snfr5
Jan 10 08:11:47.984: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:47.984: INFO: Received response from host: affinity-clusterip-transition-tcmtv
Jan 10 08:11:47.984: INFO: Received response from host: affinity-clusterip-transition-snfr5
Jan 10 08:11:47.985: INFO: Received response from host: affinity-clusterip-transition-snfr5
Jan 10 08:11:47.985: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:47.985: INFO: Received response from host: affinity-clusterip-transition-snfr5
Jan 10 08:11:47.985: INFO: Received response from host: affinity-clusterip-transition-snfr5
Jan 10 08:11:47.985: INFO: Received response from host: affinity-clusterip-transition-snfr5
Jan 10 08:11:47.985: INFO: Received response from host: affinity-clusterip-transition-tcmtv
Jan 10 08:11:47.985: INFO: Received response from host: affinity-clusterip-transition-snfr5
Jan 10 08:11:47.985: INFO: Received response from host: affinity-clusterip-transition-tcmtv
Jan 10 08:11:47.985: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:47.985: INFO: Received response from host: affinity-clusterip-transition-tcmtv
Jan 10 08:11:47.985: INFO: Received response from host: affinity-clusterip-transition-snfr5
Jan 10 08:11:47.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-8851 exec execpod-affinity5wnhc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.63.116:80/ ; done'
Jan 10 08:11:48.336: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.116:80/\n"
Jan 10 08:11:48.336: INFO: stdout: "\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-xvrkn\naffinity-clusterip-transition-xvrkn"
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Received response from host: affinity-clusterip-transition-xvrkn
Jan 10 08:11:48.336: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8851, will wait for the garbage collector to delete the pods
Jan 10 08:11:48.452: INFO: Deleting ReplicationController affinity-clusterip-transition took: 8.216597ms
Jan 10 08:11:48.552: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.244267ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 08:11:51.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8851" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.823 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":308,"skipped":5805,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:11:51.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:11:51.126: INFO: Endpoints addresses: [192.168.0.3] , ports: [6443]
Jan 10 08:11:51.126: INFO: EndpointSlices addresses: [192.168.0.3] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 10 08:11:51.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3095" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":356,"completed":309,"skipped":5817,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:11:51.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 08:11:51.171: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e0d2974-dbc3-4f83-9a2a-710e4bb7f2f2" in namespace "projected-6853" to be "Succeeded or Failed"
Jan 10 08:11:51.179: INFO: Pod "downwardapi-volume-1e0d2974-dbc3-4f83-9a2a-710e4bb7f2f2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.204787ms
Jan 10 08:11:53.186: INFO: Pod "downwardapi-volume-1e0d2974-dbc3-4f83-9a2a-710e4bb7f2f2": Phase="Running", Reason="", readiness=false. Elapsed: 2.015010308s
Jan 10 08:11:55.192: INFO: Pod "downwardapi-volume-1e0d2974-dbc3-4f83-9a2a-710e4bb7f2f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021464879s
STEP: Saw pod success
Jan 10 08:11:55.192: INFO: Pod "downwardapi-volume-1e0d2974-dbc3-4f83-9a2a-710e4bb7f2f2" satisfied condition "Succeeded or Failed"
Jan 10 08:11:55.196: INFO: Trying to get logs from node kk-instance-r65pm pod downwardapi-volume-1e0d2974-dbc3-4f83-9a2a-710e4bb7f2f2 container client-container: <nil>
STEP: delete the pod
Jan 10 08:11:55.223: INFO: Waiting for pod downwardapi-volume-1e0d2974-dbc3-4f83-9a2a-710e4bb7f2f2 to disappear
Jan 10 08:11:55.226: INFO: Pod downwardapi-volume-1e0d2974-dbc3-4f83-9a2a-710e4bb7f2f2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 10 08:11:55.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6853" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":310,"skipped":5827,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:11:55.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-1c05b473-aed2-4796-b986-ada302908bdc in namespace container-probe-7748
Jan 10 08:11:57.280: INFO: Started pod liveness-1c05b473-aed2-4796-b986-ada302908bdc in namespace container-probe-7748
STEP: checking the pod's current state and verifying that restartCount is present
Jan 10 08:11:57.282: INFO: Initial restart count of pod liveness-1c05b473-aed2-4796-b986-ada302908bdc is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 10 08:15:58.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7748" for this suite.

• [SLOW TEST:243.119 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":356,"completed":311,"skipped":5840,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:15:58.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:15:58.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 10 08:15:58.431: INFO: The status of Pod pod-exec-websocket-0ac531d9-d6e8-4afe-88d0-d10a5eb75ef6 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 08:16:00.438: INFO: The status of Pod pod-exec-websocket-0ac531d9-d6e8-4afe-88d0-d10a5eb75ef6 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 10 08:16:00.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9925" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":356,"completed":312,"skipped":5842,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:16:00.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 10 08:16:00.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7443" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":356,"completed":313,"skipped":5854,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:16:00.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-101
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Jan 10 08:16:00.649: INFO: Found 0 stateful pods, waiting for 3
Jan 10 08:16:10.656: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 10 08:16:10.656: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 10 08:16:10.657: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 10 08:16:10.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-101 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 10 08:16:10.889: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 10 08:16:10.889: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 10 08:16:10.889: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jan 10 08:16:20.934: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 10 08:16:30.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-101 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 10 08:16:31.098: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 10 08:16:31.098: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 10 08:16:31.098: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Jan 10 08:16:41.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-101 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 10 08:16:41.271: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 10 08:16:41.271: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 10 08:16:41.271: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 10 08:16:51.310: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 10 08:17:01.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=statefulset-101 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 10 08:17:01.515: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 10 08:17:01.515: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 10 08:17:01.515: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 10 08:17:11.542: INFO: Deleting all statefulset in ns statefulset-101
Jan 10 08:17:11.544: INFO: Scaling statefulset ss2 to 0
Jan 10 08:17:21.567: INFO: Waiting for statefulset status.replicas updated to 0
Jan 10 08:17:21.570: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 10 08:17:21.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-101" for this suite.

• [SLOW TEST:81.014 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":356,"completed":314,"skipped":5867,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:17:21.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-b05fb6aa-fc58-43c8-b694-c1009b5c9d76
STEP: Creating configMap with name cm-test-opt-upd-1213d1c4-efed-4aa1-81f3-1b2c6e73f312
STEP: Creating the pod
Jan 10 08:17:21.697: INFO: The status of Pod pod-configmaps-f2e92a33-081c-472a-92ae-dcb0ba788cb6 is Pending, waiting for it to be Running (with Ready = true)
Jan 10 08:17:23.703: INFO: The status of Pod pod-configmaps-f2e92a33-081c-472a-92ae-dcb0ba788cb6 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-b05fb6aa-fc58-43c8-b694-c1009b5c9d76
STEP: Updating configmap cm-test-opt-upd-1213d1c4-efed-4aa1-81f3-1b2c6e73f312
STEP: Creating configMap with name cm-test-opt-create-6dbc62f6-cde2-41f1-bf73-66a712aa9e83
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 10 08:17:25.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1427" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":315,"skipped":5886,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:17:25.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 08:17:25.840: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ce6cdcb-e09f-4c74-b064-39124e82b4d5" in namespace "downward-api-5311" to be "Succeeded or Failed"
Jan 10 08:17:25.846: INFO: Pod "downwardapi-volume-5ce6cdcb-e09f-4c74-b064-39124e82b4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.579338ms
Jan 10 08:17:27.856: INFO: Pod "downwardapi-volume-5ce6cdcb-e09f-4c74-b064-39124e82b4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015628514s
Jan 10 08:17:29.861: INFO: Pod "downwardapi-volume-5ce6cdcb-e09f-4c74-b064-39124e82b4d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02042032s
STEP: Saw pod success
Jan 10 08:17:29.861: INFO: Pod "downwardapi-volume-5ce6cdcb-e09f-4c74-b064-39124e82b4d5" satisfied condition "Succeeded or Failed"
Jan 10 08:17:29.863: INFO: Trying to get logs from node kk-instance-vgmmg pod downwardapi-volume-5ce6cdcb-e09f-4c74-b064-39124e82b4d5 container client-container: <nil>
STEP: delete the pod
Jan 10 08:17:29.904: INFO: Waiting for pod downwardapi-volume-5ce6cdcb-e09f-4c74-b064-39124e82b4d5 to disappear
Jan 10 08:17:29.907: INFO: Pod downwardapi-volume-5ce6cdcb-e09f-4c74-b064-39124e82b4d5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 10 08:17:29.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5311" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":316,"skipped":5887,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:17:29.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 08:17:29.945: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ad5f26b7-f90e-4f12-a0a7-545ae34839dd" in namespace "projected-1758" to be "Succeeded or Failed"
Jan 10 08:17:29.949: INFO: Pod "downwardapi-volume-ad5f26b7-f90e-4f12-a0a7-545ae34839dd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.313574ms
Jan 10 08:17:31.957: INFO: Pod "downwardapi-volume-ad5f26b7-f90e-4f12-a0a7-545ae34839dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012019725s
Jan 10 08:17:33.965: INFO: Pod "downwardapi-volume-ad5f26b7-f90e-4f12-a0a7-545ae34839dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019811914s
STEP: Saw pod success
Jan 10 08:17:33.966: INFO: Pod "downwardapi-volume-ad5f26b7-f90e-4f12-a0a7-545ae34839dd" satisfied condition "Succeeded or Failed"
Jan 10 08:17:33.969: INFO: Trying to get logs from node kk-instance-vgmmg pod downwardapi-volume-ad5f26b7-f90e-4f12-a0a7-545ae34839dd container client-container: <nil>
STEP: delete the pod
Jan 10 08:17:33.984: INFO: Waiting for pod downwardapi-volume-ad5f26b7-f90e-4f12-a0a7-545ae34839dd to disappear
Jan 10 08:17:33.987: INFO: Pod downwardapi-volume-ad5f26b7-f90e-4f12-a0a7-545ae34839dd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 10 08:17:33.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1758" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":356,"completed":317,"skipped":5897,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:17:34.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-ab3c02f2-3e31-4586-b79d-1ed428d2210f in namespace container-probe-5830
Jan 10 08:17:36.049: INFO: Started pod liveness-ab3c02f2-3e31-4586-b79d-1ed428d2210f in namespace container-probe-5830
STEP: checking the pod's current state and verifying that restartCount is present
Jan 10 08:17:36.052: INFO: Initial restart count of pod liveness-ab3c02f2-3e31-4586-b79d-1ed428d2210f is 0
Jan 10 08:17:56.151: INFO: Restart count of pod container-probe-5830/liveness-ab3c02f2-3e31-4586-b79d-1ed428d2210f is now 1 (20.098867602s elapsed)
Jan 10 08:18:16.231: INFO: Restart count of pod container-probe-5830/liveness-ab3c02f2-3e31-4586-b79d-1ed428d2210f is now 2 (40.179559029s elapsed)
Jan 10 08:18:36.322: INFO: Restart count of pod container-probe-5830/liveness-ab3c02f2-3e31-4586-b79d-1ed428d2210f is now 3 (1m0.269831273s elapsed)
Jan 10 08:18:56.401: INFO: Restart count of pod container-probe-5830/liveness-ab3c02f2-3e31-4586-b79d-1ed428d2210f is now 4 (1m20.349476805s elapsed)
Jan 10 08:20:08.741: INFO: Restart count of pod container-probe-5830/liveness-ab3c02f2-3e31-4586-b79d-1ed428d2210f is now 5 (2m32.689000194s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 10 08:20:08.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5830" for this suite.

• [SLOW TEST:154.770 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":356,"completed":318,"skipped":5903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:20:08.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 10 08:20:08.822: INFO: Waiting up to 5m0s for pod "pod-9c6d453f-6650-438b-8b82-1f3607c70bd4" in namespace "emptydir-6715" to be "Succeeded or Failed"
Jan 10 08:20:08.825: INFO: Pod "pod-9c6d453f-6650-438b-8b82-1f3607c70bd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.874694ms
Jan 10 08:20:10.831: INFO: Pod "pod-9c6d453f-6650-438b-8b82-1f3607c70bd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008306667s
Jan 10 08:20:12.848: INFO: Pod "pod-9c6d453f-6650-438b-8b82-1f3607c70bd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025545898s
STEP: Saw pod success
Jan 10 08:20:12.848: INFO: Pod "pod-9c6d453f-6650-438b-8b82-1f3607c70bd4" satisfied condition "Succeeded or Failed"
Jan 10 08:20:12.850: INFO: Trying to get logs from node kk-instance-r65pm pod pod-9c6d453f-6650-438b-8b82-1f3607c70bd4 container test-container: <nil>
STEP: delete the pod
Jan 10 08:20:12.876: INFO: Waiting for pod pod-9c6d453f-6650-438b-8b82-1f3607c70bd4 to disappear
Jan 10 08:20:12.880: INFO: Pod pod-9c6d453f-6650-438b-8b82-1f3607c70bd4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 10 08:20:12.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6715" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":319,"skipped":5935,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:20:12.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jan 10 08:20:14.957: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3839 PodName:var-expansion-634d7ee7-4573-4ebc-8fa4-fff8d60646cf ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 08:20:14.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 08:20:14.958: INFO: ExecWithOptions: Clientset creation
Jan 10 08:20:14.958: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-3839/pods/var-expansion-634d7ee7-4573-4ebc-8fa4-fff8d60646cf/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path
Jan 10 08:20:15.038: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3839 PodName:var-expansion-634d7ee7-4573-4ebc-8fa4-fff8d60646cf ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 10 08:20:15.039: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 08:20:15.040: INFO: ExecWithOptions: Clientset creation
Jan 10 08:20:15.040: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-3839/pods/var-expansion-634d7ee7-4573-4ebc-8fa4-fff8d60646cf/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value
Jan 10 08:20:15.632: INFO: Successfully updated pod "var-expansion-634d7ee7-4573-4ebc-8fa4-fff8d60646cf"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jan 10 08:20:15.638: INFO: Deleting pod "var-expansion-634d7ee7-4573-4ebc-8fa4-fff8d60646cf" in namespace "var-expansion-3839"
Jan 10 08:20:15.647: INFO: Wait up to 5m0s for pod "var-expansion-634d7ee7-4573-4ebc-8fa4-fff8d60646cf" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 10 08:20:49.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3839" for this suite.

• [SLOW TEST:36.771 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":356,"completed":320,"skipped":5996,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:20:49.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 10 08:20:49.719: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 10 08:21:49.753: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:21:49.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jan 10 08:21:51.825: INFO: found a healthy node: kk-instance-r65pm
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:22:01.936: INFO: pods created so far: [1 1 1]
Jan 10 08:22:01.936: INFO: length of pods created so far: 3
Jan 10 08:22:05.958: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:188
Jan 10 08:22:12.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6011" for this suite.
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 10 08:22:13.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6675" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:83.398 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":356,"completed":321,"skipped":6050,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:22:13.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service nodeport-test with type=NodePort in namespace services-8639
STEP: creating replication controller nodeport-test in namespace services-8639
I0110 08:22:13.125219      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8639, replica count: 2
I0110 08:22:16.176948      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 10 08:22:16.177: INFO: Creating new exec pod
Jan 10 08:22:19.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-8639 exec execpodqk7qb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 10 08:22:19.361: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 10 08:22:19.361: INFO: stdout: ""
Jan 10 08:22:20.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-8639 exec execpodqk7qb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 10 08:22:20.549: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 10 08:22:20.549: INFO: stdout: "nodeport-test-kgdq5"
Jan 10 08:22:20.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-8639 exec execpodqk7qb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.146 80'
Jan 10 08:22:20.695: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.41.146 80\nConnection to 10.233.41.146 80 port [tcp/http] succeeded!\n"
Jan 10 08:22:20.695: INFO: stdout: ""
Jan 10 08:22:21.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-8639 exec execpodqk7qb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.146 80'
Jan 10 08:22:21.863: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.41.146 80\nConnection to 10.233.41.146 80 port [tcp/http] succeeded!\n"
Jan 10 08:22:21.863: INFO: stdout: "nodeport-test-g4fnj"
Jan 10 08:22:21.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-8639 exec execpodqk7qb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.4 32000'
Jan 10 08:22:22.002: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.4 32000\nConnection to 192.168.0.4 32000 port [tcp/*] succeeded!\n"
Jan 10 08:22:22.002: INFO: stdout: "nodeport-test-kgdq5"
Jan 10 08:22:22.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-8639 exec execpodqk7qb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.5 32000'
Jan 10 08:22:22.165: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.5 32000\nConnection to 192.168.0.5 32000 port [tcp/*] succeeded!\n"
Jan 10 08:22:22.165: INFO: stdout: ""
Jan 10 08:22:23.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-8639 exec execpodqk7qb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.5 32000'
Jan 10 08:22:23.334: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.5 32000\nConnection to 192.168.0.5 32000 port [tcp/*] succeeded!\n"
Jan 10 08:22:23.334: INFO: stdout: "nodeport-test-g4fnj"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 08:22:23.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8639" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:10.276 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":356,"completed":322,"skipped":6063,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:22:23.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jan 10 08:22:23.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
Jan 10 08:22:26.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 10 08:22:39.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4172" for this suite.

• [SLOW TEST:16.126 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":356,"completed":323,"skipped":6079,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:22:39.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4498
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4498
I0110 08:22:39.529437      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4498, replica count: 2
Jan 10 08:22:42.580: INFO: Creating new exec pod
I0110 08:22:42.580824      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 10 08:22:45.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-4498 exec execpod5sh6r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 10 08:22:45.803: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 10 08:22:45.803: INFO: stdout: ""
Jan 10 08:22:46.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-4498 exec execpod5sh6r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 10 08:22:46.946: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 10 08:22:46.946: INFO: stdout: "externalname-service-bbdxp"
Jan 10 08:22:46.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-4498 exec execpod5sh6r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.15.154 80'
Jan 10 08:22:47.099: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.15.154 80\nConnection to 10.233.15.154 80 port [tcp/http] succeeded!\n"
Jan 10 08:22:47.099: INFO: stdout: "externalname-service-9tbvs"
Jan 10 08:22:47.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-4498 exec execpod5sh6r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.4 31692'
Jan 10 08:22:47.249: INFO: stderr: "+ nc -v -t -w 2 192.168.0.4 31692\n+ echo hostName\nConnection to 192.168.0.4 31692 port [tcp/*] succeeded!\n"
Jan 10 08:22:47.249: INFO: stdout: ""
Jan 10 08:22:48.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-4498 exec execpod5sh6r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.4 31692'
Jan 10 08:22:48.504: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.4 31692\nConnection to 192.168.0.4 31692 port [tcp/*] succeeded!\n"
Jan 10 08:22:48.504: INFO: stdout: "externalname-service-9tbvs"
Jan 10 08:22:48.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-4498 exec execpod5sh6r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.5 31692'
Jan 10 08:22:48.680: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.5 31692\nConnection to 192.168.0.5 31692 port [tcp/*] succeeded!\n"
Jan 10 08:22:48.680: INFO: stdout: ""
Jan 10 08:22:49.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-4498 exec execpod5sh6r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.5 31692'
Jan 10 08:22:49.832: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.5 31692\nConnection to 192.168.0.5 31692 port [tcp/*] succeeded!\n"
Jan 10 08:22:49.832: INFO: stdout: "externalname-service-bbdxp"
Jan 10 08:22:49.832: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 08:22:49.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4498" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:10.397 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":356,"completed":324,"skipped":6084,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:22:49.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating cluster-info
Jan 10 08:22:49.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8343 cluster-info'
Jan 10 08:22:49.985: INFO: stderr: ""
Jan 10 08:22:49.985: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 08:22:49.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8343" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":356,"completed":325,"skipped":6125,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:22:49.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Jan 10 08:22:50.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 create -f -'
Jan 10 08:22:50.827: INFO: stderr: ""
Jan 10 08:22:50.827: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 10 08:22:50.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 10 08:22:50.912: INFO: stderr: ""
Jan 10 08:22:50.912: INFO: stdout: "update-demo-nautilus-fv6ns update-demo-nautilus-glg5p "
Jan 10 08:22:50.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods update-demo-nautilus-fv6ns -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 10 08:22:50.986: INFO: stderr: ""
Jan 10 08:22:50.986: INFO: stdout: ""
Jan 10 08:22:50.986: INFO: update-demo-nautilus-fv6ns is created but not running
Jan 10 08:22:55.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 10 08:22:56.118: INFO: stderr: ""
Jan 10 08:22:56.118: INFO: stdout: "update-demo-nautilus-fv6ns update-demo-nautilus-glg5p "
Jan 10 08:22:56.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods update-demo-nautilus-fv6ns -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 10 08:22:56.226: INFO: stderr: ""
Jan 10 08:22:56.226: INFO: stdout: "true"
Jan 10 08:22:56.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods update-demo-nautilus-fv6ns -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 10 08:22:56.357: INFO: stderr: ""
Jan 10 08:22:56.357: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 10 08:22:56.357: INFO: validating pod update-demo-nautilus-fv6ns
Jan 10 08:22:56.362: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 10 08:22:56.362: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 10 08:22:56.362: INFO: update-demo-nautilus-fv6ns is verified up and running
Jan 10 08:22:56.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods update-demo-nautilus-glg5p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 10 08:22:56.470: INFO: stderr: ""
Jan 10 08:22:56.470: INFO: stdout: "true"
Jan 10 08:22:56.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods update-demo-nautilus-glg5p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 10 08:22:56.549: INFO: stderr: ""
Jan 10 08:22:56.550: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 10 08:22:56.550: INFO: validating pod update-demo-nautilus-glg5p
Jan 10 08:22:56.556: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 10 08:22:56.556: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 10 08:22:56.556: INFO: update-demo-nautilus-glg5p is verified up and running
STEP: scaling down the replication controller
Jan 10 08:22:56.557: INFO: scanned /root for discovery docs: <nil>
Jan 10 08:22:56.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 10 08:22:57.651: INFO: stderr: ""
Jan 10 08:22:57.652: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 10 08:22:57.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 10 08:22:57.733: INFO: stderr: ""
Jan 10 08:22:57.733: INFO: stdout: "update-demo-nautilus-fv6ns update-demo-nautilus-glg5p "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 10 08:23:02.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 10 08:23:02.816: INFO: stderr: ""
Jan 10 08:23:02.816: INFO: stdout: "update-demo-nautilus-glg5p "
Jan 10 08:23:02.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods update-demo-nautilus-glg5p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 10 08:23:02.887: INFO: stderr: ""
Jan 10 08:23:02.887: INFO: stdout: "true"
Jan 10 08:23:02.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods update-demo-nautilus-glg5p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 10 08:23:02.965: INFO: stderr: ""
Jan 10 08:23:02.965: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 10 08:23:02.965: INFO: validating pod update-demo-nautilus-glg5p
Jan 10 08:23:02.968: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 10 08:23:02.968: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 10 08:23:02.968: INFO: update-demo-nautilus-glg5p is verified up and running
STEP: scaling up the replication controller
Jan 10 08:23:02.969: INFO: scanned /root for discovery docs: <nil>
Jan 10 08:23:02.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 10 08:23:04.065: INFO: stderr: ""
Jan 10 08:23:04.065: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 10 08:23:04.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 10 08:23:04.148: INFO: stderr: ""
Jan 10 08:23:04.148: INFO: stdout: "update-demo-nautilus-9rmpk update-demo-nautilus-glg5p "
Jan 10 08:23:04.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods update-demo-nautilus-9rmpk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 10 08:23:04.229: INFO: stderr: ""
Jan 10 08:23:04.229: INFO: stdout: ""
Jan 10 08:23:04.229: INFO: update-demo-nautilus-9rmpk is created but not running
Jan 10 08:23:09.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 10 08:23:09.310: INFO: stderr: ""
Jan 10 08:23:09.310: INFO: stdout: "update-demo-nautilus-9rmpk update-demo-nautilus-glg5p "
Jan 10 08:23:09.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods update-demo-nautilus-9rmpk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 10 08:23:09.384: INFO: stderr: ""
Jan 10 08:23:09.384: INFO: stdout: "true"
Jan 10 08:23:09.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods update-demo-nautilus-9rmpk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 10 08:23:09.463: INFO: stderr: ""
Jan 10 08:23:09.463: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 10 08:23:09.463: INFO: validating pod update-demo-nautilus-9rmpk
Jan 10 08:23:09.469: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 10 08:23:09.469: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 10 08:23:09.469: INFO: update-demo-nautilus-9rmpk is verified up and running
Jan 10 08:23:09.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods update-demo-nautilus-glg5p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 10 08:23:09.543: INFO: stderr: ""
Jan 10 08:23:09.543: INFO: stdout: "true"
Jan 10 08:23:09.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods update-demo-nautilus-glg5p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 10 08:23:09.628: INFO: stderr: ""
Jan 10 08:23:09.628: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 10 08:23:09.628: INFO: validating pod update-demo-nautilus-glg5p
Jan 10 08:23:09.634: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 10 08:23:09.634: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 10 08:23:09.634: INFO: update-demo-nautilus-glg5p is verified up and running
STEP: using delete to clean up resources
Jan 10 08:23:09.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 delete --grace-period=0 --force -f -'
Jan 10 08:23:09.731: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 10 08:23:09.731: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 10 08:23:09.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get rc,svc -l name=update-demo --no-headers'
Jan 10 08:23:09.870: INFO: stderr: "No resources found in kubectl-8185 namespace.\n"
Jan 10 08:23:09.870: INFO: stdout: ""
Jan 10 08:23:09.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=kubectl-8185 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 10 08:23:09.980: INFO: stderr: ""
Jan 10 08:23:09.980: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 10 08:23:09.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8185" for this suite.

• [SLOW TEST:19.996 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should scale a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":356,"completed":326,"skipped":6144,"failed":0}
SSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:23:09.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name secret-emptykey-test-ebb3e1b1-df5f-4d51-a956-56dd4c5aa584
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 10 08:23:10.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6996" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":356,"completed":327,"skipped":6150,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:23:10.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 10 08:23:10.077: INFO: The status of Pod annotationupdatef6aa5f53-e58a-4d7a-8668-7ed8ab669e0f is Pending, waiting for it to be Running (with Ready = true)
Jan 10 08:23:12.086: INFO: The status of Pod annotationupdatef6aa5f53-e58a-4d7a-8668-7ed8ab669e0f is Running (Ready = true)
Jan 10 08:23:12.627: INFO: Successfully updated pod "annotationupdatef6aa5f53-e58a-4d7a-8668-7ed8ab669e0f"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 10 08:23:16.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-29" for this suite.

• [SLOW TEST:6.630 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":328,"skipped":6155,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:23:16.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-c741d75b-b383-4ea1-98a9-3a92a82fde58 in namespace container-probe-5598
Jan 10 08:23:18.712: INFO: Started pod busybox-c741d75b-b383-4ea1-98a9-3a92a82fde58 in namespace container-probe-5598
STEP: checking the pod's current state and verifying that restartCount is present
Jan 10 08:23:18.714: INFO: Initial restart count of pod busybox-c741d75b-b383-4ea1-98a9-3a92a82fde58 is 0
Jan 10 08:24:08.903: INFO: Restart count of pod container-probe-5598/busybox-c741d75b-b383-4ea1-98a9-3a92a82fde58 is now 1 (50.188305643s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 10 08:24:08.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5598" for this suite.

• [SLOW TEST:52.256 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":329,"skipped":6171,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:24:08.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 10 08:24:08.955: INFO: Waiting up to 5m0s for pod "pod-5ceefefd-b139-4e35-a8de-2e98e53e96b7" in namespace "emptydir-2637" to be "Succeeded or Failed"
Jan 10 08:24:08.959: INFO: Pod "pod-5ceefefd-b139-4e35-a8de-2e98e53e96b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.397854ms
Jan 10 08:24:10.966: INFO: Pod "pod-5ceefefd-b139-4e35-a8de-2e98e53e96b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010949797s
Jan 10 08:24:12.973: INFO: Pod "pod-5ceefefd-b139-4e35-a8de-2e98e53e96b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017622985s
STEP: Saw pod success
Jan 10 08:24:12.973: INFO: Pod "pod-5ceefefd-b139-4e35-a8de-2e98e53e96b7" satisfied condition "Succeeded or Failed"
Jan 10 08:24:12.976: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-5ceefefd-b139-4e35-a8de-2e98e53e96b7 container test-container: <nil>
STEP: delete the pod
Jan 10 08:24:13.002: INFO: Waiting for pod pod-5ceefefd-b139-4e35-a8de-2e98e53e96b7 to disappear
Jan 10 08:24:13.005: INFO: Pod pod-5ceefefd-b139-4e35-a8de-2e98e53e96b7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 10 08:24:13.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2637" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":330,"skipped":6234,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:24:13.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 10 08:24:13.041: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 10 08:24:13.050: INFO: Waiting for terminating namespaces to be deleted...
Jan 10 08:24:13.053: INFO: 
Logging pods the apiserver thinks is on node kk-instance-r65pm before test
Jan 10 08:24:13.059: INFO: calico-kube-controllers-6799f5f4b4-g6h9q from kube-system started at 2023-01-10 06:28:45 +0000 UTC (1 container statuses recorded)
Jan 10 08:24:13.059: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 10 08:24:13.059: INFO: calico-node-khhj9 from kube-system started at 2023-01-10 06:28:32 +0000 UTC (1 container statuses recorded)
Jan 10 08:24:13.059: INFO: 	Container calico-node ready: true, restart count 0
Jan 10 08:24:13.059: INFO: coredns-6d4b75cb6d-4wczh from kube-system started at 2023-01-10 06:28:45 +0000 UTC (1 container statuses recorded)
Jan 10 08:24:13.059: INFO: 	Container coredns ready: true, restart count 0
Jan 10 08:24:13.059: INFO: coredns-6d4b75cb6d-f7d8c from kube-system started at 2023-01-10 06:28:45 +0000 UTC (1 container statuses recorded)
Jan 10 08:24:13.059: INFO: 	Container coredns ready: true, restart count 0
Jan 10 08:24:13.059: INFO: kube-proxy-2v5cp from kube-system started at 2023-01-10 06:04:35 +0000 UTC (1 container statuses recorded)
Jan 10 08:24:13.059: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 10 08:24:13.060: INFO: sonobuoy-systemd-logs-daemon-set-5167b5d313ce4ad1-6zjp9 from sonobuoy started at 2023-01-10 06:56:07 +0000 UTC (2 container statuses recorded)
Jan 10 08:24:13.060: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 10 08:24:13.060: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 10 08:24:13.060: INFO: 
Logging pods the apiserver thinks is on node kk-instance-vgmmg before test
Jan 10 08:24:13.066: INFO: calico-node-wf8tr from kube-system started at 2023-01-10 06:42:17 +0000 UTC (1 container statuses recorded)
Jan 10 08:24:13.066: INFO: 	Container calico-node ready: true, restart count 0
Jan 10 08:24:13.066: INFO: kube-proxy-vrc4r from kube-system started at 2023-01-10 06:42:17 +0000 UTC (1 container statuses recorded)
Jan 10 08:24:13.066: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 10 08:24:13.066: INFO: sonobuoy from sonobuoy started at 2023-01-10 06:56:06 +0000 UTC (1 container statuses recorded)
Jan 10 08:24:13.066: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 10 08:24:13.066: INFO: sonobuoy-e2e-job-86252adb0f534a01 from sonobuoy started at 2023-01-10 06:56:07 +0000 UTC (2 container statuses recorded)
Jan 10 08:24:13.066: INFO: 	Container e2e ready: true, restart count 0
Jan 10 08:24:13.066: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 10 08:24:13.066: INFO: sonobuoy-systemd-logs-daemon-set-5167b5d313ce4ad1-smm8d from sonobuoy started at 2023-01-10 06:56:07 +0000 UTC (2 container statuses recorded)
Jan 10 08:24:13.066: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 10 08:24:13.066: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
STEP: verifying the node has the label node kk-instance-r65pm
STEP: verifying the node has the label node kk-instance-vgmmg
Jan 10 08:24:13.146: INFO: Pod calico-kube-controllers-6799f5f4b4-g6h9q requesting resource cpu=0m on Node kk-instance-r65pm
Jan 10 08:24:13.146: INFO: Pod calico-node-khhj9 requesting resource cpu=250m on Node kk-instance-r65pm
Jan 10 08:24:13.146: INFO: Pod calico-node-wf8tr requesting resource cpu=250m on Node kk-instance-vgmmg
Jan 10 08:24:13.146: INFO: Pod coredns-6d4b75cb6d-4wczh requesting resource cpu=100m on Node kk-instance-r65pm
Jan 10 08:24:13.146: INFO: Pod coredns-6d4b75cb6d-f7d8c requesting resource cpu=100m on Node kk-instance-r65pm
Jan 10 08:24:13.146: INFO: Pod kube-proxy-2v5cp requesting resource cpu=0m on Node kk-instance-r65pm
Jan 10 08:24:13.146: INFO: Pod kube-proxy-vrc4r requesting resource cpu=0m on Node kk-instance-vgmmg
Jan 10 08:24:13.146: INFO: Pod sonobuoy requesting resource cpu=0m on Node kk-instance-vgmmg
Jan 10 08:24:13.146: INFO: Pod sonobuoy-e2e-job-86252adb0f534a01 requesting resource cpu=0m on Node kk-instance-vgmmg
Jan 10 08:24:13.146: INFO: Pod sonobuoy-systemd-logs-daemon-set-5167b5d313ce4ad1-6zjp9 requesting resource cpu=0m on Node kk-instance-r65pm
Jan 10 08:24:13.146: INFO: Pod sonobuoy-systemd-logs-daemon-set-5167b5d313ce4ad1-smm8d requesting resource cpu=0m on Node kk-instance-vgmmg
STEP: Starting Pods to consume most of the cluster CPU.
Jan 10 08:24:13.146: INFO: Creating a pod which consumes cpu=1085m on Node kk-instance-r65pm
Jan 10 08:24:13.153: INFO: Creating a pod which consumes cpu=1225m on Node kk-instance-vgmmg
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-68474a04-d8c8-4acd-87a7-ce0dc3ffb8c0.1738e500ccb18294], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1578/filler-pod-68474a04-d8c8-4acd-87a7-ce0dc3ffb8c0 to kk-instance-r65pm]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-68474a04-d8c8-4acd-87a7-ce0dc3ffb8c0.1738e500f78d28cc], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-68474a04-d8c8-4acd-87a7-ce0dc3ffb8c0.1738e500f86a0f49], Reason = [Created], Message = [Created container filler-pod-68474a04-d8c8-4acd-87a7-ce0dc3ffb8c0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-68474a04-d8c8-4acd-87a7-ce0dc3ffb8c0.1738e500fe12fae1], Reason = [Started], Message = [Started container filler-pod-68474a04-d8c8-4acd-87a7-ce0dc3ffb8c0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8dcf864-ced6-42b9-bcb5-0b5a76b2aca3.1738e500cd60b557], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1578/filler-pod-a8dcf864-ced6-42b9-bcb5-0b5a76b2aca3 to kk-instance-vgmmg]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8dcf864-ced6-42b9-bcb5-0b5a76b2aca3.1738e500f787ada6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8dcf864-ced6-42b9-bcb5-0b5a76b2aca3.1738e500f8beec95], Reason = [Created], Message = [Created container filler-pod-a8dcf864-ced6-42b9-bcb5-0b5a76b2aca3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8dcf864-ced6-42b9-bcb5-0b5a76b2aca3.1738e500fe0d1537], Reason = [Started], Message = [Started container filler-pod-a8dcf864-ced6-42b9-bcb5-0b5a76b2aca3]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1738e50145a74c21], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.]
STEP: removing the label node off the node kk-instance-r65pm
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node kk-instance-vgmmg
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 10 08:24:16.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1578" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":356,"completed":331,"skipped":6235,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:24:16.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 10 08:25:16.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4882" for this suite.

• [SLOW TEST:60.055 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":356,"completed":332,"skipped":6241,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:25:16.321: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 10 08:25:27.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8459" for this suite.

• [SLOW TEST:11.094 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":356,"completed":333,"skipped":6258,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:25:27.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 10 08:25:27.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9681" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":356,"completed":334,"skipped":6268,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:25:27.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jan 10 08:25:40.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1017" for this suite.
STEP: Destroying namespace "nsdeletetest-303" for this suite.
Jan 10 08:25:40.614: INFO: Namespace nsdeletetest-303 was already deleted
STEP: Destroying namespace "nsdeletetest-7038" for this suite.

• [SLOW TEST:13.115 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":356,"completed":335,"skipped":6332,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:25:40.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 10 08:25:40.650: INFO: Waiting up to 5m0s for pod "downward-api-d8519dd7-5d16-4c5c-8a9c-543691b60ad5" in namespace "downward-api-5717" to be "Succeeded or Failed"
Jan 10 08:25:40.653: INFO: Pod "downward-api-d8519dd7-5d16-4c5c-8a9c-543691b60ad5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.532447ms
Jan 10 08:25:42.677: INFO: Pod "downward-api-d8519dd7-5d16-4c5c-8a9c-543691b60ad5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02762747s
Jan 10 08:25:44.686: INFO: Pod "downward-api-d8519dd7-5d16-4c5c-8a9c-543691b60ad5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036416905s
STEP: Saw pod success
Jan 10 08:25:44.686: INFO: Pod "downward-api-d8519dd7-5d16-4c5c-8a9c-543691b60ad5" satisfied condition "Succeeded or Failed"
Jan 10 08:25:44.689: INFO: Trying to get logs from node kk-instance-r65pm pod downward-api-d8519dd7-5d16-4c5c-8a9c-543691b60ad5 container dapi-container: <nil>
STEP: delete the pod
Jan 10 08:25:44.717: INFO: Waiting for pod downward-api-d8519dd7-5d16-4c5c-8a9c-543691b60ad5 to disappear
Jan 10 08:25:44.719: INFO: Pod downward-api-d8519dd7-5d16-4c5c-8a9c-543691b60ad5 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 10 08:25:44.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5717" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":356,"completed":336,"skipped":6350,"failed":0}
SSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:25:44.729: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jan 10 08:25:44.762: INFO: Waiting up to 5m0s for pod "security-context-61a08f30-4f1d-4645-82a5-9b3dcd4e3ee2" in namespace "security-context-7902" to be "Succeeded or Failed"
Jan 10 08:25:44.770: INFO: Pod "security-context-61a08f30-4f1d-4645-82a5-9b3dcd4e3ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.06303ms
Jan 10 08:25:46.779: INFO: Pod "security-context-61a08f30-4f1d-4645-82a5-9b3dcd4e3ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017703617s
Jan 10 08:25:48.790: INFO: Pod "security-context-61a08f30-4f1d-4645-82a5-9b3dcd4e3ee2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028136479s
STEP: Saw pod success
Jan 10 08:25:48.790: INFO: Pod "security-context-61a08f30-4f1d-4645-82a5-9b3dcd4e3ee2" satisfied condition "Succeeded or Failed"
Jan 10 08:25:48.793: INFO: Trying to get logs from node kk-instance-vgmmg pod security-context-61a08f30-4f1d-4645-82a5-9b3dcd4e3ee2 container test-container: <nil>
STEP: delete the pod
Jan 10 08:25:48.817: INFO: Waiting for pod security-context-61a08f30-4f1d-4645-82a5-9b3dcd4e3ee2 to disappear
Jan 10 08:25:48.819: INFO: Pod security-context-61a08f30-4f1d-4645-82a5-9b3dcd4e3ee2 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 10 08:25:48.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7902" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":337,"skipped":6355,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:25:48.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:25:48.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8679
I0110 08:25:48.866902      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8679, replica count: 1
I0110 08:25:49.918444      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0110 08:25:50.919491      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 10 08:25:51.031: INFO: Created: latency-svc-qf6v8
Jan 10 08:25:51.041: INFO: Got endpoints: latency-svc-qf6v8 [20.317081ms]
Jan 10 08:25:51.059: INFO: Created: latency-svc-2rmz5
Jan 10 08:25:51.071: INFO: Got endpoints: latency-svc-2rmz5 [29.840556ms]
Jan 10 08:25:51.072: INFO: Created: latency-svc-2snwv
Jan 10 08:25:51.078: INFO: Got endpoints: latency-svc-2snwv [37.657215ms]
Jan 10 08:25:51.083: INFO: Created: latency-svc-7ptzc
Jan 10 08:25:51.092: INFO: Got endpoints: latency-svc-7ptzc [50.740192ms]
Jan 10 08:25:51.102: INFO: Created: latency-svc-n4s89
Jan 10 08:25:51.107: INFO: Got endpoints: latency-svc-n4s89 [64.733328ms]
Jan 10 08:25:51.110: INFO: Created: latency-svc-pxbhv
Jan 10 08:25:51.117: INFO: Got endpoints: latency-svc-pxbhv [74.743018ms]
Jan 10 08:25:51.119: INFO: Created: latency-svc-prbd6
Jan 10 08:25:51.124: INFO: Got endpoints: latency-svc-prbd6 [81.68632ms]
Jan 10 08:25:51.130: INFO: Created: latency-svc-hg68h
Jan 10 08:25:51.136: INFO: Got endpoints: latency-svc-hg68h [94.316548ms]
Jan 10 08:25:51.140: INFO: Created: latency-svc-9mm9z
Jan 10 08:25:51.149: INFO: Got endpoints: latency-svc-9mm9z [106.492306ms]
Jan 10 08:25:51.157: INFO: Created: latency-svc-xndpf
Jan 10 08:25:51.163: INFO: Got endpoints: latency-svc-xndpf [120.345197ms]
Jan 10 08:25:51.167: INFO: Created: latency-svc-rjffm
Jan 10 08:25:51.173: INFO: Got endpoints: latency-svc-rjffm [130.362558ms]
Jan 10 08:25:51.181: INFO: Created: latency-svc-tqpf7
Jan 10 08:25:51.183: INFO: Created: latency-svc-lx74h
Jan 10 08:25:51.188: INFO: Created: latency-svc-8dph5
Jan 10 08:25:51.192: INFO: Got endpoints: latency-svc-tqpf7 [149.283312ms]
Jan 10 08:25:51.194: INFO: Got endpoints: latency-svc-lx74h [150.122368ms]
Jan 10 08:25:51.202: INFO: Created: latency-svc-dv97b
Jan 10 08:25:51.204: INFO: Got endpoints: latency-svc-8dph5 [161.690403ms]
Jan 10 08:25:51.213: INFO: Got endpoints: latency-svc-dv97b [170.244427ms]
Jan 10 08:25:51.217: INFO: Created: latency-svc-jz9f8
Jan 10 08:25:51.222: INFO: Got endpoints: latency-svc-jz9f8 [178.816855ms]
Jan 10 08:25:51.225: INFO: Created: latency-svc-wlk48
Jan 10 08:25:51.231: INFO: Created: latency-svc-slbz5
Jan 10 08:25:51.236: INFO: Got endpoints: latency-svc-wlk48 [165.363753ms]
Jan 10 08:25:51.241: INFO: Got endpoints: latency-svc-slbz5 [162.100977ms]
Jan 10 08:25:51.246: INFO: Created: latency-svc-brccx
Jan 10 08:25:51.252: INFO: Got endpoints: latency-svc-brccx [160.296878ms]
Jan 10 08:25:51.260: INFO: Created: latency-svc-zmt2b
Jan 10 08:25:51.262: INFO: Got endpoints: latency-svc-zmt2b [154.923707ms]
Jan 10 08:25:51.269: INFO: Created: latency-svc-f5z5l
Jan 10 08:25:51.275: INFO: Created: latency-svc-nmqqs
Jan 10 08:25:51.278: INFO: Got endpoints: latency-svc-f5z5l [161.384682ms]
Jan 10 08:25:51.281: INFO: Got endpoints: latency-svc-nmqqs [157.401032ms]
Jan 10 08:25:51.287: INFO: Created: latency-svc-kgg7x
Jan 10 08:25:51.293: INFO: Got endpoints: latency-svc-kgg7x [157.084983ms]
Jan 10 08:25:51.296: INFO: Created: latency-svc-6wvfj
Jan 10 08:25:51.303: INFO: Got endpoints: latency-svc-6wvfj [153.839738ms]
Jan 10 08:25:51.307: INFO: Created: latency-svc-kv28t
Jan 10 08:25:51.312: INFO: Got endpoints: latency-svc-kv28t [148.867992ms]
Jan 10 08:25:51.316: INFO: Created: latency-svc-npq7p
Jan 10 08:25:51.335: INFO: Got endpoints: latency-svc-npq7p [161.522097ms]
Jan 10 08:25:51.336: INFO: Created: latency-svc-4q74z
Jan 10 08:25:51.336: INFO: Got endpoints: latency-svc-4q74z [144.087919ms]
Jan 10 08:25:51.337: INFO: Created: latency-svc-46gzb
Jan 10 08:25:51.340: INFO: Got endpoints: latency-svc-46gzb [145.90836ms]
Jan 10 08:25:51.348: INFO: Created: latency-svc-d92jb
Jan 10 08:25:51.351: INFO: Created: latency-svc-mgz9d
Jan 10 08:25:51.354: INFO: Got endpoints: latency-svc-d92jb [150.088802ms]
Jan 10 08:25:51.359: INFO: Got endpoints: latency-svc-mgz9d [145.300172ms]
Jan 10 08:25:51.366: INFO: Created: latency-svc-2gtzm
Jan 10 08:25:51.377: INFO: Created: latency-svc-z4492
Jan 10 08:25:51.382: INFO: Got endpoints: latency-svc-2gtzm [159.834276ms]
Jan 10 08:25:51.386: INFO: Got endpoints: latency-svc-z4492 [149.120012ms]
Jan 10 08:25:51.395: INFO: Created: latency-svc-wnnv5
Jan 10 08:25:51.400: INFO: Got endpoints: latency-svc-wnnv5 [159.140905ms]
Jan 10 08:25:51.405: INFO: Created: latency-svc-lzp22
Jan 10 08:25:51.412: INFO: Created: latency-svc-mwtd2
Jan 10 08:25:51.415: INFO: Got endpoints: latency-svc-lzp22 [162.877449ms]
Jan 10 08:25:51.420: INFO: Got endpoints: latency-svc-mwtd2 [158.48562ms]
Jan 10 08:25:51.434: INFO: Created: latency-svc-c9tjt
Jan 10 08:25:51.435: INFO: Got endpoints: latency-svc-c9tjt [156.277227ms]
Jan 10 08:25:51.435: INFO: Created: latency-svc-88458
Jan 10 08:25:51.447: INFO: Created: latency-svc-dgx87
Jan 10 08:25:51.450: INFO: Got endpoints: latency-svc-88458 [168.558631ms]
Jan 10 08:25:51.453: INFO: Created: latency-svc-zpxqx
Jan 10 08:25:51.457: INFO: Got endpoints: latency-svc-dgx87 [163.927327ms]
Jan 10 08:25:51.464: INFO: Created: latency-svc-mxr6g
Jan 10 08:25:51.469: INFO: Created: latency-svc-vqwfs
Jan 10 08:25:51.477: INFO: Created: latency-svc-7xd9t
Jan 10 08:25:51.484: INFO: Created: latency-svc-dpt4z
Jan 10 08:25:51.494: INFO: Got endpoints: latency-svc-zpxqx [190.615127ms]
Jan 10 08:25:51.495: INFO: Created: latency-svc-zxv4j
Jan 10 08:25:51.503: INFO: Created: latency-svc-s92mn
Jan 10 08:25:51.508: INFO: Created: latency-svc-xftmk
Jan 10 08:25:51.513: INFO: Created: latency-svc-hqmkn
Jan 10 08:25:51.519: INFO: Created: latency-svc-q2mzl
Jan 10 08:25:51.523: INFO: Created: latency-svc-btkk9
Jan 10 08:25:51.529: INFO: Created: latency-svc-8qj2k
Jan 10 08:25:51.534: INFO: Created: latency-svc-w2dwr
Jan 10 08:25:51.542: INFO: Got endpoints: latency-svc-mxr6g [229.697975ms]
Jan 10 08:25:51.544: INFO: Created: latency-svc-qhxmc
Jan 10 08:25:51.550: INFO: Created: latency-svc-c57s8
Jan 10 08:25:51.554: INFO: Created: latency-svc-pfmkh
Jan 10 08:25:51.556: INFO: Created: latency-svc-4rgft
Jan 10 08:25:51.590: INFO: Got endpoints: latency-svc-vqwfs [254.864877ms]
Jan 10 08:25:51.599: INFO: Created: latency-svc-rvpv4
Jan 10 08:25:51.640: INFO: Got endpoints: latency-svc-7xd9t [303.462771ms]
Jan 10 08:25:51.652: INFO: Created: latency-svc-98mqn
Jan 10 08:25:51.689: INFO: Got endpoints: latency-svc-dpt4z [349.006575ms]
Jan 10 08:25:51.698: INFO: Created: latency-svc-2qz26
Jan 10 08:25:51.739: INFO: Got endpoints: latency-svc-zxv4j [384.269295ms]
Jan 10 08:25:51.752: INFO: Created: latency-svc-snl8r
Jan 10 08:25:51.788: INFO: Got endpoints: latency-svc-s92mn [428.5735ms]
Jan 10 08:25:51.798: INFO: Created: latency-svc-rhf8t
Jan 10 08:25:51.839: INFO: Got endpoints: latency-svc-xftmk [456.523075ms]
Jan 10 08:25:51.850: INFO: Created: latency-svc-6b2tv
Jan 10 08:25:51.896: INFO: Got endpoints: latency-svc-hqmkn [510.007175ms]
Jan 10 08:25:51.904: INFO: Created: latency-svc-7wjln
Jan 10 08:25:51.940: INFO: Got endpoints: latency-svc-q2mzl [539.413845ms]
Jan 10 08:25:51.950: INFO: Created: latency-svc-rnfkx
Jan 10 08:25:51.988: INFO: Got endpoints: latency-svc-btkk9 [572.984336ms]
Jan 10 08:25:51.998: INFO: Created: latency-svc-29mqz
Jan 10 08:25:52.042: INFO: Got endpoints: latency-svc-8qj2k [621.088455ms]
Jan 10 08:25:52.051: INFO: Created: latency-svc-qqh4f
Jan 10 08:25:52.091: INFO: Got endpoints: latency-svc-w2dwr [655.219474ms]
Jan 10 08:25:52.101: INFO: Created: latency-svc-kpnph
Jan 10 08:25:52.140: INFO: Got endpoints: latency-svc-qhxmc [689.411457ms]
Jan 10 08:25:52.149: INFO: Created: latency-svc-jp68l
Jan 10 08:25:52.189: INFO: Got endpoints: latency-svc-c57s8 [732.042054ms]
Jan 10 08:25:52.202: INFO: Created: latency-svc-xqx4r
Jan 10 08:25:52.241: INFO: Got endpoints: latency-svc-pfmkh [747.478186ms]
Jan 10 08:25:52.253: INFO: Created: latency-svc-bcqtf
Jan 10 08:25:52.288: INFO: Got endpoints: latency-svc-4rgft [745.848939ms]
Jan 10 08:25:52.299: INFO: Created: latency-svc-v6g9k
Jan 10 08:25:52.342: INFO: Got endpoints: latency-svc-rvpv4 [752.326286ms]
Jan 10 08:25:52.354: INFO: Created: latency-svc-fnf8b
Jan 10 08:25:52.390: INFO: Got endpoints: latency-svc-98mqn [749.212489ms]
Jan 10 08:25:52.399: INFO: Created: latency-svc-rfkwz
Jan 10 08:25:52.444: INFO: Got endpoints: latency-svc-2qz26 [754.695535ms]
Jan 10 08:25:52.455: INFO: Created: latency-svc-5f6ms
Jan 10 08:25:52.492: INFO: Got endpoints: latency-svc-snl8r [752.919985ms]
Jan 10 08:25:52.506: INFO: Created: latency-svc-lvcwb
Jan 10 08:25:52.545: INFO: Got endpoints: latency-svc-rhf8t [757.105691ms]
Jan 10 08:25:52.568: INFO: Created: latency-svc-7qrb9
Jan 10 08:25:52.594: INFO: Got endpoints: latency-svc-6b2tv [754.39411ms]
Jan 10 08:25:52.611: INFO: Created: latency-svc-tcdh2
Jan 10 08:25:52.643: INFO: Got endpoints: latency-svc-7wjln [747.567082ms]
Jan 10 08:25:52.658: INFO: Created: latency-svc-8rxzk
Jan 10 08:25:52.689: INFO: Got endpoints: latency-svc-rnfkx [749.360717ms]
Jan 10 08:25:52.700: INFO: Created: latency-svc-ddkx6
Jan 10 08:25:52.744: INFO: Got endpoints: latency-svc-29mqz [755.163923ms]
Jan 10 08:25:52.758: INFO: Created: latency-svc-lb6ql
Jan 10 08:25:52.792: INFO: Got endpoints: latency-svc-qqh4f [750.075484ms]
Jan 10 08:25:52.807: INFO: Created: latency-svc-jd68k
Jan 10 08:25:52.844: INFO: Got endpoints: latency-svc-kpnph [753.122919ms]
Jan 10 08:25:52.858: INFO: Created: latency-svc-ntglq
Jan 10 08:25:52.892: INFO: Got endpoints: latency-svc-jp68l [752.357836ms]
Jan 10 08:25:52.905: INFO: Created: latency-svc-jmcdj
Jan 10 08:25:52.944: INFO: Got endpoints: latency-svc-xqx4r [754.292154ms]
Jan 10 08:25:52.958: INFO: Created: latency-svc-jzq45
Jan 10 08:25:52.999: INFO: Got endpoints: latency-svc-bcqtf [757.134782ms]
Jan 10 08:25:53.007: INFO: Created: latency-svc-b72r5
Jan 10 08:25:53.042: INFO: Got endpoints: latency-svc-v6g9k [753.568213ms]
Jan 10 08:25:53.055: INFO: Created: latency-svc-w76zw
Jan 10 08:25:53.093: INFO: Got endpoints: latency-svc-fnf8b [749.966555ms]
Jan 10 08:25:53.101: INFO: Created: latency-svc-pxcf4
Jan 10 08:25:53.146: INFO: Got endpoints: latency-svc-rfkwz [755.576901ms]
Jan 10 08:25:53.157: INFO: Created: latency-svc-8kjnm
Jan 10 08:25:53.189: INFO: Got endpoints: latency-svc-5f6ms [745.106858ms]
Jan 10 08:25:53.203: INFO: Created: latency-svc-pfg44
Jan 10 08:25:53.242: INFO: Got endpoints: latency-svc-lvcwb [749.797645ms]
Jan 10 08:25:53.256: INFO: Created: latency-svc-cmt9s
Jan 10 08:25:53.288: INFO: Got endpoints: latency-svc-7qrb9 [742.724899ms]
Jan 10 08:25:53.300: INFO: Created: latency-svc-499sc
Jan 10 08:25:53.341: INFO: Got endpoints: latency-svc-tcdh2 [747.024796ms]
Jan 10 08:25:53.351: INFO: Created: latency-svc-twvls
Jan 10 08:25:53.399: INFO: Got endpoints: latency-svc-8rxzk [755.321373ms]
Jan 10 08:25:53.409: INFO: Created: latency-svc-psxtt
Jan 10 08:25:53.440: INFO: Got endpoints: latency-svc-ddkx6 [751.529029ms]
Jan 10 08:25:53.455: INFO: Created: latency-svc-jfvrs
Jan 10 08:25:53.490: INFO: Got endpoints: latency-svc-lb6ql [745.56907ms]
Jan 10 08:25:53.502: INFO: Created: latency-svc-5jh4v
Jan 10 08:25:53.538: INFO: Got endpoints: latency-svc-jd68k [745.962322ms]
Jan 10 08:25:53.550: INFO: Created: latency-svc-l6svl
Jan 10 08:25:53.587: INFO: Got endpoints: latency-svc-ntglq [743.039883ms]
Jan 10 08:25:53.600: INFO: Created: latency-svc-77sw6
Jan 10 08:25:53.640: INFO: Got endpoints: latency-svc-jmcdj [747.699877ms]
Jan 10 08:25:53.652: INFO: Created: latency-svc-mwsnd
Jan 10 08:25:53.691: INFO: Got endpoints: latency-svc-jzq45 [747.167876ms]
Jan 10 08:25:53.701: INFO: Created: latency-svc-vhjhn
Jan 10 08:25:53.742: INFO: Got endpoints: latency-svc-b72r5 [743.2811ms]
Jan 10 08:25:53.753: INFO: Created: latency-svc-n7425
Jan 10 08:25:53.789: INFO: Got endpoints: latency-svc-w76zw [746.923409ms]
Jan 10 08:25:53.800: INFO: Created: latency-svc-kj67p
Jan 10 08:25:53.851: INFO: Got endpoints: latency-svc-pxcf4 [758.006107ms]
Jan 10 08:25:53.862: INFO: Created: latency-svc-s6cqc
Jan 10 08:25:53.890: INFO: Got endpoints: latency-svc-8kjnm [744.193967ms]
Jan 10 08:25:53.909: INFO: Created: latency-svc-gpnq6
Jan 10 08:25:53.941: INFO: Got endpoints: latency-svc-pfg44 [751.2167ms]
Jan 10 08:25:53.949: INFO: Created: latency-svc-nbrkx
Jan 10 08:25:53.990: INFO: Got endpoints: latency-svc-cmt9s [748.147788ms]
Jan 10 08:25:53.999: INFO: Created: latency-svc-897xb
Jan 10 08:25:54.042: INFO: Got endpoints: latency-svc-499sc [753.660277ms]
Jan 10 08:25:54.053: INFO: Created: latency-svc-mt52v
Jan 10 08:25:54.092: INFO: Got endpoints: latency-svc-twvls [750.886834ms]
Jan 10 08:25:54.104: INFO: Created: latency-svc-gcbmn
Jan 10 08:25:54.144: INFO: Got endpoints: latency-svc-psxtt [745.506125ms]
Jan 10 08:25:54.154: INFO: Created: latency-svc-v62nm
Jan 10 08:25:54.190: INFO: Got endpoints: latency-svc-jfvrs [749.400586ms]
Jan 10 08:25:54.200: INFO: Created: latency-svc-fz8sh
Jan 10 08:25:54.243: INFO: Got endpoints: latency-svc-5jh4v [753.188304ms]
Jan 10 08:25:54.253: INFO: Created: latency-svc-vnl79
Jan 10 08:25:54.294: INFO: Got endpoints: latency-svc-l6svl [756.118597ms]
Jan 10 08:25:54.304: INFO: Created: latency-svc-p57tv
Jan 10 08:25:54.340: INFO: Got endpoints: latency-svc-77sw6 [752.727432ms]
Jan 10 08:25:54.355: INFO: Created: latency-svc-p7qvn
Jan 10 08:25:54.397: INFO: Got endpoints: latency-svc-mwsnd [756.246314ms]
Jan 10 08:25:54.412: INFO: Created: latency-svc-thqms
Jan 10 08:25:54.440: INFO: Got endpoints: latency-svc-vhjhn [748.117558ms]
Jan 10 08:25:54.450: INFO: Created: latency-svc-2gk62
Jan 10 08:25:54.490: INFO: Got endpoints: latency-svc-n7425 [747.08591ms]
Jan 10 08:25:54.503: INFO: Created: latency-svc-4mbzv
Jan 10 08:25:54.540: INFO: Got endpoints: latency-svc-kj67p [750.586744ms]
Jan 10 08:25:54.549: INFO: Created: latency-svc-54g8h
Jan 10 08:25:54.593: INFO: Got endpoints: latency-svc-s6cqc [742.139044ms]
Jan 10 08:25:54.605: INFO: Created: latency-svc-rt57f
Jan 10 08:25:54.639: INFO: Got endpoints: latency-svc-gpnq6 [749.089407ms]
Jan 10 08:25:54.650: INFO: Created: latency-svc-7gr8s
Jan 10 08:25:54.689: INFO: Got endpoints: latency-svc-nbrkx [748.374981ms]
Jan 10 08:25:54.701: INFO: Created: latency-svc-mxl88
Jan 10 08:25:54.741: INFO: Got endpoints: latency-svc-897xb [750.871235ms]
Jan 10 08:25:54.751: INFO: Created: latency-svc-hxf4s
Jan 10 08:25:54.789: INFO: Got endpoints: latency-svc-mt52v [747.181768ms]
Jan 10 08:25:54.800: INFO: Created: latency-svc-jw4lv
Jan 10 08:25:54.839: INFO: Got endpoints: latency-svc-gcbmn [746.712745ms]
Jan 10 08:25:54.849: INFO: Created: latency-svc-jfhk9
Jan 10 08:25:54.893: INFO: Got endpoints: latency-svc-v62nm [748.102169ms]
Jan 10 08:25:54.908: INFO: Created: latency-svc-6xkf7
Jan 10 08:25:54.941: INFO: Got endpoints: latency-svc-fz8sh [751.132605ms]
Jan 10 08:25:54.950: INFO: Created: latency-svc-dvkpw
Jan 10 08:25:54.994: INFO: Got endpoints: latency-svc-vnl79 [751.329231ms]
Jan 10 08:25:55.009: INFO: Created: latency-svc-s289m
Jan 10 08:25:55.040: INFO: Got endpoints: latency-svc-p57tv [745.996113ms]
Jan 10 08:25:55.049: INFO: Created: latency-svc-q6xnq
Jan 10 08:25:55.090: INFO: Got endpoints: latency-svc-p7qvn [749.331333ms]
Jan 10 08:25:55.100: INFO: Created: latency-svc-t9tnc
Jan 10 08:25:55.141: INFO: Got endpoints: latency-svc-thqms [744.596467ms]
Jan 10 08:25:55.154: INFO: Created: latency-svc-dlphh
Jan 10 08:25:55.194: INFO: Got endpoints: latency-svc-2gk62 [753.712422ms]
Jan 10 08:25:55.207: INFO: Created: latency-svc-4npcz
Jan 10 08:25:55.240: INFO: Got endpoints: latency-svc-4mbzv [750.07949ms]
Jan 10 08:25:55.251: INFO: Created: latency-svc-zl6bq
Jan 10 08:25:55.295: INFO: Got endpoints: latency-svc-54g8h [755.083586ms]
Jan 10 08:25:55.306: INFO: Created: latency-svc-kbsg5
Jan 10 08:25:55.342: INFO: Got endpoints: latency-svc-rt57f [748.627339ms]
Jan 10 08:25:55.352: INFO: Created: latency-svc-8sbbb
Jan 10 08:25:55.389: INFO: Got endpoints: latency-svc-7gr8s [750.290666ms]
Jan 10 08:25:55.402: INFO: Created: latency-svc-vj8gb
Jan 10 08:25:55.440: INFO: Got endpoints: latency-svc-mxl88 [750.876968ms]
Jan 10 08:25:55.451: INFO: Created: latency-svc-tq8nf
Jan 10 08:25:55.489: INFO: Got endpoints: latency-svc-hxf4s [746.901978ms]
Jan 10 08:25:55.502: INFO: Created: latency-svc-jqfkf
Jan 10 08:25:55.539: INFO: Got endpoints: latency-svc-jw4lv [749.998222ms]
Jan 10 08:25:55.556: INFO: Created: latency-svc-g5zc7
Jan 10 08:25:55.591: INFO: Got endpoints: latency-svc-jfhk9 [751.627996ms]
Jan 10 08:25:55.604: INFO: Created: latency-svc-nb6wb
Jan 10 08:25:55.642: INFO: Got endpoints: latency-svc-6xkf7 [748.68182ms]
Jan 10 08:25:55.653: INFO: Created: latency-svc-spk9r
Jan 10 08:25:55.689: INFO: Got endpoints: latency-svc-dvkpw [747.876987ms]
Jan 10 08:25:55.701: INFO: Created: latency-svc-dplwv
Jan 10 08:25:55.740: INFO: Got endpoints: latency-svc-s289m [745.52108ms]
Jan 10 08:25:55.750: INFO: Created: latency-svc-rm6j9
Jan 10 08:25:55.789: INFO: Got endpoints: latency-svc-q6xnq [748.053268ms]
Jan 10 08:25:55.799: INFO: Created: latency-svc-hzwk5
Jan 10 08:25:55.840: INFO: Got endpoints: latency-svc-t9tnc [749.645477ms]
Jan 10 08:25:55.849: INFO: Created: latency-svc-kvkwj
Jan 10 08:25:55.891: INFO: Got endpoints: latency-svc-dlphh [749.327698ms]
Jan 10 08:25:55.912: INFO: Created: latency-svc-sdz5x
Jan 10 08:25:55.940: INFO: Got endpoints: latency-svc-4npcz [746.177719ms]
Jan 10 08:25:55.953: INFO: Created: latency-svc-zxmkp
Jan 10 08:25:55.990: INFO: Got endpoints: latency-svc-zl6bq [750.243823ms]
Jan 10 08:25:56.001: INFO: Created: latency-svc-xsrhn
Jan 10 08:25:56.041: INFO: Got endpoints: latency-svc-kbsg5 [746.128328ms]
Jan 10 08:25:56.049: INFO: Created: latency-svc-s6tmx
Jan 10 08:25:56.093: INFO: Got endpoints: latency-svc-8sbbb [750.406374ms]
Jan 10 08:25:56.103: INFO: Created: latency-svc-qvck7
Jan 10 08:25:56.138: INFO: Got endpoints: latency-svc-vj8gb [748.519157ms]
Jan 10 08:25:56.147: INFO: Created: latency-svc-44z4l
Jan 10 08:25:56.194: INFO: Got endpoints: latency-svc-tq8nf [753.807843ms]
Jan 10 08:25:56.203: INFO: Created: latency-svc-cmq85
Jan 10 08:25:56.240: INFO: Got endpoints: latency-svc-jqfkf [751.64993ms]
Jan 10 08:25:56.255: INFO: Created: latency-svc-lt2qm
Jan 10 08:25:56.290: INFO: Got endpoints: latency-svc-g5zc7 [749.90811ms]
Jan 10 08:25:56.300: INFO: Created: latency-svc-4gvmt
Jan 10 08:25:56.344: INFO: Got endpoints: latency-svc-nb6wb [753.709508ms]
Jan 10 08:25:56.361: INFO: Created: latency-svc-2sw4h
Jan 10 08:25:56.394: INFO: Got endpoints: latency-svc-spk9r [752.164458ms]
Jan 10 08:25:56.406: INFO: Created: latency-svc-rljc9
Jan 10 08:25:56.440: INFO: Got endpoints: latency-svc-dplwv [750.08525ms]
Jan 10 08:25:56.455: INFO: Created: latency-svc-f75nn
Jan 10 08:25:56.490: INFO: Got endpoints: latency-svc-rm6j9 [749.879461ms]
Jan 10 08:25:56.502: INFO: Created: latency-svc-nl77l
Jan 10 08:25:56.540: INFO: Got endpoints: latency-svc-hzwk5 [750.530849ms]
Jan 10 08:25:56.549: INFO: Created: latency-svc-wpkzb
Jan 10 08:25:56.592: INFO: Got endpoints: latency-svc-kvkwj [751.784821ms]
Jan 10 08:25:56.601: INFO: Created: latency-svc-xj8s8
Jan 10 08:25:56.640: INFO: Got endpoints: latency-svc-sdz5x [749.050268ms]
Jan 10 08:25:56.649: INFO: Created: latency-svc-2bmkl
Jan 10 08:25:56.691: INFO: Got endpoints: latency-svc-zxmkp [750.629233ms]
Jan 10 08:25:56.701: INFO: Created: latency-svc-fccxk
Jan 10 08:25:56.738: INFO: Got endpoints: latency-svc-xsrhn [747.900775ms]
Jan 10 08:25:56.750: INFO: Created: latency-svc-xzlxt
Jan 10 08:25:56.789: INFO: Got endpoints: latency-svc-s6tmx [747.88443ms]
Jan 10 08:25:56.799: INFO: Created: latency-svc-mzw9s
Jan 10 08:25:56.839: INFO: Got endpoints: latency-svc-qvck7 [746.155476ms]
Jan 10 08:25:56.850: INFO: Created: latency-svc-pq2f6
Jan 10 08:25:56.889: INFO: Got endpoints: latency-svc-44z4l [750.892545ms]
Jan 10 08:25:56.899: INFO: Created: latency-svc-4d5kp
Jan 10 08:25:56.938: INFO: Got endpoints: latency-svc-cmq85 [743.543565ms]
Jan 10 08:25:56.948: INFO: Created: latency-svc-8pcjq
Jan 10 08:25:56.996: INFO: Got endpoints: latency-svc-lt2qm [755.489218ms]
Jan 10 08:25:57.007: INFO: Created: latency-svc-ld7vv
Jan 10 08:25:57.043: INFO: Got endpoints: latency-svc-4gvmt [752.260556ms]
Jan 10 08:25:57.053: INFO: Created: latency-svc-4pvxw
Jan 10 08:25:57.090: INFO: Got endpoints: latency-svc-2sw4h [744.875065ms]
Jan 10 08:25:57.098: INFO: Created: latency-svc-wp9s7
Jan 10 08:25:57.141: INFO: Got endpoints: latency-svc-rljc9 [747.035031ms]
Jan 10 08:25:57.152: INFO: Created: latency-svc-gb7mh
Jan 10 08:25:57.188: INFO: Got endpoints: latency-svc-f75nn [747.79688ms]
Jan 10 08:25:57.197: INFO: Created: latency-svc-ktnkc
Jan 10 08:25:57.240: INFO: Got endpoints: latency-svc-nl77l [749.486272ms]
Jan 10 08:25:57.250: INFO: Created: latency-svc-f24wg
Jan 10 08:25:57.296: INFO: Got endpoints: latency-svc-wpkzb [756.014365ms]
Jan 10 08:25:57.304: INFO: Created: latency-svc-glx5w
Jan 10 08:25:57.341: INFO: Got endpoints: latency-svc-xj8s8 [749.159236ms]
Jan 10 08:25:57.352: INFO: Created: latency-svc-8r6fx
Jan 10 08:25:57.388: INFO: Got endpoints: latency-svc-2bmkl [748.20856ms]
Jan 10 08:25:57.397: INFO: Created: latency-svc-tcvcr
Jan 10 08:25:57.439: INFO: Got endpoints: latency-svc-fccxk [747.343745ms]
Jan 10 08:25:57.450: INFO: Created: latency-svc-9zv7w
Jan 10 08:25:57.489: INFO: Got endpoints: latency-svc-xzlxt [750.076248ms]
Jan 10 08:25:57.501: INFO: Created: latency-svc-579jf
Jan 10 08:25:57.542: INFO: Got endpoints: latency-svc-mzw9s [752.548361ms]
Jan 10 08:25:57.552: INFO: Created: latency-svc-jmvzc
Jan 10 08:25:57.590: INFO: Got endpoints: latency-svc-pq2f6 [751.328693ms]
Jan 10 08:25:57.601: INFO: Created: latency-svc-nskpk
Jan 10 08:25:57.640: INFO: Got endpoints: latency-svc-4d5kp [750.109735ms]
Jan 10 08:25:57.648: INFO: Created: latency-svc-zjkq2
Jan 10 08:25:57.692: INFO: Got endpoints: latency-svc-8pcjq [753.958163ms]
Jan 10 08:25:57.701: INFO: Created: latency-svc-9dvf6
Jan 10 08:25:57.739: INFO: Got endpoints: latency-svc-ld7vv [742.399207ms]
Jan 10 08:25:57.749: INFO: Created: latency-svc-5n8xm
Jan 10 08:25:57.795: INFO: Got endpoints: latency-svc-4pvxw [752.308633ms]
Jan 10 08:25:57.810: INFO: Created: latency-svc-rs2nl
Jan 10 08:25:57.840: INFO: Got endpoints: latency-svc-wp9s7 [750.842413ms]
Jan 10 08:25:57.852: INFO: Created: latency-svc-mhscr
Jan 10 08:25:57.896: INFO: Got endpoints: latency-svc-gb7mh [754.618132ms]
Jan 10 08:25:57.907: INFO: Created: latency-svc-pbxgm
Jan 10 08:25:57.941: INFO: Got endpoints: latency-svc-ktnkc [753.305571ms]
Jan 10 08:25:57.949: INFO: Created: latency-svc-7v48k
Jan 10 08:25:57.989: INFO: Got endpoints: latency-svc-f24wg [749.079354ms]
Jan 10 08:25:58.006: INFO: Created: latency-svc-4nrb6
Jan 10 08:25:58.039: INFO: Got endpoints: latency-svc-glx5w [743.540431ms]
Jan 10 08:25:58.048: INFO: Created: latency-svc-xkhmq
Jan 10 08:25:58.088: INFO: Got endpoints: latency-svc-8r6fx [747.048428ms]
Jan 10 08:25:58.097: INFO: Created: latency-svc-6dxq6
Jan 10 08:25:58.139: INFO: Got endpoints: latency-svc-tcvcr [750.408049ms]
Jan 10 08:25:58.149: INFO: Created: latency-svc-k27qk
Jan 10 08:25:58.189: INFO: Got endpoints: latency-svc-9zv7w [749.754931ms]
Jan 10 08:25:58.202: INFO: Created: latency-svc-z96mk
Jan 10 08:25:58.246: INFO: Got endpoints: latency-svc-579jf [757.538607ms]
Jan 10 08:25:58.262: INFO: Created: latency-svc-67fd5
Jan 10 08:25:58.292: INFO: Got endpoints: latency-svc-jmvzc [750.206971ms]
Jan 10 08:25:58.319: INFO: Created: latency-svc-wgkpb
Jan 10 08:25:58.342: INFO: Got endpoints: latency-svc-nskpk [751.638166ms]
Jan 10 08:25:58.358: INFO: Created: latency-svc-bd247
Jan 10 08:25:58.388: INFO: Got endpoints: latency-svc-zjkq2 [748.556447ms]
Jan 10 08:25:58.402: INFO: Created: latency-svc-hdngj
Jan 10 08:25:58.439: INFO: Got endpoints: latency-svc-9dvf6 [746.392617ms]
Jan 10 08:25:58.453: INFO: Created: latency-svc-7k5hz
Jan 10 08:25:58.494: INFO: Got endpoints: latency-svc-5n8xm [754.774799ms]
Jan 10 08:25:58.507: INFO: Created: latency-svc-r75xs
Jan 10 08:25:58.542: INFO: Got endpoints: latency-svc-rs2nl [746.363404ms]
Jan 10 08:25:58.551: INFO: Created: latency-svc-jm49t
Jan 10 08:25:58.592: INFO: Got endpoints: latency-svc-mhscr [751.0258ms]
Jan 10 08:25:58.602: INFO: Created: latency-svc-lzn6p
Jan 10 08:25:58.640: INFO: Got endpoints: latency-svc-pbxgm [743.637268ms]
Jan 10 08:25:58.654: INFO: Created: latency-svc-4kl4h
Jan 10 08:25:58.689: INFO: Got endpoints: latency-svc-7v48k [747.700178ms]
Jan 10 08:25:58.698: INFO: Created: latency-svc-gs6zz
Jan 10 08:25:58.741: INFO: Got endpoints: latency-svc-4nrb6 [752.066238ms]
Jan 10 08:25:58.753: INFO: Created: latency-svc-5vfws
Jan 10 08:25:58.788: INFO: Got endpoints: latency-svc-xkhmq [748.349485ms]
Jan 10 08:25:58.803: INFO: Created: latency-svc-4dbsp
Jan 10 08:25:58.838: INFO: Got endpoints: latency-svc-6dxq6 [749.119471ms]
Jan 10 08:25:58.847: INFO: Created: latency-svc-zx4bs
Jan 10 08:25:58.892: INFO: Got endpoints: latency-svc-k27qk [752.666092ms]
Jan 10 08:25:58.940: INFO: Got endpoints: latency-svc-z96mk [751.325348ms]
Jan 10 08:25:58.989: INFO: Got endpoints: latency-svc-67fd5 [742.495837ms]
Jan 10 08:25:59.040: INFO: Got endpoints: latency-svc-wgkpb [747.039275ms]
Jan 10 08:25:59.089: INFO: Got endpoints: latency-svc-bd247 [746.515126ms]
Jan 10 08:25:59.143: INFO: Got endpoints: latency-svc-hdngj [754.252039ms]
Jan 10 08:25:59.190: INFO: Got endpoints: latency-svc-7k5hz [750.907808ms]
Jan 10 08:25:59.241: INFO: Got endpoints: latency-svc-r75xs [747.230363ms]
Jan 10 08:25:59.289: INFO: Got endpoints: latency-svc-jm49t [747.101936ms]
Jan 10 08:25:59.340: INFO: Got endpoints: latency-svc-lzn6p [748.20617ms]
Jan 10 08:25:59.388: INFO: Got endpoints: latency-svc-4kl4h [748.255851ms]
Jan 10 08:25:59.440: INFO: Got endpoints: latency-svc-gs6zz [750.031938ms]
Jan 10 08:25:59.490: INFO: Got endpoints: latency-svc-5vfws [748.15898ms]
Jan 10 08:25:59.541: INFO: Got endpoints: latency-svc-4dbsp [752.8362ms]
Jan 10 08:25:59.590: INFO: Got endpoints: latency-svc-zx4bs [752.816667ms]
Jan 10 08:25:59.591: INFO: Latencies: [29.840556ms 37.657215ms 50.740192ms 64.733328ms 74.743018ms 81.68632ms 94.316548ms 106.492306ms 120.345197ms 130.362558ms 144.087919ms 145.300172ms 145.90836ms 148.867992ms 149.120012ms 149.283312ms 150.088802ms 150.122368ms 153.839738ms 154.923707ms 156.277227ms 157.084983ms 157.401032ms 158.48562ms 159.140905ms 159.834276ms 160.296878ms 161.384682ms 161.522097ms 161.690403ms 162.100977ms 162.877449ms 163.927327ms 165.363753ms 168.558631ms 170.244427ms 178.816855ms 190.615127ms 229.697975ms 254.864877ms 303.462771ms 349.006575ms 384.269295ms 428.5735ms 456.523075ms 510.007175ms 539.413845ms 572.984336ms 621.088455ms 655.219474ms 689.411457ms 732.042054ms 742.139044ms 742.399207ms 742.495837ms 742.724899ms 743.039883ms 743.2811ms 743.540431ms 743.543565ms 743.637268ms 744.193967ms 744.596467ms 744.875065ms 745.106858ms 745.506125ms 745.52108ms 745.56907ms 745.848939ms 745.962322ms 745.996113ms 746.128328ms 746.155476ms 746.177719ms 746.363404ms 746.392617ms 746.515126ms 746.712745ms 746.901978ms 746.923409ms 747.024796ms 747.035031ms 747.039275ms 747.048428ms 747.08591ms 747.101936ms 747.167876ms 747.181768ms 747.230363ms 747.343745ms 747.478186ms 747.567082ms 747.699877ms 747.700178ms 747.79688ms 747.876987ms 747.88443ms 747.900775ms 748.053268ms 748.102169ms 748.117558ms 748.147788ms 748.15898ms 748.20617ms 748.20856ms 748.255851ms 748.349485ms 748.374981ms 748.519157ms 748.556447ms 748.627339ms 748.68182ms 749.050268ms 749.079354ms 749.089407ms 749.119471ms 749.159236ms 749.212489ms 749.327698ms 749.331333ms 749.360717ms 749.400586ms 749.486272ms 749.645477ms 749.754931ms 749.797645ms 749.879461ms 749.90811ms 749.966555ms 749.998222ms 750.031938ms 750.075484ms 750.076248ms 750.07949ms 750.08525ms 750.109735ms 750.206971ms 750.243823ms 750.290666ms 750.406374ms 750.408049ms 750.530849ms 750.586744ms 750.629233ms 750.842413ms 750.871235ms 750.876968ms 750.886834ms 750.892545ms 750.907808ms 751.0258ms 751.132605ms 751.2167ms 751.325348ms 751.328693ms 751.329231ms 751.529029ms 751.627996ms 751.638166ms 751.64993ms 751.784821ms 752.066238ms 752.164458ms 752.260556ms 752.308633ms 752.326286ms 752.357836ms 752.548361ms 752.666092ms 752.727432ms 752.816667ms 752.8362ms 752.919985ms 753.122919ms 753.188304ms 753.305571ms 753.568213ms 753.660277ms 753.709508ms 753.712422ms 753.807843ms 753.958163ms 754.252039ms 754.292154ms 754.39411ms 754.618132ms 754.695535ms 754.774799ms 755.083586ms 755.163923ms 755.321373ms 755.489218ms 755.576901ms 756.014365ms 756.118597ms 756.246314ms 757.105691ms 757.134782ms 757.538607ms 758.006107ms]
Jan 10 08:25:59.591: INFO: 50 %ile: 748.117558ms
Jan 10 08:25:59.591: INFO: 90 %ile: 753.807843ms
Jan 10 08:25:59.591: INFO: 99 %ile: 757.538607ms
Jan 10 08:25:59.591: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:188
Jan 10 08:25:59.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8679" for this suite.

• [SLOW TEST:10.780 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":356,"completed":338,"skipped":6369,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:25:59.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:25:59.651: INFO: created pod pod-service-account-defaultsa
Jan 10 08:25:59.651: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 10 08:25:59.668: INFO: created pod pod-service-account-mountsa
Jan 10 08:25:59.668: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 10 08:25:59.685: INFO: created pod pod-service-account-nomountsa
Jan 10 08:25:59.686: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 10 08:25:59.695: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 10 08:25:59.695: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 10 08:25:59.722: INFO: created pod pod-service-account-mountsa-mountspec
Jan 10 08:25:59.723: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 10 08:25:59.740: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 10 08:25:59.740: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 10 08:25:59.753: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 10 08:25:59.753: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 10 08:25:59.759: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 10 08:25:59.760: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 10 08:25:59.766: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 10 08:25:59.766: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 10 08:25:59.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2602" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":356,"completed":339,"skipped":6376,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:25:59.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 10 08:26:13.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5607" for this suite.

• [SLOW TEST:14.051 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":356,"completed":340,"skipped":6378,"failed":0}
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:26:13.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 10 08:26:13.929: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 10 08:26:15.936: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 10 08:26:15.949: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 10 08:26:17.955: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 10 08:26:17.964: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 10 08:26:17.968: INFO: Pod pod-with-prestop-http-hook still exists
Jan 10 08:26:19.969: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 10 08:26:19.975: INFO: Pod pod-with-prestop-http-hook still exists
Jan 10 08:26:21.971: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 10 08:26:21.979: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 10 08:26:21.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6048" for this suite.

• [SLOW TEST:8.163 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":356,"completed":341,"skipped":6382,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:26:21.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:26:22.045: INFO: The status of Pod pod-secrets-dce86773-c3ff-412e-952e-7e1d472bde6f is Pending, waiting for it to be Running (with Ready = true)
Jan 10 08:26:24.051: INFO: The status of Pod pod-secrets-dce86773-c3ff-412e-952e-7e1d472bde6f is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Jan 10 08:26:24.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6489" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":356,"completed":342,"skipped":6383,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:26:24.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 10 08:26:40.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3211" for this suite.

• [SLOW TEST:16.152 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":356,"completed":343,"skipped":6406,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:26:40.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 10 08:26:40.286: INFO: Waiting up to 5m0s for pod "downward-api-f81a987d-d0da-4abe-a2a1-3a2302b3baf5" in namespace "downward-api-637" to be "Succeeded or Failed"
Jan 10 08:26:40.296: INFO: Pod "downward-api-f81a987d-d0da-4abe-a2a1-3a2302b3baf5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021092ms
Jan 10 08:26:42.306: INFO: Pod "downward-api-f81a987d-d0da-4abe-a2a1-3a2302b3baf5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020079316s
Jan 10 08:26:44.312: INFO: Pod "downward-api-f81a987d-d0da-4abe-a2a1-3a2302b3baf5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026720223s
STEP: Saw pod success
Jan 10 08:26:44.313: INFO: Pod "downward-api-f81a987d-d0da-4abe-a2a1-3a2302b3baf5" satisfied condition "Succeeded or Failed"
Jan 10 08:26:44.316: INFO: Trying to get logs from node kk-instance-vgmmg pod downward-api-f81a987d-d0da-4abe-a2a1-3a2302b3baf5 container dapi-container: <nil>
STEP: delete the pod
Jan 10 08:26:44.330: INFO: Waiting for pod downward-api-f81a987d-d0da-4abe-a2a1-3a2302b3baf5 to disappear
Jan 10 08:26:44.333: INFO: Pod downward-api-f81a987d-d0da-4abe-a2a1-3a2302b3baf5 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 10 08:26:44.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-637" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":356,"completed":344,"skipped":6411,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:26:44.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 10 08:26:44.376: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5924de02-0bcb-459a-bbf1-5e6b5f77985c" in namespace "projected-3577" to be "Succeeded or Failed"
Jan 10 08:26:44.380: INFO: Pod "downwardapi-volume-5924de02-0bcb-459a-bbf1-5e6b5f77985c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.588307ms
Jan 10 08:26:46.389: INFO: Pod "downwardapi-volume-5924de02-0bcb-459a-bbf1-5e6b5f77985c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013442847s
Jan 10 08:26:48.406: INFO: Pod "downwardapi-volume-5924de02-0bcb-459a-bbf1-5e6b5f77985c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029807338s
STEP: Saw pod success
Jan 10 08:26:48.406: INFO: Pod "downwardapi-volume-5924de02-0bcb-459a-bbf1-5e6b5f77985c" satisfied condition "Succeeded or Failed"
Jan 10 08:26:48.412: INFO: Trying to get logs from node kk-instance-vgmmg pod downwardapi-volume-5924de02-0bcb-459a-bbf1-5e6b5f77985c container client-container: <nil>
STEP: delete the pod
Jan 10 08:26:48.432: INFO: Waiting for pod downwardapi-volume-5924de02-0bcb-459a-bbf1-5e6b5f77985c to disappear
Jan 10 08:26:48.436: INFO: Pod downwardapi-volume-5924de02-0bcb-459a-bbf1-5e6b5f77985c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 10 08:26:48.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3577" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":345,"skipped":6416,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:26:48.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Jan 10 08:26:59.191: INFO: 71 pods remaining
Jan 10 08:26:59.191: INFO: 71 pods has nil DeletionTimestamp
Jan 10 08:26:59.191: INFO: 
STEP: Gathering metrics
Jan 10 08:27:04.213: INFO: The status of Pod kube-controller-manager-kk-instance-rx4ck is Running (Ready = true)
Jan 10 08:27:04.386: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 10 08:27:04.386: INFO: Deleting pod "simpletest-rc-to-be-deleted-26x6f" in namespace "gc-6697"
Jan 10 08:27:04.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lbqh" in namespace "gc-6697"
Jan 10 08:27:04.413: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lz7m" in namespace "gc-6697"
Jan 10 08:27:04.433: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xs7s" in namespace "gc-6697"
Jan 10 08:27:04.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zz8h" in namespace "gc-6697"
Jan 10 08:27:04.463: INFO: Deleting pod "simpletest-rc-to-be-deleted-44xkd" in namespace "gc-6697"
Jan 10 08:27:04.473: INFO: Deleting pod "simpletest-rc-to-be-deleted-4svx4" in namespace "gc-6697"
Jan 10 08:27:04.488: INFO: Deleting pod "simpletest-rc-to-be-deleted-5rdqt" in namespace "gc-6697"
Jan 10 08:27:04.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-68lwq" in namespace "gc-6697"
Jan 10 08:27:04.519: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jwdk" in namespace "gc-6697"
Jan 10 08:27:04.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vm65" in namespace "gc-6697"
Jan 10 08:27:04.605: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ppg8" in namespace "gc-6697"
Jan 10 08:27:04.626: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qkpr" in namespace "gc-6697"
Jan 10 08:27:04.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wnwj" in namespace "gc-6697"
Jan 10 08:27:04.659: INFO: Deleting pod "simpletest-rc-to-be-deleted-882qg" in namespace "gc-6697"
Jan 10 08:27:04.678: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cpqs" in namespace "gc-6697"
Jan 10 08:27:04.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-8n48j" in namespace "gc-6697"
Jan 10 08:27:04.708: INFO: Deleting pod "simpletest-rc-to-be-deleted-8p5dk" in namespace "gc-6697"
Jan 10 08:27:04.734: INFO: Deleting pod "simpletest-rc-to-be-deleted-8t4xd" in namespace "gc-6697"
Jan 10 08:27:04.746: INFO: Deleting pod "simpletest-rc-to-be-deleted-9b564" in namespace "gc-6697"
Jan 10 08:27:04.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nq2g" in namespace "gc-6697"
Jan 10 08:27:04.786: INFO: Deleting pod "simpletest-rc-to-be-deleted-9q6qr" in namespace "gc-6697"
Jan 10 08:27:04.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnb6j" in namespace "gc-6697"
Jan 10 08:27:04.833: INFO: Deleting pod "simpletest-rc-to-be-deleted-brg4s" in namespace "gc-6697"
Jan 10 08:27:04.845: INFO: Deleting pod "simpletest-rc-to-be-deleted-chwml" in namespace "gc-6697"
Jan 10 08:27:04.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqpr9" in namespace "gc-6697"
Jan 10 08:27:04.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-crkdj" in namespace "gc-6697"
Jan 10 08:27:04.888: INFO: Deleting pod "simpletest-rc-to-be-deleted-cssch" in namespace "gc-6697"
Jan 10 08:27:04.904: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvq9z" in namespace "gc-6697"
Jan 10 08:27:04.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-cw6r7" in namespace "gc-6697"
Jan 10 08:27:04.936: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx8lh" in namespace "gc-6697"
Jan 10 08:27:04.955: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9lxj" in namespace "gc-6697"
Jan 10 08:27:04.965: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnfbv" in namespace "gc-6697"
Jan 10 08:27:04.978: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7c5v" in namespace "gc-6697"
Jan 10 08:27:04.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8hp9" in namespace "gc-6697"
Jan 10 08:27:05.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjc2c" in namespace "gc-6697"
Jan 10 08:27:05.024: INFO: Deleting pod "simpletest-rc-to-be-deleted-fw4hj" in namespace "gc-6697"
Jan 10 08:27:05.042: INFO: Deleting pod "simpletest-rc-to-be-deleted-gj6jj" in namespace "gc-6697"
Jan 10 08:27:05.062: INFO: Deleting pod "simpletest-rc-to-be-deleted-h95hz" in namespace "gc-6697"
Jan 10 08:27:05.076: INFO: Deleting pod "simpletest-rc-to-be-deleted-hccbp" in namespace "gc-6697"
Jan 10 08:27:05.085: INFO: Deleting pod "simpletest-rc-to-be-deleted-hndzq" in namespace "gc-6697"
Jan 10 08:27:05.094: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqkkj" in namespace "gc-6697"
Jan 10 08:27:05.104: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwhgf" in namespace "gc-6697"
Jan 10 08:27:05.123: INFO: Deleting pod "simpletest-rc-to-be-deleted-j4tcz" in namespace "gc-6697"
Jan 10 08:27:05.133: INFO: Deleting pod "simpletest-rc-to-be-deleted-j87xs" in namespace "gc-6697"
Jan 10 08:27:05.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-jhkv6" in namespace "gc-6697"
Jan 10 08:27:05.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-js66l" in namespace "gc-6697"
Jan 10 08:27:05.168: INFO: Deleting pod "simpletest-rc-to-be-deleted-kbx5l" in namespace "gc-6697"
Jan 10 08:27:05.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfk7l" in namespace "gc-6697"
Jan 10 08:27:05.190: INFO: Deleting pod "simpletest-rc-to-be-deleted-l9mfx" in namespace "gc-6697"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 10 08:27:05.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6697" for this suite.

• [SLOW TEST:16.780 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":356,"completed":346,"skipped":6464,"failed":0}
SSSSS
------------------------------
[sig-architecture] Conformance Tests 
  should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:27:05.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename conformance-tests
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
STEP: Getting node addresses
Jan 10 08:27:05.289: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:188
Jan 10 08:27:05.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-3969" for this suite.
•{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","total":356,"completed":347,"skipped":6469,"failed":0}

------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:27:05.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:27:05.400: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-297614a7-4a12-4cd5-9878-a327501bc5e8" in namespace "security-context-test-3512" to be "Succeeded or Failed"
Jan 10 08:27:05.403: INFO: Pod "alpine-nnp-false-297614a7-4a12-4cd5-9878-a327501bc5e8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.351675ms
Jan 10 08:27:07.415: INFO: Pod "alpine-nnp-false-297614a7-4a12-4cd5-9878-a327501bc5e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015162842s
Jan 10 08:27:09.424: INFO: Pod "alpine-nnp-false-297614a7-4a12-4cd5-9878-a327501bc5e8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023857903s
Jan 10 08:27:11.428: INFO: Pod "alpine-nnp-false-297614a7-4a12-4cd5-9878-a327501bc5e8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027886665s
Jan 10 08:27:13.439: INFO: Pod "alpine-nnp-false-297614a7-4a12-4cd5-9878-a327501bc5e8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.038818574s
Jan 10 08:27:15.445: INFO: Pod "alpine-nnp-false-297614a7-4a12-4cd5-9878-a327501bc5e8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.044705587s
Jan 10 08:27:17.454: INFO: Pod "alpine-nnp-false-297614a7-4a12-4cd5-9878-a327501bc5e8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.054213849s
Jan 10 08:27:19.462: INFO: Pod "alpine-nnp-false-297614a7-4a12-4cd5-9878-a327501bc5e8": Phase="Running", Reason="", readiness=true. Elapsed: 14.061773312s
Jan 10 08:27:21.469: INFO: Pod "alpine-nnp-false-297614a7-4a12-4cd5-9878-a327501bc5e8": Phase="Running", Reason="", readiness=true. Elapsed: 16.069535346s
Jan 10 08:27:23.480: INFO: Pod "alpine-nnp-false-297614a7-4a12-4cd5-9878-a327501bc5e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.080337511s
Jan 10 08:27:23.480: INFO: Pod "alpine-nnp-false-297614a7-4a12-4cd5-9878-a327501bc5e8" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 10 08:27:23.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3512" for this suite.

• [SLOW TEST:18.191 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:298
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":348,"skipped":6469,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:27:23.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 10 08:27:23.570: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 08:27:23.579: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 08:27:23.579: INFO: Node kk-instance-r65pm is running 0 daemon pod, expected 1
Jan 10 08:27:24.589: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 08:27:24.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 10 08:27:24.593: INFO: Node kk-instance-r65pm is running 0 daemon pod, expected 1
Jan 10 08:27:25.589: INFO: DaemonSet pods can't tolerate node kk-instance-rx4ck with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 10 08:27:25.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 10 08:27:25.593: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan 10 08:27:25.634: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"55722"},"items":null}

Jan 10 08:27:25.639: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"55722"},"items":[{"metadata":{"name":"daemon-set-7nqgc","generateName":"daemon-set-","namespace":"daemonsets-2043","uid":"592d69d4-8ae5-4e05-935e-e908ef0b34e5","resourceVersion":"55716","creationTimestamp":"2023-01-10T08:27:23Z","labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"40a2b0473fb384b35cbd4cc8ff25f8d49b35dc4dd074ef81ef3a1e77e00a1ebd","cni.projectcalico.org/podIP":"10.233.107.106/32","cni.projectcalico.org/podIPs":"10.233.107.106/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0621ca93-e946-4b73-84f4-eabad0d2bdce","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-10T08:27:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0621ca93-e946-4b73-84f4-eabad0d2bdce\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-10T08:27:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-10T08:27:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.107.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-k4mb4","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-k4mb4","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kk-instance-vgmmg","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kk-instance-vgmmg"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-10T08:27:23Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-10T08:27:25Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-10T08:27:25Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-10T08:27:23Z"}],"hostIP":"192.168.0.5","podIP":"10.233.107.106","podIPs":[{"ip":"10.233.107.106"}],"startTime":"2023-01-10T08:27:23Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-10T08:27:24Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://fd153dd922bb0feb659223dc02c04adac5c8a31c31ed21ba135168c2e0ae676c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jm2hv","generateName":"daemon-set-","namespace":"daemonsets-2043","uid":"fc02e7fb-26bb-4848-be3a-aed1a586e321","resourceVersion":"55714","creationTimestamp":"2023-01-10T08:27:23Z","labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"59f095b1c88cd9c837792792562df643342d670a82e39abbf766eaf943e36c62","cni.projectcalico.org/podIP":"10.233.82.44/32","cni.projectcalico.org/podIPs":"10.233.82.44/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0621ca93-e946-4b73-84f4-eabad0d2bdce","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-10T08:27:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0621ca93-e946-4b73-84f4-eabad0d2bdce\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-10T08:27:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-10T08:27:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.82.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-x9zwm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-x9zwm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kk-instance-r65pm","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kk-instance-r65pm"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-10T08:27:23Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-10T08:27:25Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-10T08:27:25Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-10T08:27:23Z"}],"hostIP":"192.168.0.4","podIP":"10.233.82.44","podIPs":[{"ip":"10.233.82.44"}],"startTime":"2023-01-10T08:27:23Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-10T08:27:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://4d460927f7ea9d3659e3d05f55d28af838f9c7260a4d7d52db5b81ed548fbd5f","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 10 08:27:25.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2043" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":356,"completed":349,"skipped":6479,"failed":0}
SS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:27:25.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:27:25.715: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e4fe674e-c2f2-4ef2-946a-0718f873a7a5" in namespace "security-context-test-2889" to be "Succeeded or Failed"
Jan 10 08:27:25.720: INFO: Pod "busybox-privileged-false-e4fe674e-c2f2-4ef2-946a-0718f873a7a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.903386ms
Jan 10 08:27:27.732: INFO: Pod "busybox-privileged-false-e4fe674e-c2f2-4ef2-946a-0718f873a7a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016774875s
Jan 10 08:27:29.736: INFO: Pod "busybox-privileged-false-e4fe674e-c2f2-4ef2-946a-0718f873a7a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021547191s
Jan 10 08:27:29.737: INFO: Pod "busybox-privileged-false-e4fe674e-c2f2-4ef2-946a-0718f873a7a5" satisfied condition "Succeeded or Failed"
Jan 10 08:27:29.744: INFO: Got logs for pod "busybox-privileged-false-e4fe674e-c2f2-4ef2-946a-0718f873a7a5": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 10 08:27:29.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2889" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":350,"skipped":6481,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:27:29.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption is created
Jan 10 08:27:29.794: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 10 08:27:31.798: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 10 08:27:32.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2871" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":356,"completed":351,"skipped":6530,"failed":0}
SSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:27:32.824: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 10 08:29:00.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6592" for this suite.

• [SLOW TEST:88.073 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":356,"completed":352,"skipped":6535,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:29:00.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-dd7efd22-504f-4af2-9044-3924067044ca
STEP: Creating a pod to test consume configMaps
Jan 10 08:29:00.939: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cab12090-ca6a-4d8b-8b71-28f7889f1631" in namespace "projected-7257" to be "Succeeded or Failed"
Jan 10 08:29:00.946: INFO: Pod "pod-projected-configmaps-cab12090-ca6a-4d8b-8b71-28f7889f1631": Phase="Pending", Reason="", readiness=false. Elapsed: 6.392015ms
Jan 10 08:29:02.951: INFO: Pod "pod-projected-configmaps-cab12090-ca6a-4d8b-8b71-28f7889f1631": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011238053s
Jan 10 08:29:04.957: INFO: Pod "pod-projected-configmaps-cab12090-ca6a-4d8b-8b71-28f7889f1631": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01753436s
STEP: Saw pod success
Jan 10 08:29:04.957: INFO: Pod "pod-projected-configmaps-cab12090-ca6a-4d8b-8b71-28f7889f1631" satisfied condition "Succeeded or Failed"
Jan 10 08:29:04.961: INFO: Trying to get logs from node kk-instance-r65pm pod pod-projected-configmaps-cab12090-ca6a-4d8b-8b71-28f7889f1631 container agnhost-container: <nil>
STEP: delete the pod
Jan 10 08:29:04.992: INFO: Waiting for pod pod-projected-configmaps-cab12090-ca6a-4d8b-8b71-28f7889f1631 to disappear
Jan 10 08:29:04.996: INFO: Pod pod-projected-configmaps-cab12090-ca6a-4d8b-8b71-28f7889f1631 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 10 08:29:04.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7257" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":353,"skipped":6582,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:29:05.005: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4065
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4065
STEP: creating replication controller externalsvc in namespace services-4065
I0110 08:29:05.102730      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4065, replica count: 2
I0110 08:29:08.159561      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jan 10 08:29:08.184: INFO: Creating new exec pod
Jan 10 08:29:10.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1624157078 --namespace=services-4065 exec execpodznn64 -- /bin/sh -x -c nslookup nodeport-service.services-4065.svc.cluster.local'
Jan 10 08:29:10.391: INFO: stderr: "+ nslookup nodeport-service.services-4065.svc.cluster.local\n"
Jan 10 08:29:10.391: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-4065.svc.cluster.local\tcanonical name = externalsvc.services-4065.svc.cluster.local.\nName:\texternalsvc.services-4065.svc.cluster.local\nAddress: 10.233.17.207\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4065, will wait for the garbage collector to delete the pods
Jan 10 08:29:10.453: INFO: Deleting ReplicationController externalsvc took: 5.911385ms
Jan 10 08:29:10.553: INFO: Terminating ReplicationController externalsvc pods took: 100.459806ms
Jan 10 08:29:12.777: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 10 08:29:12.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4065" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:7.805 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":356,"completed":354,"skipped":6593,"failed":0}
SSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:29:12.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 10 08:29:12.842: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-f3ca6a5a-045d-421d-a690-82b135580724" in namespace "security-context-test-2443" to be "Succeeded or Failed"
Jan 10 08:29:12.849: INFO: Pod "busybox-readonly-false-f3ca6a5a-045d-421d-a690-82b135580724": Phase="Pending", Reason="", readiness=false. Elapsed: 6.449387ms
Jan 10 08:29:14.856: INFO: Pod "busybox-readonly-false-f3ca6a5a-045d-421d-a690-82b135580724": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013656948s
Jan 10 08:29:16.866: INFO: Pod "busybox-readonly-false-f3ca6a5a-045d-421d-a690-82b135580724": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023669618s
Jan 10 08:29:16.866: INFO: Pod "busybox-readonly-false-f3ca6a5a-045d-421d-a690-82b135580724" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 10 08:29:16.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2443" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":356,"completed":355,"skipped":6596,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 10 08:29:16.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1624157078
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 10 08:29:16.916: INFO: Waiting up to 5m0s for pod "pod-80d10439-f13c-4ea0-abf9-e7e31338149d" in namespace "emptydir-2301" to be "Succeeded or Failed"
Jan 10 08:29:16.920: INFO: Pod "pod-80d10439-f13c-4ea0-abf9-e7e31338149d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.081639ms
Jan 10 08:29:18.925: INFO: Pod "pod-80d10439-f13c-4ea0-abf9-e7e31338149d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008500596s
Jan 10 08:29:20.930: INFO: Pod "pod-80d10439-f13c-4ea0-abf9-e7e31338149d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013417438s
STEP: Saw pod success
Jan 10 08:29:20.930: INFO: Pod "pod-80d10439-f13c-4ea0-abf9-e7e31338149d" satisfied condition "Succeeded or Failed"
Jan 10 08:29:20.933: INFO: Trying to get logs from node kk-instance-vgmmg pod pod-80d10439-f13c-4ea0-abf9-e7e31338149d container test-container: <nil>
STEP: delete the pod
Jan 10 08:29:20.959: INFO: Waiting for pod pod-80d10439-f13c-4ea0-abf9-e7e31338149d to disappear
Jan 10 08:29:20.963: INFO: Pod pod-80d10439-f13c-4ea0-abf9-e7e31338149d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 10 08:29:20.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2301" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":356,"skipped":6613,"failed":0}
SSSSJan 10 08:29:20.973: INFO: Running AfterSuite actions on all nodes
Jan 10 08:29:20.973: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func19.2
Jan 10 08:29:20.973: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan 10 08:29:20.973: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Jan 10 08:29:20.973: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 10 08:29:20.973: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 10 08:29:20.973: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 10 08:29:20.973: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Jan 10 08:29:20.973: INFO: Running AfterSuite actions on node 1
Jan 10 08:29:20.973: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":356,"completed":356,"skipped":6617,"failed":0}

Ran 356 of 6973 Specs in 5584.078 seconds
SUCCESS! -- 356 Passed | 0 Failed | 0 Pending | 6617 Skipped
PASS

Ginkgo ran 1 suite in 1h33m6.501808748s
Test Suite Passed
