I0131 09:42:07.771796      22 e2e.go:129] Starting e2e run "f4cbd94a-31f3-4619-a58f-931890206d12" on Ginkgo node 1
{"msg":"Test Suite starting","total":356,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1675158127 - Will randomize all specs
Will run 356 of 6973 specs

Jan 31 09:42:09.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:42:09.145: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 31 09:42:09.159: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 31 09:42:09.184: INFO: 27 / 27 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 31 09:42:09.184: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Jan 31 09:42:09.184: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 31 09:42:09.188: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 31 09:42:09.188: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 31 09:42:09.188: INFO: e2e test version: v1.24.6
Jan 31 09:42:09.189: INFO: kube-apiserver version: v1.24.6
Jan 31 09:42:09.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:42:09.191: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:42:09.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
W0131 09:42:09.210227      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jan 31 09:42:09.210: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Jan 31 09:42:09.216: INFO: PSP annotation exists on dry run pod: "e2e-test-privileged-psp"; assuming PodSecurityPolicy is enabled
W0131 09:42:09.218378      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jan 31 09:42:09.222: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8565
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting the proxy server
Jan 31 09:42:09.331: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-8565 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 09:42:09.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8565" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":356,"completed":1,"skipped":18,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:42:09.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7824
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 31 09:42:09.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7824" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":356,"completed":2,"skipped":27,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:42:09.538: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1274
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 09:42:09.669: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 31 09:42:09.675: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:09.675: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:09.675: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:09.678: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 09:42:09.678: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 09:42:10.685: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:10.685: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:10.685: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:10.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 09:42:10.689: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 09:42:11.686: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:11.686: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:11.686: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:11.688: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 09:42:11.688: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 09:42:12.684: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:12.684: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:12.684: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:12.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 09:42:12.687: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 09:42:13.683: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:13.683: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:13.683: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:13.685: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 09:42:13.685: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 09:42:14.683: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:14.683: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:14.683: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:14.686: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 31 09:42:14.686: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 31 09:42:14.716: INFO: Wrong image for pod: daemon-set-59q99. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 31 09:42:14.716: INFO: Wrong image for pod: daemon-set-wjjkv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 31 09:42:14.745: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:14.745: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:14.745: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:15.749: INFO: Wrong image for pod: daemon-set-wjjkv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 31 09:42:15.753: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:15.753: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:15.753: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:16.749: INFO: Wrong image for pod: daemon-set-wjjkv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 31 09:42:16.752: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:16.752: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:16.752: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:17.751: INFO: Pod daemon-set-nrqql is not available
Jan 31 09:42:17.751: INFO: Wrong image for pod: daemon-set-wjjkv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 31 09:42:17.755: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:17.755: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:17.755: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:18.752: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:18.752: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:18.752: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:19.751: INFO: Pod daemon-set-wlxbq is not available
Jan 31 09:42:19.754: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:19.754: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:19.754: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 31 09:42:19.757: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:19.757: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:19.757: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:19.759: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 09:42:19.759: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 09:42:20.764: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:20.764: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:20.764: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:20.767: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 09:42:20.767: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 09:42:21.763: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:21.763: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:21.763: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:21.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 09:42:21.766: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 09:42:22.763: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:22.763: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:22.763: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:42:22.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 31 09:42:22.766: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1274, will wait for the garbage collector to delete the pods
Jan 31 09:42:22.833: INFO: Deleting DaemonSet.extensions daemon-set took: 4.977246ms
Jan 31 09:42:22.934: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.230665ms
Jan 31 09:42:25.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 09:42:25.343: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 31 09:42:25.345: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5119"},"items":null}

Jan 31 09:42:25.347: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5119"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 31 09:42:25.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1274" for this suite.

• [SLOW TEST:15.825 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":356,"completed":3,"skipped":36,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:42:25.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9522
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 09:42:25.509: INFO: Waiting up to 5m0s for pod "downwardapi-volume-781e00e0-8427-4faa-81b1-55acc745ccb2" in namespace "projected-9522" to be "Succeeded or Failed"
Jan 31 09:42:25.512: INFO: Pod "downwardapi-volume-781e00e0-8427-4faa-81b1-55acc745ccb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.623365ms
Jan 31 09:42:27.517: INFO: Pod "downwardapi-volume-781e00e0-8427-4faa-81b1-55acc745ccb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008188059s
Jan 31 09:42:29.524: INFO: Pod "downwardapi-volume-781e00e0-8427-4faa-81b1-55acc745ccb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014327787s
STEP: Saw pod success
Jan 31 09:42:29.524: INFO: Pod "downwardapi-volume-781e00e0-8427-4faa-81b1-55acc745ccb2" satisfied condition "Succeeded or Failed"
Jan 31 09:42:29.526: INFO: Trying to get logs from node macpro-x86-1 pod downwardapi-volume-781e00e0-8427-4faa-81b1-55acc745ccb2 container client-container: <nil>
STEP: delete the pod
Jan 31 09:42:29.552: INFO: Waiting for pod downwardapi-volume-781e00e0-8427-4faa-81b1-55acc745ccb2 to disappear
Jan 31 09:42:29.555: INFO: Pod downwardapi-volume-781e00e0-8427-4faa-81b1-55acc745ccb2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 31 09:42:29.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9522" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":4,"skipped":36,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:42:29.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4891
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 09:42:29.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4891" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":356,"completed":5,"skipped":47,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:42:29.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9583
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9583
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-9583
I0131 09:42:29.896881      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9583, replica count: 2
I0131 09:42:32.947558      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 31 09:42:32.947: INFO: Creating new exec pod
Jan 31 09:42:35.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-9583 exec execpodhjj5c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 31 09:42:36.353: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 31 09:42:36.353: INFO: stdout: "externalname-service-7wv9s"
Jan 31 09:42:36.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-9583 exec execpodhjj5c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.131.127 80'
Jan 31 09:42:36.444: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.131.127 80\nConnection to 10.96.131.127 80 port [tcp/http] succeeded!\n"
Jan 31 09:42:36.444: INFO: stdout: "externalname-service-7wv9s"
Jan 31 09:42:36.444: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 09:42:36.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9583" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:6.769 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":356,"completed":6,"skipped":58,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:42:36.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8983
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 09:42:36.980: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 09:42:40.011: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jan 31 09:42:42.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=webhook-8983 attach --namespace=webhook-8983 to-be-attached-pod -i -c=container1'
Jan 31 09:42:42.110: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 09:42:42.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8983" for this suite.
STEP: Destroying namespace "webhook-8983-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.721 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":356,"completed":7,"skipped":67,"failed":0}
SSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:42:42.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5228
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 09:42:42.354: INFO: Creating pod...
Jan 31 09:42:44.379: INFO: Creating service...
Jan 31 09:42:44.391: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5228/pods/agnhost/proxy?method=DELETE
Jan 31 09:42:44.396: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 31 09:42:44.396: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5228/pods/agnhost/proxy?method=OPTIONS
Jan 31 09:42:44.402: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 31 09:42:44.402: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5228/pods/agnhost/proxy?method=PATCH
Jan 31 09:42:44.405: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 31 09:42:44.405: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5228/pods/agnhost/proxy?method=POST
Jan 31 09:42:44.407: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 31 09:42:44.407: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5228/pods/agnhost/proxy?method=PUT
Jan 31 09:42:44.410: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 31 09:42:44.410: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5228/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 31 09:42:44.413: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 31 09:42:44.413: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5228/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 31 09:42:44.418: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 31 09:42:44.418: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5228/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 31 09:42:44.421: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 31 09:42:44.421: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5228/services/e2e-proxy-test-service/proxy?method=POST
Jan 31 09:42:44.424: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 31 09:42:44.424: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5228/services/e2e-proxy-test-service/proxy?method=PUT
Jan 31 09:42:44.426: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 31 09:42:44.426: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5228/pods/agnhost/proxy?method=GET
Jan 31 09:42:44.429: INFO: http.Client request:GET StatusCode:301
Jan 31 09:42:44.429: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5228/services/e2e-proxy-test-service/proxy?method=GET
Jan 31 09:42:44.432: INFO: http.Client request:GET StatusCode:301
Jan 31 09:42:44.432: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5228/pods/agnhost/proxy?method=HEAD
Jan 31 09:42:44.435: INFO: http.Client request:HEAD StatusCode:301
Jan 31 09:42:44.435: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5228/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 31 09:42:44.437: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jan 31 09:42:44.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5228" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","total":356,"completed":8,"skipped":70,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:42:44.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1250
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Jan 31 09:42:44.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 create -f -'
Jan 31 09:42:45.823: INFO: stderr: ""
Jan 31 09:42:45.823: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 31 09:42:45.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 31 09:42:45.870: INFO: stderr: ""
Jan 31 09:42:45.870: INFO: stdout: "update-demo-nautilus-f2skl update-demo-nautilus-t65kh "
Jan 31 09:42:45.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods update-demo-nautilus-f2skl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 31 09:42:45.916: INFO: stderr: ""
Jan 31 09:42:45.916: INFO: stdout: ""
Jan 31 09:42:45.916: INFO: update-demo-nautilus-f2skl is created but not running
Jan 31 09:42:50.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 31 09:42:50.966: INFO: stderr: ""
Jan 31 09:42:50.966: INFO: stdout: "update-demo-nautilus-f2skl update-demo-nautilus-t65kh "
Jan 31 09:42:50.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods update-demo-nautilus-f2skl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 31 09:42:51.016: INFO: stderr: ""
Jan 31 09:42:51.016: INFO: stdout: "true"
Jan 31 09:42:51.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods update-demo-nautilus-f2skl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 31 09:42:51.063: INFO: stderr: ""
Jan 31 09:42:51.063: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 31 09:42:51.063: INFO: validating pod update-demo-nautilus-f2skl
Jan 31 09:42:51.067: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 31 09:42:51.067: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 31 09:42:51.067: INFO: update-demo-nautilus-f2skl is verified up and running
Jan 31 09:42:51.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods update-demo-nautilus-t65kh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 31 09:42:51.110: INFO: stderr: ""
Jan 31 09:42:51.110: INFO: stdout: "true"
Jan 31 09:42:51.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods update-demo-nautilus-t65kh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 31 09:42:51.153: INFO: stderr: ""
Jan 31 09:42:51.153: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 31 09:42:51.153: INFO: validating pod update-demo-nautilus-t65kh
Jan 31 09:42:51.156: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 31 09:42:51.156: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 31 09:42:51.156: INFO: update-demo-nautilus-t65kh is verified up and running
STEP: scaling down the replication controller
Jan 31 09:42:51.157: INFO: scanned /root for discovery docs: <nil>
Jan 31 09:42:51.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 31 09:42:52.217: INFO: stderr: ""
Jan 31 09:42:52.217: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 31 09:42:52.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 31 09:42:52.272: INFO: stderr: ""
Jan 31 09:42:52.272: INFO: stdout: "update-demo-nautilus-f2skl update-demo-nautilus-t65kh "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 31 09:42:57.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 31 09:42:57.318: INFO: stderr: ""
Jan 31 09:42:57.318: INFO: stdout: "update-demo-nautilus-t65kh "
Jan 31 09:42:57.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods update-demo-nautilus-t65kh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 31 09:42:57.362: INFO: stderr: ""
Jan 31 09:42:57.362: INFO: stdout: "true"
Jan 31 09:42:57.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods update-demo-nautilus-t65kh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 31 09:42:57.404: INFO: stderr: ""
Jan 31 09:42:57.404: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 31 09:42:57.404: INFO: validating pod update-demo-nautilus-t65kh
Jan 31 09:42:57.406: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 31 09:42:57.407: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 31 09:42:57.407: INFO: update-demo-nautilus-t65kh is verified up and running
STEP: scaling up the replication controller
Jan 31 09:42:57.408: INFO: scanned /root for discovery docs: <nil>
Jan 31 09:42:57.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 31 09:42:58.479: INFO: stderr: ""
Jan 31 09:42:58.479: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 31 09:42:58.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 31 09:42:58.537: INFO: stderr: ""
Jan 31 09:42:58.537: INFO: stdout: "update-demo-nautilus-t65kh update-demo-nautilus-w9bzq "
Jan 31 09:42:58.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods update-demo-nautilus-t65kh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 31 09:42:58.579: INFO: stderr: ""
Jan 31 09:42:58.579: INFO: stdout: "true"
Jan 31 09:42:58.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods update-demo-nautilus-t65kh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 31 09:42:58.623: INFO: stderr: ""
Jan 31 09:42:58.623: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 31 09:42:58.623: INFO: validating pod update-demo-nautilus-t65kh
Jan 31 09:42:58.626: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 31 09:42:58.626: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 31 09:42:58.626: INFO: update-demo-nautilus-t65kh is verified up and running
Jan 31 09:42:58.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods update-demo-nautilus-w9bzq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 31 09:42:58.669: INFO: stderr: ""
Jan 31 09:42:58.669: INFO: stdout: "true"
Jan 31 09:42:58.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods update-demo-nautilus-w9bzq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 31 09:42:58.711: INFO: stderr: ""
Jan 31 09:42:58.711: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 31 09:42:58.711: INFO: validating pod update-demo-nautilus-w9bzq
Jan 31 09:42:58.715: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 31 09:42:58.715: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 31 09:42:58.715: INFO: update-demo-nautilus-w9bzq is verified up and running
STEP: using delete to clean up resources
Jan 31 09:42:58.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 delete --grace-period=0 --force -f -'
Jan 31 09:42:58.760: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 31 09:42:58.760: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 31 09:42:58.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get rc,svc -l name=update-demo --no-headers'
Jan 31 09:42:58.809: INFO: stderr: "No resources found in kubectl-1250 namespace.\n"
Jan 31 09:42:58.809: INFO: stdout: ""
Jan 31 09:42:58.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1250 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 31 09:42:58.857: INFO: stderr: ""
Jan 31 09:42:58.857: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 09:42:58.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1250" for this suite.

• [SLOW TEST:14.420 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should scale a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":356,"completed":9,"skipped":83,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:42:58.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9402
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 09:42:59.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 09:43:02.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9402" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":356,"completed":10,"skipped":104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:43:02.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6031
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 31 09:43:02.277: INFO: Waiting up to 5m0s for pod "pod-7b978f6b-e570-4cb4-9c52-3b7fb38f3362" in namespace "emptydir-6031" to be "Succeeded or Failed"
Jan 31 09:43:02.285: INFO: Pod "pod-7b978f6b-e570-4cb4-9c52-3b7fb38f3362": Phase="Pending", Reason="", readiness=false. Elapsed: 7.30753ms
Jan 31 09:43:04.291: INFO: Pod "pod-7b978f6b-e570-4cb4-9c52-3b7fb38f3362": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014091567s
Jan 31 09:43:06.296: INFO: Pod "pod-7b978f6b-e570-4cb4-9c52-3b7fb38f3362": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018553408s
STEP: Saw pod success
Jan 31 09:43:06.296: INFO: Pod "pod-7b978f6b-e570-4cb4-9c52-3b7fb38f3362" satisfied condition "Succeeded or Failed"
Jan 31 09:43:06.298: INFO: Trying to get logs from node macpro-x86-2 pod pod-7b978f6b-e570-4cb4-9c52-3b7fb38f3362 container test-container: <nil>
STEP: delete the pod
Jan 31 09:43:06.316: INFO: Waiting for pod pod-7b978f6b-e570-4cb4-9c52-3b7fb38f3362 to disappear
Jan 31 09:43:06.318: INFO: Pod pod-7b978f6b-e570-4cb4-9c52-3b7fb38f3362 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 31 09:43:06.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6031" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":11,"skipped":127,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:43:06.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9564
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in volume subpath
Jan 31 09:43:06.454: INFO: Waiting up to 5m0s for pod "var-expansion-0b0e94d8-f5ee-4f3a-b6c5-b7d7a9ab1198" in namespace "var-expansion-9564" to be "Succeeded or Failed"
Jan 31 09:43:06.458: INFO: Pod "var-expansion-0b0e94d8-f5ee-4f3a-b6c5-b7d7a9ab1198": Phase="Pending", Reason="", readiness=false. Elapsed: 3.985148ms
Jan 31 09:43:08.462: INFO: Pod "var-expansion-0b0e94d8-f5ee-4f3a-b6c5-b7d7a9ab1198": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008190883s
Jan 31 09:43:10.467: INFO: Pod "var-expansion-0b0e94d8-f5ee-4f3a-b6c5-b7d7a9ab1198": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01302332s
STEP: Saw pod success
Jan 31 09:43:10.467: INFO: Pod "var-expansion-0b0e94d8-f5ee-4f3a-b6c5-b7d7a9ab1198" satisfied condition "Succeeded or Failed"
Jan 31 09:43:10.470: INFO: Trying to get logs from node macpro-x86-2 pod var-expansion-0b0e94d8-f5ee-4f3a-b6c5-b7d7a9ab1198 container dapi-container: <nil>
STEP: delete the pod
Jan 31 09:43:10.484: INFO: Waiting for pod var-expansion-0b0e94d8-f5ee-4f3a-b6c5-b7d7a9ab1198 to disappear
Jan 31 09:43:10.487: INFO: Pod var-expansion-0b0e94d8-f5ee-4f3a-b6c5-b7d7a9ab1198 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 31 09:43:10.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9564" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":356,"completed":12,"skipped":136,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:43:10.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5325
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 09:43:10.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jan 31 09:43:16.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-5325 --namespace=crd-publish-openapi-5325 create -f -'
Jan 31 09:43:17.165: INFO: stderr: ""
Jan 31 09:43:17.165: INFO: stdout: "e2e-test-crd-publish-openapi-7528-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 31 09:43:17.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-5325 --namespace=crd-publish-openapi-5325 delete e2e-test-crd-publish-openapi-7528-crds test-cr'
Jan 31 09:43:17.212: INFO: stderr: ""
Jan 31 09:43:17.212: INFO: stdout: "e2e-test-crd-publish-openapi-7528-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 31 09:43:17.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-5325 --namespace=crd-publish-openapi-5325 apply -f -'
Jan 31 09:43:18.009: INFO: stderr: ""
Jan 31 09:43:18.009: INFO: stdout: "e2e-test-crd-publish-openapi-7528-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 31 09:43:18.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-5325 --namespace=crd-publish-openapi-5325 delete e2e-test-crd-publish-openapi-7528-crds test-cr'
Jan 31 09:43:18.057: INFO: stderr: ""
Jan 31 09:43:18.057: INFO: stdout: "e2e-test-crd-publish-openapi-7528-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 31 09:43:18.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-5325 explain e2e-test-crd-publish-openapi-7528-crds'
Jan 31 09:43:18.218: INFO: stderr: ""
Jan 31 09:43:18.218: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7528-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 09:43:21.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5325" for this suite.

• [SLOW TEST:11.167 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":356,"completed":13,"skipped":168,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:43:21.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6529
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-1a86d7ee-e517-4ec0-a590-c5dfd6a63ede in namespace container-probe-6529
Jan 31 09:43:23.806: INFO: Started pod liveness-1a86d7ee-e517-4ec0-a590-c5dfd6a63ede in namespace container-probe-6529
STEP: checking the pod's current state and verifying that restartCount is present
Jan 31 09:43:23.808: INFO: Initial restart count of pod liveness-1a86d7ee-e517-4ec0-a590-c5dfd6a63ede is 0
Jan 31 09:43:43.884: INFO: Restart count of pod container-probe-6529/liveness-1a86d7ee-e517-4ec0-a590-c5dfd6a63ede is now 1 (20.076452964s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 31 09:43:43.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6529" for this suite.

• [SLOW TEST:22.242 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":14,"skipped":230,"failed":0}
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:43:43.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9918
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 31 09:43:44.042: INFO: The status of Pod pod-update-11c18cf5-a449-4b16-9e76-101ccee39e9e is Pending, waiting for it to be Running (with Ready = true)
Jan 31 09:43:46.049: INFO: The status of Pod pod-update-11c18cf5-a449-4b16-9e76-101ccee39e9e is Pending, waiting for it to be Running (with Ready = true)
Jan 31 09:43:48.050: INFO: The status of Pod pod-update-11c18cf5-a449-4b16-9e76-101ccee39e9e is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 31 09:43:48.573: INFO: Successfully updated pod "pod-update-11c18cf5-a449-4b16-9e76-101ccee39e9e"
STEP: verifying the updated pod is in kubernetes
Jan 31 09:43:48.577: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 31 09:43:48.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9918" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":356,"completed":15,"skipped":230,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:43:48.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6472
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating cluster-info
Jan 31 09:43:48.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6472 cluster-info'
Jan 31 09:43:48.758: INFO: stderr: ""
Jan 31 09:43:48.758: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 09:43:48.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6472" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":356,"completed":16,"skipped":240,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:43:48.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-1699
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:188
Jan 31 09:43:48.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1699" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":356,"completed":17,"skipped":249,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:43:48.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5440
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-1459c51a-1d67-4cd1-af1a-36ac9c745603
STEP: Creating secret with name s-test-opt-upd-91b14676-98f8-49d6-878e-b86e00cdf406
STEP: Creating the pod
Jan 31 09:43:49.118: INFO: The status of Pod pod-secrets-89020089-c9ab-47db-8928-7484b014b190 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 09:43:51.123: INFO: The status of Pod pod-secrets-89020089-c9ab-47db-8928-7484b014b190 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-1459c51a-1d67-4cd1-af1a-36ac9c745603
STEP: Updating secret s-test-opt-upd-91b14676-98f8-49d6-878e-b86e00cdf406
STEP: Creating secret with name s-test-opt-create-1966c98e-1f47-4f8b-9a34-c96d464230d6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 31 09:43:53.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5440" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":18,"skipped":257,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:43:53.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4359
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 31 09:43:53.372: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4359  bbbe1530-1642-4954-a82a-e6d8c5fb3bf8 5937 0 2023-01-31 09:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2023-01-31 09:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 31 09:43:53.372: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4359  bbbe1530-1642-4954-a82a-e6d8c5fb3bf8 5938 0 2023-01-31 09:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2023-01-31 09:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 31 09:43:53.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4359" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":356,"completed":19,"skipped":283,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:43:53.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3640
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test service account token: 
Jan 31 09:43:53.512: INFO: Waiting up to 5m0s for pod "test-pod-105a3613-733c-494e-8b79-9e03e5efb78e" in namespace "svcaccounts-3640" to be "Succeeded or Failed"
Jan 31 09:43:53.517: INFO: Pod "test-pod-105a3613-733c-494e-8b79-9e03e5efb78e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.659965ms
Jan 31 09:43:55.525: INFO: Pod "test-pod-105a3613-733c-494e-8b79-9e03e5efb78e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012793981s
Jan 31 09:43:57.532: INFO: Pod "test-pod-105a3613-733c-494e-8b79-9e03e5efb78e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019669285s
STEP: Saw pod success
Jan 31 09:43:57.532: INFO: Pod "test-pod-105a3613-733c-494e-8b79-9e03e5efb78e" satisfied condition "Succeeded or Failed"
Jan 31 09:43:57.534: INFO: Trying to get logs from node macpro-x86-2 pod test-pod-105a3613-733c-494e-8b79-9e03e5efb78e container agnhost-container: <nil>
STEP: delete the pod
Jan 31 09:43:57.556: INFO: Waiting for pod test-pod-105a3613-733c-494e-8b79-9e03e5efb78e to disappear
Jan 31 09:43:57.558: INFO: Pod test-pod-105a3613-733c-494e-8b79-9e03e5efb78e no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 31 09:43:57.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3640" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":356,"completed":20,"skipped":287,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:43:57.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1654
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 09:43:58.181: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 09:44:01.203: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 09:44:01.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1654" for this suite.
STEP: Destroying namespace "webhook-1654-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":356,"completed":21,"skipped":293,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:44:01.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6454
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 31 09:44:01.604: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:01.604: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:01.604: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:01.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 09:44:01.606: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 09:44:02.612: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:02.612: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:02.612: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:02.615: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 31 09:44:02.615: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 31 09:44:02.631: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:02.631: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:02.631: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:02.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 09:44:02.634: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 09:44:03.639: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:03.639: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:03.639: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:03.642: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 09:44:03.642: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 09:44:04.640: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:04.640: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:04.640: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:04.642: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 09:44:04.642: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 09:44:05.641: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:05.641: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:05.641: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:05.644: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 09:44:05.644: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 09:44:06.643: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:06.643: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:06.643: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:44:06.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 31 09:44:06.650: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6454, will wait for the garbage collector to delete the pods
Jan 31 09:44:06.711: INFO: Deleting DaemonSet.extensions daemon-set took: 5.55505ms
Jan 31 09:44:06.811: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.261769ms
Jan 31 09:44:09.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 09:44:09.515: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 31 09:44:09.517: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6235"},"items":null}

Jan 31 09:44:09.521: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6235"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 31 09:44:09.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6454" for this suite.

• [SLOW TEST:8.097 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":356,"completed":22,"skipped":297,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:44:09.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2857
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2857.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2857.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2857.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2857.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 31 09:44:15.704: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:15.706: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:15.708: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:15.710: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:15.712: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:15.715: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:15.717: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:15.718: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:15.718: INFO: Lookups using dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2857.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2857.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local jessie_udp@dns-test-service-2.dns-2857.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2857.svc.cluster.local]

Jan 31 09:44:20.722: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:20.724: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:20.728: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:20.730: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:20.733: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:20.735: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:20.737: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:20.739: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:20.739: INFO: Lookups using dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2857.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2857.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local jessie_udp@dns-test-service-2.dns-2857.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2857.svc.cluster.local]

Jan 31 09:44:25.725: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:25.728: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:25.730: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:25.733: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:25.735: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:25.738: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:25.740: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:25.742: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:25.742: INFO: Lookups using dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2857.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2857.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local jessie_udp@dns-test-service-2.dns-2857.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2857.svc.cluster.local]

Jan 31 09:44:30.724: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:30.727: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:30.730: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:30.732: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:30.736: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:30.738: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:30.741: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:30.743: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:30.743: INFO: Lookups using dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2857.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2857.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local jessie_udp@dns-test-service-2.dns-2857.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2857.svc.cluster.local]

Jan 31 09:44:35.724: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:35.726: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:35.729: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:35.732: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:35.737: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:35.739: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:35.742: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:35.750: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:35.750: INFO: Lookups using dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2857.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2857.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local jessie_udp@dns-test-service-2.dns-2857.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2857.svc.cluster.local]

Jan 31 09:44:40.722: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:40.725: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:40.728: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:40.730: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:40.733: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:40.736: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local from pod dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68: the server could not find the requested resource (get pods dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68)
Jan 31 09:44:40.740: INFO: Lookups using dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2857.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2857.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2857.svc.cluster.local]

Jan 31 09:44:45.740: INFO: DNS probes using dns-2857/dns-test-a834687e-7380-4e2d-88b5-7d9b3a3d9f68 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 31 09:44:45.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2857" for this suite.

• [SLOW TEST:36.269 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":356,"completed":23,"skipped":310,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:44:45.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-631
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 09:44:45.942: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9af5a6d4-dae4-4989-8bfc-bd4bb77044f5" in namespace "projected-631" to be "Succeeded or Failed"
Jan 31 09:44:45.948: INFO: Pod "downwardapi-volume-9af5a6d4-dae4-4989-8bfc-bd4bb77044f5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.749904ms
Jan 31 09:44:47.951: INFO: Pod "downwardapi-volume-9af5a6d4-dae4-4989-8bfc-bd4bb77044f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0091461s
Jan 31 09:44:49.956: INFO: Pod "downwardapi-volume-9af5a6d4-dae4-4989-8bfc-bd4bb77044f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013891256s
STEP: Saw pod success
Jan 31 09:44:49.956: INFO: Pod "downwardapi-volume-9af5a6d4-dae4-4989-8bfc-bd4bb77044f5" satisfied condition "Succeeded or Failed"
Jan 31 09:44:49.958: INFO: Trying to get logs from node macpro-x86-1 pod downwardapi-volume-9af5a6d4-dae4-4989-8bfc-bd4bb77044f5 container client-container: <nil>
STEP: delete the pod
Jan 31 09:44:49.972: INFO: Waiting for pod downwardapi-volume-9af5a6d4-dae4-4989-8bfc-bd4bb77044f5 to disappear
Jan 31 09:44:49.975: INFO: Pod downwardapi-volume-9af5a6d4-dae4-4989-8bfc-bd4bb77044f5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 31 09:44:49.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-631" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":24,"skipped":312,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:44:49.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5831
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-5831
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 31 09:44:50.108: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 31 09:44:50.135: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 09:44:52.138: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 09:44:54.143: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 09:44:56.142: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 09:44:58.144: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 09:45:00.140: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 09:45:02.143: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 31 09:45:02.147: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 31 09:45:04.184: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 31 09:45:04.184: INFO: Going to poll 192.168.75.21 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 31 09:45:04.186: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.75.21 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5831 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:45:04.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:45:04.186: INFO: ExecWithOptions: Clientset creation
Jan 31 09:45:04.186: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5831/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.75.21+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 31 09:45:05.243: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 31 09:45:05.243: INFO: Going to poll 192.168.38.211 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 31 09:45:05.250: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.38.211 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5831 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:45:05.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:45:05.250: INFO: ExecWithOptions: Clientset creation
Jan 31 09:45:05.250: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5831/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.38.211+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 31 09:45:06.315: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 31 09:45:06.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5831" for this suite.

• [SLOW TEST:16.346 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":25,"skipped":317,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:45:06.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8187
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 31 09:45:22.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8187" for this suite.

• [SLOW TEST:16.224 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":356,"completed":26,"skipped":325,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:45:22.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8967
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-89b0f735-9f03-41d4-885f-eed7e079f3cd
STEP: Creating a pod to test consume configMaps
Jan 31 09:45:22.702: INFO: Waiting up to 5m0s for pod "pod-configmaps-23aae7b9-9e1b-46ca-863d-d87e431ff3bb" in namespace "configmap-8967" to be "Succeeded or Failed"
Jan 31 09:45:22.708: INFO: Pod "pod-configmaps-23aae7b9-9e1b-46ca-863d-d87e431ff3bb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.899435ms
Jan 31 09:45:24.714: INFO: Pod "pod-configmaps-23aae7b9-9e1b-46ca-863d-d87e431ff3bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012422887s
Jan 31 09:45:26.723: INFO: Pod "pod-configmaps-23aae7b9-9e1b-46ca-863d-d87e431ff3bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021217856s
STEP: Saw pod success
Jan 31 09:45:26.723: INFO: Pod "pod-configmaps-23aae7b9-9e1b-46ca-863d-d87e431ff3bb" satisfied condition "Succeeded or Failed"
Jan 31 09:45:26.732: INFO: Trying to get logs from node macpro-x86-2 pod pod-configmaps-23aae7b9-9e1b-46ca-863d-d87e431ff3bb container agnhost-container: <nil>
STEP: delete the pod
Jan 31 09:45:26.772: INFO: Waiting for pod pod-configmaps-23aae7b9-9e1b-46ca-863d-d87e431ff3bb to disappear
Jan 31 09:45:26.775: INFO: Pod pod-configmaps-23aae7b9-9e1b-46ca-863d-d87e431ff3bb no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 31 09:45:26.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8967" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":27,"skipped":326,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:45:26.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5124
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jan 31 09:46:07.091: INFO: The status of Pod kube-controller-manager-master-3 is Running (Ready = true)
Jan 31 09:46:07.148: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 31 09:46:07.148: INFO: Deleting pod "simpletest.rc-22wt7" in namespace "gc-5124"
Jan 31 09:46:07.160: INFO: Deleting pod "simpletest.rc-27skc" in namespace "gc-5124"
Jan 31 09:46:07.169: INFO: Deleting pod "simpletest.rc-44qhb" in namespace "gc-5124"
Jan 31 09:46:07.185: INFO: Deleting pod "simpletest.rc-4f978" in namespace "gc-5124"
Jan 31 09:46:07.197: INFO: Deleting pod "simpletest.rc-4tq69" in namespace "gc-5124"
Jan 31 09:46:07.219: INFO: Deleting pod "simpletest.rc-4wbvl" in namespace "gc-5124"
Jan 31 09:46:07.233: INFO: Deleting pod "simpletest.rc-4zc6d" in namespace "gc-5124"
Jan 31 09:46:07.250: INFO: Deleting pod "simpletest.rc-52s6w" in namespace "gc-5124"
Jan 31 09:46:07.272: INFO: Deleting pod "simpletest.rc-56492" in namespace "gc-5124"
Jan 31 09:46:07.284: INFO: Deleting pod "simpletest.rc-586sp" in namespace "gc-5124"
Jan 31 09:46:07.301: INFO: Deleting pod "simpletest.rc-5g2p7" in namespace "gc-5124"
Jan 31 09:46:07.324: INFO: Deleting pod "simpletest.rc-5hb65" in namespace "gc-5124"
Jan 31 09:46:07.341: INFO: Deleting pod "simpletest.rc-5lkl2" in namespace "gc-5124"
Jan 31 09:46:07.352: INFO: Deleting pod "simpletest.rc-5nkmt" in namespace "gc-5124"
Jan 31 09:46:07.380: INFO: Deleting pod "simpletest.rc-5pp72" in namespace "gc-5124"
Jan 31 09:46:07.401: INFO: Deleting pod "simpletest.rc-5sp7s" in namespace "gc-5124"
Jan 31 09:46:07.416: INFO: Deleting pod "simpletest.rc-5t5bs" in namespace "gc-5124"
Jan 31 09:46:07.428: INFO: Deleting pod "simpletest.rc-5xg8j" in namespace "gc-5124"
Jan 31 09:46:07.444: INFO: Deleting pod "simpletest.rc-645g9" in namespace "gc-5124"
Jan 31 09:46:07.459: INFO: Deleting pod "simpletest.rc-688kb" in namespace "gc-5124"
Jan 31 09:46:07.472: INFO: Deleting pod "simpletest.rc-6rdpc" in namespace "gc-5124"
Jan 31 09:46:07.490: INFO: Deleting pod "simpletest.rc-6zfwz" in namespace "gc-5124"
Jan 31 09:46:07.505: INFO: Deleting pod "simpletest.rc-76fwz" in namespace "gc-5124"
Jan 31 09:46:07.522: INFO: Deleting pod "simpletest.rc-7bg5m" in namespace "gc-5124"
Jan 31 09:46:07.536: INFO: Deleting pod "simpletest.rc-7m4ls" in namespace "gc-5124"
Jan 31 09:46:07.561: INFO: Deleting pod "simpletest.rc-7qxqv" in namespace "gc-5124"
Jan 31 09:46:07.580: INFO: Deleting pod "simpletest.rc-854zv" in namespace "gc-5124"
Jan 31 09:46:07.591: INFO: Deleting pod "simpletest.rc-8bgvh" in namespace "gc-5124"
Jan 31 09:46:07.612: INFO: Deleting pod "simpletest.rc-8pn24" in namespace "gc-5124"
Jan 31 09:46:07.628: INFO: Deleting pod "simpletest.rc-94m26" in namespace "gc-5124"
Jan 31 09:46:07.641: INFO: Deleting pod "simpletest.rc-9hm9v" in namespace "gc-5124"
Jan 31 09:46:07.659: INFO: Deleting pod "simpletest.rc-9lthd" in namespace "gc-5124"
Jan 31 09:46:07.679: INFO: Deleting pod "simpletest.rc-9vmgr" in namespace "gc-5124"
Jan 31 09:46:07.691: INFO: Deleting pod "simpletest.rc-9wwrt" in namespace "gc-5124"
Jan 31 09:46:07.705: INFO: Deleting pod "simpletest.rc-bb85l" in namespace "gc-5124"
Jan 31 09:46:07.718: INFO: Deleting pod "simpletest.rc-bm8cf" in namespace "gc-5124"
Jan 31 09:46:07.744: INFO: Deleting pod "simpletest.rc-bxkx2" in namespace "gc-5124"
Jan 31 09:46:07.758: INFO: Deleting pod "simpletest.rc-cb5vs" in namespace "gc-5124"
Jan 31 09:46:07.775: INFO: Deleting pod "simpletest.rc-cg97s" in namespace "gc-5124"
Jan 31 09:46:07.786: INFO: Deleting pod "simpletest.rc-cnmpm" in namespace "gc-5124"
Jan 31 09:46:07.796: INFO: Deleting pod "simpletest.rc-cp9rn" in namespace "gc-5124"
Jan 31 09:46:07.814: INFO: Deleting pod "simpletest.rc-d6mlp" in namespace "gc-5124"
Jan 31 09:46:07.830: INFO: Deleting pod "simpletest.rc-dsrgn" in namespace "gc-5124"
Jan 31 09:46:07.840: INFO: Deleting pod "simpletest.rc-f4vvs" in namespace "gc-5124"
Jan 31 09:46:07.859: INFO: Deleting pod "simpletest.rc-f7tp9" in namespace "gc-5124"
Jan 31 09:46:07.877: INFO: Deleting pod "simpletest.rc-fv7p6" in namespace "gc-5124"
Jan 31 09:46:07.889: INFO: Deleting pod "simpletest.rc-fz8mn" in namespace "gc-5124"
Jan 31 09:46:07.899: INFO: Deleting pod "simpletest.rc-gj7jf" in namespace "gc-5124"
Jan 31 09:46:07.913: INFO: Deleting pod "simpletest.rc-h2zqx" in namespace "gc-5124"
Jan 31 09:46:07.939: INFO: Deleting pod "simpletest.rc-hzlg9" in namespace "gc-5124"
Jan 31 09:46:07.960: INFO: Deleting pod "simpletest.rc-jftss" in namespace "gc-5124"
Jan 31 09:46:07.973: INFO: Deleting pod "simpletest.rc-jgtmf" in namespace "gc-5124"
Jan 31 09:46:07.989: INFO: Deleting pod "simpletest.rc-jkbzn" in namespace "gc-5124"
Jan 31 09:46:07.998: INFO: Deleting pod "simpletest.rc-k267w" in namespace "gc-5124"
Jan 31 09:46:08.015: INFO: Deleting pod "simpletest.rc-kkh8g" in namespace "gc-5124"
Jan 31 09:46:08.042: INFO: Deleting pod "simpletest.rc-l88wj" in namespace "gc-5124"
Jan 31 09:46:08.117: INFO: Deleting pod "simpletest.rc-lc5cs" in namespace "gc-5124"
Jan 31 09:46:08.179: INFO: Deleting pod "simpletest.rc-ldhkc" in namespace "gc-5124"
Jan 31 09:46:08.233: INFO: Deleting pod "simpletest.rc-llzr2" in namespace "gc-5124"
Jan 31 09:46:08.273: INFO: Deleting pod "simpletest.rc-lsj69" in namespace "gc-5124"
Jan 31 09:46:08.298: INFO: Deleting pod "simpletest.rc-lx68s" in namespace "gc-5124"
Jan 31 09:46:08.322: INFO: Deleting pod "simpletest.rc-m5k64" in namespace "gc-5124"
Jan 31 09:46:08.361: INFO: Deleting pod "simpletest.rc-mjtmh" in namespace "gc-5124"
Jan 31 09:46:08.408: INFO: Deleting pod "simpletest.rc-mq24x" in namespace "gc-5124"
Jan 31 09:46:08.421: INFO: Deleting pod "simpletest.rc-mxxnz" in namespace "gc-5124"
Jan 31 09:46:08.432: INFO: Deleting pod "simpletest.rc-n6x6j" in namespace "gc-5124"
Jan 31 09:46:08.442: INFO: Deleting pod "simpletest.rc-nnzb8" in namespace "gc-5124"
Jan 31 09:46:08.457: INFO: Deleting pod "simpletest.rc-p4c4f" in namespace "gc-5124"
Jan 31 09:46:08.475: INFO: Deleting pod "simpletest.rc-p4vhb" in namespace "gc-5124"
Jan 31 09:46:08.506: INFO: Deleting pod "simpletest.rc-p54ds" in namespace "gc-5124"
Jan 31 09:46:08.522: INFO: Deleting pod "simpletest.rc-p8crm" in namespace "gc-5124"
Jan 31 09:46:08.534: INFO: Deleting pod "simpletest.rc-ph4pw" in namespace "gc-5124"
Jan 31 09:46:08.544: INFO: Deleting pod "simpletest.rc-phkdb" in namespace "gc-5124"
Jan 31 09:46:08.605: INFO: Deleting pod "simpletest.rc-plw7z" in namespace "gc-5124"
Jan 31 09:46:08.636: INFO: Deleting pod "simpletest.rc-pwnvr" in namespace "gc-5124"
Jan 31 09:46:08.648: INFO: Deleting pod "simpletest.rc-q9qtc" in namespace "gc-5124"
Jan 31 09:46:08.679: INFO: Deleting pod "simpletest.rc-qgkvd" in namespace "gc-5124"
Jan 31 09:46:08.694: INFO: Deleting pod "simpletest.rc-qqw5n" in namespace "gc-5124"
Jan 31 09:46:08.721: INFO: Deleting pod "simpletest.rc-qxqxc" in namespace "gc-5124"
Jan 31 09:46:08.732: INFO: Deleting pod "simpletest.rc-r4brz" in namespace "gc-5124"
Jan 31 09:46:08.754: INFO: Deleting pod "simpletest.rc-rsp25" in namespace "gc-5124"
Jan 31 09:46:08.767: INFO: Deleting pod "simpletest.rc-s7m7w" in namespace "gc-5124"
Jan 31 09:46:08.783: INFO: Deleting pod "simpletest.rc-sfcvl" in namespace "gc-5124"
Jan 31 09:46:08.830: INFO: Deleting pod "simpletest.rc-sl6cd" in namespace "gc-5124"
Jan 31 09:46:08.918: INFO: Deleting pod "simpletest.rc-t8wf2" in namespace "gc-5124"
Jan 31 09:46:08.935: INFO: Deleting pod "simpletest.rc-tkhvm" in namespace "gc-5124"
Jan 31 09:46:09.001: INFO: Deleting pod "simpletest.rc-tpk5s" in namespace "gc-5124"
Jan 31 09:46:09.025: INFO: Deleting pod "simpletest.rc-tqrkn" in namespace "gc-5124"
Jan 31 09:46:09.087: INFO: Deleting pod "simpletest.rc-tw64j" in namespace "gc-5124"
Jan 31 09:46:09.148: INFO: Deleting pod "simpletest.rc-v6s4n" in namespace "gc-5124"
Jan 31 09:46:09.182: INFO: Deleting pod "simpletest.rc-v8wtw" in namespace "gc-5124"
Jan 31 09:46:09.235: INFO: Deleting pod "simpletest.rc-vbb4t" in namespace "gc-5124"
Jan 31 09:46:09.300: INFO: Deleting pod "simpletest.rc-vn5sd" in namespace "gc-5124"
Jan 31 09:46:09.326: INFO: Deleting pod "simpletest.rc-vsl6d" in namespace "gc-5124"
Jan 31 09:46:09.386: INFO: Deleting pod "simpletest.rc-ww5k6" in namespace "gc-5124"
Jan 31 09:46:09.429: INFO: Deleting pod "simpletest.rc-xfmnn" in namespace "gc-5124"
Jan 31 09:46:09.484: INFO: Deleting pod "simpletest.rc-xjzcl" in namespace "gc-5124"
Jan 31 09:46:09.535: INFO: Deleting pod "simpletest.rc-zjlhb" in namespace "gc-5124"
Jan 31 09:46:09.578: INFO: Deleting pod "simpletest.rc-zphg5" in namespace "gc-5124"
Jan 31 09:46:09.637: INFO: Deleting pod "simpletest.rc-zwzf4" in namespace "gc-5124"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 31 09:46:09.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5124" for this suite.

• [SLOW TEST:42.981 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":356,"completed":28,"skipped":365,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:46:09.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5298
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-d74e9359-7660-4413-b575-602dc538756f
STEP: Creating a pod to test consume configMaps
Jan 31 09:46:09.929: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-13df2238-9f9f-4ebe-9d39-d5d0711d42b5" in namespace "projected-5298" to be "Succeeded or Failed"
Jan 31 09:46:09.939: INFO: Pod "pod-projected-configmaps-13df2238-9f9f-4ebe-9d39-d5d0711d42b5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.73052ms
Jan 31 09:46:11.946: INFO: Pod "pod-projected-configmaps-13df2238-9f9f-4ebe-9d39-d5d0711d42b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016517591s
Jan 31 09:46:13.958: INFO: Pod "pod-projected-configmaps-13df2238-9f9f-4ebe-9d39-d5d0711d42b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028556825s
Jan 31 09:46:15.966: INFO: Pod "pod-projected-configmaps-13df2238-9f9f-4ebe-9d39-d5d0711d42b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036670966s
STEP: Saw pod success
Jan 31 09:46:15.966: INFO: Pod "pod-projected-configmaps-13df2238-9f9f-4ebe-9d39-d5d0711d42b5" satisfied condition "Succeeded or Failed"
Jan 31 09:46:15.969: INFO: Trying to get logs from node macpro-x86-2 pod pod-projected-configmaps-13df2238-9f9f-4ebe-9d39-d5d0711d42b5 container agnhost-container: <nil>
STEP: delete the pod
Jan 31 09:46:15.991: INFO: Waiting for pod pod-projected-configmaps-13df2238-9f9f-4ebe-9d39-d5d0711d42b5 to disappear
Jan 31 09:46:15.994: INFO: Pod pod-projected-configmaps-13df2238-9f9f-4ebe-9d39-d5d0711d42b5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 31 09:46:15.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5298" for this suite.

• [SLOW TEST:6.230 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":29,"skipped":376,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:46:16.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6742
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 09:46:16.161: INFO: created pod pod-service-account-defaultsa
Jan 31 09:46:16.161: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 31 09:46:16.175: INFO: created pod pod-service-account-mountsa
Jan 31 09:46:16.175: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 31 09:46:16.188: INFO: created pod pod-service-account-nomountsa
Jan 31 09:46:16.188: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 31 09:46:16.229: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 31 09:46:16.229: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 31 09:46:16.240: INFO: created pod pod-service-account-mountsa-mountspec
Jan 31 09:46:16.240: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 31 09:46:16.250: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 31 09:46:16.250: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 31 09:46:16.258: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 31 09:46:16.258: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 31 09:46:16.268: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 31 09:46:16.268: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 31 09:46:16.281: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 31 09:46:16.281: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 31 09:46:16.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6742" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":356,"completed":30,"skipped":401,"failed":0}
SSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:46:16.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-0cacf504-4b9c-4a9f-acd8-ea3967cd212f
STEP: Creating a pod to test consume secrets
Jan 31 09:46:16.487: INFO: Waiting up to 5m0s for pod "pod-secrets-fe626fa4-68be-4581-9634-4e2587a1e4cd" in namespace "secrets-1" to be "Succeeded or Failed"
Jan 31 09:46:16.504: INFO: Pod "pod-secrets-fe626fa4-68be-4581-9634-4e2587a1e4cd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.917664ms
Jan 31 09:46:18.510: INFO: Pod "pod-secrets-fe626fa4-68be-4581-9634-4e2587a1e4cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022615869s
Jan 31 09:46:20.516: INFO: Pod "pod-secrets-fe626fa4-68be-4581-9634-4e2587a1e4cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028846655s
Jan 31 09:46:22.524: INFO: Pod "pod-secrets-fe626fa4-68be-4581-9634-4e2587a1e4cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036820745s
STEP: Saw pod success
Jan 31 09:46:22.524: INFO: Pod "pod-secrets-fe626fa4-68be-4581-9634-4e2587a1e4cd" satisfied condition "Succeeded or Failed"
Jan 31 09:46:22.526: INFO: Trying to get logs from node macpro-x86-2 pod pod-secrets-fe626fa4-68be-4581-9634-4e2587a1e4cd container secret-env-test: <nil>
STEP: delete the pod
Jan 31 09:46:22.542: INFO: Waiting for pod pod-secrets-fe626fa4-68be-4581-9634-4e2587a1e4cd to disappear
Jan 31 09:46:22.544: INFO: Pod pod-secrets-fe626fa4-68be-4581-9634-4e2587a1e4cd no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 31 09:46:22.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1" for this suite.

• [SLOW TEST:6.255 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":356,"completed":31,"skipped":408,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:46:22.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-134
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a collection of services
Jan 31 09:46:22.679: INFO: Creating e2e-svc-a-tfcg8
Jan 31 09:46:22.695: INFO: Creating e2e-svc-b-jxc9f
Jan 31 09:46:22.710: INFO: Creating e2e-svc-c-876fj
STEP: deleting service collection
Jan 31 09:46:22.814: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 09:46:22.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-134" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":356,"completed":32,"skipped":421,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:46:22.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2153
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-a9a788ed-4a18-49bd-b925-6c8d546e7ce7
STEP: Creating a pod to test consume configMaps
Jan 31 09:46:22.961: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e969bae4-4eec-4960-a29b-edbb9eb2ba64" in namespace "projected-2153" to be "Succeeded or Failed"
Jan 31 09:46:22.964: INFO: Pod "pod-projected-configmaps-e969bae4-4eec-4960-a29b-edbb9eb2ba64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.787692ms
Jan 31 09:46:24.971: INFO: Pod "pod-projected-configmaps-e969bae4-4eec-4960-a29b-edbb9eb2ba64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010279554s
Jan 31 09:46:26.978: INFO: Pod "pod-projected-configmaps-e969bae4-4eec-4960-a29b-edbb9eb2ba64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016947494s
STEP: Saw pod success
Jan 31 09:46:26.978: INFO: Pod "pod-projected-configmaps-e969bae4-4eec-4960-a29b-edbb9eb2ba64" satisfied condition "Succeeded or Failed"
Jan 31 09:46:26.981: INFO: Trying to get logs from node macpro-x86-1 pod pod-projected-configmaps-e969bae4-4eec-4960-a29b-edbb9eb2ba64 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 31 09:46:27.003: INFO: Waiting for pod pod-projected-configmaps-e969bae4-4eec-4960-a29b-edbb9eb2ba64 to disappear
Jan 31 09:46:27.006: INFO: Pod pod-projected-configmaps-e969bae4-4eec-4960-a29b-edbb9eb2ba64 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 31 09:46:27.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2153" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":33,"skipped":424,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:46:27.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5834
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 31 09:46:27.151: INFO: Waiting up to 5m0s for pod "pod-2e612cb1-58f6-4cda-94e3-a724e35e0109" in namespace "emptydir-5834" to be "Succeeded or Failed"
Jan 31 09:46:27.154: INFO: Pod "pod-2e612cb1-58f6-4cda-94e3-a724e35e0109": Phase="Pending", Reason="", readiness=false. Elapsed: 2.932922ms
Jan 31 09:46:29.160: INFO: Pod "pod-2e612cb1-58f6-4cda-94e3-a724e35e0109": Phase="Running", Reason="", readiness=false. Elapsed: 2.00935605s
Jan 31 09:46:31.168: INFO: Pod "pod-2e612cb1-58f6-4cda-94e3-a724e35e0109": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017339813s
STEP: Saw pod success
Jan 31 09:46:31.168: INFO: Pod "pod-2e612cb1-58f6-4cda-94e3-a724e35e0109" satisfied condition "Succeeded or Failed"
Jan 31 09:46:31.171: INFO: Trying to get logs from node macpro-x86-2 pod pod-2e612cb1-58f6-4cda-94e3-a724e35e0109 container test-container: <nil>
STEP: delete the pod
Jan 31 09:46:31.190: INFO: Waiting for pod pod-2e612cb1-58f6-4cda-94e3-a724e35e0109 to disappear
Jan 31 09:46:31.193: INFO: Pod pod-2e612cb1-58f6-4cda-94e3-a724e35e0109 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 31 09:46:31.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5834" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":34,"skipped":438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:46:31.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8788
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override arguments
Jan 31 09:46:31.337: INFO: Waiting up to 5m0s for pod "client-containers-f4d555a7-5fec-4bc5-99d1-ea7349f569fe" in namespace "containers-8788" to be "Succeeded or Failed"
Jan 31 09:46:31.339: INFO: Pod "client-containers-f4d555a7-5fec-4bc5-99d1-ea7349f569fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.477615ms
Jan 31 09:46:33.345: INFO: Pod "client-containers-f4d555a7-5fec-4bc5-99d1-ea7349f569fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008673671s
Jan 31 09:46:35.357: INFO: Pod "client-containers-f4d555a7-5fec-4bc5-99d1-ea7349f569fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020216336s
STEP: Saw pod success
Jan 31 09:46:35.357: INFO: Pod "client-containers-f4d555a7-5fec-4bc5-99d1-ea7349f569fe" satisfied condition "Succeeded or Failed"
Jan 31 09:46:35.364: INFO: Trying to get logs from node macpro-x86-2 pod client-containers-f4d555a7-5fec-4bc5-99d1-ea7349f569fe container agnhost-container: <nil>
STEP: delete the pod
Jan 31 09:46:35.403: INFO: Waiting for pod client-containers-f4d555a7-5fec-4bc5-99d1-ea7349f569fe to disappear
Jan 31 09:46:35.407: INFO: Pod client-containers-f4d555a7-5fec-4bc5-99d1-ea7349f569fe no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 31 09:46:35.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8788" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","total":356,"completed":35,"skipped":465,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:46:35.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-993
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Jan 31 09:46:35.580: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 31 09:46:40.590: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Jan 31 09:46:40.594: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 31 09:46:40.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-993" for this suite.

• [SLOW TEST:5.203 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":356,"completed":36,"skipped":476,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:46:40.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2658
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 09:46:40.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 31 09:46:40.789: INFO: The status of Pod pod-logs-websocket-7b7f0463-f781-4e2a-9f50-492f9141991f is Pending, waiting for it to be Running (with Ready = true)
Jan 31 09:46:42.794: INFO: The status of Pod pod-logs-websocket-7b7f0463-f781-4e2a-9f50-492f9141991f is Pending, waiting for it to be Running (with Ready = true)
Jan 31 09:46:44.794: INFO: The status of Pod pod-logs-websocket-7b7f0463-f781-4e2a-9f50-492f9141991f is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 31 09:46:44.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2658" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":356,"completed":37,"skipped":503,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:46:44.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1637
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jan 31 09:46:44.963: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 31 09:46:46.970: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jan 31 09:46:46.985: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 31 09:46:48.992: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 31 09:46:48.995: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1637 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:46:48.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:46:48.995: INFO: ExecWithOptions: Clientset creation
Jan 31 09:46:48.995: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1637/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 31 09:46:49.053: INFO: Exec stderr: ""
Jan 31 09:46:49.053: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1637 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:46:49.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:46:49.054: INFO: ExecWithOptions: Clientset creation
Jan 31 09:46:49.054: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1637/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 31 09:46:49.101: INFO: Exec stderr: ""
Jan 31 09:46:49.101: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1637 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:46:49.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:46:49.102: INFO: ExecWithOptions: Clientset creation
Jan 31 09:46:49.102: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1637/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 31 09:46:49.146: INFO: Exec stderr: ""
Jan 31 09:46:49.146: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1637 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:46:49.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:46:49.146: INFO: ExecWithOptions: Clientset creation
Jan 31 09:46:49.146: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1637/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 31 09:46:49.190: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 31 09:46:49.190: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1637 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:46:49.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:46:49.191: INFO: ExecWithOptions: Clientset creation
Jan 31 09:46:49.191: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1637/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 31 09:46:49.239: INFO: Exec stderr: ""
Jan 31 09:46:49.239: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1637 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:46:49.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:46:49.239: INFO: ExecWithOptions: Clientset creation
Jan 31 09:46:49.239: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1637/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 31 09:46:49.280: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 31 09:46:49.280: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1637 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:46:49.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:46:49.281: INFO: ExecWithOptions: Clientset creation
Jan 31 09:46:49.281: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1637/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 31 09:46:49.335: INFO: Exec stderr: ""
Jan 31 09:46:49.335: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1637 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:46:49.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:46:49.336: INFO: ExecWithOptions: Clientset creation
Jan 31 09:46:49.336: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1637/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 31 09:46:49.381: INFO: Exec stderr: ""
Jan 31 09:46:49.381: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1637 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:46:49.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:46:49.381: INFO: ExecWithOptions: Clientset creation
Jan 31 09:46:49.381: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1637/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 31 09:46:49.426: INFO: Exec stderr: ""
Jan 31 09:46:49.426: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1637 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:46:49.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:46:49.427: INFO: ExecWithOptions: Clientset creation
Jan 31 09:46:49.427: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1637/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 31 09:46:49.469: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:188
Jan 31 09:46:49.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1637" for this suite.
•{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":38,"skipped":528,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:46:49.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6735
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 09:46:49.630: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 31 09:46:54.650: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 31 09:46:54.651: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 31 09:46:56.659: INFO: Creating deployment "test-rollover-deployment"
Jan 31 09:46:56.669: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 31 09:46:58.687: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 31 09:46:58.693: INFO: Ensure that both replica sets have 1 created replica
Jan 31 09:46:58.698: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 31 09:46:58.708: INFO: Updating deployment test-rollover-deployment
Jan 31 09:46:58.708: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 31 09:47:00.720: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 31 09:47:00.726: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 31 09:47:00.731: INFO: all replica sets need to contain the pod-template-hash label
Jan 31 09:47:00.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 9, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 9, 46, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 9, 46, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 9, 46, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 31 09:47:02.739: INFO: all replica sets need to contain the pod-template-hash label
Jan 31 09:47:02.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 9, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 9, 46, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 9, 46, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 9, 46, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 31 09:47:04.742: INFO: all replica sets need to contain the pod-template-hash label
Jan 31 09:47:04.742: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 9, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 9, 46, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 9, 46, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 9, 46, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 31 09:47:06.744: INFO: all replica sets need to contain the pod-template-hash label
Jan 31 09:47:06.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 9, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 9, 46, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 9, 46, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 9, 46, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 31 09:47:08.743: INFO: all replica sets need to contain the pod-template-hash label
Jan 31 09:47:08.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 9, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 9, 46, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 9, 46, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 9, 46, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 31 09:47:10.740: INFO: 
Jan 31 09:47:10.740: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 31 09:47:10.754: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6735  1047536f-8060-4ebc-8223-9d51e2cf6b5a 9615 2 2023-01-31 09:46:56 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-01-31 09:46:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 09:47:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00434cde8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-31 09:46:56 +0000 UTC,LastTransitionTime:2023-01-31 09:46:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-779c67f4f8" has successfully progressed.,LastUpdateTime:2023-01-31 09:47:09 +0000 UTC,LastTransitionTime:2023-01-31 09:46:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 31 09:47:10.757: INFO: New ReplicaSet "test-rollover-deployment-779c67f4f8" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-779c67f4f8  deployment-6735  9056d558-0e36-4463-a948-f50ba91983c7 9605 2 2023-01-31 09:46:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 1047536f-8060-4ebc-8223-9d51e2cf6b5a 0xc004972947 0xc004972948}] []  [{kube-controller-manager Update apps/v1 2023-01-31 09:46:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1047536f-8060-4ebc-8223-9d51e2cf6b5a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 09:47:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 779c67f4f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049729f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 31 09:47:10.757: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 31 09:47:10.757: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6735  41a6afb4-b82f-429c-8dd3-ff567418255e 9614 2 2023-01-31 09:46:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 1047536f-8060-4ebc-8223-9d51e2cf6b5a 0xc00497281f 0xc004972830}] []  [{e2e.test Update apps/v1 2023-01-31 09:46:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 09:47:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1047536f-8060-4ebc-8223-9d51e2cf6b5a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-31 09:47:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0049728e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 31 09:47:10.757: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-87f8f6dcf  deployment-6735  8ebd03b0-50ce-452f-8757-b195a3fdd692 9563 2 2023-01-31 09:46:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 1047536f-8060-4ebc-8223-9d51e2cf6b5a 0xc004972a50 0xc004972a51}] []  [{kube-controller-manager Update apps/v1 2023-01-31 09:46:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1047536f-8060-4ebc-8223-9d51e2cf6b5a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 09:46:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 87f8f6dcf,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004972af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 31 09:47:10.759: INFO: Pod "test-rollover-deployment-779c67f4f8-lxbq2" is available:
&Pod{ObjectMeta:{test-rollover-deployment-779c67f4f8-lxbq2 test-rollover-deployment-779c67f4f8- deployment-6735  5aaf7798-74d5-41d6-ae13-fbe4b38a3333 9576 0 2023-01-31 09:46:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[cni.projectcalico.org/containerID:09ab10b7ce806120dc149d1eb6e98d048c07690576e79016ce57c564c4f2c486 cni.projectcalico.org/podIP:192.168.75.17/32 cni.projectcalico.org/podIPs:192.168.75.17/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-779c67f4f8 9056d558-0e36-4463-a948-f50ba91983c7 0xc00434d177 0xc00434d178}] []  [{kube-controller-manager Update v1 2023-01-31 09:46:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9056d558-0e36-4463-a948-f50ba91983c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 09:46:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 09:46:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.75.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ntr5s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ntr5s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 09:46:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 09:46:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 09:46:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 09:46:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.31,PodIP:192.168.75.17,StartTime:2023-01-31 09:46:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 09:46:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://7659c1b03aa26f920d4a83012fd5b956aa50393fad067cde4f142e8957302915,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.75.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 31 09:47:10.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6735" for this suite.

• [SLOW TEST:21.282 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":356,"completed":39,"skipped":536,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:47:10.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5338
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 31 09:47:10.904: INFO: Waiting up to 5m0s for pod "pod-e7f33da8-387b-4a78-a219-facb847eaf96" in namespace "emptydir-5338" to be "Succeeded or Failed"
Jan 31 09:47:10.907: INFO: Pod "pod-e7f33da8-387b-4a78-a219-facb847eaf96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.916497ms
Jan 31 09:47:12.915: INFO: Pod "pod-e7f33da8-387b-4a78-a219-facb847eaf96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011346589s
Jan 31 09:47:14.923: INFO: Pod "pod-e7f33da8-387b-4a78-a219-facb847eaf96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019448651s
STEP: Saw pod success
Jan 31 09:47:14.923: INFO: Pod "pod-e7f33da8-387b-4a78-a219-facb847eaf96" satisfied condition "Succeeded or Failed"
Jan 31 09:47:14.926: INFO: Trying to get logs from node macpro-x86-2 pod pod-e7f33da8-387b-4a78-a219-facb847eaf96 container test-container: <nil>
STEP: delete the pod
Jan 31 09:47:14.956: INFO: Waiting for pod pod-e7f33da8-387b-4a78-a219-facb847eaf96 to disappear
Jan 31 09:47:14.961: INFO: Pod pod-e7f33da8-387b-4a78-a219-facb847eaf96 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 31 09:47:14.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5338" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":40,"skipped":542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:47:14.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-849
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jan 31 09:47:16.183: INFO: The status of Pod kube-controller-manager-master-3 is Running (Ready = true)
Jan 31 09:47:16.229: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 31 09:47:16.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-849" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":356,"completed":41,"skipped":607,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:47:16.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6342
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: reading a file in the container
Jan 31 09:47:18.396: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6342 pod-service-account-424ed0ad-5c4a-4667-a5b4-550f147d692d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan 31 09:47:18.498: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6342 pod-service-account-424ed0ad-5c4a-4667-a5b4-550f147d692d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan 31 09:47:18.589: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6342 pod-service-account-424ed0ad-5c4a-4667-a5b4-550f147d692d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 31 09:47:18.682: INFO: Got root ca configmap in namespace "svcaccounts-6342"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 31 09:47:18.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6342" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":356,"completed":42,"skipped":649,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:47:18.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1521
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 09:47:18.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 09:47:19.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1521" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":356,"completed":43,"skipped":650,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:47:19.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1718
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jan 31 09:47:19.702: INFO: observed Pod pod-test in namespace pods-1718 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 31 09:47:19.723: INFO: observed Pod pod-test in namespace pods-1718 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:19 +0000 UTC  }]
Jan 31 09:47:19.745: INFO: observed Pod pod-test in namespace pods-1718 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:19 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:19 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:19 +0000 UTC  }]
Jan 31 09:47:20.279: INFO: observed Pod pod-test in namespace pods-1718 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:19 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:19 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:19 +0000 UTC  }]
Jan 31 09:47:20.954: INFO: Found Pod pod-test in namespace pods-1718 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:19 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jan 31 09:47:20.988: INFO: observed event type MODIFIED
Jan 31 09:47:22.932: INFO: observed event type MODIFIED
Jan 31 09:47:23.054: INFO: observed event type MODIFIED
Jan 31 09:47:23.930: INFO: observed event type MODIFIED
Jan 31 09:47:23.940: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 31 09:47:23.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1718" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":356,"completed":44,"skipped":653,"failed":0}
SS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:47:23.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5058
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 09:47:24.089: INFO: Creating pod...
Jan 31 09:47:26.107: INFO: Creating service...
Jan 31 09:47:26.126: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5058/pods/agnhost/proxy/some/path/with/DELETE
Jan 31 09:47:26.130: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 31 09:47:26.130: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5058/pods/agnhost/proxy/some/path/with/GET
Jan 31 09:47:26.140: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 31 09:47:26.140: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5058/pods/agnhost/proxy/some/path/with/HEAD
Jan 31 09:47:26.145: INFO: http.Client request:HEAD | StatusCode:200
Jan 31 09:47:26.145: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5058/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 31 09:47:26.150: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 31 09:47:26.150: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5058/pods/agnhost/proxy/some/path/with/PATCH
Jan 31 09:47:26.154: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 31 09:47:26.154: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5058/pods/agnhost/proxy/some/path/with/POST
Jan 31 09:47:26.160: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 31 09:47:26.160: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5058/pods/agnhost/proxy/some/path/with/PUT
Jan 31 09:47:26.165: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 31 09:47:26.165: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5058/services/test-service/proxy/some/path/with/DELETE
Jan 31 09:47:26.169: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 31 09:47:26.169: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5058/services/test-service/proxy/some/path/with/GET
Jan 31 09:47:26.179: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 31 09:47:26.179: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5058/services/test-service/proxy/some/path/with/HEAD
Jan 31 09:47:26.183: INFO: http.Client request:HEAD | StatusCode:200
Jan 31 09:47:26.183: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5058/services/test-service/proxy/some/path/with/OPTIONS
Jan 31 09:47:26.191: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 31 09:47:26.191: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5058/services/test-service/proxy/some/path/with/PATCH
Jan 31 09:47:26.197: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 31 09:47:26.197: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5058/services/test-service/proxy/some/path/with/POST
Jan 31 09:47:26.201: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 31 09:47:26.201: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5058/services/test-service/proxy/some/path/with/PUT
Jan 31 09:47:26.208: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jan 31 09:47:26.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5058" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":356,"completed":45,"skipped":655,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:47:26.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5752
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pods
Jan 31 09:47:26.376: INFO: created test-pod-1
Jan 31 09:47:26.383: INFO: created test-pod-2
Jan 31 09:47:26.404: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running
Jan 31 09:47:26.404: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5752' to be running and ready
Jan 31 09:47:26.419: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 31 09:47:26.419: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 31 09:47:26.419: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 31 09:47:26.419: INFO: 0 / 3 pods in namespace 'pods-5752' are running and ready (0 seconds elapsed)
Jan 31 09:47:26.419: INFO: expected 0 pod replicas in namespace 'pods-5752', 0 are Running and Ready.
Jan 31 09:47:26.419: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
Jan 31 09:47:26.419: INFO: test-pod-1  macpro-x86-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:26 +0000 UTC  }]
Jan 31 09:47:26.419: INFO: test-pod-2  macpro-x86-2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:26 +0000 UTC  }]
Jan 31 09:47:26.419: INFO: test-pod-3  macpro-x86-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 09:47:26 +0000 UTC  }]
Jan 31 09:47:26.419: INFO: 
Jan 31 09:47:28.431: INFO: 3 / 3 pods in namespace 'pods-5752' are running and ready (2 seconds elapsed)
Jan 31 09:47:28.431: INFO: expected 0 pod replicas in namespace 'pods-5752', 0 are Running and Ready.
STEP: waiting for all pods to be deleted
Jan 31 09:47:28.456: INFO: Pod quantity 3 is different from expected quantity 0
Jan 31 09:47:29.465: INFO: Pod quantity 3 is different from expected quantity 0
Jan 31 09:47:30.461: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 31 09:47:31.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5752" for this suite.

• [SLOW TEST:5.256 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":356,"completed":46,"skipped":659,"failed":0}
SSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:47:31.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2462
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 09:47:31.635: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-687909aa-478b-4329-9e13-733d2a47d797" in namespace "security-context-test-2462" to be "Succeeded or Failed"
Jan 31 09:47:31.640: INFO: Pod "busybox-privileged-false-687909aa-478b-4329-9e13-733d2a47d797": Phase="Pending", Reason="", readiness=false. Elapsed: 5.11376ms
Jan 31 09:47:33.649: INFO: Pod "busybox-privileged-false-687909aa-478b-4329-9e13-733d2a47d797": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0132439s
Jan 31 09:47:35.655: INFO: Pod "busybox-privileged-false-687909aa-478b-4329-9e13-733d2a47d797": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019897417s
Jan 31 09:47:35.655: INFO: Pod "busybox-privileged-false-687909aa-478b-4329-9e13-733d2a47d797" satisfied condition "Succeeded or Failed"
Jan 31 09:47:35.664: INFO: Got logs for pod "busybox-privileged-false-687909aa-478b-4329-9e13-733d2a47d797": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 31 09:47:35.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2462" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":47,"skipped":663,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:47:35.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9899
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 31 09:47:35.810: INFO: Waiting up to 5m0s for pod "pod-8687ff68-8e9c-45a7-807f-cbba40145bad" in namespace "emptydir-9899" to be "Succeeded or Failed"
Jan 31 09:47:35.817: INFO: Pod "pod-8687ff68-8e9c-45a7-807f-cbba40145bad": Phase="Pending", Reason="", readiness=false. Elapsed: 7.193396ms
Jan 31 09:47:37.830: INFO: Pod "pod-8687ff68-8e9c-45a7-807f-cbba40145bad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019798686s
Jan 31 09:47:39.836: INFO: Pod "pod-8687ff68-8e9c-45a7-807f-cbba40145bad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026085381s
STEP: Saw pod success
Jan 31 09:47:39.836: INFO: Pod "pod-8687ff68-8e9c-45a7-807f-cbba40145bad" satisfied condition "Succeeded or Failed"
Jan 31 09:47:39.840: INFO: Trying to get logs from node macpro-x86-2 pod pod-8687ff68-8e9c-45a7-807f-cbba40145bad container test-container: <nil>
STEP: delete the pod
Jan 31 09:47:39.856: INFO: Waiting for pod pod-8687ff68-8e9c-45a7-807f-cbba40145bad to disappear
Jan 31 09:47:39.859: INFO: Pod pod-8687ff68-8e9c-45a7-807f-cbba40145bad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 31 09:47:39.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9899" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":48,"skipped":682,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:47:39.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8546
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-3e1a21e4-f8fb-4fc4-8132-a9c2ac39c6b3
STEP: Creating a pod to test consume secrets
Jan 31 09:47:40.015: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e8b910dd-3fe5-4c5d-b8dd-55eede001140" in namespace "projected-8546" to be "Succeeded or Failed"
Jan 31 09:47:40.019: INFO: Pod "pod-projected-secrets-e8b910dd-3fe5-4c5d-b8dd-55eede001140": Phase="Pending", Reason="", readiness=false. Elapsed: 4.577756ms
Jan 31 09:47:42.023: INFO: Pod "pod-projected-secrets-e8b910dd-3fe5-4c5d-b8dd-55eede001140": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0082371s
Jan 31 09:47:44.027: INFO: Pod "pod-projected-secrets-e8b910dd-3fe5-4c5d-b8dd-55eede001140": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012733981s
STEP: Saw pod success
Jan 31 09:47:44.027: INFO: Pod "pod-projected-secrets-e8b910dd-3fe5-4c5d-b8dd-55eede001140" satisfied condition "Succeeded or Failed"
Jan 31 09:47:44.034: INFO: Trying to get logs from node macpro-x86-2 pod pod-projected-secrets-e8b910dd-3fe5-4c5d-b8dd-55eede001140 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 31 09:47:44.052: INFO: Waiting for pod pod-projected-secrets-e8b910dd-3fe5-4c5d-b8dd-55eede001140 to disappear
Jan 31 09:47:44.055: INFO: Pod pod-projected-secrets-e8b910dd-3fe5-4c5d-b8dd-55eede001140 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 31 09:47:44.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8546" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":49,"skipped":691,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:47:44.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3360
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption is created
Jan 31 09:47:44.205: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 31 09:47:46.213: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 31 09:47:47.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3360" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":356,"completed":50,"skipped":709,"failed":0}
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:47:47.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-819
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 31 09:48:15.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-819" for this suite.

• [SLOW TEST:28.403 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":356,"completed":51,"skipped":716,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:48:15.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-8708
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jan 31 09:48:35.928: INFO: EndpointSlice for Service endpointslice-8708/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 31 09:48:45.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8708" for this suite.

• [SLOW TEST:30.304 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":356,"completed":52,"skipped":793,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:48:45.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-3014
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:48:46.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename disruption-2
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2-1325
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-3014
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:188
Jan 31 09:48:52.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-1325" for this suite.
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 31 09:48:52.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3014" for this suite.

• [SLOW TEST:6.359 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":356,"completed":53,"skipped":815,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:48:52.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8607
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Jan 31 09:48:52.458: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 31 09:48:57.464: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Jan 31 09:48:57.467: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Jan 31 09:48:57.482: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Jan 31 09:48:57.484: INFO: Observed &ReplicaSet event: ADDED
Jan 31 09:48:57.484: INFO: Observed &ReplicaSet event: MODIFIED
Jan 31 09:48:57.484: INFO: Observed &ReplicaSet event: MODIFIED
Jan 31 09:48:57.484: INFO: Observed &ReplicaSet event: MODIFIED
Jan 31 09:48:57.484: INFO: Found replicaset test-rs in namespace replicaset-8607 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 31 09:48:57.484: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Jan 31 09:48:57.484: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 31 09:48:57.490: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Jan 31 09:48:57.492: INFO: Observed &ReplicaSet event: ADDED
Jan 31 09:48:57.492: INFO: Observed &ReplicaSet event: MODIFIED
Jan 31 09:48:57.492: INFO: Observed &ReplicaSet event: MODIFIED
Jan 31 09:48:57.492: INFO: Observed &ReplicaSet event: MODIFIED
Jan 31 09:48:57.492: INFO: Observed replicaset test-rs in namespace replicaset-8607 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 31 09:48:57.492: INFO: Observed &ReplicaSet event: MODIFIED
Jan 31 09:48:57.492: INFO: Found replicaset test-rs in namespace replicaset-8607 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 31 09:48:57.492: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 31 09:48:57.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8607" for this suite.

• [SLOW TEST:5.190 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":356,"completed":54,"skipped":818,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:48:57.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5476
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 31 09:48:57.638: INFO: Waiting up to 5m0s for pod "pod-4e53b126-0d9e-402d-b02e-1510a21faf3c" in namespace "emptydir-5476" to be "Succeeded or Failed"
Jan 31 09:48:57.642: INFO: Pod "pod-4e53b126-0d9e-402d-b02e-1510a21faf3c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.897947ms
Jan 31 09:48:59.649: INFO: Pod "pod-4e53b126-0d9e-402d-b02e-1510a21faf3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010940556s
Jan 31 09:49:01.656: INFO: Pod "pod-4e53b126-0d9e-402d-b02e-1510a21faf3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017233854s
STEP: Saw pod success
Jan 31 09:49:01.656: INFO: Pod "pod-4e53b126-0d9e-402d-b02e-1510a21faf3c" satisfied condition "Succeeded or Failed"
Jan 31 09:49:01.658: INFO: Trying to get logs from node macpro-x86-1 pod pod-4e53b126-0d9e-402d-b02e-1510a21faf3c container test-container: <nil>
STEP: delete the pod
Jan 31 09:49:01.677: INFO: Waiting for pod pod-4e53b126-0d9e-402d-b02e-1510a21faf3c to disappear
Jan 31 09:49:01.684: INFO: Pod pod-4e53b126-0d9e-402d-b02e-1510a21faf3c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 31 09:49:01.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5476" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":55,"skipped":818,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:49:01.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4852
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-4852
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 31 09:49:01.818: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 31 09:49:01.849: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 09:49:03.855: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 09:49:05.859: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 09:49:07.857: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 09:49:09.859: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 09:49:11.855: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 09:49:13.856: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 31 09:49:13.861: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 31 09:49:15.885: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 31 09:49:15.885: INFO: Breadth first check of 192.168.75.39 on host 10.221.188.31...
Jan 31 09:49:15.887: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.75.29:9080/dial?request=hostname&protocol=http&host=192.168.75.39&port=8083&tries=1'] Namespace:pod-network-test-4852 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:49:15.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:49:15.888: INFO: ExecWithOptions: Clientset creation
Jan 31 09:49:15.888: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4852/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.75.29%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.75.39%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 31 09:49:15.946: INFO: Waiting for responses: map[]
Jan 31 09:49:15.946: INFO: reached 192.168.75.39 after 0/1 tries
Jan 31 09:49:15.946: INFO: Breadth first check of 192.168.38.249 on host 10.221.188.32...
Jan 31 09:49:15.949: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.75.29:9080/dial?request=hostname&protocol=http&host=192.168.38.249&port=8083&tries=1'] Namespace:pod-network-test-4852 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:49:15.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:49:15.950: INFO: ExecWithOptions: Clientset creation
Jan 31 09:49:15.950: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4852/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.75.29%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.38.249%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 31 09:49:15.991: INFO: Waiting for responses: map[]
Jan 31 09:49:15.991: INFO: reached 192.168.38.249 after 0/1 tries
Jan 31 09:49:15.991: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 31 09:49:15.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4852" for this suite.

• [SLOW TEST:14.308 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":356,"completed":56,"skipped":829,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:49:16.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9056
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-1362a525-d5ff-4bf6-a30b-f9bf62432448
STEP: Creating a pod to test consume configMaps
Jan 31 09:49:16.146: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6955d6f0-2294-48bb-a5d2-6666a349f6bb" in namespace "projected-9056" to be "Succeeded or Failed"
Jan 31 09:49:16.153: INFO: Pod "pod-projected-configmaps-6955d6f0-2294-48bb-a5d2-6666a349f6bb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.169537ms
Jan 31 09:49:18.162: INFO: Pod "pod-projected-configmaps-6955d6f0-2294-48bb-a5d2-6666a349f6bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015521454s
Jan 31 09:49:20.167: INFO: Pod "pod-projected-configmaps-6955d6f0-2294-48bb-a5d2-6666a349f6bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020840464s
STEP: Saw pod success
Jan 31 09:49:20.167: INFO: Pod "pod-projected-configmaps-6955d6f0-2294-48bb-a5d2-6666a349f6bb" satisfied condition "Succeeded or Failed"
Jan 31 09:49:20.170: INFO: Trying to get logs from node macpro-x86-1 pod pod-projected-configmaps-6955d6f0-2294-48bb-a5d2-6666a349f6bb container agnhost-container: <nil>
STEP: delete the pod
Jan 31 09:49:20.187: INFO: Waiting for pod pod-projected-configmaps-6955d6f0-2294-48bb-a5d2-6666a349f6bb to disappear
Jan 31 09:49:20.190: INFO: Pod pod-projected-configmaps-6955d6f0-2294-48bb-a5d2-6666a349f6bb no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 31 09:49:20.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9056" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":57,"skipped":830,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:49:20.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1621
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 09:49:20.341: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 31 09:49:25.350: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 31 09:49:25.350: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 31 09:49:25.388: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1621  19427ff8-dd37-4982-9716-cb95895689b3 10820 1 2023-01-31 09:49:25 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2023-01-31 09:49:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004481858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 31 09:49:25.395: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 31 09:49:25.395: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 31 09:49:25.395: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1621  28831ac2-70f9-49e0-a222-608659406cdb 10821 1 2023-01-31 09:49:20 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 19427ff8-dd37-4982-9716-cb95895689b3 0xc004481bb7 0xc004481bb8}] []  [{e2e.test Update apps/v1 2023-01-31 09:49:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 09:49:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-31 09:49:25 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"19427ff8-dd37-4982-9716-cb95895689b3\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004481c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 31 09:49:25.405: INFO: Pod "test-cleanup-controller-mgk6d" is available:
&Pod{ObjectMeta:{test-cleanup-controller-mgk6d test-cleanup-controller- deployment-1621  a64e6221-244c-4a58-99c4-bac18c7c0ac0 10766 0 2023-01-31 09:49:20 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:ce93ab59d7dde30577cd2b1dd5126c315a382625692f55f549dd8b1a68efa04b cni.projectcalico.org/podIP:192.168.75.10/32 cni.projectcalico.org/podIPs:192.168.75.10/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller 28831ac2-70f9-49e0-a222-608659406cdb 0xc004481fa7 0xc004481fa8}] []  [{calico Update v1 2023-01-31 09:49:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-31 09:49:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28831ac2-70f9-49e0-a222-608659406cdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 09:49:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.75.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-clcxl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-clcxl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 09:49:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 09:49:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 09:49:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 09:49:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.31,PodIP:192.168.75.10,StartTime:2023-01-31 09:49:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 09:49:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0069fea4c525626ff74ccaa70e6137a5c527c1820866cdacc6572d77c3c092a2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.75.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 31 09:49:25.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1621" for this suite.

• [SLOW TEST:5.219 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":356,"completed":58,"skipped":840,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:49:25.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2671
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 31 09:49:25.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2671" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":356,"completed":59,"skipped":845,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:49:25.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8632
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 09:49:25.753: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89fe2125-6f2b-4108-995f-cb45503b80fc" in namespace "downward-api-8632" to be "Succeeded or Failed"
Jan 31 09:49:25.762: INFO: Pod "downwardapi-volume-89fe2125-6f2b-4108-995f-cb45503b80fc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.062009ms
Jan 31 09:49:27.766: INFO: Pod "downwardapi-volume-89fe2125-6f2b-4108-995f-cb45503b80fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012832277s
Jan 31 09:49:29.773: INFO: Pod "downwardapi-volume-89fe2125-6f2b-4108-995f-cb45503b80fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020213555s
STEP: Saw pod success
Jan 31 09:49:29.773: INFO: Pod "downwardapi-volume-89fe2125-6f2b-4108-995f-cb45503b80fc" satisfied condition "Succeeded or Failed"
Jan 31 09:49:29.776: INFO: Trying to get logs from node macpro-x86-1 pod downwardapi-volume-89fe2125-6f2b-4108-995f-cb45503b80fc container client-container: <nil>
STEP: delete the pod
Jan 31 09:49:29.795: INFO: Waiting for pod downwardapi-volume-89fe2125-6f2b-4108-995f-cb45503b80fc to disappear
Jan 31 09:49:29.799: INFO: Pod downwardapi-volume-89fe2125-6f2b-4108-995f-cb45503b80fc no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 31 09:49:29.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8632" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":60,"skipped":862,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:49:29.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1941
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 31 09:49:29.947: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1941  9de9f539-be4c-4444-bbd2-f848e4bd2943 10886 0 2023-01-31 09:49:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-31 09:49:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 31 09:49:29.947: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1941  9de9f539-be4c-4444-bbd2-f848e4bd2943 10887 0 2023-01-31 09:49:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-31 09:49:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 31 09:49:29.958: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1941  9de9f539-be4c-4444-bbd2-f848e4bd2943 10888 0 2023-01-31 09:49:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-31 09:49:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 31 09:49:29.958: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1941  9de9f539-be4c-4444-bbd2-f848e4bd2943 10889 0 2023-01-31 09:49:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-31 09:49:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 31 09:49:29.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1941" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":356,"completed":61,"skipped":891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:49:29.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5410
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 31 09:49:30.116: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:49:30.116: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:49:30.116: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:49:30.119: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 09:49:30.119: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 09:49:31.127: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:49:31.127: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:49:31.127: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:49:31.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 09:49:31.129: INFO: Node macpro-x86-2 is running 0 daemon pod, expected 1
Jan 31 09:49:32.125: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:49:32.125: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:49:32.125: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 09:49:32.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 31 09:49:32.131: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status
Jan 31 09:49:32.138: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Jan 31 09:49:32.145: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Jan 31 09:49:32.147: INFO: Observed &DaemonSet event: ADDED
Jan 31 09:49:32.147: INFO: Observed &DaemonSet event: MODIFIED
Jan 31 09:49:32.147: INFO: Observed &DaemonSet event: MODIFIED
Jan 31 09:49:32.147: INFO: Observed &DaemonSet event: MODIFIED
Jan 31 09:49:32.147: INFO: Observed &DaemonSet event: MODIFIED
Jan 31 09:49:32.147: INFO: Found daemon set daemon-set in namespace daemonsets-5410 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 31 09:49:32.147: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Jan 31 09:49:32.157: INFO: Observed &DaemonSet event: ADDED
Jan 31 09:49:32.157: INFO: Observed &DaemonSet event: MODIFIED
Jan 31 09:49:32.157: INFO: Observed &DaemonSet event: MODIFIED
Jan 31 09:49:32.157: INFO: Observed &DaemonSet event: MODIFIED
Jan 31 09:49:32.157: INFO: Observed &DaemonSet event: MODIFIED
Jan 31 09:49:32.157: INFO: Observed daemon set daemon-set in namespace daemonsets-5410 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 31 09:49:32.158: INFO: Observed &DaemonSet event: MODIFIED
Jan 31 09:49:32.158: INFO: Found daemon set daemon-set in namespace daemonsets-5410 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 31 09:49:32.158: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5410, will wait for the garbage collector to delete the pods
Jan 31 09:49:32.222: INFO: Deleting DaemonSet.extensions daemon-set took: 6.629283ms
Jan 31 09:49:32.322: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.233747ms
Jan 31 09:49:34.228: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 09:49:34.228: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 31 09:49:34.230: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10985"},"items":null}

Jan 31 09:49:34.232: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10985"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 31 09:49:34.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5410" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":356,"completed":62,"skipped":938,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:49:34.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5467
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 31 09:49:37.412: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 31 09:49:37.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5467" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":63,"skipped":962,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:49:37.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7257
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 31 09:49:37.561: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 31 09:49:37.567: INFO: Waiting for terminating namespaces to be deleted...
Jan 31 09:49:37.570: INFO: 
Logging pods the apiserver thinks is on node macpro-x86-1 before test
Jan 31 09:49:37.576: INFO: calico-node-nd7tv from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 09:49:37.576: INFO: 	Container calico-node ready: true, restart count 0
Jan 31 09:49:37.576: INFO: kube-proxy-tmcdj from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 09:49:37.576: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 31 09:49:37.576: INFO: helm-charts-fluent-bit-lqp7w from logging started at 2023-01-31 09:32:49 +0000 UTC (1 container statuses recorded)
Jan 31 09:49:37.576: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 31 09:49:37.576: INFO: speaker-9kx82 from metallb-system started at 2023-01-31 09:31:56 +0000 UTC (1 container statuses recorded)
Jan 31 09:49:37.576: INFO: 	Container speaker ready: true, restart count 0
Jan 31 09:49:37.576: INFO: kube-prometheus-stack-prometheus-node-exporter-lmx85 from orka-monitoring started at 2023-01-31 09:34:27 +0000 UTC (1 container statuses recorded)
Jan 31 09:49:37.576: INFO: 	Container node-exporter ready: true, restart count 0
Jan 31 09:49:37.576: INFO: sonobuoy from sonobuoy started at 2023-01-31 09:42:03 +0000 UTC (1 container statuses recorded)
Jan 31 09:49:37.576: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 31 09:49:37.576: INFO: sonobuoy-systemd-logs-daemon-set-e1cae5bb05d74403-zlhvz from sonobuoy started at 2023-01-31 09:42:03 +0000 UTC (2 container statuses recorded)
Jan 31 09:49:37.576: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 09:49:37.576: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 31 09:49:37.576: INFO: 
Logging pods the apiserver thinks is on node macpro-x86-2 before test
Jan 31 09:49:37.583: INFO: calico-node-w8d7z from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 09:49:37.583: INFO: 	Container calico-node ready: true, restart count 0
Jan 31 09:49:37.583: INFO: kube-proxy-fj2ms from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 09:49:37.583: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 31 09:49:37.583: INFO: helm-charts-fluent-bit-bwlwb from logging started at 2023-01-31 09:32:49 +0000 UTC (1 container statuses recorded)
Jan 31 09:49:37.583: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 31 09:49:37.583: INFO: speaker-7pfgg from metallb-system started at 2023-01-31 09:31:56 +0000 UTC (1 container statuses recorded)
Jan 31 09:49:37.583: INFO: 	Container speaker ready: true, restart count 0
Jan 31 09:49:37.583: INFO: kube-prometheus-stack-prometheus-node-exporter-b8d2q from orka-monitoring started at 2023-01-31 09:34:26 +0000 UTC (1 container statuses recorded)
Jan 31 09:49:37.583: INFO: 	Container node-exporter ready: true, restart count 0
Jan 31 09:49:37.583: INFO: sonobuoy-e2e-job-1083219078514678 from sonobuoy started at 2023-01-31 09:42:03 +0000 UTC (2 container statuses recorded)
Jan 31 09:49:37.583: INFO: 	Container e2e ready: true, restart count 0
Jan 31 09:49:37.583: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 09:49:37.583: INFO: sonobuoy-systemd-logs-daemon-set-e1cae5bb05d74403-fhxh8 from sonobuoy started at 2023-01-31 09:42:04 +0000 UTC (2 container statuses recorded)
Jan 31 09:49:37.583: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 09:49:37.583: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
STEP: verifying the node has the label node macpro-x86-1
STEP: verifying the node has the label node macpro-x86-2
Jan 31 09:49:37.638: INFO: Pod calico-node-nd7tv requesting resource cpu=250m on Node macpro-x86-1
Jan 31 09:49:37.638: INFO: Pod calico-node-w8d7z requesting resource cpu=250m on Node macpro-x86-2
Jan 31 09:49:37.638: INFO: Pod kube-proxy-fj2ms requesting resource cpu=0m on Node macpro-x86-2
Jan 31 09:49:37.638: INFO: Pod kube-proxy-tmcdj requesting resource cpu=0m on Node macpro-x86-1
Jan 31 09:49:37.638: INFO: Pod helm-charts-fluent-bit-bwlwb requesting resource cpu=200m on Node macpro-x86-2
Jan 31 09:49:37.638: INFO: Pod helm-charts-fluent-bit-lqp7w requesting resource cpu=200m on Node macpro-x86-1
Jan 31 09:49:37.638: INFO: Pod speaker-7pfgg requesting resource cpu=0m on Node macpro-x86-2
Jan 31 09:49:37.638: INFO: Pod speaker-9kx82 requesting resource cpu=0m on Node macpro-x86-1
Jan 31 09:49:37.638: INFO: Pod kube-prometheus-stack-prometheus-node-exporter-b8d2q requesting resource cpu=100m on Node macpro-x86-2
Jan 31 09:49:37.638: INFO: Pod kube-prometheus-stack-prometheus-node-exporter-lmx85 requesting resource cpu=100m on Node macpro-x86-1
Jan 31 09:49:37.638: INFO: Pod sonobuoy requesting resource cpu=0m on Node macpro-x86-1
Jan 31 09:49:37.638: INFO: Pod sonobuoy-e2e-job-1083219078514678 requesting resource cpu=0m on Node macpro-x86-2
Jan 31 09:49:37.638: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1cae5bb05d74403-fhxh8 requesting resource cpu=0m on Node macpro-x86-2
Jan 31 09:49:37.638: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1cae5bb05d74403-zlhvz requesting resource cpu=0m on Node macpro-x86-1
STEP: Starting Pods to consume most of the cluster CPU.
Jan 31 09:49:37.638: INFO: Creating a pod which consumes cpu=8015m on Node macpro-x86-1
Jan 31 09:49:37.648: INFO: Creating a pod which consumes cpu=8015m on Node macpro-x86-2
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54f881da-a340-4e4e-b02e-68820a18b5f9.173f5bd9db3b8932], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7257/filler-pod-54f881da-a340-4e4e-b02e-68820a18b5f9 to macpro-x86-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54f881da-a340-4e4e-b02e-68820a18b5f9.173f5bd9fd45d458], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54f881da-a340-4e4e-b02e-68820a18b5f9.173f5bd9feb5f273], Reason = [Created], Message = [Created container filler-pod-54f881da-a340-4e4e-b02e-68820a18b5f9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54f881da-a340-4e4e-b02e-68820a18b5f9.173f5bda01e5f35e], Reason = [Started], Message = [Started container filler-pod-54f881da-a340-4e4e-b02e-68820a18b5f9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c0666d3-a6dc-43b5-af4d-8602e6889080.173f5bd9dbcbabf8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7257/filler-pod-6c0666d3-a6dc-43b5-af4d-8602e6889080 to macpro-x86-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c0666d3-a6dc-43b5-af4d-8602e6889080.173f5bd9fce70da2], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c0666d3-a6dc-43b5-af4d-8602e6889080.173f5bd9fe25b108], Reason = [Created], Message = [Created container filler-pod-6c0666d3-a6dc-43b5-af4d-8602e6889080]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c0666d3-a6dc-43b5-af4d-8602e6889080.173f5bda0180d29d], Reason = [Started], Message = [Started container filler-pod-6c0666d3-a6dc-43b5-af4d-8602e6889080]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173f5bda54cdeccc], Reason = [FailedScheduling], Message = [0/5 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 5 Insufficient cpu. preemption: 0/5 nodes are available: 2 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.]
STEP: removing the label node off the node macpro-x86-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node macpro-x86-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 31 09:49:40.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7257" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":356,"completed":64,"skipped":981,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:49:40.741: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5494
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 31 09:49:43.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5494" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":356,"completed":65,"skipped":989,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:49:43.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2125
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-4f780ee7-23f1-41b9-8629-d61b52b4bcd7
STEP: Creating a pod to test consume configMaps
Jan 31 09:49:43.408: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-275eb12c-e3cb-4345-9e87-9adf8872ee73" in namespace "projected-2125" to be "Succeeded or Failed"
Jan 31 09:49:43.412: INFO: Pod "pod-projected-configmaps-275eb12c-e3cb-4345-9e87-9adf8872ee73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.446824ms
Jan 31 09:49:45.420: INFO: Pod "pod-projected-configmaps-275eb12c-e3cb-4345-9e87-9adf8872ee73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01199244s
Jan 31 09:49:47.427: INFO: Pod "pod-projected-configmaps-275eb12c-e3cb-4345-9e87-9adf8872ee73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018899057s
STEP: Saw pod success
Jan 31 09:49:47.427: INFO: Pod "pod-projected-configmaps-275eb12c-e3cb-4345-9e87-9adf8872ee73" satisfied condition "Succeeded or Failed"
Jan 31 09:49:47.429: INFO: Trying to get logs from node macpro-x86-1 pod pod-projected-configmaps-275eb12c-e3cb-4345-9e87-9adf8872ee73 container agnhost-container: <nil>
STEP: delete the pod
Jan 31 09:49:47.443: INFO: Waiting for pod pod-projected-configmaps-275eb12c-e3cb-4345-9e87-9adf8872ee73 to disappear
Jan 31 09:49:47.446: INFO: Pod pod-projected-configmaps-275eb12c-e3cb-4345-9e87-9adf8872ee73 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 31 09:49:47.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2125" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":66,"skipped":991,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:49:47.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7073
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 09:49:48.009: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 09:49:51.037: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 09:49:51.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7073" for this suite.
STEP: Destroying namespace "webhook-7073-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":356,"completed":67,"skipped":1090,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:49:51.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1579
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-728acb46-cdeb-43d3-805f-206fc8e7f2f2 in namespace container-probe-1579
Jan 31 09:49:53.325: INFO: Started pod liveness-728acb46-cdeb-43d3-805f-206fc8e7f2f2 in namespace container-probe-1579
STEP: checking the pod's current state and verifying that restartCount is present
Jan 31 09:49:53.327: INFO: Initial restart count of pod liveness-728acb46-cdeb-43d3-805f-206fc8e7f2f2 is 0
Jan 31 09:50:13.396: INFO: Restart count of pod container-probe-1579/liveness-728acb46-cdeb-43d3-805f-206fc8e7f2f2 is now 1 (20.068928556s elapsed)
Jan 31 09:50:33.461: INFO: Restart count of pod container-probe-1579/liveness-728acb46-cdeb-43d3-805f-206fc8e7f2f2 is now 2 (40.134497642s elapsed)
Jan 31 09:50:53.533: INFO: Restart count of pod container-probe-1579/liveness-728acb46-cdeb-43d3-805f-206fc8e7f2f2 is now 3 (1m0.206060499s elapsed)
Jan 31 09:51:13.610: INFO: Restart count of pod container-probe-1579/liveness-728acb46-cdeb-43d3-805f-206fc8e7f2f2 is now 4 (1m20.283027051s elapsed)
Jan 31 09:52:13.846: INFO: Restart count of pod container-probe-1579/liveness-728acb46-cdeb-43d3-805f-206fc8e7f2f2 is now 5 (2m20.519348082s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 31 09:52:13.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1579" for this suite.

• [SLOW TEST:142.697 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":356,"completed":68,"skipped":1094,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:52:13.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-587
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 09:52:14.017: INFO: The status of Pod pod-secrets-ca138ced-5249-4bbf-a8d4-4cd7ff784894 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 09:52:16.022: INFO: The status of Pod pod-secrets-ca138ced-5249-4bbf-a8d4-4cd7ff784894 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Jan 31 09:52:16.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-587" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":356,"completed":69,"skipped":1099,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:52:16.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3011
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-9154c2a1-5190-40a9-ac53-ac3538f539bf
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 31 09:52:18.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3011" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":70,"skipped":1103,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:52:18.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9374
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 09:52:18.703: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 09:52:21.730: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 09:52:21.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9374" for this suite.
STEP: Destroying namespace "webhook-9374-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":356,"completed":71,"skipped":1112,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:52:21.825: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3272
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-downwardapi-d7wf
STEP: Creating a pod to test atomic-volume-subpath
Jan 31 09:52:21.985: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-d7wf" in namespace "subpath-3272" to be "Succeeded or Failed"
Jan 31 09:52:21.989: INFO: Pod "pod-subpath-test-downwardapi-d7wf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.356815ms
Jan 31 09:52:23.994: INFO: Pod "pod-subpath-test-downwardapi-d7wf": Phase="Running", Reason="", readiness=true. Elapsed: 2.009042262s
Jan 31 09:52:26.001: INFO: Pod "pod-subpath-test-downwardapi-d7wf": Phase="Running", Reason="", readiness=true. Elapsed: 4.015722336s
Jan 31 09:52:28.009: INFO: Pod "pod-subpath-test-downwardapi-d7wf": Phase="Running", Reason="", readiness=true. Elapsed: 6.023630681s
Jan 31 09:52:30.014: INFO: Pod "pod-subpath-test-downwardapi-d7wf": Phase="Running", Reason="", readiness=true. Elapsed: 8.029146681s
Jan 31 09:52:32.022: INFO: Pod "pod-subpath-test-downwardapi-d7wf": Phase="Running", Reason="", readiness=true. Elapsed: 10.03684695s
Jan 31 09:52:34.032: INFO: Pod "pod-subpath-test-downwardapi-d7wf": Phase="Running", Reason="", readiness=true. Elapsed: 12.047294164s
Jan 31 09:52:36.040: INFO: Pod "pod-subpath-test-downwardapi-d7wf": Phase="Running", Reason="", readiness=true. Elapsed: 14.055039521s
Jan 31 09:52:38.047: INFO: Pod "pod-subpath-test-downwardapi-d7wf": Phase="Running", Reason="", readiness=true. Elapsed: 16.06210488s
Jan 31 09:52:40.061: INFO: Pod "pod-subpath-test-downwardapi-d7wf": Phase="Running", Reason="", readiness=true. Elapsed: 18.075940136s
Jan 31 09:52:42.069: INFO: Pod "pod-subpath-test-downwardapi-d7wf": Phase="Running", Reason="", readiness=true. Elapsed: 20.084106216s
Jan 31 09:52:44.078: INFO: Pod "pod-subpath-test-downwardapi-d7wf": Phase="Running", Reason="", readiness=false. Elapsed: 22.092665583s
Jan 31 09:52:46.086: INFO: Pod "pod-subpath-test-downwardapi-d7wf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.10068717s
STEP: Saw pod success
Jan 31 09:52:46.086: INFO: Pod "pod-subpath-test-downwardapi-d7wf" satisfied condition "Succeeded or Failed"
Jan 31 09:52:46.090: INFO: Trying to get logs from node macpro-x86-2 pod pod-subpath-test-downwardapi-d7wf container test-container-subpath-downwardapi-d7wf: <nil>
STEP: delete the pod
Jan 31 09:52:46.114: INFO: Waiting for pod pod-subpath-test-downwardapi-d7wf to disappear
Jan 31 09:52:46.116: INFO: Pod pod-subpath-test-downwardapi-d7wf no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-d7wf
Jan 31 09:52:46.117: INFO: Deleting pod "pod-subpath-test-downwardapi-d7wf" in namespace "subpath-3272"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 31 09:52:46.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3272" for this suite.

• [SLOW TEST:24.302 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","total":356,"completed":72,"skipped":1154,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:52:46.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-7336
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:79
Jan 31 09:52:46.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the sample API server.
Jan 31 09:52:46.642: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jan 31 09:52:48.709: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 9, 52, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 9, 52, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 9, 52, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 9, 52, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 31 09:52:50.717: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 9, 52, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 9, 52, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 9, 52, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 9, 52, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 31 09:52:52.935: INFO: Waited 216.500012ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jan 31 09:52:53.089: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:69
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:188
Jan 31 09:52:53.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7336" for this suite.

• [SLOW TEST:7.864 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":356,"completed":73,"skipped":1167,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:52:53.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2241
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 09:52:54.124: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd94cd7a-fad9-48eb-84d7-6c31ecbf6658" in namespace "downward-api-2241" to be "Succeeded or Failed"
Jan 31 09:52:54.130: INFO: Pod "downwardapi-volume-dd94cd7a-fad9-48eb-84d7-6c31ecbf6658": Phase="Pending", Reason="", readiness=false. Elapsed: 5.777084ms
Jan 31 09:52:56.135: INFO: Pod "downwardapi-volume-dd94cd7a-fad9-48eb-84d7-6c31ecbf6658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010419554s
Jan 31 09:52:58.143: INFO: Pod "downwardapi-volume-dd94cd7a-fad9-48eb-84d7-6c31ecbf6658": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018652423s
STEP: Saw pod success
Jan 31 09:52:58.143: INFO: Pod "downwardapi-volume-dd94cd7a-fad9-48eb-84d7-6c31ecbf6658" satisfied condition "Succeeded or Failed"
Jan 31 09:52:58.145: INFO: Trying to get logs from node macpro-x86-2 pod downwardapi-volume-dd94cd7a-fad9-48eb-84d7-6c31ecbf6658 container client-container: <nil>
STEP: delete the pod
Jan 31 09:52:58.163: INFO: Waiting for pod downwardapi-volume-dd94cd7a-fad9-48eb-84d7-6c31ecbf6658 to disappear
Jan 31 09:52:58.165: INFO: Pod downwardapi-volume-dd94cd7a-fad9-48eb-84d7-6c31ecbf6658 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 31 09:52:58.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2241" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":74,"skipped":1190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:52:58.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5072
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 09:52:58.918: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 09:53:01.941: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 09:53:02.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5072" for this suite.
STEP: Destroying namespace "webhook-5072-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":356,"completed":75,"skipped":1226,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:53:02.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-999
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 31 09:53:06.248: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 31 09:53:06.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-999" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":76,"skipped":1339,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:53:06.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8285
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-projected-lgcv
STEP: Creating a pod to test atomic-volume-subpath
Jan 31 09:53:06.419: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-lgcv" in namespace "subpath-8285" to be "Succeeded or Failed"
Jan 31 09:53:06.422: INFO: Pod "pod-subpath-test-projected-lgcv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.464515ms
Jan 31 09:53:08.427: INFO: Pod "pod-subpath-test-projected-lgcv": Phase="Running", Reason="", readiness=true. Elapsed: 2.007987816s
Jan 31 09:53:10.438: INFO: Pod "pod-subpath-test-projected-lgcv": Phase="Running", Reason="", readiness=true. Elapsed: 4.018217529s
Jan 31 09:53:12.441: INFO: Pod "pod-subpath-test-projected-lgcv": Phase="Running", Reason="", readiness=true. Elapsed: 6.021930374s
Jan 31 09:53:14.449: INFO: Pod "pod-subpath-test-projected-lgcv": Phase="Running", Reason="", readiness=true. Elapsed: 8.029472617s
Jan 31 09:53:16.457: INFO: Pod "pod-subpath-test-projected-lgcv": Phase="Running", Reason="", readiness=true. Elapsed: 10.037463955s
Jan 31 09:53:18.465: INFO: Pod "pod-subpath-test-projected-lgcv": Phase="Running", Reason="", readiness=true. Elapsed: 12.045257429s
Jan 31 09:53:20.474: INFO: Pod "pod-subpath-test-projected-lgcv": Phase="Running", Reason="", readiness=true. Elapsed: 14.054262122s
Jan 31 09:53:22.480: INFO: Pod "pod-subpath-test-projected-lgcv": Phase="Running", Reason="", readiness=true. Elapsed: 16.060196322s
Jan 31 09:53:24.490: INFO: Pod "pod-subpath-test-projected-lgcv": Phase="Running", Reason="", readiness=true. Elapsed: 18.070710092s
Jan 31 09:53:26.498: INFO: Pod "pod-subpath-test-projected-lgcv": Phase="Running", Reason="", readiness=true. Elapsed: 20.078622931s
Jan 31 09:53:28.506: INFO: Pod "pod-subpath-test-projected-lgcv": Phase="Running", Reason="", readiness=false. Elapsed: 22.086179377s
Jan 31 09:53:30.515: INFO: Pod "pod-subpath-test-projected-lgcv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.095732069s
STEP: Saw pod success
Jan 31 09:53:30.515: INFO: Pod "pod-subpath-test-projected-lgcv" satisfied condition "Succeeded or Failed"
Jan 31 09:53:30.517: INFO: Trying to get logs from node macpro-x86-2 pod pod-subpath-test-projected-lgcv container test-container-subpath-projected-lgcv: <nil>
STEP: delete the pod
Jan 31 09:53:30.532: INFO: Waiting for pod pod-subpath-test-projected-lgcv to disappear
Jan 31 09:53:30.535: INFO: Pod pod-subpath-test-projected-lgcv no longer exists
STEP: Deleting pod pod-subpath-test-projected-lgcv
Jan 31 09:53:30.535: INFO: Deleting pod "pod-subpath-test-projected-lgcv" in namespace "subpath-8285"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 31 09:53:30.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8285" for this suite.

• [SLOW TEST:24.275 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","total":356,"completed":77,"skipped":1342,"failed":0}
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:53:30.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7429
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 31 09:53:30.687: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 31 09:53:32.692: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 31 09:53:32.702: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 31 09:53:34.711: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 31 09:53:34.722: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 31 09:53:34.731: INFO: Pod pod-with-prestop-http-hook still exists
Jan 31 09:53:36.732: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 31 09:53:36.740: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 31 09:53:36.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7429" for this suite.

• [SLOW TEST:6.210 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":356,"completed":78,"skipped":1343,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:53:36.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6671
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 09:53:36.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6671" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":356,"completed":79,"skipped":1379,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:53:36.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-236
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 09:53:37.572: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 09:53:40.597: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 09:53:40.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-236" for this suite.
STEP: Destroying namespace "webhook-236-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":356,"completed":80,"skipped":1389,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:53:40.669: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-9570
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 31 09:53:40.825: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 31 09:54:40.866: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Jan 31 09:54:40.890: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 31 09:54:40.906: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 31 09:54:40.924: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 31 09:54:40.933: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 31 09:54:48.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9570" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:68.364 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":356,"completed":81,"skipped":1404,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:54:49.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-6893
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 31 09:54:49.564: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 09:54:52.587: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 09:54:52.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 09:54:55.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6893" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:6.841 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":356,"completed":82,"skipped":1408,"failed":0}
SSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:54:55.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3341
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jan 31 09:54:58.035: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3341 PodName:var-expansion-08d0dbbc-db34-42b0-8049-05fb0e999464 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:54:58.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:54:58.036: INFO: ExecWithOptions: Clientset creation
Jan 31 09:54:58.036: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3341/pods/var-expansion-08d0dbbc-db34-42b0-8049-05fb0e999464/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path
Jan 31 09:54:58.086: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3341 PodName:var-expansion-08d0dbbc-db34-42b0-8049-05fb0e999464 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 09:54:58.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 09:54:58.086: INFO: ExecWithOptions: Clientset creation
Jan 31 09:54:58.086: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3341/pods/var-expansion-08d0dbbc-db34-42b0-8049-05fb0e999464/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value
Jan 31 09:54:58.642: INFO: Successfully updated pod "var-expansion-08d0dbbc-db34-42b0-8049-05fb0e999464"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jan 31 09:54:58.650: INFO: Deleting pod "var-expansion-08d0dbbc-db34-42b0-8049-05fb0e999464" in namespace "var-expansion-3341"
Jan 31 09:54:58.658: INFO: Wait up to 5m0s for pod "var-expansion-08d0dbbc-db34-42b0-8049-05fb0e999464" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 31 09:55:32.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3341" for this suite.

• [SLOW TEST:36.799 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":356,"completed":83,"skipped":1415,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:55:32.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8962
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod with failed condition
STEP: updating the pod
Jan 31 09:57:33.339: INFO: Successfully updated pod "var-expansion-b1a64260-53e0-49cf-a7eb-22e931d3d7d6"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jan 31 09:57:35.352: INFO: Deleting pod "var-expansion-b1a64260-53e0-49cf-a7eb-22e931d3d7d6" in namespace "var-expansion-8962"
Jan 31 09:57:35.360: INFO: Wait up to 5m0s for pod "var-expansion-b1a64260-53e0-49cf-a7eb-22e931d3d7d6" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 31 09:58:07.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8962" for this suite.

• [SLOW TEST:154.707 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":356,"completed":84,"skipped":1416,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:58:07.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9237
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-4302dc08-1c75-427f-9975-7137397b1b41
STEP: Creating a pod to test consume secrets
Jan 31 09:58:07.522: INFO: Waiting up to 5m0s for pod "pod-secrets-2522a408-dcc4-48d2-81e6-121766468140" in namespace "secrets-9237" to be "Succeeded or Failed"
Jan 31 09:58:07.527: INFO: Pod "pod-secrets-2522a408-dcc4-48d2-81e6-121766468140": Phase="Pending", Reason="", readiness=false. Elapsed: 5.200247ms
Jan 31 09:58:09.533: INFO: Pod "pod-secrets-2522a408-dcc4-48d2-81e6-121766468140": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011534478s
Jan 31 09:58:11.539: INFO: Pod "pod-secrets-2522a408-dcc4-48d2-81e6-121766468140": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01699813s
STEP: Saw pod success
Jan 31 09:58:11.539: INFO: Pod "pod-secrets-2522a408-dcc4-48d2-81e6-121766468140" satisfied condition "Succeeded or Failed"
Jan 31 09:58:11.541: INFO: Trying to get logs from node macpro-x86-2 pod pod-secrets-2522a408-dcc4-48d2-81e6-121766468140 container secret-volume-test: <nil>
STEP: delete the pod
Jan 31 09:58:11.566: INFO: Waiting for pod pod-secrets-2522a408-dcc4-48d2-81e6-121766468140 to disappear
Jan 31 09:58:11.568: INFO: Pod pod-secrets-2522a408-dcc4-48d2-81e6-121766468140 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 31 09:58:11.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9237" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":85,"skipped":1417,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:58:11.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5917
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-secret-v8qk
STEP: Creating a pod to test atomic-volume-subpath
Jan 31 09:58:11.715: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-v8qk" in namespace "subpath-5917" to be "Succeeded or Failed"
Jan 31 09:58:11.717: INFO: Pod "pod-subpath-test-secret-v8qk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.579538ms
Jan 31 09:58:13.723: INFO: Pod "pod-subpath-test-secret-v8qk": Phase="Running", Reason="", readiness=true. Elapsed: 2.008277591s
Jan 31 09:58:15.729: INFO: Pod "pod-subpath-test-secret-v8qk": Phase="Running", Reason="", readiness=true. Elapsed: 4.013833787s
Jan 31 09:58:17.732: INFO: Pod "pod-subpath-test-secret-v8qk": Phase="Running", Reason="", readiness=true. Elapsed: 6.017307862s
Jan 31 09:58:19.739: INFO: Pod "pod-subpath-test-secret-v8qk": Phase="Running", Reason="", readiness=true. Elapsed: 8.024167712s
Jan 31 09:58:21.748: INFO: Pod "pod-subpath-test-secret-v8qk": Phase="Running", Reason="", readiness=true. Elapsed: 10.032824157s
Jan 31 09:58:23.756: INFO: Pod "pod-subpath-test-secret-v8qk": Phase="Running", Reason="", readiness=true. Elapsed: 12.041321104s
Jan 31 09:58:25.762: INFO: Pod "pod-subpath-test-secret-v8qk": Phase="Running", Reason="", readiness=true. Elapsed: 14.047734731s
Jan 31 09:58:27.771: INFO: Pod "pod-subpath-test-secret-v8qk": Phase="Running", Reason="", readiness=true. Elapsed: 16.05625456s
Jan 31 09:58:29.778: INFO: Pod "pod-subpath-test-secret-v8qk": Phase="Running", Reason="", readiness=true. Elapsed: 18.063735825s
Jan 31 09:58:31.786: INFO: Pod "pod-subpath-test-secret-v8qk": Phase="Running", Reason="", readiness=true. Elapsed: 20.071734107s
Jan 31 09:58:33.795: INFO: Pod "pod-subpath-test-secret-v8qk": Phase="Running", Reason="", readiness=false. Elapsed: 22.080545334s
Jan 31 09:58:35.802: INFO: Pod "pod-subpath-test-secret-v8qk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.087612939s
STEP: Saw pod success
Jan 31 09:58:35.802: INFO: Pod "pod-subpath-test-secret-v8qk" satisfied condition "Succeeded or Failed"
Jan 31 09:58:35.805: INFO: Trying to get logs from node macpro-x86-1 pod pod-subpath-test-secret-v8qk container test-container-subpath-secret-v8qk: <nil>
STEP: delete the pod
Jan 31 09:58:35.823: INFO: Waiting for pod pod-subpath-test-secret-v8qk to disappear
Jan 31 09:58:35.826: INFO: Pod pod-subpath-test-secret-v8qk no longer exists
STEP: Deleting pod pod-subpath-test-secret-v8qk
Jan 31 09:58:35.826: INFO: Deleting pod "pod-subpath-test-secret-v8qk" in namespace "subpath-5917"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 31 09:58:35.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5917" for this suite.

• [SLOW TEST:24.260 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","total":356,"completed":86,"skipped":1424,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:58:35.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6409
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Jan 31 09:58:35.960: INFO: namespace kubectl-6409
Jan 31 09:58:35.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6409 create -f -'
Jan 31 09:58:37.207: INFO: stderr: ""
Jan 31 09:58:37.207: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 31 09:58:38.214: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 31 09:58:38.214: INFO: Found 1 / 1
Jan 31 09:58:38.214: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 31 09:58:38.216: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 31 09:58:38.216: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 31 09:58:38.216: INFO: wait on agnhost-primary startup in kubectl-6409 
Jan 31 09:58:38.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6409 logs agnhost-primary-cbf55 agnhost-primary'
Jan 31 09:58:38.266: INFO: stderr: ""
Jan 31 09:58:38.266: INFO: stdout: "Paused\n"
STEP: exposing RC
Jan 31 09:58:38.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6409 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 31 09:58:38.326: INFO: stderr: ""
Jan 31 09:58:38.326: INFO: stdout: "service/rm2 exposed\n"
Jan 31 09:58:38.333: INFO: Service rm2 in namespace kubectl-6409 found.
STEP: exposing service
Jan 31 09:58:40.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6409 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 31 09:58:40.432: INFO: stderr: ""
Jan 31 09:58:40.432: INFO: stdout: "service/rm3 exposed\n"
Jan 31 09:58:40.442: INFO: Service rm3 in namespace kubectl-6409 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 09:58:42.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6409" for this suite.

• [SLOW TEST:6.630 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1249
    should create services for rc  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":356,"completed":87,"skipped":1428,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:58:42.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5218
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-projected-all-test-volume-76741764-a0f1-4230-8cd6-c1d7f1495eb8
STEP: Creating secret with name secret-projected-all-test-volume-e0cb52b8-e99f-4466-be3d-3a39d37e49c8
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 31 09:58:42.612: INFO: Waiting up to 5m0s for pod "projected-volume-78d275f2-3579-4914-bcbb-eadfb7b13877" in namespace "projected-5218" to be "Succeeded or Failed"
Jan 31 09:58:42.620: INFO: Pod "projected-volume-78d275f2-3579-4914-bcbb-eadfb7b13877": Phase="Pending", Reason="", readiness=false. Elapsed: 7.398686ms
Jan 31 09:58:44.627: INFO: Pod "projected-volume-78d275f2-3579-4914-bcbb-eadfb7b13877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014364277s
Jan 31 09:58:46.636: INFO: Pod "projected-volume-78d275f2-3579-4914-bcbb-eadfb7b13877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023575719s
STEP: Saw pod success
Jan 31 09:58:46.636: INFO: Pod "projected-volume-78d275f2-3579-4914-bcbb-eadfb7b13877" satisfied condition "Succeeded or Failed"
Jan 31 09:58:46.638: INFO: Trying to get logs from node macpro-x86-2 pod projected-volume-78d275f2-3579-4914-bcbb-eadfb7b13877 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 31 09:58:46.654: INFO: Waiting for pod projected-volume-78d275f2-3579-4914-bcbb-eadfb7b13877 to disappear
Jan 31 09:58:46.657: INFO: Pod projected-volume-78d275f2-3579-4914-bcbb-eadfb7b13877 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:188
Jan 31 09:58:46.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5218" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":356,"completed":88,"skipped":1442,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:58:46.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-918
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 09:58:47.271: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 09:58:50.292: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 09:58:50.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7908-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 09:58:53.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-918" for this suite.
STEP: Destroying namespace "webhook-918-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.351 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":356,"completed":89,"skipped":1447,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:58:54.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4609
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 09:58:54.185: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cbc71dbe-6703-420d-a4e7-2b29d903476b" in namespace "projected-4609" to be "Succeeded or Failed"
Jan 31 09:58:54.189: INFO: Pod "downwardapi-volume-cbc71dbe-6703-420d-a4e7-2b29d903476b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.173103ms
Jan 31 09:58:56.195: INFO: Pod "downwardapi-volume-cbc71dbe-6703-420d-a4e7-2b29d903476b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009513508s
Jan 31 09:58:58.202: INFO: Pod "downwardapi-volume-cbc71dbe-6703-420d-a4e7-2b29d903476b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016785478s
STEP: Saw pod success
Jan 31 09:58:58.202: INFO: Pod "downwardapi-volume-cbc71dbe-6703-420d-a4e7-2b29d903476b" satisfied condition "Succeeded or Failed"
Jan 31 09:58:58.204: INFO: Trying to get logs from node macpro-x86-2 pod downwardapi-volume-cbc71dbe-6703-420d-a4e7-2b29d903476b container client-container: <nil>
STEP: delete the pod
Jan 31 09:58:58.220: INFO: Waiting for pod downwardapi-volume-cbc71dbe-6703-420d-a4e7-2b29d903476b to disappear
Jan 31 09:58:58.222: INFO: Pod downwardapi-volume-cbc71dbe-6703-420d-a4e7-2b29d903476b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 31 09:58:58.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4609" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":356,"completed":90,"skipped":1496,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:58:58.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-685
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 31 09:58:58.361: INFO: PodSpec: initContainers in spec.initContainers
Jan 31 09:59:40.995: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-448ddf60-ccd6-4e62-9bc4-2794a32cf16c", GenerateName:"", Namespace:"init-container-685", SelfLink:"", UID:"73619658-e9eb-409e-8d28-111d3f6c4596", ResourceVersion:"13714", Generation:0, CreationTimestamp:time.Date(2023, time.January, 31, 9, 58, 58, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"361413658"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"c0051ad6a30902944dffdfdd957bd1e1b7a23fb05483219fb2efd9e43d0025de", "cni.projectcalico.org/podIP":"192.168.75.26/32", "cni.projectcalico.org/podIPs":"192.168.75.26/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 31, 9, 58, 58, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003018798), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 31, 9, 58, 58, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0030187c8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 31, 9, 58, 59, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0030187f8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-qmrlz", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc002150e20), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qmrlz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qmrlz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.7", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qmrlz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0043d23d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"macpro-x86-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002133500), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0043d2460)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0043d2480)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0043d2488), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0043d248c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0040e2410), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 31, 9, 58, 58, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 31, 9, 58, 58, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 31, 9, 58, 58, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 31, 9, 58, 58, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.221.188.31", PodIP:"192.168.75.26", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.75.26"}}, StartTime:time.Date(2023, time.January, 31, 9, 58, 58, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0021335e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0021336c0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://7fffa30a6294d72f07e382449a6334db9ce464f6673b8c4befd66c1938a582ff", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002150ea0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002150e80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.7", ImageID:"", ContainerID:"", Started:(*bool)(0xc0043d2504)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 31 09:59:40.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-685" for this suite.

• [SLOW TEST:42.779 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":356,"completed":91,"skipped":1574,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 09:59:41.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1104
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-7376405e-5be9-4ac0-8d56-2a4e1564f37b
STEP: Creating the pod
Jan 31 09:59:41.158: INFO: The status of Pod pod-configmaps-656cd94c-fe9b-45f7-906a-b521d62b05f7 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 09:59:43.163: INFO: The status of Pod pod-configmaps-656cd94c-fe9b-45f7-906a-b521d62b05f7 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-7376405e-5be9-4ac0-8d56-2a4e1564f37b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 31 10:01:15.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1104" for this suite.

• [SLOW TEST:94.509 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":92,"skipped":1576,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:01:15.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3014
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 10:01:15.654: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d002186-3153-4c47-a1ba-98003a7208de" in namespace "downward-api-3014" to be "Succeeded or Failed"
Jan 31 10:01:15.657: INFO: Pod "downwardapi-volume-1d002186-3153-4c47-a1ba-98003a7208de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.421474ms
Jan 31 10:01:17.664: INFO: Pod "downwardapi-volume-1d002186-3153-4c47-a1ba-98003a7208de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009765638s
Jan 31 10:01:19.672: INFO: Pod "downwardapi-volume-1d002186-3153-4c47-a1ba-98003a7208de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018272254s
STEP: Saw pod success
Jan 31 10:01:19.673: INFO: Pod "downwardapi-volume-1d002186-3153-4c47-a1ba-98003a7208de" satisfied condition "Succeeded or Failed"
Jan 31 10:01:19.675: INFO: Trying to get logs from node macpro-x86-1 pod downwardapi-volume-1d002186-3153-4c47-a1ba-98003a7208de container client-container: <nil>
STEP: delete the pod
Jan 31 10:01:19.690: INFO: Waiting for pod downwardapi-volume-1d002186-3153-4c47-a1ba-98003a7208de to disappear
Jan 31 10:01:19.693: INFO: Pod downwardapi-volume-1d002186-3153-4c47-a1ba-98003a7208de no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 31 10:01:19.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3014" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":93,"skipped":1596,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:01:19.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3131
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Starting the proxy
Jan 31 10:01:19.830: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-3131 proxy --unix-socket=/tmp/kubectl-proxy-unix1432289505/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 10:01:19.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3131" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":356,"completed":94,"skipped":1598,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:01:19.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6584
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-c6593d82-ad44-460c-9400-21f0066856d6 in namespace container-probe-6584
Jan 31 10:01:22.020: INFO: Started pod busybox-c6593d82-ad44-460c-9400-21f0066856d6 in namespace container-probe-6584
STEP: checking the pod's current state and verifying that restartCount is present
Jan 31 10:01:22.022: INFO: Initial restart count of pod busybox-c6593d82-ad44-460c-9400-21f0066856d6 is 0
Jan 31 10:02:12.216: INFO: Restart count of pod container-probe-6584/busybox-c6593d82-ad44-460c-9400-21f0066856d6 is now 1 (50.194198391s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 31 10:02:12.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6584" for this suite.

• [SLOW TEST:52.372 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":95,"skipped":1611,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:02:12.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2254
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 31 10:02:12.388: INFO: Waiting up to 5m0s for pod "pod-ff56d1ce-fb4c-4fd3-9978-dc8388ec4e80" in namespace "emptydir-2254" to be "Succeeded or Failed"
Jan 31 10:02:12.391: INFO: Pod "pod-ff56d1ce-fb4c-4fd3-9978-dc8388ec4e80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.946581ms
Jan 31 10:02:14.398: INFO: Pod "pod-ff56d1ce-fb4c-4fd3-9978-dc8388ec4e80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010034285s
Jan 31 10:02:16.404: INFO: Pod "pod-ff56d1ce-fb4c-4fd3-9978-dc8388ec4e80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015619598s
STEP: Saw pod success
Jan 31 10:02:16.404: INFO: Pod "pod-ff56d1ce-fb4c-4fd3-9978-dc8388ec4e80" satisfied condition "Succeeded or Failed"
Jan 31 10:02:16.406: INFO: Trying to get logs from node macpro-x86-2 pod pod-ff56d1ce-fb4c-4fd3-9978-dc8388ec4e80 container test-container: <nil>
STEP: delete the pod
Jan 31 10:02:16.428: INFO: Waiting for pod pod-ff56d1ce-fb4c-4fd3-9978-dc8388ec4e80 to disappear
Jan 31 10:02:16.430: INFO: Pod pod-ff56d1ce-fb4c-4fd3-9978-dc8388ec4e80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 31 10:02:16.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2254" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":96,"skipped":1619,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:02:16.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2379
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2379.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2379.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2379.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2379.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 31 10:02:18.601: INFO: DNS probes using dns-test-7728480d-2e1c-4dad-b745-56e9155e014b succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2379.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2379.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2379.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2379.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 31 10:02:20.641: INFO: File wheezy_udp@dns-test-service-3.dns-2379.svc.cluster.local from pod  dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 31 10:02:20.643: INFO: File jessie_udp@dns-test-service-3.dns-2379.svc.cluster.local from pod  dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 31 10:02:20.643: INFO: Lookups using dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e failed for: [wheezy_udp@dns-test-service-3.dns-2379.svc.cluster.local jessie_udp@dns-test-service-3.dns-2379.svc.cluster.local]

Jan 31 10:02:25.649: INFO: File wheezy_udp@dns-test-service-3.dns-2379.svc.cluster.local from pod  dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 31 10:02:25.652: INFO: File jessie_udp@dns-test-service-3.dns-2379.svc.cluster.local from pod  dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 31 10:02:25.652: INFO: Lookups using dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e failed for: [wheezy_udp@dns-test-service-3.dns-2379.svc.cluster.local jessie_udp@dns-test-service-3.dns-2379.svc.cluster.local]

Jan 31 10:02:30.650: INFO: File wheezy_udp@dns-test-service-3.dns-2379.svc.cluster.local from pod  dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 31 10:02:30.652: INFO: File jessie_udp@dns-test-service-3.dns-2379.svc.cluster.local from pod  dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 31 10:02:30.652: INFO: Lookups using dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e failed for: [wheezy_udp@dns-test-service-3.dns-2379.svc.cluster.local jessie_udp@dns-test-service-3.dns-2379.svc.cluster.local]

Jan 31 10:02:35.651: INFO: File wheezy_udp@dns-test-service-3.dns-2379.svc.cluster.local from pod  dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 31 10:02:35.654: INFO: File jessie_udp@dns-test-service-3.dns-2379.svc.cluster.local from pod  dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 31 10:02:35.654: INFO: Lookups using dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e failed for: [wheezy_udp@dns-test-service-3.dns-2379.svc.cluster.local jessie_udp@dns-test-service-3.dns-2379.svc.cluster.local]

Jan 31 10:02:40.649: INFO: File wheezy_udp@dns-test-service-3.dns-2379.svc.cluster.local from pod  dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 31 10:02:40.653: INFO: File jessie_udp@dns-test-service-3.dns-2379.svc.cluster.local from pod  dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 31 10:02:40.653: INFO: Lookups using dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e failed for: [wheezy_udp@dns-test-service-3.dns-2379.svc.cluster.local jessie_udp@dns-test-service-3.dns-2379.svc.cluster.local]

Jan 31 10:02:45.648: INFO: File wheezy_udp@dns-test-service-3.dns-2379.svc.cluster.local from pod  dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 31 10:02:45.651: INFO: File jessie_udp@dns-test-service-3.dns-2379.svc.cluster.local from pod  dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 31 10:02:45.651: INFO: Lookups using dns-2379/dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e failed for: [wheezy_udp@dns-test-service-3.dns-2379.svc.cluster.local jessie_udp@dns-test-service-3.dns-2379.svc.cluster.local]

Jan 31 10:02:50.652: INFO: DNS probes using dns-test-1c688783-ed8c-429d-a65e-6f1fa4eae33e succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2379.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2379.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2379.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2379.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 31 10:02:52.720: INFO: DNS probes using dns-test-40c18cfa-ee6c-4a8b-98e3-4d5ed3443a8f succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 31 10:02:52.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2379" for this suite.

• [SLOW TEST:36.347 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":356,"completed":97,"skipped":1623,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:02:52.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5532
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:02:52.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jan 31 10:02:58.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 create -f -'
Jan 31 10:02:59.622: INFO: stderr: ""
Jan 31 10:02:59.622: INFO: stdout: "e2e-test-crd-publish-openapi-9702-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 31 10:02:59.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 delete e2e-test-crd-publish-openapi-9702-crds test-cr'
Jan 31 10:02:59.697: INFO: stderr: ""
Jan 31 10:02:59.697: INFO: stdout: "e2e-test-crd-publish-openapi-9702-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 31 10:02:59.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 apply -f -'
Jan 31 10:03:00.400: INFO: stderr: ""
Jan 31 10:03:00.400: INFO: stdout: "e2e-test-crd-publish-openapi-9702-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 31 10:03:00.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 delete e2e-test-crd-publish-openapi-9702-crds test-cr'
Jan 31 10:03:00.449: INFO: stderr: ""
Jan 31 10:03:00.449: INFO: stdout: "e2e-test-crd-publish-openapi-9702-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jan 31 10:03:00.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-5532 explain e2e-test-crd-publish-openapi-9702-crds'
Jan 31 10:03:00.578: INFO: stderr: ""
Jan 31 10:03:00.578: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9702-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:03:04.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5532" for this suite.

• [SLOW TEST:11.257 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":356,"completed":98,"skipped":1670,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:03:04.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8820
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Jan 31 10:03:04.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-8820 create -f -'
Jan 31 10:03:04.979: INFO: stderr: ""
Jan 31 10:03:04.979: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 31 10:03:05.991: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 31 10:03:05.991: INFO: Found 0 / 1
Jan 31 10:03:06.983: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 31 10:03:06.983: INFO: Found 1 / 1
Jan 31 10:03:06.983: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 31 10:03:06.985: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 31 10:03:06.985: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 31 10:03:06.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-8820 patch pod agnhost-primary-k8f2h -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 31 10:03:07.050: INFO: stderr: ""
Jan 31 10:03:07.050: INFO: stdout: "pod/agnhost-primary-k8f2h patched\n"
STEP: checking annotations
Jan 31 10:03:07.053: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 31 10:03:07.053: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 10:03:07.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8820" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":356,"completed":99,"skipped":1681,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:03:07.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4597
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4597.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4597.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4597.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4597.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 31 10:03:09.221: INFO: DNS probes using dns-4597/dns-test-b3f0c944-d6ce-4f9e-8e27-50936e2ada9e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 31 10:03:09.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4597" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","total":356,"completed":100,"skipped":1716,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:03:09.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1203
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:03:09.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jan 31 10:03:14.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-1203 --namespace=crd-publish-openapi-1203 create -f -'
Jan 31 10:03:15.534: INFO: stderr: ""
Jan 31 10:03:15.534: INFO: stdout: "e2e-test-crd-publish-openapi-2579-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 31 10:03:15.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-1203 --namespace=crd-publish-openapi-1203 delete e2e-test-crd-publish-openapi-2579-crds test-cr'
Jan 31 10:03:15.611: INFO: stderr: ""
Jan 31 10:03:15.611: INFO: stdout: "e2e-test-crd-publish-openapi-2579-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 31 10:03:15.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-1203 --namespace=crd-publish-openapi-1203 apply -f -'
Jan 31 10:03:16.380: INFO: stderr: ""
Jan 31 10:03:16.380: INFO: stdout: "e2e-test-crd-publish-openapi-2579-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 31 10:03:16.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-1203 --namespace=crd-publish-openapi-1203 delete e2e-test-crd-publish-openapi-2579-crds test-cr'
Jan 31 10:03:16.430: INFO: stderr: ""
Jan 31 10:03:16.430: INFO: stdout: "e2e-test-crd-publish-openapi-2579-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 31 10:03:16.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-1203 explain e2e-test-crd-publish-openapi-2579-crds'
Jan 31 10:03:17.043: INFO: stderr: ""
Jan 31 10:03:17.043: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2579-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:03:22.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1203" for this suite.

• [SLOW TEST:13.001 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":356,"completed":101,"skipped":1729,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:03:22.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3576
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3576 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3576;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3576 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3576;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3576.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3576.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3576.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3576.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3576.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3576.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3576.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3576.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3576.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3576.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3576.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3576.svc;check="$$(dig +notcp +noall +answer +search 123.201.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.201.123_udp@PTR;check="$$(dig +tcp +noall +answer +search 123.201.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.201.123_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3576 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3576;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3576 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3576;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3576.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3576.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3576.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3576.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3576.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3576.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3576.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3576.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3576.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3576.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3576.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3576.svc;check="$$(dig +notcp +noall +answer +search 123.201.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.201.123_udp@PTR;check="$$(dig +tcp +noall +answer +search 123.201.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.201.123_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 31 10:03:24.418: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.420: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.422: INFO: Unable to read wheezy_udp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.425: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.427: INFO: Unable to read wheezy_udp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.429: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.431: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.433: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.435: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.437: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.444: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.446: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.448: INFO: Unable to read jessie_udp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.449: INFO: Unable to read jessie_tcp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.452: INFO: Unable to read jessie_udp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.455: INFO: Unable to read jessie_tcp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.457: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.459: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:24.468: INFO: Lookups using dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3576 wheezy_tcp@dns-test-service.dns-3576 wheezy_udp@dns-test-service.dns-3576.svc wheezy_tcp@dns-test-service.dns-3576.svc wheezy_udp@_http._tcp.dns-test-service.dns-3576.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3576.svc wheezy_udp@_http._tcp.test-service-2.dns-3576.svc wheezy_tcp@_http._tcp.test-service-2.dns-3576.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3576 jessie_tcp@dns-test-service.dns-3576 jessie_udp@dns-test-service.dns-3576.svc jessie_tcp@dns-test-service.dns-3576.svc jessie_udp@_http._tcp.dns-test-service.dns-3576.svc jessie_tcp@_http._tcp.dns-test-service.dns-3576.svc]

Jan 31 10:03:29.471: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:29.474: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:29.476: INFO: Unable to read wheezy_udp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:29.478: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:29.480: INFO: Unable to read wheezy_udp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:29.482: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:29.497: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:29.500: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:29.502: INFO: Unable to read jessie_udp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:29.504: INFO: Unable to read jessie_tcp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:29.506: INFO: Unable to read jessie_udp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:29.508: INFO: Unable to read jessie_tcp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:29.522: INFO: Lookups using dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3576 wheezy_tcp@dns-test-service.dns-3576 wheezy_udp@dns-test-service.dns-3576.svc wheezy_tcp@dns-test-service.dns-3576.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3576 jessie_tcp@dns-test-service.dns-3576 jessie_udp@dns-test-service.dns-3576.svc jessie_tcp@dns-test-service.dns-3576.svc]

Jan 31 10:03:34.472: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:34.474: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:34.482: INFO: Unable to read wheezy_udp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:34.497: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:34.504: INFO: Unable to read wheezy_udp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:34.506: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:34.520: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:34.522: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:34.524: INFO: Unable to read jessie_udp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:34.526: INFO: Unable to read jessie_tcp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:34.528: INFO: Unable to read jessie_udp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:34.532: INFO: Unable to read jessie_tcp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:34.544: INFO: Lookups using dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3576 wheezy_tcp@dns-test-service.dns-3576 wheezy_udp@dns-test-service.dns-3576.svc wheezy_tcp@dns-test-service.dns-3576.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3576 jessie_tcp@dns-test-service.dns-3576 jessie_udp@dns-test-service.dns-3576.svc jessie_tcp@dns-test-service.dns-3576.svc]

Jan 31 10:03:39.472: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:39.474: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:39.476: INFO: Unable to read wheezy_udp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:39.478: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:39.480: INFO: Unable to read wheezy_udp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:39.482: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:39.497: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:39.499: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:39.501: INFO: Unable to read jessie_udp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:39.503: INFO: Unable to read jessie_tcp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:39.505: INFO: Unable to read jessie_udp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:39.507: INFO: Unable to read jessie_tcp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:39.520: INFO: Lookups using dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3576 wheezy_tcp@dns-test-service.dns-3576 wheezy_udp@dns-test-service.dns-3576.svc wheezy_tcp@dns-test-service.dns-3576.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3576 jessie_tcp@dns-test-service.dns-3576 jessie_udp@dns-test-service.dns-3576.svc jessie_tcp@dns-test-service.dns-3576.svc]

Jan 31 10:03:44.471: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:44.474: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:44.476: INFO: Unable to read wheezy_udp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:44.478: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:44.480: INFO: Unable to read wheezy_udp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:44.482: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:44.498: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:44.500: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:44.502: INFO: Unable to read jessie_udp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:44.504: INFO: Unable to read jessie_tcp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:44.507: INFO: Unable to read jessie_udp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:44.509: INFO: Unable to read jessie_tcp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:44.522: INFO: Lookups using dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3576 wheezy_tcp@dns-test-service.dns-3576 wheezy_udp@dns-test-service.dns-3576.svc wheezy_tcp@dns-test-service.dns-3576.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3576 jessie_tcp@dns-test-service.dns-3576 jessie_udp@dns-test-service.dns-3576.svc jessie_tcp@dns-test-service.dns-3576.svc]

Jan 31 10:03:49.471: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:49.473: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:49.476: INFO: Unable to read wheezy_udp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:49.478: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:49.480: INFO: Unable to read wheezy_udp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:49.482: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:49.496: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:49.498: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:49.500: INFO: Unable to read jessie_udp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:49.502: INFO: Unable to read jessie_tcp@dns-test-service.dns-3576 from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:49.504: INFO: Unable to read jessie_udp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:49.506: INFO: Unable to read jessie_tcp@dns-test-service.dns-3576.svc from pod dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae: the server could not find the requested resource (get pods dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae)
Jan 31 10:03:49.518: INFO: Lookups using dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3576 wheezy_tcp@dns-test-service.dns-3576 wheezy_udp@dns-test-service.dns-3576.svc wheezy_tcp@dns-test-service.dns-3576.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3576 jessie_tcp@dns-test-service.dns-3576 jessie_udp@dns-test-service.dns-3576.svc jessie_tcp@dns-test-service.dns-3576.svc]

Jan 31 10:03:54.521: INFO: DNS probes using dns-3576/dns-test-13accfc5-4b42-4ce5-a6da-7aecc8aa7dae succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 31 10:03:54.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3576" for this suite.

• [SLOW TEST:32.417 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":356,"completed":102,"skipped":1744,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:03:54.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6578
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6578
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Jan 31 10:03:54.823: INFO: Found 0 stateful pods, waiting for 3
Jan 31 10:04:04.827: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 31 10:04:04.827: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 31 10:04:04.827: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jan 31 10:04:04.851: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 31 10:04:14.883: INFO: Updating stateful set ss2
Jan 31 10:04:14.892: INFO: Waiting for Pod statefulset-6578/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Jan 31 10:04:24.963: INFO: Found 2 stateful pods, waiting for 3
Jan 31 10:04:34.968: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 31 10:04:34.968: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 31 10:04:34.968: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 31 10:04:34.991: INFO: Updating stateful set ss2
Jan 31 10:04:35.000: INFO: Waiting for Pod statefulset-6578/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Jan 31 10:04:45.025: INFO: Updating stateful set ss2
Jan 31 10:04:45.036: INFO: Waiting for StatefulSet statefulset-6578/ss2 to complete update
Jan 31 10:04:45.036: INFO: Waiting for Pod statefulset-6578/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 31 10:04:55.045: INFO: Deleting all statefulset in ns statefulset-6578
Jan 31 10:04:55.047: INFO: Scaling statefulset ss2 to 0
Jan 31 10:05:05.064: INFO: Waiting for statefulset status.replicas updated to 0
Jan 31 10:05:05.073: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 31 10:05:05.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6578" for this suite.

• [SLOW TEST:70.459 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":356,"completed":103,"skipped":1748,"failed":0}
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:05:05.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6842
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:05:05.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6842 create -f -'
Jan 31 10:05:06.060: INFO: stderr: ""
Jan 31 10:05:06.060: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 31 10:05:06.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6842 create -f -'
Jan 31 10:05:06.193: INFO: stderr: ""
Jan 31 10:05:06.193: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 31 10:05:07.200: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 31 10:05:07.200: INFO: Found 0 / 1
Jan 31 10:05:08.198: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 31 10:05:08.198: INFO: Found 1 / 1
Jan 31 10:05:08.198: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 31 10:05:08.200: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 31 10:05:08.200: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 31 10:05:08.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6842 describe pod agnhost-primary-kvz4l'
Jan 31 10:05:08.266: INFO: stderr: ""
Jan 31 10:05:08.266: INFO: stdout: "Name:         agnhost-primary-kvz4l\nNamespace:    kubectl-6842\nPriority:     0\nNode:         macpro-x86-1/10.221.188.31\nStart Time:   Tue, 31 Jan 2023 10:05:06 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 9eef39cc24b78993f831cd834e91e28ff3fb2fec67a13405886347c506f7c027\n              cni.projectcalico.org/podIP: 192.168.75.45/32\n              cni.projectcalico.org/podIPs: 192.168.75.45/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           192.168.75.45\nIPs:\n  IP:           192.168.75.45\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://d5bd2289dbb3287fd255ba5a1b753d5b13b9dbd61e24affe49639a905802fcc8\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 31 Jan 2023 10:05:06 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hwwz6 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-hwwz6:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-6842/agnhost-primary-kvz4l to macpro-x86-1\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.39\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jan 31 10:05:08.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6842 describe rc agnhost-primary'
Jan 31 10:05:08.318: INFO: stderr: ""
Jan 31 10:05:08.318: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6842\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-kvz4l\n"
Jan 31 10:05:08.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6842 describe service agnhost-primary'
Jan 31 10:05:08.372: INFO: stderr: ""
Jan 31 10:05:08.372: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6842\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.108.35.143\nIPs:               10.108.35.143\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.75.45:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 31 10:05:08.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6842 describe node macpro-x86-1'
Jan 31 10:05:08.459: INFO: stderr: ""
Jan 31 10:05:08.459: INFO: stdout: "Name:               macpro-x86-1\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=macpro-x86-1\n                    kubernetes.io/os=linux\n                    node-role.orka/worker-node=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.221.188.31/23\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.75.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 31 Jan 2023 09:31:46 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  macpro-x86-1\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 31 Jan 2023 10:05:02 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 31 Jan 2023 09:31:52 +0000   Tue, 31 Jan 2023 09:31:52 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 31 Jan 2023 10:04:40 +0000   Tue, 31 Jan 2023 09:31:45 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 31 Jan 2023 10:04:40 +0000   Tue, 31 Jan 2023 09:31:45 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 31 Jan 2023 10:04:40 +0000   Tue, 31 Jan 2023 09:31:45 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 31 Jan 2023 10:04:40 +0000   Tue, 31 Jan 2023 09:31:56 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.221.188.31\n  Hostname:    macpro-x86-1\nCapacity:\n  cpu:                12\n  ephemeral-storage:  976227012Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             32846488Ki\n  pods:               110\nAllocatable:\n  cpu:                12\n  ephemeral-storage:  899690812770\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             32744088Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 239e6a6f630045a990e653204d21ae7a\n  System UUID:                273d4264-b96c-e257-bb6b-a3abd075524f\n  Boot ID:                    1eb4edac-9a6a-4b39-8c0e-23dd28717bf6\n  Kernel Version:             5.6.14-300.fc32.x86_64\n  OS Image:                   Fedora CoreOS 32.20200601.3.0\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.8\n  Kubelet Version:            v1.24.6\n  Kube-Proxy Version:         v1.24.6\nPodCIDR:                      192.168.4.0/24\nPodCIDRs:                     192.168.4.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-nd7tv                                          250m (2%)     0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 kube-proxy-tmcdj                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\n  kubectl-6842                agnhost-primary-kvz4l                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  logging                     helm-charts-fluent-bit-lqp7w                               200m (1%)     200m (1%)   512Mi (1%)       512Mi (1%)     32m\n  metallb-system              speaker-9kx82                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\n  orka-monitoring             kube-prometheus-stack-prometheus-node-exporter-lmx85       100m (0%)     100m (0%)   100Mi (0%)       100Mi (0%)     30m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         23m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-e1cae5bb05d74403-zlhvz    0 (0%)        0 (0%)      0 (0%)           0 (0%)         23m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                550m (4%)   300m (2%)\n  memory             612Mi (1%)  612Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 33m                kube-proxy       \n  Normal   Starting                 33m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      33m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  33m (x2 over 33m)  kubelet          Node macpro-x86-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    33m (x2 over 33m)  kubelet          Node macpro-x86-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     33m (x2 over 33m)  kubelet          Node macpro-x86-1 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  33m                kubelet          Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           33m                node-controller  Node macpro-x86-1 event: Registered Node macpro-x86-1 in Controller\n  Normal   NodeReady                33m                kubelet          Node macpro-x86-1 status is now: NodeReady\n  Normal   Starting                 29m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      29m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  29m                kubelet          Node macpro-x86-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    29m                kubelet          Node macpro-x86-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     29m                kubelet          Node macpro-x86-1 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  29m                kubelet          Updated Node Allocatable limit across pods\n"
Jan 31 10:05:08.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6842 describe namespace kubectl-6842'
Jan 31 10:05:08.514: INFO: stderr: ""
Jan 31 10:05:08.514: INFO: stdout: "Name:         kubectl-6842\nLabels:       e2e-framework=kubectl\n              e2e-run=f4cbd94a-31f3-4619-a58f-931890206d12\n              kubernetes.io/metadata.name=kubectl-6842\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 10:05:08.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6842" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":356,"completed":104,"skipped":1748,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:05:08.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-274
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 31 10:05:08.666: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:05:10.673: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 31 10:05:10.687: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:05:12.691: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 31 10:05:12.714: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 31 10:05:12.717: INFO: Pod pod-with-poststart-http-hook still exists
Jan 31 10:05:14.718: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 31 10:05:14.725: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 31 10:05:14.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-274" for this suite.

• [SLOW TEST:6.210 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":356,"completed":105,"skipped":1785,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:05:14.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8056
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 31 10:05:24.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8056" for this suite.

• [SLOW TEST:10.138 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":356,"completed":106,"skipped":1810,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:05:24.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2574
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Jan 31 10:05:25.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2574" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":107,"skipped":1825,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:05:25.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4914
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Jan 31 10:05:35.773: INFO: 69 pods remaining
Jan 31 10:05:35.773: INFO: 69 pods has nil DeletionTimestamp
Jan 31 10:05:35.773: INFO: 
STEP: Gathering metrics
Jan 31 10:05:40.808: INFO: The status of Pod kube-controller-manager-master-3 is Running (Ready = true)
Jan 31 10:05:40.850: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 31 10:05:40.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-24m7w" in namespace "gc-4914"
Jan 31 10:05:40.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mvl5" in namespace "gc-4914"
Jan 31 10:05:40.873: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vvhs" in namespace "gc-4914"
Jan 31 10:05:40.881: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wzpz" in namespace "gc-4914"
Jan 31 10:05:40.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xnsz" in namespace "gc-4914"
Jan 31 10:05:40.900: INFO: Deleting pod "simpletest-rc-to-be-deleted-4bm9v" in namespace "gc-4914"
Jan 31 10:05:40.908: INFO: Deleting pod "simpletest-rc-to-be-deleted-4csjf" in namespace "gc-4914"
Jan 31 10:05:40.914: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fxdb" in namespace "gc-4914"
Jan 31 10:05:40.923: INFO: Deleting pod "simpletest-rc-to-be-deleted-52jfc" in namespace "gc-4914"
Jan 31 10:05:40.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-55d98" in namespace "gc-4914"
Jan 31 10:05:40.942: INFO: Deleting pod "simpletest-rc-to-be-deleted-59vtf" in namespace "gc-4914"
Jan 31 10:05:40.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pk4w" in namespace "gc-4914"
Jan 31 10:05:40.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pwcb" in namespace "gc-4914"
Jan 31 10:05:40.977: INFO: Deleting pod "simpletest-rc-to-be-deleted-5rsph" in namespace "gc-4914"
Jan 31 10:05:40.987: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tvxt" in namespace "gc-4914"
Jan 31 10:05:40.995: INFO: Deleting pod "simpletest-rc-to-be-deleted-5v6t9" in namespace "gc-4914"
Jan 31 10:05:41.004: INFO: Deleting pod "simpletest-rc-to-be-deleted-64njd" in namespace "gc-4914"
Jan 31 10:05:41.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nwx7" in namespace "gc-4914"
Jan 31 10:05:41.019: INFO: Deleting pod "simpletest-rc-to-be-deleted-77fxv" in namespace "gc-4914"
Jan 31 10:05:41.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hlrg" in namespace "gc-4914"
Jan 31 10:05:41.036: INFO: Deleting pod "simpletest-rc-to-be-deleted-7mrlb" in namespace "gc-4914"
Jan 31 10:05:41.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vzr9" in namespace "gc-4914"
Jan 31 10:05:41.060: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qk8p" in namespace "gc-4914"
Jan 31 10:05:41.075: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tsfg" in namespace "gc-4914"
Jan 31 10:05:41.085: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lgmg" in namespace "gc-4914"
Jan 31 10:05:41.094: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lsq7" in namespace "gc-4914"
Jan 31 10:05:41.103: INFO: Deleting pod "simpletest-rc-to-be-deleted-bzfwc" in namespace "gc-4914"
Jan 31 10:05:41.112: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckspl" in namespace "gc-4914"
Jan 31 10:05:41.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqqqc" in namespace "gc-4914"
Jan 31 10:05:41.129: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxtdw" in namespace "gc-4914"
Jan 31 10:05:41.139: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgk2j" in namespace "gc-4914"
Jan 31 10:05:41.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-dh865" in namespace "gc-4914"
Jan 31 10:05:41.161: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhnp9" in namespace "gc-4914"
Jan 31 10:05:41.177: INFO: Deleting pod "simpletest-rc-to-be-deleted-fk8d7" in namespace "gc-4914"
Jan 31 10:05:41.191: INFO: Deleting pod "simpletest-rc-to-be-deleted-gcj6n" in namespace "gc-4914"
Jan 31 10:05:41.203: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkgb5" in namespace "gc-4914"
Jan 31 10:05:41.221: INFO: Deleting pod "simpletest-rc-to-be-deleted-gl9mf" in namespace "gc-4914"
Jan 31 10:05:41.235: INFO: Deleting pod "simpletest-rc-to-be-deleted-glqw9" in namespace "gc-4914"
Jan 31 10:05:41.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-h95sd" in namespace "gc-4914"
Jan 31 10:05:41.258: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcgxh" in namespace "gc-4914"
Jan 31 10:05:41.266: INFO: Deleting pod "simpletest-rc-to-be-deleted-hp7pw" in namespace "gc-4914"
Jan 31 10:05:41.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqdwh" in namespace "gc-4914"
Jan 31 10:05:41.287: INFO: Deleting pod "simpletest-rc-to-be-deleted-hs6gt" in namespace "gc-4914"
Jan 31 10:05:41.301: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzgcb" in namespace "gc-4914"
Jan 31 10:05:41.330: INFO: Deleting pod "simpletest-rc-to-be-deleted-jftnf" in namespace "gc-4914"
Jan 31 10:05:41.344: INFO: Deleting pod "simpletest-rc-to-be-deleted-jsmqr" in namespace "gc-4914"
Jan 31 10:05:41.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-k7mdv" in namespace "gc-4914"
Jan 31 10:05:41.371: INFO: Deleting pod "simpletest-rc-to-be-deleted-k9k6d" in namespace "gc-4914"
Jan 31 10:05:41.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-knxbk" in namespace "gc-4914"
Jan 31 10:05:41.404: INFO: Deleting pod "simpletest-rc-to-be-deleted-kqgzd" in namespace "gc-4914"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 31 10:05:41.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4914" for this suite.

• [SLOW TEST:16.367 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":356,"completed":108,"skipped":1826,"failed":0}
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:05:41.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1848
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1540
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 31 10:05:41.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1848 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Jan 31 10:05:41.647: INFO: stderr: ""
Jan 31 10:05:41.647: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1544
Jan 31 10:05:41.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1848 delete pods e2e-test-httpd-pod'
Jan 31 10:05:49.646: INFO: stderr: ""
Jan 31 10:05:49.646: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 10:05:49.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1848" for this suite.

• [SLOW TEST:8.232 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1537
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":356,"completed":109,"skipped":1826,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:05:49.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-7998
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Deleting RuntimeClass runtimeclass-7998-delete-me
STEP: Waiting for the RuntimeClass to disappear
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 31 10:05:49.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7998" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":110,"skipped":1885,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:05:49.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8078
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 31 10:05:49.942: INFO: The status of Pod labelsupdate486e65fd-7d13-4abe-b97e-e6f35cc3d40e is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:05:51.945: INFO: The status of Pod labelsupdate486e65fd-7d13-4abe-b97e-e6f35cc3d40e is Running (Ready = true)
Jan 31 10:05:52.475: INFO: Successfully updated pod "labelsupdate486e65fd-7d13-4abe-b97e-e6f35cc3d40e"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 31 10:05:56.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8078" for this suite.

• [SLOW TEST:6.691 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":111,"skipped":1894,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:05:56.499: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-9256
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 31 10:06:10.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9256" for this suite.

• [SLOW TEST:14.155 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":112,"skipped":1906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:06:10.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9299
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9299
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9299
STEP: Waiting until pod test-pod will start running in namespace statefulset-9299
STEP: Creating statefulset with conflicting port in namespace statefulset-9299
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9299
Jan 31 10:06:12.828: INFO: Observed stateful pod in namespace: statefulset-9299, name: ss-0, uid: 55967d64-7291-4159-80aa-2a8650703d1a, status phase: Pending. Waiting for statefulset controller to delete.
Jan 31 10:06:12.839: INFO: Observed stateful pod in namespace: statefulset-9299, name: ss-0, uid: 55967d64-7291-4159-80aa-2a8650703d1a, status phase: Failed. Waiting for statefulset controller to delete.
Jan 31 10:06:12.861: INFO: Observed stateful pod in namespace: statefulset-9299, name: ss-0, uid: 55967d64-7291-4159-80aa-2a8650703d1a, status phase: Failed. Waiting for statefulset controller to delete.
Jan 31 10:06:12.864: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9299
STEP: Removing pod with conflicting port in namespace statefulset-9299
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9299 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 31 10:06:14.926: INFO: Deleting all statefulset in ns statefulset-9299
Jan 31 10:06:14.929: INFO: Scaling statefulset ss to 0
Jan 31 10:06:24.951: INFO: Waiting for statefulset status.replicas updated to 0
Jan 31 10:06:24.954: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 31 10:06:24.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9299" for this suite.

• [SLOW TEST:14.327 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":356,"completed":113,"skipped":1928,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:06:24.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2536
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-2536/configmap-test-24126ec7-2c52-414a-8b75-d0773f48f69a
STEP: Creating a pod to test consume configMaps
Jan 31 10:06:25.115: INFO: Waiting up to 5m0s for pod "pod-configmaps-4ea02d84-fbf0-45a1-9362-4dac8305af8e" in namespace "configmap-2536" to be "Succeeded or Failed"
Jan 31 10:06:25.118: INFO: Pod "pod-configmaps-4ea02d84-fbf0-45a1-9362-4dac8305af8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.425876ms
Jan 31 10:06:27.122: INFO: Pod "pod-configmaps-4ea02d84-fbf0-45a1-9362-4dac8305af8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006652138s
Jan 31 10:06:29.128: INFO: Pod "pod-configmaps-4ea02d84-fbf0-45a1-9362-4dac8305af8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01283834s
STEP: Saw pod success
Jan 31 10:06:29.128: INFO: Pod "pod-configmaps-4ea02d84-fbf0-45a1-9362-4dac8305af8e" satisfied condition "Succeeded or Failed"
Jan 31 10:06:29.130: INFO: Trying to get logs from node macpro-x86-2 pod pod-configmaps-4ea02d84-fbf0-45a1-9362-4dac8305af8e container env-test: <nil>
STEP: delete the pod
Jan 31 10:06:29.143: INFO: Waiting for pod pod-configmaps-4ea02d84-fbf0-45a1-9362-4dac8305af8e to disappear
Jan 31 10:06:29.145: INFO: Pod pod-configmaps-4ea02d84-fbf0-45a1-9362-4dac8305af8e no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 31 10:06:29.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2536" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":356,"completed":114,"skipped":1931,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:06:29.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2193
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:06:29.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2193" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":356,"completed":115,"skipped":1937,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:06:29.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7408
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 10:06:29.618: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 10:06:32.641: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:06:32.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:06:35.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7408" for this suite.
STEP: Destroying namespace "webhook-7408-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.556 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":356,"completed":116,"skipped":1998,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:06:35.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5256
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:06:35.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 31 10:06:35.992: INFO: The status of Pod pod-exec-websocket-39fc3139-cd7e-4761-bdc6-c26985d75300 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:06:37.996: INFO: The status of Pod pod-exec-websocket-39fc3139-cd7e-4761-bdc6-c26985d75300 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 31 10:06:38.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5256" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":356,"completed":117,"skipped":2000,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:06:38.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2260
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-2260
STEP: creating service affinity-nodeport-transition in namespace services-2260
STEP: creating replication controller affinity-nodeport-transition in namespace services-2260
I0131 10:06:38.223701      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2260, replica count: 3
I0131 10:06:41.274603      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 31 10:06:41.286: INFO: Creating new exec pod
Jan 31 10:06:44.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-2260 exec execpod-affinityc2bwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 31 10:06:44.413: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 31 10:06:44.413: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:06:44.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-2260 exec execpod-affinityc2bwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.195.161 80'
Jan 31 10:06:44.496: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.195.161 80\nConnection to 10.99.195.161 80 port [tcp/http] succeeded!\n"
Jan 31 10:06:44.496: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:06:44.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-2260 exec execpod-affinityc2bwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.31 30366'
Jan 31 10:06:44.592: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.31 30366\nConnection to 10.221.188.31 30366 port [tcp/*] succeeded!\n"
Jan 31 10:06:44.592: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:06:44.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-2260 exec execpod-affinityc2bwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.32 30366'
Jan 31 10:06:44.683: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.32 30366\nConnection to 10.221.188.32 30366 port [tcp/*] succeeded!\n"
Jan 31 10:06:44.683: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:06:44.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-2260 exec execpod-affinityc2bwm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.221.188.31:30366/ ; done'
Jan 31 10:06:44.840: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n"
Jan 31 10:06:44.840: INFO: stdout: "\naffinity-nodeport-transition-sf8rn\naffinity-nodeport-transition-sf8rn\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-7svsg\naffinity-nodeport-transition-sf8rn\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-sf8rn\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-sf8rn\naffinity-nodeport-transition-7svsg\naffinity-nodeport-transition-sf8rn\naffinity-nodeport-transition-7svsg\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-sf8rn\naffinity-nodeport-transition-7svsg\naffinity-nodeport-transition-7svsg"
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-sf8rn
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-sf8rn
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-7svsg
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-sf8rn
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-sf8rn
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-sf8rn
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-7svsg
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-sf8rn
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-7svsg
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-sf8rn
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-7svsg
Jan 31 10:06:44.840: INFO: Received response from host: affinity-nodeport-transition-7svsg
Jan 31 10:06:44.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-2260 exec execpod-affinityc2bwm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.221.188.31:30366/ ; done'
Jan 31 10:06:44.982: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30366/\n"
Jan 31 10:06:44.982: INFO: stdout: "\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-pfff7\naffinity-nodeport-transition-pfff7"
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Received response from host: affinity-nodeport-transition-pfff7
Jan 31 10:06:44.982: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2260, will wait for the garbage collector to delete the pods
Jan 31 10:06:45.053: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.152879ms
Jan 31 10:06:45.154: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.621529ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 10:06:46.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2260" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:8.830 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":118,"skipped":2009,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:06:46.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4514
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 31 10:06:47.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4514" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":356,"completed":119,"skipped":2011,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:06:47.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5840
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5840
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5840
STEP: creating replication controller externalsvc in namespace services-5840
I0131 10:06:47.230583      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5840, replica count: 2
I0131 10:06:50.282404      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jan 31 10:06:50.302: INFO: Creating new exec pod
Jan 31 10:06:52.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-5840 exec execpodjddzp -- /bin/sh -x -c nslookup clusterip-service.services-5840.svc.cluster.local'
Jan 31 10:06:52.440: INFO: stderr: "+ nslookup clusterip-service.services-5840.svc.cluster.local\n"
Jan 31 10:06:52.440: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-5840.svc.cluster.local\tcanonical name = externalsvc.services-5840.svc.cluster.local.\nName:\texternalsvc.services-5840.svc.cluster.local\nAddress: 10.96.40.210\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5840, will wait for the garbage collector to delete the pods
Jan 31 10:06:52.501: INFO: Deleting ReplicationController externalsvc took: 6.970473ms
Jan 31 10:06:52.602: INFO: Terminating ReplicationController externalsvc pods took: 100.804613ms
Jan 31 10:06:54.836: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 10:06:54.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5840" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:7.816 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":356,"completed":120,"skipped":2022,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:06:54.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6491
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:06:54.997: INFO: Creating simple deployment test-new-deployment
Jan 31 10:06:55.039: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 31 10:06:57.083: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-6491  7d7f4e94-719e-4fb0-8b8b-9fd89e1854da 18120 3 2023-01-31 10:06:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-31 10:06:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 10:06:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052666a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-31 10:06:55 +0000 UTC,LastTransitionTime:2023-01-31 10:06:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-55df494869" has successfully progressed.,LastUpdateTime:2023-01-31 10:06:55 +0000 UTC,LastTransitionTime:2023-01-31 10:06:55 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 31 10:06:57.090: INFO: New ReplicaSet "test-new-deployment-55df494869" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-55df494869  deployment-6491  8927a63c-7bfb-412f-b763-d3ce83b2b226 18123 3 2023-01-31 10:06:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 7d7f4e94-719e-4fb0-8b8b-9fd89e1854da 0xc005266b27 0xc005266b28}] []  [{kube-controller-manager Update apps/v1 2023-01-31 10:06:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d7f4e94-719e-4fb0-8b8b-9fd89e1854da\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 10:06:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005266bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 31 10:06:57.102: INFO: Pod "test-new-deployment-55df494869-rhnjs" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-rhnjs test-new-deployment-55df494869- deployment-6491  6ff78f69-2464-4a0e-b2ad-6ea21f10d739 18125 0 2023-01-31 10:06:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-55df494869 8927a63c-7bfb-412f-b763-d3ce83b2b226 0xc005266fb7 0xc005266fb8}] []  [{kube-controller-manager Update v1 2023-01-31 10:06:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8927a63c-7bfb-412f-b763-d3ce83b2b226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rrn4w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rrn4w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 10:06:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 10:06:57.102: INFO: Pod "test-new-deployment-55df494869-tmjsf" is available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-tmjsf test-new-deployment-55df494869- deployment-6491  4b21f376-7bab-4372-a4d7-f99339836063 18115 0 2023-01-31 10:06:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:3a91a1492098292f679c3f5bd0f6fb45c4827d09e027368eebe146d87f2327de cni.projectcalico.org/podIP:192.168.38.216/32 cni.projectcalico.org/podIPs:192.168.38.216/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-55df494869 8927a63c-7bfb-412f-b763-d3ce83b2b226 0xc005267120 0xc005267121}] []  [{calico Update v1 2023-01-31 10:06:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-31 10:06:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8927a63c-7bfb-412f-b763-d3ce83b2b226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 10:06:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.38.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6qdh6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6qdh6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 10:06:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 10:06:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 10:06:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 10:06:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.32,PodIP:192.168.38.216,StartTime:2023-01-31 10:06:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 10:06:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b790be0a812ce49d203cec397bfdee6aed4c7bdeb9775013d092a576b05c7afc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.38.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 31 10:06:57.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6491" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":356,"completed":121,"skipped":2055,"failed":0}
SSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:06:57.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-7210
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan 31 10:06:57.255: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 31 10:07:57.293: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:07:57.295: INFO: Starting informer...
STEP: Starting pod...
Jan 31 10:07:57.510: INFO: Pod is running on macpro-x86-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jan 31 10:07:57.533: INFO: Pod wasn't evicted. Proceeding
Jan 31 10:07:57.533: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jan 31 10:09:12.579: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:188
Jan 31 10:09:12.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7210" for this suite.

• [SLOW TEST:135.477 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":356,"completed":122,"skipped":2058,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:09:12.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8130
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 31 10:09:12.747: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8130  4577a996-28d9-4d65-a0f5-7464d33b478c 18583 0 2023-01-31 10:09:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-31 10:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 31 10:09:12.747: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8130  4577a996-28d9-4d65-a0f5-7464d33b478c 18583 0 2023-01-31 10:09:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-31 10:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 31 10:09:12.752: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8130  4577a996-28d9-4d65-a0f5-7464d33b478c 18584 0 2023-01-31 10:09:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-31 10:09:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 31 10:09:12.752: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8130  4577a996-28d9-4d65-a0f5-7464d33b478c 18584 0 2023-01-31 10:09:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-31 10:09:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 31 10:09:12.757: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8130  4577a996-28d9-4d65-a0f5-7464d33b478c 18585 0 2023-01-31 10:09:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-31 10:09:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 31 10:09:12.757: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8130  4577a996-28d9-4d65-a0f5-7464d33b478c 18585 0 2023-01-31 10:09:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-31 10:09:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 31 10:09:12.761: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8130  4577a996-28d9-4d65-a0f5-7464d33b478c 18586 0 2023-01-31 10:09:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-31 10:09:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 31 10:09:12.761: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8130  4577a996-28d9-4d65-a0f5-7464d33b478c 18586 0 2023-01-31 10:09:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-31 10:09:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 31 10:09:12.763: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8130  f7736c3d-2d71-41f2-bd2b-809cb0b97d5e 18587 0 2023-01-31 10:09:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-31 10:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 31 10:09:12.763: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8130  f7736c3d-2d71-41f2-bd2b-809cb0b97d5e 18587 0 2023-01-31 10:09:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-31 10:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 31 10:09:22.773: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8130  f7736c3d-2d71-41f2-bd2b-809cb0b97d5e 18624 0 2023-01-31 10:09:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-31 10:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 31 10:09:22.773: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8130  f7736c3d-2d71-41f2-bd2b-809cb0b97d5e 18624 0 2023-01-31 10:09:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-31 10:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 31 10:09:32.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8130" for this suite.

• [SLOW TEST:20.189 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":356,"completed":123,"skipped":2066,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:09:32.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9803
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 10:09:33.558: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 10:09:36.580: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:09:36.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5080-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:09:39.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9803" for this suite.
STEP: Destroying namespace "webhook-9803-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.997 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":356,"completed":124,"skipped":2088,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:09:39.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8531
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 31 10:09:39.929: INFO: Waiting up to 5m0s for pod "downward-api-c0cbfeef-840f-4df2-87e2-3e3b6292bafb" in namespace "downward-api-8531" to be "Succeeded or Failed"
Jan 31 10:09:39.934: INFO: Pod "downward-api-c0cbfeef-840f-4df2-87e2-3e3b6292bafb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.13174ms
Jan 31 10:09:41.941: INFO: Pod "downward-api-c0cbfeef-840f-4df2-87e2-3e3b6292bafb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012096547s
Jan 31 10:09:43.947: INFO: Pod "downward-api-c0cbfeef-840f-4df2-87e2-3e3b6292bafb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017835896s
STEP: Saw pod success
Jan 31 10:09:43.947: INFO: Pod "downward-api-c0cbfeef-840f-4df2-87e2-3e3b6292bafb" satisfied condition "Succeeded or Failed"
Jan 31 10:09:43.950: INFO: Trying to get logs from node macpro-x86-1 pod downward-api-c0cbfeef-840f-4df2-87e2-3e3b6292bafb container dapi-container: <nil>
STEP: delete the pod
Jan 31 10:09:43.972: INFO: Waiting for pod downward-api-c0cbfeef-840f-4df2-87e2-3e3b6292bafb to disappear
Jan 31 10:09:43.975: INFO: Pod downward-api-c0cbfeef-840f-4df2-87e2-3e3b6292bafb no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 31 10:09:43.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8531" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":356,"completed":125,"skipped":2099,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:09:43.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4068
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 10:09:44.780: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 10:09:47.802: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jan 31 10:09:47.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:09:47.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4068" for this suite.
STEP: Destroying namespace "webhook-4068-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":356,"completed":126,"skipped":2104,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:09:47.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-8645
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 31 10:09:48.047: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 31 10:10:48.086: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Jan 31 10:10:48.114: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 31 10:10:48.129: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 31 10:10:48.167: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 31 10:10:48.177: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 31 10:11:06.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8645" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:78.369 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":356,"completed":127,"skipped":2108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:11:06.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9862
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:11:06.415: INFO: The status of Pod server-envvars-d6cdff99-0f46-4f9e-87c1-5676c6aab329 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:11:08.419: INFO: The status of Pod server-envvars-d6cdff99-0f46-4f9e-87c1-5676c6aab329 is Running (Ready = true)
Jan 31 10:11:08.445: INFO: Waiting up to 5m0s for pod "client-envvars-e8d73521-ecc0-44b2-91da-78b26635356d" in namespace "pods-9862" to be "Succeeded or Failed"
Jan 31 10:11:08.452: INFO: Pod "client-envvars-e8d73521-ecc0-44b2-91da-78b26635356d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.922725ms
Jan 31 10:11:10.459: INFO: Pod "client-envvars-e8d73521-ecc0-44b2-91da-78b26635356d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014847453s
Jan 31 10:11:12.463: INFO: Pod "client-envvars-e8d73521-ecc0-44b2-91da-78b26635356d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018913895s
STEP: Saw pod success
Jan 31 10:11:12.463: INFO: Pod "client-envvars-e8d73521-ecc0-44b2-91da-78b26635356d" satisfied condition "Succeeded or Failed"
Jan 31 10:11:12.466: INFO: Trying to get logs from node macpro-x86-1 pod client-envvars-e8d73521-ecc0-44b2-91da-78b26635356d container env3cont: <nil>
STEP: delete the pod
Jan 31 10:11:12.481: INFO: Waiting for pod client-envvars-e8d73521-ecc0-44b2-91da-78b26635356d to disappear
Jan 31 10:11:12.485: INFO: Pod client-envvars-e8d73521-ecc0-44b2-91da-78b26635356d no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 31 10:11:12.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9862" for this suite.

• [SLOW TEST:6.214 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":356,"completed":128,"skipped":2165,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:11:12.493: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9180
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:11:12.636: INFO: The status of Pod busybox-readonly-fsa8252c12-a7e0-44c7-a626-e4023c70e549 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:11:14.644: INFO: The status of Pod busybox-readonly-fsa8252c12-a7e0-44c7-a626-e4023c70e549 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 31 10:11:14.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9180" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":129,"skipped":2191,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:11:14.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5860
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
Jan 31 10:11:14.789: INFO: Creating simple deployment test-deployment-mhwvm
Jan 31 10:11:14.800: INFO: deployment "test-deployment-mhwvm" doesn't have the required revision set
STEP: Getting /status
Jan 31 10:11:16.812: INFO: Deployment test-deployment-mhwvm has Conditions: [{Available True 2023-01-31 10:11:16 +0000 UTC 2023-01-31 10:11:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-31 10:11:16 +0000 UTC 2023-01-31 10:11:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mhwvm-688c4d6789" has successfully progressed.}]
STEP: updating Deployment Status
Jan 31 10:11:16.819: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 10, 11, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 10, 11, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 10, 11, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 10, 11, 14, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-mhwvm-688c4d6789\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Jan 31 10:11:16.821: INFO: Observed &Deployment event: ADDED
Jan 31 10:11:16.821: INFO: Observed Deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-31 10:11:14 +0000 UTC 2023-01-31 10:11:14 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mhwvm-688c4d6789"}
Jan 31 10:11:16.821: INFO: Observed &Deployment event: MODIFIED
Jan 31 10:11:16.821: INFO: Observed Deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-31 10:11:14 +0000 UTC 2023-01-31 10:11:14 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mhwvm-688c4d6789"}
Jan 31 10:11:16.821: INFO: Observed Deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-31 10:11:14 +0000 UTC 2023-01-31 10:11:14 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 31 10:11:16.821: INFO: Observed &Deployment event: MODIFIED
Jan 31 10:11:16.821: INFO: Observed Deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-31 10:11:14 +0000 UTC 2023-01-31 10:11:14 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 31 10:11:16.821: INFO: Observed Deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-31 10:11:14 +0000 UTC 2023-01-31 10:11:14 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mhwvm-688c4d6789" is progressing.}
Jan 31 10:11:16.821: INFO: Observed &Deployment event: MODIFIED
Jan 31 10:11:16.821: INFO: Observed Deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-31 10:11:16 +0000 UTC 2023-01-31 10:11:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 31 10:11:16.821: INFO: Observed Deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-31 10:11:16 +0000 UTC 2023-01-31 10:11:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mhwvm-688c4d6789" has successfully progressed.}
Jan 31 10:11:16.821: INFO: Observed &Deployment event: MODIFIED
Jan 31 10:11:16.821: INFO: Observed Deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-31 10:11:16 +0000 UTC 2023-01-31 10:11:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 31 10:11:16.821: INFO: Observed Deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-31 10:11:16 +0000 UTC 2023-01-31 10:11:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mhwvm-688c4d6789" has successfully progressed.}
Jan 31 10:11:16.821: INFO: Found Deployment test-deployment-mhwvm in namespace deployment-5860 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 31 10:11:16.821: INFO: Deployment test-deployment-mhwvm has an updated status
STEP: patching the Statefulset Status
Jan 31 10:11:16.821: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 31 10:11:16.826: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Jan 31 10:11:16.827: INFO: Observed &Deployment event: ADDED
Jan 31 10:11:16.827: INFO: Observed deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-31 10:11:14 +0000 UTC 2023-01-31 10:11:14 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mhwvm-688c4d6789"}
Jan 31 10:11:16.827: INFO: Observed &Deployment event: MODIFIED
Jan 31 10:11:16.827: INFO: Observed deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-31 10:11:14 +0000 UTC 2023-01-31 10:11:14 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mhwvm-688c4d6789"}
Jan 31 10:11:16.827: INFO: Observed deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-31 10:11:14 +0000 UTC 2023-01-31 10:11:14 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 31 10:11:16.827: INFO: Observed &Deployment event: MODIFIED
Jan 31 10:11:16.827: INFO: Observed deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-31 10:11:14 +0000 UTC 2023-01-31 10:11:14 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 31 10:11:16.827: INFO: Observed deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-31 10:11:14 +0000 UTC 2023-01-31 10:11:14 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mhwvm-688c4d6789" is progressing.}
Jan 31 10:11:16.827: INFO: Observed &Deployment event: MODIFIED
Jan 31 10:11:16.827: INFO: Observed deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-31 10:11:16 +0000 UTC 2023-01-31 10:11:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 31 10:11:16.827: INFO: Observed deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-31 10:11:16 +0000 UTC 2023-01-31 10:11:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mhwvm-688c4d6789" has successfully progressed.}
Jan 31 10:11:16.827: INFO: Observed &Deployment event: MODIFIED
Jan 31 10:11:16.827: INFO: Observed deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-31 10:11:16 +0000 UTC 2023-01-31 10:11:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 31 10:11:16.827: INFO: Observed deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-31 10:11:16 +0000 UTC 2023-01-31 10:11:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mhwvm-688c4d6789" has successfully progressed.}
Jan 31 10:11:16.827: INFO: Observed deployment test-deployment-mhwvm in namespace deployment-5860 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 31 10:11:16.827: INFO: Observed &Deployment event: MODIFIED
Jan 31 10:11:16.827: INFO: Found deployment test-deployment-mhwvm in namespace deployment-5860 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 31 10:11:16.827: INFO: Deployment test-deployment-mhwvm has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 31 10:11:16.833: INFO: Deployment "test-deployment-mhwvm":
&Deployment{ObjectMeta:{test-deployment-mhwvm  deployment-5860  fb4a9d43-a88d-4d9b-98cc-c08f4e58e45a 19289 1 2023-01-31 10:11:14 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2023-01-31 10:11:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-31 10:11:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-31 10:11:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004224c88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 31 10:11:16.840: INFO: New ReplicaSet "test-deployment-mhwvm-688c4d6789" of Deployment "test-deployment-mhwvm":
&ReplicaSet{ObjectMeta:{test-deployment-mhwvm-688c4d6789  deployment-5860  e11c6106-5af7-425a-b614-319253f2303c 19281 1 2023-01-31 10:11:14 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-mhwvm fb4a9d43-a88d-4d9b-98cc-c08f4e58e45a 0xc004225040 0xc004225041}] []  [{kube-controller-manager Update apps/v1 2023-01-31 10:11:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb4a9d43-a88d-4d9b-98cc-c08f4e58e45a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 10:11:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 688c4d6789,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042250e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 31 10:11:16.843: INFO: Pod "test-deployment-mhwvm-688c4d6789-nlxgm" is available:
&Pod{ObjectMeta:{test-deployment-mhwvm-688c4d6789-nlxgm test-deployment-mhwvm-688c4d6789- deployment-5860  2044df0f-afb3-4a85-87b1-f51041710a1d 19280 0 2023-01-31 10:11:14 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[cni.projectcalico.org/containerID:2c7607e0a5e2dfe158552f37a5611c52e70147e2366fb5b14468491932f23aea cni.projectcalico.org/podIP:192.168.38.248/32 cni.projectcalico.org/podIPs:192.168.38.248/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-mhwvm-688c4d6789 e11c6106-5af7-425a-b614-319253f2303c 0xc0058f89f0 0xc0058f89f1}] []  [{kube-controller-manager Update v1 2023-01-31 10:11:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e11c6106-5af7-425a-b614-319253f2303c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 10:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 10:11:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.38.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lk228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lk228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 10:11:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 10:11:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 10:11:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 10:11:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.32,PodIP:192.168.38.248,StartTime:2023-01-31 10:11:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 10:11:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4c6eba64a250ae35aace0238d40d54ba221c1d243cada141458a666284db120f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.38.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 31 10:11:16.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5860" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":356,"completed":130,"skipped":2208,"failed":0}
S
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:11:16.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-8722
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 31 10:17:01.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8722" for this suite.

• [SLOW TEST:344.176 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":356,"completed":131,"skipped":2209,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:17:01.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7473
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:17:01.172: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-15a7ffa4-2966-487d-a6ee-935bb134269a" in namespace "security-context-test-7473" to be "Succeeded or Failed"
Jan 31 10:17:01.178: INFO: Pod "alpine-nnp-false-15a7ffa4-2966-487d-a6ee-935bb134269a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.444136ms
Jan 31 10:17:03.181: INFO: Pod "alpine-nnp-false-15a7ffa4-2966-487d-a6ee-935bb134269a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009030043s
Jan 31 10:17:05.187: INFO: Pod "alpine-nnp-false-15a7ffa4-2966-487d-a6ee-935bb134269a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014667187s
Jan 31 10:17:07.191: INFO: Pod "alpine-nnp-false-15a7ffa4-2966-487d-a6ee-935bb134269a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019397796s
Jan 31 10:17:07.191: INFO: Pod "alpine-nnp-false-15a7ffa4-2966-487d-a6ee-935bb134269a" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 31 10:17:07.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7473" for this suite.

• [SLOW TEST:6.192 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:298
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":132,"skipped":2258,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:17:07.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1044
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 31 10:17:07.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1044 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 31 10:17:07.409: INFO: stderr: ""
Jan 31 10:17:07.409: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jan 31 10:17:07.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1044 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 31 10:17:08.541: INFO: stderr: ""
Jan 31 10:17:08.541: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 31 10:17:08.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-1044 delete pods e2e-test-httpd-pod'
Jan 31 10:17:11.549: INFO: stderr: ""
Jan 31 10:17:11.549: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 10:17:11.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1044" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":356,"completed":133,"skipped":2285,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity 
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:17:11.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename csistoragecapacity
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in csistoragecapacity-1470
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/storage.k8s.io
STEP: getting /apis/storage.k8s.io/v1
STEP: creating
STEP: watching
Jan 31 10:17:11.696: INFO: starting watch
STEP: getting
STEP: listing in namespace
STEP: listing across namespaces
STEP: patching
STEP: updating
Jan 31 10:17:11.714: INFO: waiting for watch events with expected annotations in namespace
Jan 31 10:17:11.714: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:188
Jan 31 10:17:11.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-1470" for this suite.
•{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","total":356,"completed":134,"skipped":2296,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:17:11.741: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1967
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 31 10:17:15.911: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 31 10:17:15.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1967" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":356,"completed":135,"skipped":2306,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:17:15.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7298
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:17:16.067: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 31 10:17:16.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:17:16.076: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Jan 31 10:17:16.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:17:16.098: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 10:17:17.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:17:17.103: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 10:17:18.105: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 10:17:18.105: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 31 10:17:18.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 10:17:18.126: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 31 10:17:19.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:17:19.131: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 31 10:17:19.142: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:17:19.142: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 10:17:20.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:17:20.146: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 10:17:21.147: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:17:21.147: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 10:17:22.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 10:17:22.146: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7298, will wait for the garbage collector to delete the pods
Jan 31 10:17:22.207: INFO: Deleting DaemonSet.extensions daemon-set took: 6.001174ms
Jan 31 10:17:22.307: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.237327ms
Jan 31 10:17:24.715: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:17:24.715: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 31 10:17:24.717: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20319"},"items":null}

Jan 31 10:17:24.719: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20319"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 31 10:17:24.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7298" for this suite.

• [SLOW TEST:8.822 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":356,"completed":136,"skipped":2312,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:17:24.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7672
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 31 10:17:24.893: INFO: Waiting up to 5m0s for pod "downward-api-e002a6cf-6ed5-4f8d-b7cc-9cce7d11c6c1" in namespace "downward-api-7672" to be "Succeeded or Failed"
Jan 31 10:17:24.897: INFO: Pod "downward-api-e002a6cf-6ed5-4f8d-b7cc-9cce7d11c6c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051957ms
Jan 31 10:17:26.903: INFO: Pod "downward-api-e002a6cf-6ed5-4f8d-b7cc-9cce7d11c6c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009965944s
Jan 31 10:17:28.911: INFO: Pod "downward-api-e002a6cf-6ed5-4f8d-b7cc-9cce7d11c6c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017619968s
STEP: Saw pod success
Jan 31 10:17:28.911: INFO: Pod "downward-api-e002a6cf-6ed5-4f8d-b7cc-9cce7d11c6c1" satisfied condition "Succeeded or Failed"
Jan 31 10:17:28.919: INFO: Trying to get logs from node macpro-x86-1 pod downward-api-e002a6cf-6ed5-4f8d-b7cc-9cce7d11c6c1 container dapi-container: <nil>
STEP: delete the pod
Jan 31 10:17:28.942: INFO: Waiting for pod downward-api-e002a6cf-6ed5-4f8d-b7cc-9cce7d11c6c1 to disappear
Jan 31 10:17:28.946: INFO: Pod downward-api-e002a6cf-6ed5-4f8d-b7cc-9cce7d11c6c1 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 31 10:17:28.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7672" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":356,"completed":137,"skipped":2333,"failed":0}
S
------------------------------
[sig-apps] Job 
  should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:17:28.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7835
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensure pods equal to paralellism count is attached to the job
STEP: patching /status
STEP: updating /status
STEP: get /status
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 31 10:17:31.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7835" for this suite.
•{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","total":356,"completed":138,"skipped":2334,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:17:31.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9600
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9600
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9600
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9600
Jan 31 10:17:31.277: INFO: Found 0 stateful pods, waiting for 1
Jan 31 10:17:41.286: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 31 10:17:41.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-9600 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 31 10:17:41.428: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 31 10:17:41.428: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 31 10:17:41.428: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 31 10:17:41.435: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 31 10:17:51.444: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 31 10:17:51.444: INFO: Waiting for statefulset status.replicas updated to 0
Jan 31 10:17:51.460: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999852s
Jan 31 10:17:52.464: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994853567s
Jan 31 10:17:53.476: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989601219s
Jan 31 10:17:54.481: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.978640084s
Jan 31 10:17:55.485: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.973272815s
Jan 31 10:17:56.490: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.969685006s
Jan 31 10:17:57.494: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.964764384s
Jan 31 10:17:58.498: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.960591755s
Jan 31 10:17:59.504: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.95661264s
Jan 31 10:18:00.509: INFO: Verifying statefulset ss doesn't scale past 1 for another 951.23259ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9600
Jan 31 10:18:01.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-9600 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 31 10:18:01.634: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 31 10:18:01.634: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 31 10:18:01.634: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 31 10:18:01.637: INFO: Found 1 stateful pods, waiting for 3
Jan 31 10:18:11.642: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 31 10:18:11.642: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 31 10:18:11.642: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 31 10:18:11.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-9600 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 31 10:18:11.738: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 31 10:18:11.738: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 31 10:18:11.738: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 31 10:18:11.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-9600 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 31 10:18:11.843: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 31 10:18:11.843: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 31 10:18:11.843: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 31 10:18:11.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-9600 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 31 10:18:11.939: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 31 10:18:11.939: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 31 10:18:11.939: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 31 10:18:11.939: INFO: Waiting for statefulset status.replicas updated to 0
Jan 31 10:18:11.943: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 31 10:18:21.955: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 31 10:18:21.955: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 31 10:18:21.955: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 31 10:18:21.968: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999835s
Jan 31 10:18:22.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99557871s
Jan 31 10:18:23.979: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99094242s
Jan 31 10:18:24.987: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985555856s
Jan 31 10:18:25.991: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977160992s
Jan 31 10:18:26.998: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973186771s
Jan 31 10:18:28.002: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.966489694s
Jan 31 10:18:29.007: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.962459336s
Jan 31 10:18:30.013: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.956826626s
Jan 31 10:18:31.032: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.182913ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9600
Jan 31 10:18:32.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-9600 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 31 10:18:32.127: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 31 10:18:32.127: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 31 10:18:32.127: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 31 10:18:32.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-9600 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 31 10:18:32.244: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 31 10:18:32.244: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 31 10:18:32.244: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 31 10:18:32.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-9600 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 31 10:18:32.329: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 31 10:18:32.329: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 31 10:18:32.329: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 31 10:18:32.329: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 31 10:18:42.353: INFO: Deleting all statefulset in ns statefulset-9600
Jan 31 10:18:42.355: INFO: Scaling statefulset ss to 0
Jan 31 10:18:42.364: INFO: Waiting for statefulset status.replicas updated to 0
Jan 31 10:18:42.366: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 31 10:18:42.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9600" for this suite.

• [SLOW TEST:71.272 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":356,"completed":139,"skipped":2358,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:18:42.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7948
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-7948
STEP: creating service affinity-clusterip in namespace services-7948
STEP: creating replication controller affinity-clusterip in namespace services-7948
I0131 10:18:42.554481      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7948, replica count: 3
I0131 10:18:45.606411      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 31 10:18:45.612: INFO: Creating new exec pod
Jan 31 10:18:48.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-7948 exec execpod-affinity4fb75 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 31 10:18:48.735: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 31 10:18:48.735: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:18:48.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-7948 exec execpod-affinity4fb75 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.1.93 80'
Jan 31 10:18:48.834: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.1.93 80\nConnection to 10.99.1.93 80 port [tcp/http] succeeded!\n"
Jan 31 10:18:48.834: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:18:48.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-7948 exec execpod-affinity4fb75 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.1.93:80/ ; done'
Jan 31 10:18:48.978: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.1.93:80/\n"
Jan 31 10:18:48.978: INFO: stdout: "\naffinity-clusterip-cddgc\naffinity-clusterip-cddgc\naffinity-clusterip-cddgc\naffinity-clusterip-cddgc\naffinity-clusterip-cddgc\naffinity-clusterip-cddgc\naffinity-clusterip-cddgc\naffinity-clusterip-cddgc\naffinity-clusterip-cddgc\naffinity-clusterip-cddgc\naffinity-clusterip-cddgc\naffinity-clusterip-cddgc\naffinity-clusterip-cddgc\naffinity-clusterip-cddgc\naffinity-clusterip-cddgc\naffinity-clusterip-cddgc"
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Received response from host: affinity-clusterip-cddgc
Jan 31 10:18:48.978: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7948, will wait for the garbage collector to delete the pods
Jan 31 10:18:49.049: INFO: Deleting ReplicationController affinity-clusterip took: 5.293071ms
Jan 31 10:18:49.150: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.774796ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 10:18:50.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7948" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:8.481 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":140,"skipped":2361,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:18:50.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4901
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Service
STEP: watching for the Service to be added
Jan 31 10:18:51.023: INFO: Found Service test-service-jnlrm in namespace services-4901 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 31 10:18:51.023: INFO: Service test-service-jnlrm created
STEP: Getting /status
Jan 31 10:18:51.026: INFO: Service test-service-jnlrm has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jan 31 10:18:51.037: INFO: observed Service test-service-jnlrm in namespace services-4901 with annotations: map[] & LoadBalancer: {[]}
Jan 31 10:18:51.037: INFO: Found Service test-service-jnlrm in namespace services-4901 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 31 10:18:51.037: INFO: Service test-service-jnlrm has service status patched
STEP: updating the ServiceStatus
Jan 31 10:18:51.063: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jan 31 10:18:51.066: INFO: Observed Service test-service-jnlrm in namespace services-4901 with annotations: map[] & Conditions: {[]}
Jan 31 10:18:51.066: INFO: Observed event: &Service{ObjectMeta:{test-service-jnlrm  services-4901  94e4a843-7e1f-4a0e-bd97-def12f8fc68a 20975 0 2023-01-31 10:18:51 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2023-01-31 10:18:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-31 10:18:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.100.216.233,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.100.216.233],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 31 10:18:51.066: INFO: Observed event: &Service{ObjectMeta:{test-service-jnlrm  services-4901  94e4a843-7e1f-4a0e-bd97-def12f8fc68a 20977 0 2023-01-31 10:18:51 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2023-01-31 10:18:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-31 10:18:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.100.216.233,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.100.216.233],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{},},Conditions:[]Condition{},},}
Jan 31 10:18:51.066: INFO: Found Service test-service-jnlrm in namespace services-4901 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 31 10:18:51.066: INFO: Service test-service-jnlrm has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jan 31 10:18:51.074: INFO: observed Service test-service-jnlrm in namespace services-4901 with labels: map[test-service-static:true]
Jan 31 10:18:51.074: INFO: observed Service test-service-jnlrm in namespace services-4901 with labels: map[test-service-static:true]
Jan 31 10:18:51.074: INFO: observed Service test-service-jnlrm in namespace services-4901 with labels: map[test-service-static:true]
Jan 31 10:18:51.074: INFO: observed Service test-service-jnlrm in namespace services-4901 with labels: map[test-service-static:true]
Jan 31 10:18:51.074: INFO: Found Service test-service-jnlrm in namespace services-4901 with labels: map[test-service:patched test-service-static:true]
Jan 31 10:18:51.074: INFO: Service test-service-jnlrm patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jan 31 10:18:51.090: INFO: Observed event: ADDED
Jan 31 10:18:51.090: INFO: Observed event: MODIFIED
Jan 31 10:18:51.090: INFO: Observed event: MODIFIED
Jan 31 10:18:51.090: INFO: Observed event: MODIFIED
Jan 31 10:18:51.091: INFO: Observed event: MODIFIED
Jan 31 10:18:51.091: INFO: Found Service test-service-jnlrm in namespace services-4901 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 31 10:18:51.091: INFO: Service test-service-jnlrm deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 10:18:51.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4901" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":356,"completed":141,"skipped":2514,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:18:51.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7997
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's command
Jan 31 10:18:51.227: INFO: Waiting up to 5m0s for pod "var-expansion-7ab530ea-e81f-40b8-ac7c-fb85770bbfc3" in namespace "var-expansion-7997" to be "Succeeded or Failed"
Jan 31 10:18:51.233: INFO: Pod "var-expansion-7ab530ea-e81f-40b8-ac7c-fb85770bbfc3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.269638ms
Jan 31 10:18:53.237: INFO: Pod "var-expansion-7ab530ea-e81f-40b8-ac7c-fb85770bbfc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009479773s
Jan 31 10:18:55.247: INFO: Pod "var-expansion-7ab530ea-e81f-40b8-ac7c-fb85770bbfc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019553845s
STEP: Saw pod success
Jan 31 10:18:55.247: INFO: Pod "var-expansion-7ab530ea-e81f-40b8-ac7c-fb85770bbfc3" satisfied condition "Succeeded or Failed"
Jan 31 10:18:55.251: INFO: Trying to get logs from node macpro-x86-2 pod var-expansion-7ab530ea-e81f-40b8-ac7c-fb85770bbfc3 container dapi-container: <nil>
STEP: delete the pod
Jan 31 10:18:55.274: INFO: Waiting for pod var-expansion-7ab530ea-e81f-40b8-ac7c-fb85770bbfc3 to disappear
Jan 31 10:18:55.279: INFO: Pod var-expansion-7ab530ea-e81f-40b8-ac7c-fb85770bbfc3 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 31 10:18:55.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7997" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":356,"completed":142,"skipped":2515,"failed":0}
SS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:18:55.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-5844
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 31 10:18:55.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5844" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":143,"skipped":2517,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:18:55.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-965
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 31 10:18:55.570: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 31 10:19:55.611: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:19:55.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-4655
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:19:55.749: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jan 31 10:19:55.752: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:188
Jan 31 10:19:55.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4655" for this suite.
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 31 10:19:55.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-965" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:60.389 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":356,"completed":144,"skipped":2523,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:19:55.825: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1943
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:19:55.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1943
I0131 10:19:55.964438      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1943, replica count: 1
I0131 10:19:57.015438      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 31 10:19:57.126: INFO: Created: latency-svc-26rl5
Jan 31 10:19:57.135: INFO: Got endpoints: latency-svc-26rl5 [19.16666ms]
Jan 31 10:19:57.153: INFO: Created: latency-svc-mbcpt
Jan 31 10:19:57.164: INFO: Got endpoints: latency-svc-mbcpt [29.510402ms]
Jan 31 10:19:57.165: INFO: Created: latency-svc-2r55k
Jan 31 10:19:57.179: INFO: Got endpoints: latency-svc-2r55k [44.358426ms]
Jan 31 10:19:57.186: INFO: Created: latency-svc-skffr
Jan 31 10:19:57.204: INFO: Got endpoints: latency-svc-skffr [69.398321ms]
Jan 31 10:19:57.216: INFO: Created: latency-svc-pdzfm
Jan 31 10:19:57.222: INFO: Got endpoints: latency-svc-pdzfm [86.893585ms]
Jan 31 10:19:57.227: INFO: Created: latency-svc-tg4bq
Jan 31 10:19:57.236: INFO: Got endpoints: latency-svc-tg4bq [101.008318ms]
Jan 31 10:19:57.242: INFO: Created: latency-svc-m5j6g
Jan 31 10:19:57.249: INFO: Got endpoints: latency-svc-m5j6g [114.548174ms]
Jan 31 10:19:57.255: INFO: Created: latency-svc-fn28j
Jan 31 10:19:57.264: INFO: Got endpoints: latency-svc-fn28j [129.446336ms]
Jan 31 10:19:57.270: INFO: Created: latency-svc-nkzmm
Jan 31 10:19:57.285: INFO: Got endpoints: latency-svc-nkzmm [150.452499ms]
Jan 31 10:19:57.291: INFO: Created: latency-svc-r9d7s
Jan 31 10:19:57.310: INFO: Got endpoints: latency-svc-r9d7s [175.283523ms]
Jan 31 10:19:57.319: INFO: Created: latency-svc-st9wv
Jan 31 10:19:57.332: INFO: Got endpoints: latency-svc-st9wv [197.567979ms]
Jan 31 10:19:57.336: INFO: Created: latency-svc-7zxvn
Jan 31 10:19:57.344: INFO: Got endpoints: latency-svc-7zxvn [208.92852ms]
Jan 31 10:19:57.348: INFO: Created: latency-svc-4d2lj
Jan 31 10:19:57.359: INFO: Got endpoints: latency-svc-4d2lj [224.610653ms]
Jan 31 10:19:57.361: INFO: Created: latency-svc-4gmxd
Jan 31 10:19:57.372: INFO: Got endpoints: latency-svc-4gmxd [236.929391ms]
Jan 31 10:19:57.381: INFO: Created: latency-svc-m6qgv
Jan 31 10:19:57.394: INFO: Created: latency-svc-9tl7b
Jan 31 10:19:57.395: INFO: Got endpoints: latency-svc-m6qgv [259.876423ms]
Jan 31 10:19:57.407: INFO: Got endpoints: latency-svc-9tl7b [271.618865ms]
Jan 31 10:19:57.415: INFO: Created: latency-svc-nt7hn
Jan 31 10:19:57.422: INFO: Created: latency-svc-vsdjf
Jan 31 10:19:57.427: INFO: Got endpoints: latency-svc-nt7hn [262.769597ms]
Jan 31 10:19:57.431: INFO: Got endpoints: latency-svc-vsdjf [251.538824ms]
Jan 31 10:19:57.443: INFO: Created: latency-svc-kz6x6
Jan 31 10:19:57.451: INFO: Got endpoints: latency-svc-kz6x6 [246.615084ms]
Jan 31 10:19:57.457: INFO: Created: latency-svc-pjl7q
Jan 31 10:19:57.470: INFO: Got endpoints: latency-svc-pjl7q [248.559148ms]
Jan 31 10:19:57.477: INFO: Created: latency-svc-jmqmz
Jan 31 10:19:57.490: INFO: Got endpoints: latency-svc-jmqmz [254.63463ms]
Jan 31 10:19:57.498: INFO: Created: latency-svc-sqzqn
Jan 31 10:19:57.505: INFO: Got endpoints: latency-svc-sqzqn [255.924873ms]
Jan 31 10:19:57.523: INFO: Created: latency-svc-xd4jb
Jan 31 10:19:57.536: INFO: Got endpoints: latency-svc-xd4jb [271.302434ms]
Jan 31 10:19:57.542: INFO: Created: latency-svc-j2w2c
Jan 31 10:19:57.551: INFO: Got endpoints: latency-svc-j2w2c [265.367823ms]
Jan 31 10:19:57.562: INFO: Created: latency-svc-n57db
Jan 31 10:19:57.569: INFO: Got endpoints: latency-svc-n57db [259.245245ms]
Jan 31 10:19:57.576: INFO: Created: latency-svc-dvqbh
Jan 31 10:19:57.582: INFO: Got endpoints: latency-svc-dvqbh [249.626611ms]
Jan 31 10:19:57.589: INFO: Created: latency-svc-cvjgr
Jan 31 10:19:57.596: INFO: Got endpoints: latency-svc-cvjgr [252.290472ms]
Jan 31 10:19:57.606: INFO: Created: latency-svc-47k54
Jan 31 10:19:57.612: INFO: Got endpoints: latency-svc-47k54 [252.180069ms]
Jan 31 10:19:57.616: INFO: Created: latency-svc-bjgcv
Jan 31 10:19:57.638: INFO: Got endpoints: latency-svc-bjgcv [266.44731ms]
Jan 31 10:19:57.642: INFO: Created: latency-svc-npmrz
Jan 31 10:19:57.653: INFO: Got endpoints: latency-svc-npmrz [258.416053ms]
Jan 31 10:19:57.666: INFO: Created: latency-svc-5t7vf
Jan 31 10:19:57.673: INFO: Got endpoints: latency-svc-5t7vf [266.519719ms]
Jan 31 10:19:57.679: INFO: Created: latency-svc-8z72p
Jan 31 10:19:57.687: INFO: Got endpoints: latency-svc-8z72p [259.918954ms]
Jan 31 10:19:57.692: INFO: Created: latency-svc-k22hr
Jan 31 10:19:57.699: INFO: Got endpoints: latency-svc-k22hr [268.629739ms]
Jan 31 10:19:57.703: INFO: Created: latency-svc-8gvcx
Jan 31 10:19:57.711: INFO: Got endpoints: latency-svc-8gvcx [259.99617ms]
Jan 31 10:19:57.716: INFO: Created: latency-svc-xq77s
Jan 31 10:19:57.724: INFO: Got endpoints: latency-svc-xq77s [253.320541ms]
Jan 31 10:19:57.729: INFO: Created: latency-svc-s87fz
Jan 31 10:19:57.738: INFO: Got endpoints: latency-svc-s87fz [247.806309ms]
Jan 31 10:19:57.760: INFO: Created: latency-svc-sggf5
Jan 31 10:19:57.769: INFO: Got endpoints: latency-svc-sggf5 [263.576884ms]
Jan 31 10:19:57.775: INFO: Created: latency-svc-p94xx
Jan 31 10:19:57.782: INFO: Got endpoints: latency-svc-p94xx [246.806472ms]
Jan 31 10:19:57.792: INFO: Created: latency-svc-hc29h
Jan 31 10:19:57.805: INFO: Created: latency-svc-vzcq2
Jan 31 10:19:57.807: INFO: Got endpoints: latency-svc-hc29h [255.813197ms]
Jan 31 10:19:57.808: INFO: Got endpoints: latency-svc-vzcq2 [238.296441ms]
Jan 31 10:19:57.829: INFO: Created: latency-svc-sb4pt
Jan 31 10:19:57.838: INFO: Got endpoints: latency-svc-sb4pt [255.668762ms]
Jan 31 10:19:57.859: INFO: Created: latency-svc-55pz4
Jan 31 10:19:57.866: INFO: Got endpoints: latency-svc-55pz4 [270.30605ms]
Jan 31 10:19:57.873: INFO: Created: latency-svc-czxlz
Jan 31 10:19:57.881: INFO: Got endpoints: latency-svc-czxlz [269.6279ms]
Jan 31 10:19:57.907: INFO: Created: latency-svc-qpjpt
Jan 31 10:19:57.912: INFO: Got endpoints: latency-svc-qpjpt [274.010117ms]
Jan 31 10:19:57.918: INFO: Created: latency-svc-scrkc
Jan 31 10:19:57.926: INFO: Got endpoints: latency-svc-scrkc [273.01758ms]
Jan 31 10:19:58.050: INFO: Created: latency-svc-w2t9f
Jan 31 10:19:58.050: INFO: Created: latency-svc-h45m7
Jan 31 10:19:58.050: INFO: Created: latency-svc-jsh8x
Jan 31 10:19:58.055: INFO: Created: latency-svc-gf85x
Jan 31 10:19:58.056: INFO: Created: latency-svc-9mxfs
Jan 31 10:19:58.056: INFO: Created: latency-svc-4c8xn
Jan 31 10:19:58.056: INFO: Created: latency-svc-lwbvg
Jan 31 10:19:58.061: INFO: Created: latency-svc-qfwdr
Jan 31 10:19:58.061: INFO: Created: latency-svc-tgd8p
Jan 31 10:19:58.061: INFO: Created: latency-svc-vvvlp
Jan 31 10:19:58.061: INFO: Created: latency-svc-bl4nd
Jan 31 10:19:58.067: INFO: Created: latency-svc-qd4v6
Jan 31 10:19:58.078: INFO: Got endpoints: latency-svc-jsh8x [308.670436ms]
Jan 31 10:19:58.078: INFO: Got endpoints: latency-svc-h45m7 [151.344872ms]
Jan 31 10:19:58.078: INFO: Got endpoints: latency-svc-w2t9f [239.869242ms]
Jan 31 10:19:58.078: INFO: Created: latency-svc-f8585
Jan 31 10:19:58.078: INFO: Created: latency-svc-bgk6l
Jan 31 10:19:58.078: INFO: Created: latency-svc-bkbnx
Jan 31 10:19:58.091: INFO: Got endpoints: latency-svc-9mxfs [391.663111ms]
Jan 31 10:19:58.097: INFO: Created: latency-svc-brghw
Jan 31 10:19:58.107: INFO: Created: latency-svc-rkplt
Jan 31 10:19:58.114: INFO: Created: latency-svc-xwp4n
Jan 31 10:19:58.134: INFO: Created: latency-svc-g89kr
Jan 31 10:19:58.139: INFO: Got endpoints: latency-svc-lwbvg [356.490354ms]
Jan 31 10:19:58.149: INFO: Created: latency-svc-v7wpk
Jan 31 10:19:58.185: INFO: Got endpoints: latency-svc-4c8xn [446.882243ms]
Jan 31 10:19:58.197: INFO: Created: latency-svc-pk2mv
Jan 31 10:19:58.231: INFO: Got endpoints: latency-svc-gf85x [364.96986ms]
Jan 31 10:19:58.246: INFO: Created: latency-svc-mhf4n
Jan 31 10:19:58.283: INFO: Got endpoints: latency-svc-qd4v6 [476.236496ms]
Jan 31 10:19:58.295: INFO: Created: latency-svc-64j9b
Jan 31 10:19:58.335: INFO: Got endpoints: latency-svc-tgd8p [662.205416ms]
Jan 31 10:19:58.348: INFO: Created: latency-svc-6vnf5
Jan 31 10:19:58.385: INFO: Got endpoints: latency-svc-bl4nd [577.192997ms]
Jan 31 10:19:58.398: INFO: Created: latency-svc-hwgsc
Jan 31 10:19:58.433: INFO: Got endpoints: latency-svc-qfwdr [746.142616ms]
Jan 31 10:19:58.445: INFO: Created: latency-svc-g29qq
Jan 31 10:19:58.483: INFO: Got endpoints: latency-svc-vvvlp [771.948267ms]
Jan 31 10:19:58.498: INFO: Created: latency-svc-qxsqx
Jan 31 10:19:58.532: INFO: Got endpoints: latency-svc-bgk6l [650.337277ms]
Jan 31 10:19:58.543: INFO: Created: latency-svc-r4fnb
Jan 31 10:19:58.583: INFO: Got endpoints: latency-svc-bkbnx [670.608651ms]
Jan 31 10:19:58.596: INFO: Created: latency-svc-s4r6f
Jan 31 10:19:58.634: INFO: Got endpoints: latency-svc-f8585 [910.651724ms]
Jan 31 10:19:58.647: INFO: Created: latency-svc-q44sm
Jan 31 10:19:58.684: INFO: Got endpoints: latency-svc-brghw [606.224691ms]
Jan 31 10:19:58.704: INFO: Created: latency-svc-dhpsj
Jan 31 10:19:58.733: INFO: Got endpoints: latency-svc-rkplt [655.362895ms]
Jan 31 10:19:58.743: INFO: Created: latency-svc-5gphk
Jan 31 10:19:58.783: INFO: Got endpoints: latency-svc-xwp4n [704.920589ms]
Jan 31 10:19:58.795: INFO: Created: latency-svc-f2zz7
Jan 31 10:19:58.832: INFO: Got endpoints: latency-svc-g89kr [741.235793ms]
Jan 31 10:19:58.843: INFO: Created: latency-svc-npgkl
Jan 31 10:19:58.886: INFO: Got endpoints: latency-svc-v7wpk [746.636698ms]
Jan 31 10:19:58.907: INFO: Created: latency-svc-55psc
Jan 31 10:19:58.937: INFO: Got endpoints: latency-svc-pk2mv [751.347355ms]
Jan 31 10:19:58.954: INFO: Created: latency-svc-xgrrq
Jan 31 10:19:58.984: INFO: Got endpoints: latency-svc-mhf4n [752.969119ms]
Jan 31 10:19:58.998: INFO: Created: latency-svc-6dxfg
Jan 31 10:19:59.034: INFO: Got endpoints: latency-svc-64j9b [750.762373ms]
Jan 31 10:19:59.054: INFO: Created: latency-svc-9mwth
Jan 31 10:19:59.085: INFO: Got endpoints: latency-svc-6vnf5 [749.566934ms]
Jan 31 10:19:59.096: INFO: Created: latency-svc-hlgkz
Jan 31 10:19:59.133: INFO: Got endpoints: latency-svc-hwgsc [748.26882ms]
Jan 31 10:19:59.149: INFO: Created: latency-svc-2whcx
Jan 31 10:19:59.184: INFO: Got endpoints: latency-svc-g29qq [750.598681ms]
Jan 31 10:19:59.205: INFO: Created: latency-svc-55tl9
Jan 31 10:19:59.234: INFO: Got endpoints: latency-svc-qxsqx [750.867193ms]
Jan 31 10:19:59.247: INFO: Created: latency-svc-xg7js
Jan 31 10:19:59.283: INFO: Got endpoints: latency-svc-r4fnb [751.740532ms]
Jan 31 10:19:59.297: INFO: Created: latency-svc-5r5mn
Jan 31 10:19:59.334: INFO: Got endpoints: latency-svc-s4r6f [750.533429ms]
Jan 31 10:19:59.348: INFO: Created: latency-svc-x6wzr
Jan 31 10:19:59.387: INFO: Got endpoints: latency-svc-q44sm [752.883548ms]
Jan 31 10:19:59.410: INFO: Created: latency-svc-66vxd
Jan 31 10:19:59.435: INFO: Got endpoints: latency-svc-dhpsj [751.23545ms]
Jan 31 10:19:59.450: INFO: Created: latency-svc-hhk7b
Jan 31 10:19:59.483: INFO: Got endpoints: latency-svc-5gphk [749.720598ms]
Jan 31 10:19:59.494: INFO: Created: latency-svc-6sxtq
Jan 31 10:19:59.532: INFO: Got endpoints: latency-svc-f2zz7 [749.471627ms]
Jan 31 10:19:59.544: INFO: Created: latency-svc-xjkn6
Jan 31 10:19:59.584: INFO: Got endpoints: latency-svc-npgkl [751.356754ms]
Jan 31 10:19:59.597: INFO: Created: latency-svc-b9xnm
Jan 31 10:19:59.633: INFO: Got endpoints: latency-svc-55psc [747.608903ms]
Jan 31 10:19:59.644: INFO: Created: latency-svc-ltdr4
Jan 31 10:19:59.685: INFO: Got endpoints: latency-svc-xgrrq [747.945004ms]
Jan 31 10:19:59.715: INFO: Created: latency-svc-9phqs
Jan 31 10:19:59.748: INFO: Got endpoints: latency-svc-6dxfg [763.310945ms]
Jan 31 10:19:59.794: INFO: Created: latency-svc-d9zct
Jan 31 10:19:59.804: INFO: Got endpoints: latency-svc-9mwth [770.121977ms]
Jan 31 10:19:59.847: INFO: Created: latency-svc-6xxnq
Jan 31 10:19:59.854: INFO: Got endpoints: latency-svc-hlgkz [769.079386ms]
Jan 31 10:19:59.891: INFO: Created: latency-svc-qcq6w
Jan 31 10:19:59.895: INFO: Got endpoints: latency-svc-2whcx [762.032004ms]
Jan 31 10:19:59.913: INFO: Created: latency-svc-wst9v
Jan 31 10:19:59.935: INFO: Got endpoints: latency-svc-55tl9 [750.60615ms]
Jan 31 10:19:59.953: INFO: Created: latency-svc-5mn29
Jan 31 10:19:59.984: INFO: Got endpoints: latency-svc-xg7js [749.972063ms]
Jan 31 10:19:59.995: INFO: Created: latency-svc-pmp6v
Jan 31 10:20:00.033: INFO: Got endpoints: latency-svc-5r5mn [749.093013ms]
Jan 31 10:20:00.045: INFO: Created: latency-svc-pk9wj
Jan 31 10:20:00.085: INFO: Got endpoints: latency-svc-x6wzr [751.300694ms]
Jan 31 10:20:00.097: INFO: Created: latency-svc-zp2jw
Jan 31 10:20:00.133: INFO: Got endpoints: latency-svc-66vxd [746.060106ms]
Jan 31 10:20:00.146: INFO: Created: latency-svc-fmt64
Jan 31 10:20:00.187: INFO: Got endpoints: latency-svc-hhk7b [751.329958ms]
Jan 31 10:20:00.203: INFO: Created: latency-svc-7sjr9
Jan 31 10:20:00.234: INFO: Got endpoints: latency-svc-6sxtq [750.865923ms]
Jan 31 10:20:00.245: INFO: Created: latency-svc-mhzbj
Jan 31 10:20:00.283: INFO: Got endpoints: latency-svc-xjkn6 [750.79379ms]
Jan 31 10:20:00.300: INFO: Created: latency-svc-4nwqb
Jan 31 10:20:00.333: INFO: Got endpoints: latency-svc-b9xnm [749.122649ms]
Jan 31 10:20:00.348: INFO: Created: latency-svc-2dqrg
Jan 31 10:20:00.385: INFO: Got endpoints: latency-svc-ltdr4 [751.431249ms]
Jan 31 10:20:00.398: INFO: Created: latency-svc-7rx6v
Jan 31 10:20:00.435: INFO: Got endpoints: latency-svc-9phqs [750.00528ms]
Jan 31 10:20:00.447: INFO: Created: latency-svc-5hbtp
Jan 31 10:20:00.483: INFO: Got endpoints: latency-svc-d9zct [735.461354ms]
Jan 31 10:20:00.492: INFO: Created: latency-svc-kt6nj
Jan 31 10:20:00.533: INFO: Got endpoints: latency-svc-6xxnq [729.488749ms]
Jan 31 10:20:00.544: INFO: Created: latency-svc-b9xs4
Jan 31 10:20:00.582: INFO: Got endpoints: latency-svc-qcq6w [728.008592ms]
Jan 31 10:20:00.596: INFO: Created: latency-svc-ccklc
Jan 31 10:20:00.634: INFO: Got endpoints: latency-svc-wst9v [738.275668ms]
Jan 31 10:20:00.644: INFO: Created: latency-svc-8cvmt
Jan 31 10:20:00.684: INFO: Got endpoints: latency-svc-5mn29 [749.173175ms]
Jan 31 10:20:00.700: INFO: Created: latency-svc-w6kqt
Jan 31 10:20:00.735: INFO: Got endpoints: latency-svc-pmp6v [751.039913ms]
Jan 31 10:20:00.747: INFO: Created: latency-svc-8sx8g
Jan 31 10:20:00.793: INFO: Got endpoints: latency-svc-pk9wj [760.762351ms]
Jan 31 10:20:00.818: INFO: Created: latency-svc-ftzmr
Jan 31 10:20:00.831: INFO: Got endpoints: latency-svc-zp2jw [746.257915ms]
Jan 31 10:20:00.864: INFO: Created: latency-svc-xglfd
Jan 31 10:20:00.886: INFO: Got endpoints: latency-svc-fmt64 [753.020361ms]
Jan 31 10:20:00.903: INFO: Created: latency-svc-8psr9
Jan 31 10:20:00.936: INFO: Got endpoints: latency-svc-7sjr9 [749.432264ms]
Jan 31 10:20:00.949: INFO: Created: latency-svc-989w5
Jan 31 10:20:00.983: INFO: Got endpoints: latency-svc-mhzbj [749.751198ms]
Jan 31 10:20:01.000: INFO: Created: latency-svc-pbv86
Jan 31 10:20:01.036: INFO: Got endpoints: latency-svc-4nwqb [753.086902ms]
Jan 31 10:20:01.054: INFO: Created: latency-svc-6976w
Jan 31 10:20:01.085: INFO: Got endpoints: latency-svc-2dqrg [751.752548ms]
Jan 31 10:20:01.099: INFO: Created: latency-svc-2wnh4
Jan 31 10:20:01.131: INFO: Got endpoints: latency-svc-7rx6v [745.826519ms]
Jan 31 10:20:01.145: INFO: Created: latency-svc-klqv6
Jan 31 10:20:01.188: INFO: Got endpoints: latency-svc-5hbtp [753.278755ms]
Jan 31 10:20:01.216: INFO: Created: latency-svc-fh7lq
Jan 31 10:20:01.234: INFO: Got endpoints: latency-svc-kt6nj [750.945277ms]
Jan 31 10:20:01.246: INFO: Created: latency-svc-tcfcf
Jan 31 10:20:01.285: INFO: Got endpoints: latency-svc-b9xs4 [751.838686ms]
Jan 31 10:20:01.302: INFO: Created: latency-svc-6gbmg
Jan 31 10:20:01.333: INFO: Got endpoints: latency-svc-ccklc [751.331093ms]
Jan 31 10:20:01.349: INFO: Created: latency-svc-876nd
Jan 31 10:20:01.389: INFO: Got endpoints: latency-svc-8cvmt [755.648728ms]
Jan 31 10:20:01.405: INFO: Created: latency-svc-5ptsm
Jan 31 10:20:01.436: INFO: Got endpoints: latency-svc-w6kqt [752.628635ms]
Jan 31 10:20:01.454: INFO: Created: latency-svc-m756k
Jan 31 10:20:01.483: INFO: Got endpoints: latency-svc-8sx8g [748.533903ms]
Jan 31 10:20:01.497: INFO: Created: latency-svc-tghhm
Jan 31 10:20:01.533: INFO: Got endpoints: latency-svc-ftzmr [739.475386ms]
Jan 31 10:20:01.544: INFO: Created: latency-svc-jqt9w
Jan 31 10:20:01.582: INFO: Got endpoints: latency-svc-xglfd [750.779903ms]
Jan 31 10:20:01.594: INFO: Created: latency-svc-2gnwx
Jan 31 10:20:01.633: INFO: Got endpoints: latency-svc-8psr9 [746.465714ms]
Jan 31 10:20:01.647: INFO: Created: latency-svc-gm66s
Jan 31 10:20:01.682: INFO: Got endpoints: latency-svc-989w5 [746.041815ms]
Jan 31 10:20:01.696: INFO: Created: latency-svc-5bfjk
Jan 31 10:20:01.733: INFO: Got endpoints: latency-svc-pbv86 [749.685008ms]
Jan 31 10:20:01.744: INFO: Created: latency-svc-67mbr
Jan 31 10:20:01.783: INFO: Got endpoints: latency-svc-6976w [747.482921ms]
Jan 31 10:20:01.794: INFO: Created: latency-svc-kssxq
Jan 31 10:20:01.833: INFO: Got endpoints: latency-svc-2wnh4 [748.299267ms]
Jan 31 10:20:01.845: INFO: Created: latency-svc-8dhnl
Jan 31 10:20:01.885: INFO: Got endpoints: latency-svc-klqv6 [753.992396ms]
Jan 31 10:20:01.900: INFO: Created: latency-svc-kpp4c
Jan 31 10:20:01.936: INFO: Got endpoints: latency-svc-fh7lq [748.090784ms]
Jan 31 10:20:01.949: INFO: Created: latency-svc-qmksz
Jan 31 10:20:01.986: INFO: Got endpoints: latency-svc-tcfcf [751.665429ms]
Jan 31 10:20:01.999: INFO: Created: latency-svc-lq94k
Jan 31 10:20:02.035: INFO: Got endpoints: latency-svc-6gbmg [749.474141ms]
Jan 31 10:20:02.053: INFO: Created: latency-svc-nwhlm
Jan 31 10:20:02.084: INFO: Got endpoints: latency-svc-876nd [750.7031ms]
Jan 31 10:20:02.096: INFO: Created: latency-svc-ckkvk
Jan 31 10:20:02.134: INFO: Got endpoints: latency-svc-5ptsm [744.680948ms]
Jan 31 10:20:02.153: INFO: Created: latency-svc-5zccz
Jan 31 10:20:02.185: INFO: Got endpoints: latency-svc-m756k [748.321511ms]
Jan 31 10:20:02.197: INFO: Created: latency-svc-zkjft
Jan 31 10:20:02.235: INFO: Got endpoints: latency-svc-tghhm [751.539442ms]
Jan 31 10:20:02.258: INFO: Created: latency-svc-fb6xt
Jan 31 10:20:02.284: INFO: Got endpoints: latency-svc-jqt9w [750.826207ms]
Jan 31 10:20:02.296: INFO: Created: latency-svc-hh48b
Jan 31 10:20:02.333: INFO: Got endpoints: latency-svc-2gnwx [751.49215ms]
Jan 31 10:20:02.351: INFO: Created: latency-svc-58m64
Jan 31 10:20:02.386: INFO: Got endpoints: latency-svc-gm66s [752.943802ms]
Jan 31 10:20:02.399: INFO: Created: latency-svc-7gg2d
Jan 31 10:20:02.433: INFO: Got endpoints: latency-svc-5bfjk [751.037345ms]
Jan 31 10:20:02.445: INFO: Created: latency-svc-qtpkt
Jan 31 10:20:02.482: INFO: Got endpoints: latency-svc-67mbr [749.068569ms]
Jan 31 10:20:02.493: INFO: Created: latency-svc-bf7wb
Jan 31 10:20:02.533: INFO: Got endpoints: latency-svc-kssxq [749.470023ms]
Jan 31 10:20:02.544: INFO: Created: latency-svc-lpw6g
Jan 31 10:20:02.584: INFO: Got endpoints: latency-svc-8dhnl [751.035077ms]
Jan 31 10:20:02.595: INFO: Created: latency-svc-zx7vz
Jan 31 10:20:02.632: INFO: Got endpoints: latency-svc-kpp4c [747.669458ms]
Jan 31 10:20:02.643: INFO: Created: latency-svc-k5jwl
Jan 31 10:20:02.683: INFO: Got endpoints: latency-svc-qmksz [747.481834ms]
Jan 31 10:20:02.696: INFO: Created: latency-svc-w62m9
Jan 31 10:20:02.732: INFO: Got endpoints: latency-svc-lq94k [746.573726ms]
Jan 31 10:20:02.747: INFO: Created: latency-svc-kctq6
Jan 31 10:20:02.784: INFO: Got endpoints: latency-svc-nwhlm [749.104562ms]
Jan 31 10:20:02.797: INFO: Created: latency-svc-fxmft
Jan 31 10:20:02.832: INFO: Got endpoints: latency-svc-ckkvk [748.117848ms]
Jan 31 10:20:02.846: INFO: Created: latency-svc-bggr6
Jan 31 10:20:02.887: INFO: Got endpoints: latency-svc-5zccz [752.533288ms]
Jan 31 10:20:02.897: INFO: Created: latency-svc-qzxgd
Jan 31 10:20:02.936: INFO: Got endpoints: latency-svc-zkjft [750.974489ms]
Jan 31 10:20:02.945: INFO: Created: latency-svc-rqklw
Jan 31 10:20:02.984: INFO: Got endpoints: latency-svc-fb6xt [748.786608ms]
Jan 31 10:20:03.000: INFO: Created: latency-svc-hjgpw
Jan 31 10:20:03.035: INFO: Got endpoints: latency-svc-hh48b [751.024799ms]
Jan 31 10:20:03.049: INFO: Created: latency-svc-jf996
Jan 31 10:20:03.086: INFO: Got endpoints: latency-svc-58m64 [752.856865ms]
Jan 31 10:20:03.116: INFO: Created: latency-svc-vpqtr
Jan 31 10:20:03.133: INFO: Got endpoints: latency-svc-7gg2d [747.05856ms]
Jan 31 10:20:03.148: INFO: Created: latency-svc-926xk
Jan 31 10:20:03.184: INFO: Got endpoints: latency-svc-qtpkt [750.615645ms]
Jan 31 10:20:03.210: INFO: Created: latency-svc-hrw7v
Jan 31 10:20:03.234: INFO: Got endpoints: latency-svc-bf7wb [751.993364ms]
Jan 31 10:20:03.247: INFO: Created: latency-svc-hjr5k
Jan 31 10:20:03.283: INFO: Got endpoints: latency-svc-lpw6g [750.487372ms]
Jan 31 10:20:03.300: INFO: Created: latency-svc-ssmhv
Jan 31 10:20:03.336: INFO: Got endpoints: latency-svc-zx7vz [751.410529ms]
Jan 31 10:20:03.347: INFO: Created: latency-svc-rv8qv
Jan 31 10:20:03.385: INFO: Got endpoints: latency-svc-k5jwl [753.195785ms]
Jan 31 10:20:03.405: INFO: Created: latency-svc-dtfh6
Jan 31 10:20:03.435: INFO: Got endpoints: latency-svc-w62m9 [751.44989ms]
Jan 31 10:20:03.445: INFO: Created: latency-svc-vvvk8
Jan 31 10:20:03.486: INFO: Got endpoints: latency-svc-kctq6 [753.31953ms]
Jan 31 10:20:03.509: INFO: Created: latency-svc-p258s
Jan 31 10:20:03.533: INFO: Got endpoints: latency-svc-fxmft [749.378112ms]
Jan 31 10:20:03.548: INFO: Created: latency-svc-9d48p
Jan 31 10:20:03.586: INFO: Got endpoints: latency-svc-bggr6 [753.634557ms]
Jan 31 10:20:03.598: INFO: Created: latency-svc-sfkr7
Jan 31 10:20:03.635: INFO: Got endpoints: latency-svc-qzxgd [748.081268ms]
Jan 31 10:20:03.647: INFO: Created: latency-svc-5vf2c
Jan 31 10:20:03.685: INFO: Got endpoints: latency-svc-rqklw [749.043359ms]
Jan 31 10:20:03.706: INFO: Created: latency-svc-vr8wz
Jan 31 10:20:03.733: INFO: Got endpoints: latency-svc-hjgpw [749.726281ms]
Jan 31 10:20:03.745: INFO: Created: latency-svc-pv9v4
Jan 31 10:20:03.785: INFO: Got endpoints: latency-svc-jf996 [750.46438ms]
Jan 31 10:20:03.796: INFO: Created: latency-svc-72fx8
Jan 31 10:20:03.834: INFO: Got endpoints: latency-svc-vpqtr [747.200328ms]
Jan 31 10:20:03.846: INFO: Created: latency-svc-fk67b
Jan 31 10:20:03.883: INFO: Got endpoints: latency-svc-926xk [750.452554ms]
Jan 31 10:20:03.897: INFO: Created: latency-svc-qxgfc
Jan 31 10:20:03.934: INFO: Got endpoints: latency-svc-hrw7v [750.323556ms]
Jan 31 10:20:03.945: INFO: Created: latency-svc-5m4rl
Jan 31 10:20:03.985: INFO: Got endpoints: latency-svc-hjr5k [750.734058ms]
Jan 31 10:20:04.009: INFO: Created: latency-svc-bcbcn
Jan 31 10:20:04.033: INFO: Got endpoints: latency-svc-ssmhv [749.46588ms]
Jan 31 10:20:04.047: INFO: Created: latency-svc-8th95
Jan 31 10:20:04.085: INFO: Got endpoints: latency-svc-rv8qv [749.681257ms]
Jan 31 10:20:04.100: INFO: Created: latency-svc-qp8tn
Jan 31 10:20:04.135: INFO: Got endpoints: latency-svc-dtfh6 [749.271715ms]
Jan 31 10:20:04.147: INFO: Created: latency-svc-97kgg
Jan 31 10:20:04.182: INFO: Got endpoints: latency-svc-vvvk8 [747.287086ms]
Jan 31 10:20:04.195: INFO: Created: latency-svc-xtbz9
Jan 31 10:20:04.236: INFO: Got endpoints: latency-svc-p258s [749.790814ms]
Jan 31 10:20:04.255: INFO: Created: latency-svc-gkfxv
Jan 31 10:20:04.282: INFO: Got endpoints: latency-svc-9d48p [749.110048ms]
Jan 31 10:20:04.306: INFO: Created: latency-svc-c29vh
Jan 31 10:20:04.356: INFO: Got endpoints: latency-svc-sfkr7 [769.804326ms]
Jan 31 10:20:04.394: INFO: Got endpoints: latency-svc-5vf2c [759.208686ms]
Jan 31 10:20:04.397: INFO: Created: latency-svc-vbg2b
Jan 31 10:20:04.435: INFO: Created: latency-svc-ljhxs
Jan 31 10:20:04.438: INFO: Got endpoints: latency-svc-vr8wz [752.953199ms]
Jan 31 10:20:04.497: INFO: Created: latency-svc-gdwbh
Jan 31 10:20:04.499: INFO: Got endpoints: latency-svc-pv9v4 [765.468195ms]
Jan 31 10:20:04.520: INFO: Created: latency-svc-b7lm5
Jan 31 10:20:04.533: INFO: Got endpoints: latency-svc-72fx8 [747.381655ms]
Jan 31 10:20:04.543: INFO: Created: latency-svc-w95q8
Jan 31 10:20:04.582: INFO: Got endpoints: latency-svc-fk67b [748.583054ms]
Jan 31 10:20:04.596: INFO: Created: latency-svc-wcbnz
Jan 31 10:20:04.633: INFO: Got endpoints: latency-svc-qxgfc [749.921544ms]
Jan 31 10:20:04.657: INFO: Created: latency-svc-7nslr
Jan 31 10:20:04.683: INFO: Got endpoints: latency-svc-5m4rl [749.150258ms]
Jan 31 10:20:04.697: INFO: Created: latency-svc-rhb5x
Jan 31 10:20:04.734: INFO: Got endpoints: latency-svc-bcbcn [749.380807ms]
Jan 31 10:20:04.745: INFO: Created: latency-svc-6zpnq
Jan 31 10:20:04.784: INFO: Got endpoints: latency-svc-8th95 [750.98894ms]
Jan 31 10:20:04.797: INFO: Created: latency-svc-xc55d
Jan 31 10:20:04.834: INFO: Got endpoints: latency-svc-qp8tn [748.711945ms]
Jan 31 10:20:04.847: INFO: Created: latency-svc-629jp
Jan 31 10:20:04.882: INFO: Got endpoints: latency-svc-97kgg [746.994596ms]
Jan 31 10:20:04.894: INFO: Created: latency-svc-np84w
Jan 31 10:20:04.935: INFO: Got endpoints: latency-svc-xtbz9 [752.375339ms]
Jan 31 10:20:04.947: INFO: Created: latency-svc-m8z7m
Jan 31 10:20:04.983: INFO: Got endpoints: latency-svc-gkfxv [747.097305ms]
Jan 31 10:20:05.034: INFO: Got endpoints: latency-svc-c29vh [751.259382ms]
Jan 31 10:20:05.085: INFO: Got endpoints: latency-svc-vbg2b [728.927337ms]
Jan 31 10:20:05.134: INFO: Got endpoints: latency-svc-ljhxs [740.473018ms]
Jan 31 10:20:05.187: INFO: Got endpoints: latency-svc-gdwbh [748.91304ms]
Jan 31 10:20:05.234: INFO: Got endpoints: latency-svc-b7lm5 [735.521334ms]
Jan 31 10:20:05.287: INFO: Got endpoints: latency-svc-w95q8 [753.925086ms]
Jan 31 10:20:05.335: INFO: Got endpoints: latency-svc-wcbnz [752.448749ms]
Jan 31 10:20:05.387: INFO: Got endpoints: latency-svc-7nslr [754.186838ms]
Jan 31 10:20:05.433: INFO: Got endpoints: latency-svc-rhb5x [749.637131ms]
Jan 31 10:20:05.483: INFO: Got endpoints: latency-svc-6zpnq [748.185308ms]
Jan 31 10:20:05.533: INFO: Got endpoints: latency-svc-xc55d [749.493115ms]
Jan 31 10:20:05.584: INFO: Got endpoints: latency-svc-629jp [749.572554ms]
Jan 31 10:20:05.639: INFO: Got endpoints: latency-svc-np84w [757.073694ms]
Jan 31 10:20:05.684: INFO: Got endpoints: latency-svc-m8z7m [749.806471ms]
Jan 31 10:20:05.685: INFO: Latencies: [29.510402ms 44.358426ms 69.398321ms 86.893585ms 101.008318ms 114.548174ms 129.446336ms 150.452499ms 151.344872ms 175.283523ms 197.567979ms 208.92852ms 224.610653ms 236.929391ms 238.296441ms 239.869242ms 246.615084ms 246.806472ms 247.806309ms 248.559148ms 249.626611ms 251.538824ms 252.180069ms 252.290472ms 253.320541ms 254.63463ms 255.668762ms 255.813197ms 255.924873ms 258.416053ms 259.245245ms 259.876423ms 259.918954ms 259.99617ms 262.769597ms 263.576884ms 265.367823ms 266.44731ms 266.519719ms 268.629739ms 269.6279ms 270.30605ms 271.302434ms 271.618865ms 273.01758ms 274.010117ms 308.670436ms 356.490354ms 364.96986ms 391.663111ms 446.882243ms 476.236496ms 577.192997ms 606.224691ms 650.337277ms 655.362895ms 662.205416ms 670.608651ms 704.920589ms 728.008592ms 728.927337ms 729.488749ms 735.461354ms 735.521334ms 738.275668ms 739.475386ms 740.473018ms 741.235793ms 744.680948ms 745.826519ms 746.041815ms 746.060106ms 746.142616ms 746.257915ms 746.465714ms 746.573726ms 746.636698ms 746.994596ms 747.05856ms 747.097305ms 747.200328ms 747.287086ms 747.381655ms 747.481834ms 747.482921ms 747.608903ms 747.669458ms 747.945004ms 748.081268ms 748.090784ms 748.117848ms 748.185308ms 748.26882ms 748.299267ms 748.321511ms 748.533903ms 748.583054ms 748.711945ms 748.786608ms 748.91304ms 749.043359ms 749.068569ms 749.093013ms 749.104562ms 749.110048ms 749.122649ms 749.150258ms 749.173175ms 749.271715ms 749.378112ms 749.380807ms 749.432264ms 749.46588ms 749.470023ms 749.471627ms 749.474141ms 749.493115ms 749.566934ms 749.572554ms 749.637131ms 749.681257ms 749.685008ms 749.720598ms 749.726281ms 749.751198ms 749.790814ms 749.806471ms 749.921544ms 749.972063ms 750.00528ms 750.323556ms 750.452554ms 750.46438ms 750.487372ms 750.533429ms 750.598681ms 750.60615ms 750.615645ms 750.7031ms 750.734058ms 750.762373ms 750.779903ms 750.79379ms 750.826207ms 750.865923ms 750.867193ms 750.945277ms 750.974489ms 750.98894ms 751.024799ms 751.035077ms 751.037345ms 751.039913ms 751.23545ms 751.259382ms 751.300694ms 751.329958ms 751.331093ms 751.347355ms 751.356754ms 751.410529ms 751.431249ms 751.44989ms 751.49215ms 751.539442ms 751.665429ms 751.740532ms 751.752548ms 751.838686ms 751.993364ms 752.375339ms 752.448749ms 752.533288ms 752.628635ms 752.856865ms 752.883548ms 752.943802ms 752.953199ms 752.969119ms 753.020361ms 753.086902ms 753.195785ms 753.278755ms 753.31953ms 753.634557ms 753.925086ms 753.992396ms 754.186838ms 755.648728ms 757.073694ms 759.208686ms 760.762351ms 762.032004ms 763.310945ms 765.468195ms 769.079386ms 769.804326ms 770.121977ms 771.948267ms 910.651724ms]
Jan 31 10:20:05.685: INFO: 50 %ile: 749.043359ms
Jan 31 10:20:05.685: INFO: 90 %ile: 753.086902ms
Jan 31 10:20:05.685: INFO: 99 %ile: 771.948267ms
Jan 31 10:20:05.685: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:188
Jan 31 10:20:05.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1943" for this suite.

• [SLOW TEST:9.872 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":356,"completed":145,"skipped":2531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:20:05.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9879
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1574
[It] should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 31 10:20:05.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-9879 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 31 10:20:05.878: INFO: stderr: ""
Jan 31 10:20:05.878: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jan 31 10:20:10.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-9879 get pod e2e-test-httpd-pod -o json'
Jan 31 10:20:10.973: INFO: stderr: ""
Jan 31 10:20:10.973: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"4c0e76e8c347708f77f065eddbbec5998bf7eb0c0af5c83cbf86e088753b2dbc\",\n            \"cni.projectcalico.org/podIP\": \"192.168.75.16/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.75.16/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2023-01-31T10:20:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9879\",\n        \"resourceVersion\": \"22088\",\n        \"uid\": \"97f3d843-3422-43d4-871f-404f1325a582\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-ks2sj\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"macpro-x86-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-ks2sj\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-31T10:20:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-31T10:20:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-31T10:20:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-31T10:20:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://c740eb8a39feb05b14695ef2606713c6d4d01575113d11ed7cfe2a5c90a57fd3\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-31T10:20:06Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.221.188.31\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.75.16\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.75.16\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-31T10:20:05Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 31 10:20:10.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-9879 replace -f -'
Jan 31 10:20:12.243: INFO: stderr: ""
Jan 31 10:20:12.243: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1578
Jan 31 10:20:12.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-9879 delete pods e2e-test-httpd-pod'
Jan 31 10:20:13.895: INFO: stderr: ""
Jan 31 10:20:13.895: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 10:20:13.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9879" for this suite.

• [SLOW TEST:8.232 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1571
    should update a single-container pod's image  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":356,"completed":146,"skipped":2565,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:20:13.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-2972
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 31 10:20:14.131: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 31 10:20:14.136: INFO: starting watch
STEP: patching
STEP: updating
Jan 31 10:20:14.159: INFO: waiting for watch events with expected annotations
Jan 31 10:20:14.159: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:188
Jan 31 10:20:14.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-2972" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":356,"completed":147,"skipped":2574,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:20:14.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2267
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-817f3c00-8aeb-42ab-aa72-c36a30f2586b
STEP: Creating secret with name s-test-opt-upd-8c24f68f-b09e-40f2-a474-949e3ee21b8b
STEP: Creating the pod
Jan 31 10:20:14.422: INFO: The status of Pod pod-projected-secrets-c613a664-9844-4cfe-b6cc-a5a6dea60bcf is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:20:16.427: INFO: The status of Pod pod-projected-secrets-c613a664-9844-4cfe-b6cc-a5a6dea60bcf is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-817f3c00-8aeb-42ab-aa72-c36a30f2586b
STEP: Updating secret s-test-opt-upd-8c24f68f-b09e-40f2-a474-949e3ee21b8b
STEP: Creating secret with name s-test-opt-create-3ad1c93d-4189-4beb-b63e-585ac54bea0b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 31 10:20:18.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2267" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":148,"skipped":2580,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:20:18.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7761
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 10:20:18.665: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cc319a99-cc42-4f95-bddf-6eb7fc3bf71f" in namespace "projected-7761" to be "Succeeded or Failed"
Jan 31 10:20:18.669: INFO: Pod "downwardapi-volume-cc319a99-cc42-4f95-bddf-6eb7fc3bf71f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.620182ms
Jan 31 10:20:20.676: INFO: Pod "downwardapi-volume-cc319a99-cc42-4f95-bddf-6eb7fc3bf71f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010704082s
Jan 31 10:20:22.680: INFO: Pod "downwardapi-volume-cc319a99-cc42-4f95-bddf-6eb7fc3bf71f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014579777s
STEP: Saw pod success
Jan 31 10:20:22.680: INFO: Pod "downwardapi-volume-cc319a99-cc42-4f95-bddf-6eb7fc3bf71f" satisfied condition "Succeeded or Failed"
Jan 31 10:20:22.682: INFO: Trying to get logs from node macpro-x86-2 pod downwardapi-volume-cc319a99-cc42-4f95-bddf-6eb7fc3bf71f container client-container: <nil>
STEP: delete the pod
Jan 31 10:20:22.717: INFO: Waiting for pod downwardapi-volume-cc319a99-cc42-4f95-bddf-6eb7fc3bf71f to disappear
Jan 31 10:20:22.724: INFO: Pod downwardapi-volume-cc319a99-cc42-4f95-bddf-6eb7fc3bf71f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 31 10:20:22.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7761" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":149,"skipped":2590,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:20:22.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-8501
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 31 10:20:24.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8501" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","total":356,"completed":150,"skipped":2610,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:20:24.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7679
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 31 10:20:32.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7679" for this suite.

• [SLOW TEST:7.149 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":356,"completed":151,"skipped":2614,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:20:32.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3739
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3739
[It] should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-3739
Jan 31 10:20:32.202: INFO: Found 0 stateful pods, waiting for 1
Jan 31 10:20:42.206: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 31 10:20:42.241: INFO: Deleting all statefulset in ns statefulset-3739
Jan 31 10:20:42.243: INFO: Scaling statefulset ss to 0
Jan 31 10:20:52.262: INFO: Waiting for statefulset status.replicas updated to 0
Jan 31 10:20:52.264: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 31 10:20:52.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3739" for this suite.

• [SLOW TEST:20.236 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":356,"completed":152,"skipped":2635,"failed":0}
SSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:20:52.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6512
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 31 10:21:52.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6512" for this suite.

• [SLOW TEST:60.147 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":356,"completed":153,"skipped":2642,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:21:52.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3176
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 31 10:21:52.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3176" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":356,"completed":154,"skipped":2656,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:21:52.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4587
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 10:21:52.740: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd94e213-33b5-439c-9f3b-11dbd0b067e5" in namespace "downward-api-4587" to be "Succeeded or Failed"
Jan 31 10:21:52.744: INFO: Pod "downwardapi-volume-cd94e213-33b5-439c-9f3b-11dbd0b067e5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.240907ms
Jan 31 10:21:54.748: INFO: Pod "downwardapi-volume-cd94e213-33b5-439c-9f3b-11dbd0b067e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007972863s
Jan 31 10:21:56.755: INFO: Pod "downwardapi-volume-cd94e213-33b5-439c-9f3b-11dbd0b067e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014170395s
STEP: Saw pod success
Jan 31 10:21:56.755: INFO: Pod "downwardapi-volume-cd94e213-33b5-439c-9f3b-11dbd0b067e5" satisfied condition "Succeeded or Failed"
Jan 31 10:21:56.757: INFO: Trying to get logs from node macpro-x86-1 pod downwardapi-volume-cd94e213-33b5-439c-9f3b-11dbd0b067e5 container client-container: <nil>
STEP: delete the pod
Jan 31 10:21:56.777: INFO: Waiting for pod downwardapi-volume-cd94e213-33b5-439c-9f3b-11dbd0b067e5 to disappear
Jan 31 10:21:56.780: INFO: Pod downwardapi-volume-cd94e213-33b5-439c-9f3b-11dbd0b067e5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 31 10:21:56.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4587" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":356,"completed":155,"skipped":2670,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:21:56.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9488
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-16ff3bb4-d85e-47a2-a329-87993ee57f60
STEP: Creating a pod to test consume configMaps
Jan 31 10:21:56.925: INFO: Waiting up to 5m0s for pod "pod-configmaps-7626974e-ba63-4532-90f3-d89b3b1733a6" in namespace "configmap-9488" to be "Succeeded or Failed"
Jan 31 10:21:56.931: INFO: Pod "pod-configmaps-7626974e-ba63-4532-90f3-d89b3b1733a6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.460232ms
Jan 31 10:21:58.935: INFO: Pod "pod-configmaps-7626974e-ba63-4532-90f3-d89b3b1733a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009353918s
Jan 31 10:22:00.941: INFO: Pod "pod-configmaps-7626974e-ba63-4532-90f3-d89b3b1733a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015442161s
STEP: Saw pod success
Jan 31 10:22:00.941: INFO: Pod "pod-configmaps-7626974e-ba63-4532-90f3-d89b3b1733a6" satisfied condition "Succeeded or Failed"
Jan 31 10:22:00.943: INFO: Trying to get logs from node macpro-x86-2 pod pod-configmaps-7626974e-ba63-4532-90f3-d89b3b1733a6 container agnhost-container: <nil>
STEP: delete the pod
Jan 31 10:22:00.962: INFO: Waiting for pod pod-configmaps-7626974e-ba63-4532-90f3-d89b3b1733a6 to disappear
Jan 31 10:22:00.964: INFO: Pod pod-configmaps-7626974e-ba63-4532-90f3-d89b3b1733a6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 31 10:22:00.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9488" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":156,"skipped":2701,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:22:00.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5399
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-bffabbb3-b8c8-4c72-99d9-645bf95de668
STEP: Creating a pod to test consume configMaps
Jan 31 10:22:01.104: INFO: Waiting up to 5m0s for pod "pod-configmaps-6f6d4c8f-5986-4c34-8d44-a219d3e8cb51" in namespace "configmap-5399" to be "Succeeded or Failed"
Jan 31 10:22:01.109: INFO: Pod "pod-configmaps-6f6d4c8f-5986-4c34-8d44-a219d3e8cb51": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046749ms
Jan 31 10:22:03.112: INFO: Pod "pod-configmaps-6f6d4c8f-5986-4c34-8d44-a219d3e8cb51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007836617s
Jan 31 10:22:05.116: INFO: Pod "pod-configmaps-6f6d4c8f-5986-4c34-8d44-a219d3e8cb51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011937291s
STEP: Saw pod success
Jan 31 10:22:05.116: INFO: Pod "pod-configmaps-6f6d4c8f-5986-4c34-8d44-a219d3e8cb51" satisfied condition "Succeeded or Failed"
Jan 31 10:22:05.119: INFO: Trying to get logs from node macpro-x86-1 pod pod-configmaps-6f6d4c8f-5986-4c34-8d44-a219d3e8cb51 container agnhost-container: <nil>
STEP: delete the pod
Jan 31 10:22:05.132: INFO: Waiting for pod pod-configmaps-6f6d4c8f-5986-4c34-8d44-a219d3e8cb51 to disappear
Jan 31 10:22:05.135: INFO: Pod pod-configmaps-6f6d4c8f-5986-4c34-8d44-a219d3e8cb51 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 31 10:22:05.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5399" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":157,"skipped":2736,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:22:05.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7876
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan 31 10:22:05.269: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 31 10:22:09.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7876" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":356,"completed":158,"skipped":2763,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:22:09.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-296
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override all
Jan 31 10:22:10.103: INFO: Waiting up to 5m0s for pod "client-containers-d07c0f7f-e947-4892-b7c0-8e62fa5a0e15" in namespace "containers-296" to be "Succeeded or Failed"
Jan 31 10:22:10.107: INFO: Pod "client-containers-d07c0f7f-e947-4892-b7c0-8e62fa5a0e15": Phase="Pending", Reason="", readiness=false. Elapsed: 3.292464ms
Jan 31 10:22:12.111: INFO: Pod "client-containers-d07c0f7f-e947-4892-b7c0-8e62fa5a0e15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007310147s
Jan 31 10:22:14.115: INFO: Pod "client-containers-d07c0f7f-e947-4892-b7c0-8e62fa5a0e15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012188067s
STEP: Saw pod success
Jan 31 10:22:14.116: INFO: Pod "client-containers-d07c0f7f-e947-4892-b7c0-8e62fa5a0e15" satisfied condition "Succeeded or Failed"
Jan 31 10:22:14.118: INFO: Trying to get logs from node macpro-x86-1 pod client-containers-d07c0f7f-e947-4892-b7c0-8e62fa5a0e15 container agnhost-container: <nil>
STEP: delete the pod
Jan 31 10:22:14.135: INFO: Waiting for pod client-containers-d07c0f7f-e947-4892-b7c0-8e62fa5a0e15 to disappear
Jan 31 10:22:14.137: INFO: Pod client-containers-d07c0f7f-e947-4892-b7c0-8e62fa5a0e15 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 31 10:22:14.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-296" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":356,"completed":159,"skipped":2797,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:22:14.144: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1645
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-1645/configmap-test-bdd8fa48-f821-4a3c-b958-0619e9bdf07b
STEP: Creating a pod to test consume configMaps
Jan 31 10:22:14.278: INFO: Waiting up to 5m0s for pod "pod-configmaps-16c395cf-bcca-455f-b478-99e5fe4cac01" in namespace "configmap-1645" to be "Succeeded or Failed"
Jan 31 10:22:14.280: INFO: Pod "pod-configmaps-16c395cf-bcca-455f-b478-99e5fe4cac01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045527ms
Jan 31 10:22:16.284: INFO: Pod "pod-configmaps-16c395cf-bcca-455f-b478-99e5fe4cac01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006366905s
Jan 31 10:22:18.287: INFO: Pod "pod-configmaps-16c395cf-bcca-455f-b478-99e5fe4cac01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009920924s
STEP: Saw pod success
Jan 31 10:22:18.287: INFO: Pod "pod-configmaps-16c395cf-bcca-455f-b478-99e5fe4cac01" satisfied condition "Succeeded or Failed"
Jan 31 10:22:18.290: INFO: Trying to get logs from node macpro-x86-2 pod pod-configmaps-16c395cf-bcca-455f-b478-99e5fe4cac01 container env-test: <nil>
STEP: delete the pod
Jan 31 10:22:18.302: INFO: Waiting for pod pod-configmaps-16c395cf-bcca-455f-b478-99e5fe4cac01 to disappear
Jan 31 10:22:18.305: INFO: Pod pod-configmaps-16c395cf-bcca-455f-b478-99e5fe4cac01 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 31 10:22:18.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1645" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":160,"skipped":2798,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:22:18.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7187
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-db62cf52-ec73-4056-b065-826c56d073b4
STEP: Creating configMap with name cm-test-opt-upd-81949a17-9ff9-42de-989b-8ae741ca8cda
STEP: Creating the pod
Jan 31 10:22:18.462: INFO: The status of Pod pod-projected-configmaps-7609e435-980a-4171-9435-325f600f110f is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:22:20.468: INFO: The status of Pod pod-projected-configmaps-7609e435-980a-4171-9435-325f600f110f is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-db62cf52-ec73-4056-b065-826c56d073b4
STEP: Updating configmap cm-test-opt-upd-81949a17-9ff9-42de-989b-8ae741ca8cda
STEP: Creating configMap with name cm-test-opt-create-8c978b62-7963-4f3c-804b-7ef9ebfe1f3d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 31 10:22:24.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7187" for this suite.

• [SLOW TEST:6.211 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":161,"skipped":2803,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:22:24.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7341
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 10:22:24.657: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ae6bea4-9017-48b5-aebd-80c367c02507" in namespace "downward-api-7341" to be "Succeeded or Failed"
Jan 31 10:22:24.663: INFO: Pod "downwardapi-volume-1ae6bea4-9017-48b5-aebd-80c367c02507": Phase="Pending", Reason="", readiness=false. Elapsed: 5.862733ms
Jan 31 10:22:26.670: INFO: Pod "downwardapi-volume-1ae6bea4-9017-48b5-aebd-80c367c02507": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012474755s
Jan 31 10:22:28.675: INFO: Pod "downwardapi-volume-1ae6bea4-9017-48b5-aebd-80c367c02507": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017240105s
STEP: Saw pod success
Jan 31 10:22:28.675: INFO: Pod "downwardapi-volume-1ae6bea4-9017-48b5-aebd-80c367c02507" satisfied condition "Succeeded or Failed"
Jan 31 10:22:28.677: INFO: Trying to get logs from node macpro-x86-2 pod downwardapi-volume-1ae6bea4-9017-48b5-aebd-80c367c02507 container client-container: <nil>
STEP: delete the pod
Jan 31 10:22:28.690: INFO: Waiting for pod downwardapi-volume-1ae6bea4-9017-48b5-aebd-80c367c02507 to disappear
Jan 31 10:22:28.693: INFO: Pod downwardapi-volume-1ae6bea4-9017-48b5-aebd-80c367c02507 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 31 10:22:28.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7341" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":162,"skipped":2840,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:22:28.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-4439
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 31 10:22:34.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4439" for this suite.

• [SLOW TEST:6.190 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":356,"completed":163,"skipped":2851,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:22:34.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5501
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5501
STEP: creating service affinity-nodeport in namespace services-5501
STEP: creating replication controller affinity-nodeport in namespace services-5501
I0131 10:22:35.055805      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5501, replica count: 3
I0131 10:22:38.107472      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 31 10:22:38.124: INFO: Creating new exec pod
Jan 31 10:22:41.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-5501 exec execpod-affinitywzv4j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 31 10:22:41.240: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 31 10:22:41.240: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:22:41.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-5501 exec execpod-affinitywzv4j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.219.31 80'
Jan 31 10:22:41.336: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.219.31 80\nConnection to 10.103.219.31 80 port [tcp/http] succeeded!\n"
Jan 31 10:22:41.336: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:22:41.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-5501 exec execpod-affinitywzv4j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.31 30876'
Jan 31 10:22:41.423: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.31 30876\nConnection to 10.221.188.31 30876 port [tcp/*] succeeded!\n"
Jan 31 10:22:41.423: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:22:41.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-5501 exec execpod-affinitywzv4j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.32 30876'
Jan 31 10:22:41.516: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.32 30876\nConnection to 10.221.188.32 30876 port [tcp/*] succeeded!\n"
Jan 31 10:22:41.516: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:22:41.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-5501 exec execpod-affinitywzv4j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.221.188.31:30876/ ; done'
Jan 31 10:22:41.644: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:30876/\n"
Jan 31 10:22:41.644: INFO: stdout: "\naffinity-nodeport-dsxld\naffinity-nodeport-dsxld\naffinity-nodeport-dsxld\naffinity-nodeport-dsxld\naffinity-nodeport-dsxld\naffinity-nodeport-dsxld\naffinity-nodeport-dsxld\naffinity-nodeport-dsxld\naffinity-nodeport-dsxld\naffinity-nodeport-dsxld\naffinity-nodeport-dsxld\naffinity-nodeport-dsxld\naffinity-nodeport-dsxld\naffinity-nodeport-dsxld\naffinity-nodeport-dsxld\naffinity-nodeport-dsxld"
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.644: INFO: Received response from host: affinity-nodeport-dsxld
Jan 31 10:22:41.645: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5501, will wait for the garbage collector to delete the pods
Jan 31 10:22:41.716: INFO: Deleting ReplicationController affinity-nodeport took: 7.004268ms
Jan 31 10:22:41.817: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.815865ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 10:22:44.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5501" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.272 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":164,"skipped":2888,"failed":0}
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:22:44.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1232
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 10:22:44.295: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f7b98ba-5235-40c4-882b-d1ea541e898a" in namespace "downward-api-1232" to be "Succeeded or Failed"
Jan 31 10:22:44.297: INFO: Pod "downwardapi-volume-7f7b98ba-5235-40c4-882b-d1ea541e898a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.248596ms
Jan 31 10:22:46.301: INFO: Pod "downwardapi-volume-7f7b98ba-5235-40c4-882b-d1ea541e898a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006743887s
Jan 31 10:22:48.305: INFO: Pod "downwardapi-volume-7f7b98ba-5235-40c4-882b-d1ea541e898a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010537571s
STEP: Saw pod success
Jan 31 10:22:48.305: INFO: Pod "downwardapi-volume-7f7b98ba-5235-40c4-882b-d1ea541e898a" satisfied condition "Succeeded or Failed"
Jan 31 10:22:48.307: INFO: Trying to get logs from node macpro-x86-2 pod downwardapi-volume-7f7b98ba-5235-40c4-882b-d1ea541e898a container client-container: <nil>
STEP: delete the pod
Jan 31 10:22:48.322: INFO: Waiting for pod downwardapi-volume-7f7b98ba-5235-40c4-882b-d1ea541e898a to disappear
Jan 31 10:22:48.331: INFO: Pod downwardapi-volume-7f7b98ba-5235-40c4-882b-d1ea541e898a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 31 10:22:48.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1232" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":165,"skipped":2888,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:22:48.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-376
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 31 10:22:48.485: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:22:48.485: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:22:48.485: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:22:48.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:22:48.486: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 10:22:49.491: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:22:49.491: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:22:49.491: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:22:49.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:22:49.493: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 10:22:50.491: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:22:50.492: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:22:50.492: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:22:50.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 31 10:22:50.494: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan 31 10:22:50.514: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24128"},"items":null}

Jan 31 10:22:50.519: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24130"},"items":[{"metadata":{"name":"daemon-set-gmz7s","generateName":"daemon-set-","namespace":"daemonsets-376","uid":"7abe60dc-5000-4b29-b066-bd2f7011cb62","resourceVersion":"24130","creationTimestamp":"2023-01-31T10:22:48Z","deletionTimestamp":"2023-01-31T10:23:20Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"9e2c0a0d0474c0ae76dcced307423af09af80678b2a0af5a64c30abd3b233bd3","cni.projectcalico.org/podIP":"192.168.75.42/32","cni.projectcalico.org/podIPs":"192.168.75.42/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e6f8518e-9e1b-49c1-807e-161afed97b3d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-31T10:22:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-31T10:22:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6f8518e-9e1b-49c1-807e-161afed97b3d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-31T10:22:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.75.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-psnq5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-psnq5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"macpro-x86-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["macpro-x86-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-31T10:22:48Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-31T10:22:50Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-31T10:22:50Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-31T10:22:48Z"}],"hostIP":"10.221.188.31","podIP":"192.168.75.42","podIPs":[{"ip":"192.168.75.42"}],"startTime":"2023-01-31T10:22:48Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-31T10:22:49Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://c6dc8c43703f67905695c98e2150490aca030ae7a2da49a8c4a589d3ee62c5bb","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jgn98","generateName":"daemon-set-","namespace":"daemonsets-376","uid":"5e2d3de7-3006-4911-8d6e-38903278ae45","resourceVersion":"24129","creationTimestamp":"2023-01-31T10:22:48Z","deletionTimestamp":"2023-01-31T10:23:20Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"50aaa595991561557445b51be1913cdeb2b679d17d2fff3c462ba2fc10949326","cni.projectcalico.org/podIP":"192.168.38.230/32","cni.projectcalico.org/podIPs":"192.168.38.230/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e6f8518e-9e1b-49c1-807e-161afed97b3d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-31T10:22:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-31T10:22:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6f8518e-9e1b-49c1-807e-161afed97b3d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-31T10:22:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.38.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xl4wp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xl4wp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"macpro-x86-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["macpro-x86-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-31T10:22:48Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-31T10:22:49Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-31T10:22:49Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-31T10:22:48Z"}],"hostIP":"10.221.188.32","podIP":"192.168.38.230","podIPs":[{"ip":"192.168.38.230"}],"startTime":"2023-01-31T10:22:48Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-31T10:22:49Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://4787389d5325ec9df05c2cf7ba1d886174c7cf2e68fa0ed3ec7f63832034456a","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 31 10:22:50.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-376" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":356,"completed":166,"skipped":2903,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:22:50.533: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8666
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-8666
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 31 10:22:50.655: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 31 10:22:50.697: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:22:52.702: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 10:22:54.701: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 10:22:56.703: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 10:22:58.701: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 10:23:00.701: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 10:23:02.702: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 10:23:04.703: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 10:23:06.701: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 10:23:08.702: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 10:23:10.702: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 10:23:12.701: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 31 10:23:12.704: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 31 10:23:14.723: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 31 10:23:14.723: INFO: Breadth first check of 192.168.75.30 on host 10.221.188.31...
Jan 31 10:23:14.726: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.75.19:9080/dial?request=hostname&protocol=udp&host=192.168.75.30&port=8081&tries=1'] Namespace:pod-network-test-8666 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 10:23:14.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 10:23:14.726: INFO: ExecWithOptions: Clientset creation
Jan 31 10:23:14.726: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8666/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.75.19%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.75.30%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 31 10:23:14.790: INFO: Waiting for responses: map[]
Jan 31 10:23:14.790: INFO: reached 192.168.75.30 after 0/1 tries
Jan 31 10:23:14.790: INFO: Breadth first check of 192.168.38.233 on host 10.221.188.32...
Jan 31 10:23:14.793: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.75.19:9080/dial?request=hostname&protocol=udp&host=192.168.38.233&port=8081&tries=1'] Namespace:pod-network-test-8666 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 10:23:14.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 10:23:14.794: INFO: ExecWithOptions: Clientset creation
Jan 31 10:23:14.794: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8666/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.75.19%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.38.233%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 31 10:23:14.844: INFO: Waiting for responses: map[]
Jan 31 10:23:14.844: INFO: reached 192.168.38.233 after 0/1 tries
Jan 31 10:23:14.844: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 31 10:23:14.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8666" for this suite.

• [SLOW TEST:24.322 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":356,"completed":167,"skipped":2912,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:23:14.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8443
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-ded8ed90-7516-4942-be55-f405a7ba8470
STEP: Creating a pod to test consume configMaps
Jan 31 10:23:14.990: INFO: Waiting up to 5m0s for pod "pod-configmaps-d0c4a59e-329f-4849-baf7-2398fd9474d0" in namespace "configmap-8443" to be "Succeeded or Failed"
Jan 31 10:23:14.994: INFO: Pod "pod-configmaps-d0c4a59e-329f-4849-baf7-2398fd9474d0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.99109ms
Jan 31 10:23:17.000: INFO: Pod "pod-configmaps-d0c4a59e-329f-4849-baf7-2398fd9474d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010268538s
Jan 31 10:23:19.005: INFO: Pod "pod-configmaps-d0c4a59e-329f-4849-baf7-2398fd9474d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015609295s
STEP: Saw pod success
Jan 31 10:23:19.005: INFO: Pod "pod-configmaps-d0c4a59e-329f-4849-baf7-2398fd9474d0" satisfied condition "Succeeded or Failed"
Jan 31 10:23:19.008: INFO: Trying to get logs from node macpro-x86-2 pod pod-configmaps-d0c4a59e-329f-4849-baf7-2398fd9474d0 container agnhost-container: <nil>
STEP: delete the pod
Jan 31 10:23:19.027: INFO: Waiting for pod pod-configmaps-d0c4a59e-329f-4849-baf7-2398fd9474d0 to disappear
Jan 31 10:23:19.030: INFO: Pod pod-configmaps-d0c4a59e-329f-4849-baf7-2398fd9474d0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 31 10:23:19.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8443" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":168,"skipped":2915,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:23:19.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5485
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 31 10:23:19.177: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 31 10:23:24.184: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 31 10:23:25.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5485" for this suite.

• [SLOW TEST:6.179 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":356,"completed":169,"skipped":2942,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:23:25.218: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6673
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-mgj9
STEP: Creating a pod to test atomic-volume-subpath
Jan 31 10:23:25.372: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mgj9" in namespace "subpath-6673" to be "Succeeded or Failed"
Jan 31 10:23:25.376: INFO: Pod "pod-subpath-test-configmap-mgj9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.785828ms
Jan 31 10:23:27.383: INFO: Pod "pod-subpath-test-configmap-mgj9": Phase="Running", Reason="", readiness=true. Elapsed: 2.010896571s
Jan 31 10:23:29.386: INFO: Pod "pod-subpath-test-configmap-mgj9": Phase="Running", Reason="", readiness=true. Elapsed: 4.01453625s
Jan 31 10:23:31.391: INFO: Pod "pod-subpath-test-configmap-mgj9": Phase="Running", Reason="", readiness=true. Elapsed: 6.019535023s
Jan 31 10:23:33.395: INFO: Pod "pod-subpath-test-configmap-mgj9": Phase="Running", Reason="", readiness=true. Elapsed: 8.023467009s
Jan 31 10:23:35.400: INFO: Pod "pod-subpath-test-configmap-mgj9": Phase="Running", Reason="", readiness=true. Elapsed: 10.028104809s
Jan 31 10:23:37.406: INFO: Pod "pod-subpath-test-configmap-mgj9": Phase="Running", Reason="", readiness=true. Elapsed: 12.034157113s
Jan 31 10:23:39.411: INFO: Pod "pod-subpath-test-configmap-mgj9": Phase="Running", Reason="", readiness=true. Elapsed: 14.038927518s
Jan 31 10:23:41.415: INFO: Pod "pod-subpath-test-configmap-mgj9": Phase="Running", Reason="", readiness=true. Elapsed: 16.043288726s
Jan 31 10:23:43.420: INFO: Pod "pod-subpath-test-configmap-mgj9": Phase="Running", Reason="", readiness=true. Elapsed: 18.048535356s
Jan 31 10:23:45.425: INFO: Pod "pod-subpath-test-configmap-mgj9": Phase="Running", Reason="", readiness=true. Elapsed: 20.053591863s
Jan 31 10:23:47.432: INFO: Pod "pod-subpath-test-configmap-mgj9": Phase="Running", Reason="", readiness=false. Elapsed: 22.060636178s
Jan 31 10:23:49.438: INFO: Pod "pod-subpath-test-configmap-mgj9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.065823079s
STEP: Saw pod success
Jan 31 10:23:49.438: INFO: Pod "pod-subpath-test-configmap-mgj9" satisfied condition "Succeeded or Failed"
Jan 31 10:23:49.440: INFO: Trying to get logs from node macpro-x86-1 pod pod-subpath-test-configmap-mgj9 container test-container-subpath-configmap-mgj9: <nil>
STEP: delete the pod
Jan 31 10:23:49.454: INFO: Waiting for pod pod-subpath-test-configmap-mgj9 to disappear
Jan 31 10:23:49.457: INFO: Pod pod-subpath-test-configmap-mgj9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mgj9
Jan 31 10:23:49.457: INFO: Deleting pod "pod-subpath-test-configmap-mgj9" in namespace "subpath-6673"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 31 10:23:49.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6673" for this suite.

• [SLOW TEST:24.250 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","total":356,"completed":170,"skipped":2946,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:23:49.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-4913
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jan 31 10:23:49.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4913" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":356,"completed":171,"skipped":2970,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:23:49.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9459
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 31 10:23:49.745: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 31 10:23:49.751: INFO: Waiting for terminating namespaces to be deleted...
Jan 31 10:23:49.753: INFO: 
Logging pods the apiserver thinks is on node macpro-x86-1 before test
Jan 31 10:23:49.759: INFO: calico-node-nd7tv from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 10:23:49.759: INFO: 	Container calico-node ready: true, restart count 0
Jan 31 10:23:49.759: INFO: kube-proxy-tmcdj from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 10:23:49.759: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 31 10:23:49.759: INFO: helm-charts-fluent-bit-ndvnj from logging started at 2023-01-31 10:08:01 +0000 UTC (1 container statuses recorded)
Jan 31 10:23:49.759: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 31 10:23:49.759: INFO: speaker-rnwpp from metallb-system started at 2023-01-31 10:07:58 +0000 UTC (1 container statuses recorded)
Jan 31 10:23:49.759: INFO: 	Container speaker ready: true, restart count 0
Jan 31 10:23:49.759: INFO: kube-prometheus-stack-prometheus-node-exporter-8c76s from orka-monitoring started at 2023-01-31 10:07:57 +0000 UTC (1 container statuses recorded)
Jan 31 10:23:49.759: INFO: 	Container node-exporter ready: true, restart count 0
Jan 31 10:23:49.759: INFO: sonobuoy from sonobuoy started at 2023-01-31 09:42:03 +0000 UTC (1 container statuses recorded)
Jan 31 10:23:49.759: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 31 10:23:49.759: INFO: sonobuoy-systemd-logs-daemon-set-e1cae5bb05d74403-zlhvz from sonobuoy started at 2023-01-31 09:42:03 +0000 UTC (2 container statuses recorded)
Jan 31 10:23:49.759: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 10:23:49.759: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 31 10:23:49.759: INFO: 
Logging pods the apiserver thinks is on node macpro-x86-2 before test
Jan 31 10:23:49.785: INFO: calico-node-w8d7z from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 10:23:49.785: INFO: 	Container calico-node ready: true, restart count 0
Jan 31 10:23:49.785: INFO: kube-proxy-fj2ms from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 10:23:49.785: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 31 10:23:49.785: INFO: helm-charts-fluent-bit-bwlwb from logging started at 2023-01-31 09:32:49 +0000 UTC (1 container statuses recorded)
Jan 31 10:23:49.785: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 31 10:23:49.785: INFO: speaker-7pfgg from metallb-system started at 2023-01-31 09:31:56 +0000 UTC (1 container statuses recorded)
Jan 31 10:23:49.785: INFO: 	Container speaker ready: true, restart count 0
Jan 31 10:23:49.785: INFO: kube-prometheus-stack-prometheus-node-exporter-b8d2q from orka-monitoring started at 2023-01-31 09:34:26 +0000 UTC (1 container statuses recorded)
Jan 31 10:23:49.785: INFO: 	Container node-exporter ready: true, restart count 0
Jan 31 10:23:49.785: INFO: sonobuoy-e2e-job-1083219078514678 from sonobuoy started at 2023-01-31 09:42:03 +0000 UTC (2 container statuses recorded)
Jan 31 10:23:49.785: INFO: 	Container e2e ready: true, restart count 0
Jan 31 10:23:49.785: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 10:23:49.785: INFO: sonobuoy-systemd-logs-daemon-set-e1cae5bb05d74403-fhxh8 from sonobuoy started at 2023-01-31 09:42:04 +0000 UTC (2 container statuses recorded)
Jan 31 10:23:49.785: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 10:23:49.785: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173f5db7a9978f8a], Reason = [FailedScheduling], Message = [0/5 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 31 10:23:50.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9459" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":356,"completed":172,"skipped":2976,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:23:50.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1713
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 31 10:23:50.971: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:50.971: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:50.971: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:50.973: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:23:50.973: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 10:23:51.980: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:51.980: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:51.981: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:51.983: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:23:51.983: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 10:23:52.978: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:52.979: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:52.979: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:52.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 31 10:23:52.982: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 31 10:23:53.002: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:53.002: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:53.002: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:53.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 10:23:53.010: INFO: Node macpro-x86-2 is running 0 daemon pod, expected 1
Jan 31 10:23:54.016: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:54.016: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:54.016: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:54.020: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 10:23:54.020: INFO: Node macpro-x86-2 is running 0 daemon pod, expected 1
Jan 31 10:23:55.014: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:55.014: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:55.014: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:23:55.016: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 31 10:23:55.016: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1713, will wait for the garbage collector to delete the pods
Jan 31 10:23:55.077: INFO: Deleting DaemonSet.extensions daemon-set took: 4.880734ms
Jan 31 10:23:55.178: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.574761ms
Jan 31 10:23:57.183: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:23:57.183: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 31 10:23:57.185: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24618"},"items":null}

Jan 31 10:23:57.187: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24618"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 31 10:23:57.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1713" for this suite.

• [SLOW TEST:6.380 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":356,"completed":173,"skipped":3011,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:23:57.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9240
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:23:57.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:23:58.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9240" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":356,"completed":174,"skipped":3021,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests 
  should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:23:58.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename conformance-tests
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in conformance-tests-592
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
STEP: Getting node addresses
Jan 31 10:23:58.537: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:188
Jan 31 10:23:58.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-592" for this suite.
•{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","total":356,"completed":175,"skipped":3041,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:23:58.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8022
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-c676a673-47e6-442b-be6c-3c4896966e3f
STEP: Creating a pod to test consume secrets
Jan 31 10:23:58.683: INFO: Waiting up to 5m0s for pod "pod-secrets-c0676569-807b-4771-8de0-9b76a1ea22b3" in namespace "secrets-8022" to be "Succeeded or Failed"
Jan 31 10:23:58.685: INFO: Pod "pod-secrets-c0676569-807b-4771-8de0-9b76a1ea22b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.229531ms
Jan 31 10:24:00.690: INFO: Pod "pod-secrets-c0676569-807b-4771-8de0-9b76a1ea22b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006656934s
Jan 31 10:24:02.694: INFO: Pod "pod-secrets-c0676569-807b-4771-8de0-9b76a1ea22b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01049669s
STEP: Saw pod success
Jan 31 10:24:02.694: INFO: Pod "pod-secrets-c0676569-807b-4771-8de0-9b76a1ea22b3" satisfied condition "Succeeded or Failed"
Jan 31 10:24:02.695: INFO: Trying to get logs from node macpro-x86-1 pod pod-secrets-c0676569-807b-4771-8de0-9b76a1ea22b3 container secret-volume-test: <nil>
STEP: delete the pod
Jan 31 10:24:02.708: INFO: Waiting for pod pod-secrets-c0676569-807b-4771-8de0-9b76a1ea22b3 to disappear
Jan 31 10:24:02.710: INFO: Pod pod-secrets-c0676569-807b-4771-8de0-9b76a1ea22b3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 31 10:24:02.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8022" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":176,"skipped":3042,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:24:02.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-4497
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 31 10:24:02.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4497" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":177,"skipped":3054,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:24:02.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-4272
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jan 31 10:24:02.980: INFO: Waiting up to 5m0s for pod "security-context-086bad2c-7dac-4bc9-8507-e704ffe5d416" in namespace "security-context-4272" to be "Succeeded or Failed"
Jan 31 10:24:02.986: INFO: Pod "security-context-086bad2c-7dac-4bc9-8507-e704ffe5d416": Phase="Pending", Reason="", readiness=false. Elapsed: 6.249959ms
Jan 31 10:24:04.990: INFO: Pod "security-context-086bad2c-7dac-4bc9-8507-e704ffe5d416": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010524413s
Jan 31 10:24:06.995: INFO: Pod "security-context-086bad2c-7dac-4bc9-8507-e704ffe5d416": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015546746s
STEP: Saw pod success
Jan 31 10:24:06.996: INFO: Pod "security-context-086bad2c-7dac-4bc9-8507-e704ffe5d416" satisfied condition "Succeeded or Failed"
Jan 31 10:24:06.998: INFO: Trying to get logs from node macpro-x86-2 pod security-context-086bad2c-7dac-4bc9-8507-e704ffe5d416 container test-container: <nil>
STEP: delete the pod
Jan 31 10:24:07.017: INFO: Waiting for pod security-context-086bad2c-7dac-4bc9-8507-e704ffe5d416 to disappear
Jan 31 10:24:07.019: INFO: Pod security-context-086bad2c-7dac-4bc9-8507-e704ffe5d416 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 31 10:24:07.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4272" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":178,"skipped":3083,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:24:07.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-626
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 31 10:24:09.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-626" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":356,"completed":179,"skipped":3093,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:24:10.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-4704
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 31 10:24:10.763: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 10:24:13.803: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:24:13.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:24:16.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4704" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:6.962 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":356,"completed":180,"skipped":3122,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:24:16.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8507
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-be30cc05-92dc-4176-90d6-11903ee2a5d0
STEP: Creating a pod to test consume configMaps
Jan 31 10:24:17.177: INFO: Waiting up to 5m0s for pod "pod-configmaps-bee84f20-9e2c-4c0a-9f88-ebf568fca2a3" in namespace "configmap-8507" to be "Succeeded or Failed"
Jan 31 10:24:17.187: INFO: Pod "pod-configmaps-bee84f20-9e2c-4c0a-9f88-ebf568fca2a3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.05669ms
Jan 31 10:24:19.191: INFO: Pod "pod-configmaps-bee84f20-9e2c-4c0a-9f88-ebf568fca2a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014139072s
Jan 31 10:24:21.198: INFO: Pod "pod-configmaps-bee84f20-9e2c-4c0a-9f88-ebf568fca2a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020938161s
STEP: Saw pod success
Jan 31 10:24:21.198: INFO: Pod "pod-configmaps-bee84f20-9e2c-4c0a-9f88-ebf568fca2a3" satisfied condition "Succeeded or Failed"
Jan 31 10:24:21.200: INFO: Trying to get logs from node macpro-x86-2 pod pod-configmaps-bee84f20-9e2c-4c0a-9f88-ebf568fca2a3 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 31 10:24:21.215: INFO: Waiting for pod pod-configmaps-bee84f20-9e2c-4c0a-9f88-ebf568fca2a3 to disappear
Jan 31 10:24:21.218: INFO: Pod pod-configmaps-bee84f20-9e2c-4c0a-9f88-ebf568fca2a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 31 10:24:21.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8507" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":181,"skipped":3123,"failed":0}
SS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:24:21.225: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1972
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching services
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 10:24:21.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1972" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":356,"completed":182,"skipped":3125,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:24:21.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9216
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
Jan 31 10:24:21.559: INFO: created test-event-1
Jan 31 10:24:21.563: INFO: created test-event-2
Jan 31 10:24:21.566: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jan 31 10:24:21.569: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jan 31 10:24:21.582: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Jan 31 10:24:21.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9216" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":356,"completed":183,"skipped":3158,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:24:21.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5755
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 10:24:22.123: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 10:24:25.150: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:24:35.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5755" for this suite.
STEP: Destroying namespace "webhook-5755-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:13.743 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":356,"completed":184,"skipped":3161,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:24:35.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-843
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-843
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Jan 31 10:24:35.504: INFO: Found 0 stateful pods, waiting for 3
Jan 31 10:24:45.508: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 31 10:24:45.508: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 31 10:24:45.508: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 31 10:24:45.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-843 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 31 10:24:45.632: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 31 10:24:45.632: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 31 10:24:45.632: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jan 31 10:24:55.665: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 31 10:25:05.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-843 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 31 10:25:05.778: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 31 10:25:05.778: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 31 10:25:05.778: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Jan 31 10:25:15.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-843 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 31 10:25:15.916: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 31 10:25:15.916: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 31 10:25:15.916: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 31 10:25:25.948: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 31 10:25:35.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-843 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 31 10:25:36.087: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 31 10:25:36.087: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 31 10:25:36.087: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 31 10:25:46.103: INFO: Deleting all statefulset in ns statefulset-843
Jan 31 10:25:46.105: INFO: Scaling statefulset ss2 to 0
Jan 31 10:25:56.123: INFO: Waiting for statefulset status.replicas updated to 0
Jan 31 10:25:56.125: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 31 10:25:56.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-843" for this suite.

• [SLOW TEST:80.823 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":356,"completed":185,"skipped":3277,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:25:56.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9883
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:25:56.281: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jan 31 10:25:57.331: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 31 10:25:58.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9883" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":356,"completed":186,"skipped":3308,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:25:58.354: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3978
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 10:25:58.500: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0781fd2-dbea-41a0-b07c-611ab312e0fe" in namespace "projected-3978" to be "Succeeded or Failed"
Jan 31 10:25:58.505: INFO: Pod "downwardapi-volume-f0781fd2-dbea-41a0-b07c-611ab312e0fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.67709ms
Jan 31 10:26:00.513: INFO: Pod "downwardapi-volume-f0781fd2-dbea-41a0-b07c-611ab312e0fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01278761s
Jan 31 10:26:02.516: INFO: Pod "downwardapi-volume-f0781fd2-dbea-41a0-b07c-611ab312e0fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016580449s
STEP: Saw pod success
Jan 31 10:26:02.516: INFO: Pod "downwardapi-volume-f0781fd2-dbea-41a0-b07c-611ab312e0fe" satisfied condition "Succeeded or Failed"
Jan 31 10:26:02.519: INFO: Trying to get logs from node macpro-x86-1 pod downwardapi-volume-f0781fd2-dbea-41a0-b07c-611ab312e0fe container client-container: <nil>
STEP: delete the pod
Jan 31 10:26:02.542: INFO: Waiting for pod downwardapi-volume-f0781fd2-dbea-41a0-b07c-611ab312e0fe to disappear
Jan 31 10:26:02.545: INFO: Pod downwardapi-volume-f0781fd2-dbea-41a0-b07c-611ab312e0fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 31 10:26:02.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3978" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":187,"skipped":3317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:26:02.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4204
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating secret secrets-4204/secret-test-03e6d1f5-54a1-4d2b-98ba-2d80a9c36893
STEP: Creating a pod to test consume secrets
Jan 31 10:26:02.698: INFO: Waiting up to 5m0s for pod "pod-configmaps-9a6d3253-5dde-439e-9072-96c91f23913d" in namespace "secrets-4204" to be "Succeeded or Failed"
Jan 31 10:26:02.702: INFO: Pod "pod-configmaps-9a6d3253-5dde-439e-9072-96c91f23913d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.341918ms
Jan 31 10:26:04.707: INFO: Pod "pod-configmaps-9a6d3253-5dde-439e-9072-96c91f23913d": Phase="Running", Reason="", readiness=false. Elapsed: 2.008787695s
Jan 31 10:26:06.711: INFO: Pod "pod-configmaps-9a6d3253-5dde-439e-9072-96c91f23913d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012901291s
STEP: Saw pod success
Jan 31 10:26:06.711: INFO: Pod "pod-configmaps-9a6d3253-5dde-439e-9072-96c91f23913d" satisfied condition "Succeeded or Failed"
Jan 31 10:26:06.713: INFO: Trying to get logs from node macpro-x86-1 pod pod-configmaps-9a6d3253-5dde-439e-9072-96c91f23913d container env-test: <nil>
STEP: delete the pod
Jan 31 10:26:06.726: INFO: Waiting for pod pod-configmaps-9a6d3253-5dde-439e-9072-96c91f23913d to disappear
Jan 31 10:26:06.727: INFO: Pod pod-configmaps-9a6d3253-5dde-439e-9072-96c91f23913d no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 31 10:26:06.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4204" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":188,"skipped":3347,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:26:06.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9671
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-9671
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 31 10:26:06.860: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 31 10:26:06.898: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:26:08.903: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 10:26:10.906: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 10:26:12.901: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 10:26:14.904: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 10:26:16.904: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 31 10:26:18.905: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 31 10:26:18.913: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 31 10:26:20.959: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 31 10:26:20.959: INFO: Going to poll 192.168.75.32 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 31 10:26:20.961: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.75.32:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9671 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 10:26:20.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 10:26:20.961: INFO: ExecWithOptions: Clientset creation
Jan 31 10:26:20.961: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9671/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.75.32%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 31 10:26:21.023: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 31 10:26:21.023: INFO: Going to poll 192.168.38.197 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 31 10:26:21.025: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.38.197:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9671 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 10:26:21.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 10:26:21.026: INFO: ExecWithOptions: Clientset creation
Jan 31 10:26:21.026: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9671/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.38.197%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 31 10:26:21.068: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 31 10:26:21.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9671" for this suite.

• [SLOW TEST:14.343 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":189,"skipped":3352,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:26:21.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2443
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 31 10:26:21.214: INFO: The status of Pod labelsupdatebcdd1bbd-e63f-4b2c-adc9-cabfa96fc551 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:26:23.218: INFO: The status of Pod labelsupdatebcdd1bbd-e63f-4b2c-adc9-cabfa96fc551 is Running (Ready = true)
Jan 31 10:26:23.737: INFO: Successfully updated pod "labelsupdatebcdd1bbd-e63f-4b2c-adc9-cabfa96fc551"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 31 10:26:27.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2443" for this suite.

• [SLOW TEST:6.684 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":190,"skipped":3368,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:26:27.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-2799
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  test/e2e/framework/framework.go:652
STEP: Request ServerVersion
STEP: Confirm major version
Jan 31 10:26:27.887: INFO: Major version: 1
STEP: Confirm minor version
Jan 31 10:26:27.887: INFO: cleanMinorVersion: 24
Jan 31 10:26:27.887: INFO: Minor version: 24
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:188
Jan 31 10:26:27.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2799" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":356,"completed":191,"skipped":3378,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:26:27.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3295
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:26:28.032: INFO: The status of Pod busybox-host-aliases17e4e91b-8a6a-46bf-b4da-f2f3c1e52cb8 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:26:30.035: INFO: The status of Pod busybox-host-aliases17e4e91b-8a6a-46bf-b4da-f2f3c1e52cb8 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 31 10:26:30.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3295" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":192,"skipped":3389,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:26:30.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2188
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 31 10:26:46.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2188" for this suite.

• [SLOW TEST:16.241 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":356,"completed":193,"skipped":3402,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:26:46.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-881
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:26:46.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:26:52.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-881" for this suite.

• [SLOW TEST:6.554 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":356,"completed":194,"skipped":3410,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:26:52.851: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4290
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jan 31 10:26:53.070: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 31 10:26:58.074: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 31 10:26:58.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4290" for this suite.

• [SLOW TEST:5.269 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":356,"completed":195,"skipped":3414,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:26:58.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7601
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7601, will wait for the garbage collector to delete the pods
Jan 31 10:27:00.323: INFO: Deleting Job.batch foo took: 5.397378ms
Jan 31 10:27:00.423: INFO: Terminating Job.batch foo pods took: 100.073677ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 31 10:27:32.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7601" for this suite.

• [SLOW TEST:34.516 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":356,"completed":196,"skipped":3429,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:27:32.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7684
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 31 10:27:43.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7684" for this suite.

• [SLOW TEST:11.177 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":356,"completed":197,"skipped":3431,"failed":0}
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:27:43.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-7734
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:188
Jan 31 10:27:43.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7734" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":356,"completed":198,"skipped":3432,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:27:43.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6978
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 31 10:27:44.090: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:27:46.094: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 31 10:27:46.114: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:27:48.118: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 31 10:27:48.130: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 31 10:27:48.135: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 31 10:27:50.136: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 31 10:27:50.140: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 31 10:27:52.136: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 31 10:27:52.139: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 31 10:27:52.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6978" for this suite.

• [SLOW TEST:8.214 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":356,"completed":199,"skipped":3455,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:27:52.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7687
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
STEP: create deployment with httpd image
Jan 31 10:27:52.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-7687 create -f -'
Jan 31 10:27:53.427: INFO: stderr: ""
Jan 31 10:27:53.427: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jan 31 10:27:53.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-7687 diff -f -'
Jan 31 10:27:54.621: INFO: rc: 1
Jan 31 10:27:54.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-7687 delete -f -'
Jan 31 10:27:54.674: INFO: stderr: ""
Jan 31 10:27:54.674: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 10:27:54.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7687" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":356,"completed":200,"skipped":3467,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:27:54.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8258
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name projected-secret-test-3d8c865e-a919-4df4-b1e3-a0e701224b6e
STEP: Creating a pod to test consume secrets
Jan 31 10:27:54.857: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-021c70ae-9fbe-4262-8d64-615f89cee2fb" in namespace "projected-8258" to be "Succeeded or Failed"
Jan 31 10:27:54.861: INFO: Pod "pod-projected-secrets-021c70ae-9fbe-4262-8d64-615f89cee2fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.557867ms
Jan 31 10:27:56.864: INFO: Pod "pod-projected-secrets-021c70ae-9fbe-4262-8d64-615f89cee2fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0072356s
Jan 31 10:27:58.868: INFO: Pod "pod-projected-secrets-021c70ae-9fbe-4262-8d64-615f89cee2fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011071279s
STEP: Saw pod success
Jan 31 10:27:58.868: INFO: Pod "pod-projected-secrets-021c70ae-9fbe-4262-8d64-615f89cee2fb" satisfied condition "Succeeded or Failed"
Jan 31 10:27:58.870: INFO: Trying to get logs from node macpro-x86-2 pod pod-projected-secrets-021c70ae-9fbe-4262-8d64-615f89cee2fb container secret-volume-test: <nil>
STEP: delete the pod
Jan 31 10:27:58.901: INFO: Waiting for pod pod-projected-secrets-021c70ae-9fbe-4262-8d64-615f89cee2fb to disappear
Jan 31 10:27:58.909: INFO: Pod pod-projected-secrets-021c70ae-9fbe-4262-8d64-615f89cee2fb no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 31 10:27:58.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8258" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":201,"skipped":3511,"failed":0}

------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:27:58.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4945
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jan 31 10:27:59.067: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Jan 31 10:27:59.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4945" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":356,"completed":202,"skipped":3511,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:27:59.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5699
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 31 10:27:59.223: INFO: Waiting up to 5m0s for pod "pod-bf196b33-6cad-4622-a6ac-73bc4d123959" in namespace "emptydir-5699" to be "Succeeded or Failed"
Jan 31 10:27:59.227: INFO: Pod "pod-bf196b33-6cad-4622-a6ac-73bc4d123959": Phase="Pending", Reason="", readiness=false. Elapsed: 3.395743ms
Jan 31 10:28:01.231: INFO: Pod "pod-bf196b33-6cad-4622-a6ac-73bc4d123959": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008263638s
Jan 31 10:28:03.235: INFO: Pod "pod-bf196b33-6cad-4622-a6ac-73bc4d123959": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012157259s
STEP: Saw pod success
Jan 31 10:28:03.235: INFO: Pod "pod-bf196b33-6cad-4622-a6ac-73bc4d123959" satisfied condition "Succeeded or Failed"
Jan 31 10:28:03.238: INFO: Trying to get logs from node macpro-x86-1 pod pod-bf196b33-6cad-4622-a6ac-73bc4d123959 container test-container: <nil>
STEP: delete the pod
Jan 31 10:28:03.253: INFO: Waiting for pod pod-bf196b33-6cad-4622-a6ac-73bc4d123959 to disappear
Jan 31 10:28:03.256: INFO: Pod pod-bf196b33-6cad-4622-a6ac-73bc4d123959 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 31 10:28:03.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5699" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":203,"skipped":3525,"failed":0}
SSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:28:03.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4976
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:28:03.404: INFO: The status of Pod test-webserver-83a1cb66-864f-4b70-8469-3357dd16d933 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:28:05.409: INFO: The status of Pod test-webserver-83a1cb66-864f-4b70-8469-3357dd16d933 is Running (Ready = false)
Jan 31 10:28:07.412: INFO: The status of Pod test-webserver-83a1cb66-864f-4b70-8469-3357dd16d933 is Running (Ready = false)
Jan 31 10:28:09.409: INFO: The status of Pod test-webserver-83a1cb66-864f-4b70-8469-3357dd16d933 is Running (Ready = false)
Jan 31 10:28:11.410: INFO: The status of Pod test-webserver-83a1cb66-864f-4b70-8469-3357dd16d933 is Running (Ready = false)
Jan 31 10:28:13.408: INFO: The status of Pod test-webserver-83a1cb66-864f-4b70-8469-3357dd16d933 is Running (Ready = false)
Jan 31 10:28:15.411: INFO: The status of Pod test-webserver-83a1cb66-864f-4b70-8469-3357dd16d933 is Running (Ready = false)
Jan 31 10:28:17.410: INFO: The status of Pod test-webserver-83a1cb66-864f-4b70-8469-3357dd16d933 is Running (Ready = false)
Jan 31 10:28:19.410: INFO: The status of Pod test-webserver-83a1cb66-864f-4b70-8469-3357dd16d933 is Running (Ready = false)
Jan 31 10:28:21.409: INFO: The status of Pod test-webserver-83a1cb66-864f-4b70-8469-3357dd16d933 is Running (Ready = false)
Jan 31 10:28:23.409: INFO: The status of Pod test-webserver-83a1cb66-864f-4b70-8469-3357dd16d933 is Running (Ready = false)
Jan 31 10:28:25.410: INFO: The status of Pod test-webserver-83a1cb66-864f-4b70-8469-3357dd16d933 is Running (Ready = true)
Jan 31 10:28:25.412: INFO: Container started at 2023-01-31 10:28:03 +0000 UTC, pod became ready at 2023-01-31 10:28:23 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 31 10:28:25.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4976" for this suite.

• [SLOW TEST:22.158 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":356,"completed":204,"skipped":3532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:28:25.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7928
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 10:28:25.555: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12ce8a60-240a-494d-ab1e-fd865f04bcaf" in namespace "downward-api-7928" to be "Succeeded or Failed"
Jan 31 10:28:25.564: INFO: Pod "downwardapi-volume-12ce8a60-240a-494d-ab1e-fd865f04bcaf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.252253ms
Jan 31 10:28:27.571: INFO: Pod "downwardapi-volume-12ce8a60-240a-494d-ab1e-fd865f04bcaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015668339s
Jan 31 10:28:29.579: INFO: Pod "downwardapi-volume-12ce8a60-240a-494d-ab1e-fd865f04bcaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023581484s
STEP: Saw pod success
Jan 31 10:28:29.579: INFO: Pod "downwardapi-volume-12ce8a60-240a-494d-ab1e-fd865f04bcaf" satisfied condition "Succeeded or Failed"
Jan 31 10:28:29.581: INFO: Trying to get logs from node macpro-x86-1 pod downwardapi-volume-12ce8a60-240a-494d-ab1e-fd865f04bcaf container client-container: <nil>
STEP: delete the pod
Jan 31 10:28:29.598: INFO: Waiting for pod downwardapi-volume-12ce8a60-240a-494d-ab1e-fd865f04bcaf to disappear
Jan 31 10:28:29.600: INFO: Pod downwardapi-volume-12ce8a60-240a-494d-ab1e-fd865f04bcaf no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 31 10:28:29.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7928" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":205,"skipped":3577,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:28:29.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8504
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating api versions
Jan 31 10:28:29.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-8504 api-versions'
Jan 31 10:28:29.805: INFO: stderr: ""
Jan 31 10:28:29.805: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nlogicmonitor.com/v1alpha1\nlogicmonitor.com/v1alpha2\nmacstadium.orka.com/v1alpha1\nmetallb.io/v1alpha1\nmetallb.io/v1beta1\nmetallb.io/v1beta2\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntraefik.containo.us/v1alpha1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 10:28:29.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8504" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":356,"completed":206,"skipped":3620,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:28:29.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3757
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 31 10:28:31.988: INFO: DNS probes using dns-3757/dns-test-b481af31-c344-4646-aa23-2750063f6f4f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 31 10:28:32.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3757" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":356,"completed":207,"skipped":3626,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:28:32.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1745
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1745
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating stateful set ss in namespace statefulset-1745
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1745
Jan 31 10:28:32.151: INFO: Found 0 stateful pods, waiting for 1
Jan 31 10:28:42.155: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 31 10:28:42.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-1745 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 31 10:28:42.264: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 31 10:28:42.264: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 31 10:28:42.264: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 31 10:28:42.267: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 31 10:28:52.276: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 31 10:28:52.276: INFO: Waiting for statefulset status.replicas updated to 0
Jan 31 10:28:52.291: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 31 10:28:52.291: INFO: ss-0  macpro-x86-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:28:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:28:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:28:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:28:32 +0000 UTC  }]
Jan 31 10:28:52.291: INFO: 
Jan 31 10:28:52.291: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 31 10:28:53.296: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996111629s
Jan 31 10:28:54.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990308635s
Jan 31 10:28:55.308: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986424487s
Jan 31 10:28:56.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978696533s
Jan 31 10:28:57.317: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974160844s
Jan 31 10:28:58.323: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969537813s
Jan 31 10:28:59.328: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964775598s
Jan 31 10:29:00.332: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.959634389s
Jan 31 10:29:01.338: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.744436ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1745
Jan 31 10:29:02.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-1745 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 31 10:29:02.437: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 31 10:29:02.437: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 31 10:29:02.437: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 31 10:29:02.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-1745 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 31 10:29:02.532: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 31 10:29:02.532: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 31 10:29:02.532: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 31 10:29:02.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-1745 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 31 10:29:02.613: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 31 10:29:02.613: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 31 10:29:02.613: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 31 10:29:02.617: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 31 10:29:02.617: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 31 10:29:02.617: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 31 10:29:02.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-1745 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 31 10:29:02.707: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 31 10:29:02.707: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 31 10:29:02.707: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 31 10:29:02.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-1745 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 31 10:29:02.810: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 31 10:29:02.810: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 31 10:29:02.810: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 31 10:29:02.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=statefulset-1745 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 31 10:29:02.912: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 31 10:29:02.912: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 31 10:29:02.912: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 31 10:29:02.912: INFO: Waiting for statefulset status.replicas updated to 0
Jan 31 10:29:02.915: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 31 10:29:12.922: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 31 10:29:12.922: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 31 10:29:12.922: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 31 10:29:12.932: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 31 10:29:12.932: INFO: ss-0  macpro-x86-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:28:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:29:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:29:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:28:32 +0000 UTC  }]
Jan 31 10:29:12.932: INFO: ss-1  macpro-x86-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:28:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:29:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:29:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:28:52 +0000 UTC  }]
Jan 31 10:29:12.932: INFO: ss-2  macpro-x86-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:28:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:29:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:29:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-31 10:28:52 +0000 UTC  }]
Jan 31 10:29:12.932: INFO: 
Jan 31 10:29:12.932: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 31 10:29:13.936: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.996179478s
Jan 31 10:29:14.940: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.991885824s
Jan 31 10:29:15.945: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987131943s
Jan 31 10:29:16.949: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.983505494s
Jan 31 10:29:17.953: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.979438073s
Jan 31 10:29:18.958: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.975581907s
Jan 31 10:29:19.963: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.970151543s
Jan 31 10:29:20.967: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.964845577s
Jan 31 10:29:21.970: INFO: Verifying statefulset ss doesn't scale past 0 for another 961.055152ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1745
Jan 31 10:29:22.974: INFO: Scaling statefulset ss to 0
Jan 31 10:29:22.983: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 31 10:29:22.986: INFO: Deleting all statefulset in ns statefulset-1745
Jan 31 10:29:22.990: INFO: Scaling statefulset ss to 0
Jan 31 10:29:22.998: INFO: Waiting for statefulset status.replicas updated to 0
Jan 31 10:29:23.000: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 31 10:29:23.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1745" for this suite.

• [SLOW TEST:51.026 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":356,"completed":208,"skipped":3635,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:29:23.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9869
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 31 10:29:23.166: INFO: Waiting up to 5m0s for pod "pod-fcf3570f-768b-43c0-93c2-063ee84ca132" in namespace "emptydir-9869" to be "Succeeded or Failed"
Jan 31 10:29:23.173: INFO: Pod "pod-fcf3570f-768b-43c0-93c2-063ee84ca132": Phase="Pending", Reason="", readiness=false. Elapsed: 7.631464ms
Jan 31 10:29:25.178: INFO: Pod "pod-fcf3570f-768b-43c0-93c2-063ee84ca132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012636683s
Jan 31 10:29:27.184: INFO: Pod "pod-fcf3570f-768b-43c0-93c2-063ee84ca132": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018109427s
STEP: Saw pod success
Jan 31 10:29:27.184: INFO: Pod "pod-fcf3570f-768b-43c0-93c2-063ee84ca132" satisfied condition "Succeeded or Failed"
Jan 31 10:29:27.186: INFO: Trying to get logs from node macpro-x86-1 pod pod-fcf3570f-768b-43c0-93c2-063ee84ca132 container test-container: <nil>
STEP: delete the pod
Jan 31 10:29:27.204: INFO: Waiting for pod pod-fcf3570f-768b-43c0-93c2-063ee84ca132 to disappear
Jan 31 10:29:27.206: INFO: Pod pod-fcf3570f-768b-43c0-93c2-063ee84ca132 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 31 10:29:27.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9869" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":209,"skipped":3666,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:29:27.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-6206
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jan 31 10:29:29.390: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 31 10:29:31.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6206" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":356,"completed":210,"skipped":3679,"failed":0}

------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:29:31.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6181
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Jan 31 10:29:31.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6181 create -f -'
Jan 31 10:29:31.705: INFO: stderr: ""
Jan 31 10:29:31.705: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 31 10:29:31.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6181 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 31 10:29:31.756: INFO: stderr: ""
Jan 31 10:29:31.756: INFO: stdout: "update-demo-nautilus-d5xrv update-demo-nautilus-dl5dj "
Jan 31 10:29:31.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6181 get pods update-demo-nautilus-d5xrv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 31 10:29:31.799: INFO: stderr: ""
Jan 31 10:29:31.799: INFO: stdout: ""
Jan 31 10:29:31.799: INFO: update-demo-nautilus-d5xrv is created but not running
Jan 31 10:29:36.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6181 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 31 10:29:36.853: INFO: stderr: ""
Jan 31 10:29:36.853: INFO: stdout: "update-demo-nautilus-d5xrv update-demo-nautilus-dl5dj "
Jan 31 10:29:36.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6181 get pods update-demo-nautilus-d5xrv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 31 10:29:36.901: INFO: stderr: ""
Jan 31 10:29:36.901: INFO: stdout: "true"
Jan 31 10:29:36.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6181 get pods update-demo-nautilus-d5xrv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 31 10:29:36.946: INFO: stderr: ""
Jan 31 10:29:36.946: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 31 10:29:36.946: INFO: validating pod update-demo-nautilus-d5xrv
Jan 31 10:29:36.951: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 31 10:29:36.951: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 31 10:29:36.951: INFO: update-demo-nautilus-d5xrv is verified up and running
Jan 31 10:29:36.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6181 get pods update-demo-nautilus-dl5dj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 31 10:29:36.997: INFO: stderr: ""
Jan 31 10:29:36.997: INFO: stdout: "true"
Jan 31 10:29:36.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6181 get pods update-demo-nautilus-dl5dj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 31 10:29:37.039: INFO: stderr: ""
Jan 31 10:29:37.039: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 31 10:29:37.039: INFO: validating pod update-demo-nautilus-dl5dj
Jan 31 10:29:37.043: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 31 10:29:37.043: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 31 10:29:37.043: INFO: update-demo-nautilus-dl5dj is verified up and running
STEP: using delete to clean up resources
Jan 31 10:29:37.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6181 delete --grace-period=0 --force -f -'
Jan 31 10:29:37.089: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 31 10:29:37.089: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 31 10:29:37.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6181 get rc,svc -l name=update-demo --no-headers'
Jan 31 10:29:37.143: INFO: stderr: "No resources found in kubectl-6181 namespace.\n"
Jan 31 10:29:37.143: INFO: stdout: ""
Jan 31 10:29:37.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-6181 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 31 10:29:37.193: INFO: stderr: ""
Jan 31 10:29:37.193: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 10:29:37.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6181" for this suite.

• [SLOW TEST:5.770 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should create and stop a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":356,"completed":211,"skipped":3679,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:29:37.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4776
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-5f12fd4e-a294-42b4-8a61-fb86ce8388b9
STEP: Creating a pod to test consume configMaps
Jan 31 10:29:37.346: INFO: Waiting up to 5m0s for pod "pod-configmaps-fd5232a2-cf99-4a20-a657-502873219945" in namespace "configmap-4776" to be "Succeeded or Failed"
Jan 31 10:29:37.358: INFO: Pod "pod-configmaps-fd5232a2-cf99-4a20-a657-502873219945": Phase="Pending", Reason="", readiness=false. Elapsed: 11.815194ms
Jan 31 10:29:39.389: INFO: Pod "pod-configmaps-fd5232a2-cf99-4a20-a657-502873219945": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042982217s
Jan 31 10:29:41.398: INFO: Pod "pod-configmaps-fd5232a2-cf99-4a20-a657-502873219945": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051683286s
STEP: Saw pod success
Jan 31 10:29:41.398: INFO: Pod "pod-configmaps-fd5232a2-cf99-4a20-a657-502873219945" satisfied condition "Succeeded or Failed"
Jan 31 10:29:41.401: INFO: Trying to get logs from node macpro-x86-1 pod pod-configmaps-fd5232a2-cf99-4a20-a657-502873219945 container agnhost-container: <nil>
STEP: delete the pod
Jan 31 10:29:41.416: INFO: Waiting for pod pod-configmaps-fd5232a2-cf99-4a20-a657-502873219945 to disappear
Jan 31 10:29:41.418: INFO: Pod pod-configmaps-fd5232a2-cf99-4a20-a657-502873219945 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 31 10:29:41.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4776" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":212,"skipped":3688,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:29:41.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5456
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3037
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4172
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jan 31 10:29:54.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5456" for this suite.
STEP: Destroying namespace "nsdeletetest-3037" for this suite.
Jan 31 10:29:54.858: INFO: Namespace nsdeletetest-3037 was already deleted
STEP: Destroying namespace "nsdeletetest-4172" for this suite.

• [SLOW TEST:13.437 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":356,"completed":213,"skipped":3730,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:29:54.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9385
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 31 10:29:55.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9385" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":356,"completed":214,"skipped":3745,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:29:55.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8982
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 10:29:55.659: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 10:29:58.685: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:29:58.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8982" for this suite.
STEP: Destroying namespace "webhook-8982-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":356,"completed":215,"skipped":3806,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:29:58.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3018
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 31 10:30:15.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3018" for this suite.

• [SLOW TEST:16.251 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":356,"completed":216,"skipped":3810,"failed":0}
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:30:15.041: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6923
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 31 10:30:15.167: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 31 10:30:18.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6923" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":356,"completed":217,"skipped":3817,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:30:18.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4110
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 31 10:30:18.816: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 31 10:30:22.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4110" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":356,"completed":218,"skipped":3827,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:30:22.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-7108
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 31 10:30:24.264: INFO: starting watch
STEP: patching
STEP: updating
Jan 31 10:30:24.273: INFO: waiting for watch events with expected annotations
Jan 31 10:30:24.273: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:30:24.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7108" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":356,"completed":219,"skipped":3868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:30:24.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8401
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jan 31 10:30:26.983: INFO: Successfully updated pod "adopt-release-4k8d5"
STEP: Checking that the Job readopts the Pod
Jan 31 10:30:26.983: INFO: Waiting up to 15m0s for pod "adopt-release-4k8d5" in namespace "job-8401" to be "adopted"
Jan 31 10:30:26.994: INFO: Pod "adopt-release-4k8d5": Phase="Running", Reason="", readiness=true. Elapsed: 10.908678ms
Jan 31 10:30:29.003: INFO: Pod "adopt-release-4k8d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.020457181s
Jan 31 10:30:29.003: INFO: Pod "adopt-release-4k8d5" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jan 31 10:30:29.515: INFO: Successfully updated pod "adopt-release-4k8d5"
STEP: Checking that the Job releases the Pod
Jan 31 10:30:29.515: INFO: Waiting up to 15m0s for pod "adopt-release-4k8d5" in namespace "job-8401" to be "released"
Jan 31 10:30:29.518: INFO: Pod "adopt-release-4k8d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.247141ms
Jan 31 10:30:31.523: INFO: Pod "adopt-release-4k8d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007339788s
Jan 31 10:30:31.523: INFO: Pod "adopt-release-4k8d5" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 31 10:30:31.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8401" for this suite.

• [SLOW TEST:7.205 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":356,"completed":220,"skipped":3903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:30:31.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9091
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-9091
Jan 31 10:30:31.665: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:30:33.670: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 31 10:30:33.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-9091 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 31 10:30:33.756: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 31 10:30:33.756: INFO: stdout: "iptables"
Jan 31 10:30:33.756: INFO: proxyMode: iptables
Jan 31 10:30:33.766: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 31 10:30:33.769: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-9091
STEP: creating replication controller affinity-clusterip-timeout in namespace services-9091
I0131 10:30:33.793433      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9091, replica count: 3
I0131 10:30:36.844687      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 31 10:30:36.851: INFO: Creating new exec pod
Jan 31 10:30:39.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-9091 exec execpod-affinityhsx89 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 31 10:30:39.968: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 31 10:30:39.968: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:30:39.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-9091 exec execpod-affinityhsx89 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.17.244 80'
Jan 31 10:30:40.057: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.17.244 80\nConnection to 10.97.17.244 80 port [tcp/http] succeeded!\n"
Jan 31 10:30:40.057: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:30:40.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-9091 exec execpod-affinityhsx89 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.97.17.244:80/ ; done'
Jan 31 10:30:40.184: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n"
Jan 31 10:30:40.184: INFO: stdout: "\naffinity-clusterip-timeout-wzzw9\naffinity-clusterip-timeout-wzzw9\naffinity-clusterip-timeout-wzzw9\naffinity-clusterip-timeout-wzzw9\naffinity-clusterip-timeout-wzzw9\naffinity-clusterip-timeout-wzzw9\naffinity-clusterip-timeout-wzzw9\naffinity-clusterip-timeout-wzzw9\naffinity-clusterip-timeout-wzzw9\naffinity-clusterip-timeout-wzzw9\naffinity-clusterip-timeout-wzzw9\naffinity-clusterip-timeout-wzzw9\naffinity-clusterip-timeout-wzzw9\naffinity-clusterip-timeout-wzzw9\naffinity-clusterip-timeout-wzzw9\naffinity-clusterip-timeout-wzzw9"
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Received response from host: affinity-clusterip-timeout-wzzw9
Jan 31 10:30:40.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-9091 exec execpod-affinityhsx89 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.97.17.244:80/'
Jan 31 10:30:40.270: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n"
Jan 31 10:30:40.271: INFO: stdout: "affinity-clusterip-timeout-wzzw9"
Jan 31 10:31:00.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-9091 exec execpod-affinityhsx89 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.97.17.244:80/'
Jan 31 10:31:00.375: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.97.17.244:80/\n"
Jan 31 10:31:00.375: INFO: stdout: "affinity-clusterip-timeout-xdskd"
Jan 31 10:31:00.375: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9091, will wait for the garbage collector to delete the pods
Jan 31 10:31:00.447: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.583237ms
Jan 31 10:31:00.547: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.631927ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 10:31:02.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9091" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:31.362 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":221,"skipped":3936,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:31:02.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-1751
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 31 10:33:01.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1751" for this suite.

• [SLOW TEST:118.174 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":356,"completed":222,"skipped":3967,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:33:01.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1166
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service multi-endpoint-test in namespace services-1166
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1166 to expose endpoints map[]
Jan 31 10:33:01.213: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 31 10:33:02.221: INFO: successfully validated that service multi-endpoint-test in namespace services-1166 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1166
Jan 31 10:33:02.232: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:33:04.236: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1166 to expose endpoints map[pod1:[100]]
Jan 31 10:33:04.244: INFO: successfully validated that service multi-endpoint-test in namespace services-1166 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1166
Jan 31 10:33:04.255: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:33:06.261: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1166 to expose endpoints map[pod1:[100] pod2:[101]]
Jan 31 10:33:06.275: INFO: successfully validated that service multi-endpoint-test in namespace services-1166 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Jan 31 10:33:06.275: INFO: Creating new exec pod
Jan 31 10:33:09.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-1166 exec execpod5vf4h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 31 10:33:09.396: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 31 10:33:09.396: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:33:09.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-1166 exec execpod5vf4h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.116.253 80'
Jan 31 10:33:09.482: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.116.253 80\nConnection to 10.102.116.253 80 port [tcp/http] succeeded!\n"
Jan 31 10:33:09.482: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:33:09.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-1166 exec execpod5vf4h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 31 10:33:09.570: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 31 10:33:09.570: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:33:09.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-1166 exec execpod5vf4h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.116.253 81'
Jan 31 10:33:09.655: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.116.253 81\nConnection to 10.102.116.253 81 port [tcp/*] succeeded!\n"
Jan 31 10:33:09.655: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-1166
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1166 to expose endpoints map[pod2:[101]]
Jan 31 10:33:10.693: INFO: successfully validated that service multi-endpoint-test in namespace services-1166 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1166
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1166 to expose endpoints map[]
Jan 31 10:33:11.716: INFO: successfully validated that service multi-endpoint-test in namespace services-1166 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 10:33:11.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1166" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:10.695 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":356,"completed":223,"skipped":3971,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:33:11.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8368
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 31 10:33:11.902: INFO: The status of Pod annotationupdatedf463c25-5adf-4642-aba7-7d1318a3f185 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:33:13.907: INFO: The status of Pod annotationupdatedf463c25-5adf-4642-aba7-7d1318a3f185 is Running (Ready = true)
Jan 31 10:33:14.437: INFO: Successfully updated pod "annotationupdatedf463c25-5adf-4642-aba7-7d1318a3f185"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 31 10:33:18.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8368" for this suite.

• [SLOW TEST:6.700 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":224,"skipped":4027,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:33:18.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6110
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:33:18.605: INFO: Create a RollingUpdate DaemonSet
Jan 31 10:33:18.616: INFO: Check that daemon pods launch on every node of the cluster
Jan 31 10:33:18.627: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:18.627: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:18.627: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:18.630: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:33:18.630: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 10:33:19.634: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:19.634: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:19.634: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:19.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:33:19.637: INFO: Node macpro-x86-1 is running 0 daemon pod, expected 1
Jan 31 10:33:20.635: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:20.635: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:20.635: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:20.639: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 31 10:33:20.639: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jan 31 10:33:20.639: INFO: Update the DaemonSet to trigger a rollout
Jan 31 10:33:20.645: INFO: Updating DaemonSet daemon-set
Jan 31 10:33:23.665: INFO: Roll back the DaemonSet before rollout is complete
Jan 31 10:33:23.673: INFO: Updating DaemonSet daemon-set
Jan 31 10:33:23.673: INFO: Make sure DaemonSet rollback is complete
Jan 31 10:33:23.679: INFO: Wrong image for pod: daemon-set-65z7r. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 31 10:33:23.679: INFO: Pod daemon-set-65z7r is not available
Jan 31 10:33:23.690: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:23.690: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:23.690: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:24.698: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:24.698: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:24.698: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:25.696: INFO: Pod daemon-set-5n5pl is not available
Jan 31 10:33:25.708: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:25.708: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 31 10:33:25.708: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6110, will wait for the garbage collector to delete the pods
Jan 31 10:33:25.771: INFO: Deleting DaemonSet.extensions daemon-set took: 5.676947ms
Jan 31 10:33:25.872: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.570978ms
Jan 31 10:33:27.975: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 10:33:27.975: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 31 10:33:27.977: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28768"},"items":null}

Jan 31 10:33:27.981: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28768"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 31 10:33:27.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6110" for this suite.

• [SLOW TEST:9.534 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":356,"completed":225,"skipped":4049,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:33:27.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7619
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:33:28.181: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"717d5907-b2f6-452f-a69d-7b8bdf58a7df", Controller:(*bool)(0xc0042ecfd6), BlockOwnerDeletion:(*bool)(0xc0042ecfd7)}}
Jan 31 10:33:28.189: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8061b907-a8a1-440b-975e-ed311b7a25d7", Controller:(*bool)(0xc0042ed2a6), BlockOwnerDeletion:(*bool)(0xc0042ed2a7)}}
Jan 31 10:33:28.200: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f6d7ae78-644f-44f3-9faa-a293777edc43", Controller:(*bool)(0xc000019856), BlockOwnerDeletion:(*bool)(0xc000019857)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 31 10:33:33.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7619" for this suite.

• [SLOW TEST:5.223 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":356,"completed":226,"skipped":4088,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:33:33.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-1293
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:33:33.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Creating first CR 
Jan 31 10:33:35.897: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-31T10:33:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-31T10:33:35Z]] name:name1 resourceVersion:28874 uid:f9301f51-9085-47c5-9618-7de40a91e70b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jan 31 10:33:45.908: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-31T10:33:45Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-31T10:33:45Z]] name:name2 resourceVersion:28909 uid:a6c37e16-c272-4f02-9972-077b83de9d09] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jan 31 10:33:55.915: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-31T10:33:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-31T10:33:55Z]] name:name1 resourceVersion:28931 uid:f9301f51-9085-47c5-9618-7de40a91e70b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jan 31 10:34:05.927: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-31T10:33:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-31T10:34:05Z]] name:name2 resourceVersion:28950 uid:a6c37e16-c272-4f02-9972-077b83de9d09] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jan 31 10:34:15.943: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-31T10:33:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-31T10:33:55Z]] name:name1 resourceVersion:28969 uid:f9301f51-9085-47c5-9618-7de40a91e70b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jan 31 10:34:25.956: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-31T10:33:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-31T10:34:05Z]] name:name2 resourceVersion:28988 uid:a6c37e16-c272-4f02-9972-077b83de9d09] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:34:36.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1293" for this suite.

• [SLOW TEST:63.267 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":356,"completed":227,"skipped":4120,"failed":0}
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:34:36.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-6461
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan 31 10:34:36.618: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 31 10:35:36.654: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:35:36.657: INFO: Starting informer...
STEP: Starting pods...
Jan 31 10:35:36.874: INFO: Pod1 is running on macpro-x86-1. Tainting Node
Jan 31 10:35:39.094: INFO: Pod2 is running on macpro-x86-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jan 31 10:35:45.229: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 31 10:36:05.267: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:188
Jan 31 10:36:05.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6461" for this suite.

• [SLOW TEST:88.850 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":356,"completed":228,"skipped":4120,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:36:05.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-919
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-919
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-919
Jan 31 10:36:05.524: INFO: Found 0 stateful pods, waiting for 1
Jan 31 10:36:15.530: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Jan 31 10:36:15.547: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Jan 31 10:36:15.553: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Jan 31 10:36:15.554: INFO: Observed &StatefulSet event: ADDED
Jan 31 10:36:15.554: INFO: Found Statefulset ss in namespace statefulset-919 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 31 10:36:15.554: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Jan 31 10:36:15.554: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 31 10:36:15.560: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Jan 31 10:36:15.561: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 31 10:36:15.561: INFO: Deleting all statefulset in ns statefulset-919
Jan 31 10:36:15.563: INFO: Scaling statefulset ss to 0
Jan 31 10:36:25.580: INFO: Waiting for statefulset status.replicas updated to 0
Jan 31 10:36:25.582: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 31 10:36:25.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-919" for this suite.

• [SLOW TEST:20.276 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":356,"completed":229,"skipped":4138,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:36:25.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4683
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:36:25.751: INFO: Waiting up to 5m0s for pod "busybox-user-65534-31ce6156-c28d-4d7d-9f78-fb6c07844303" in namespace "security-context-test-4683" to be "Succeeded or Failed"
Jan 31 10:36:25.755: INFO: Pod "busybox-user-65534-31ce6156-c28d-4d7d-9f78-fb6c07844303": Phase="Pending", Reason="", readiness=false. Elapsed: 4.895453ms
Jan 31 10:36:27.762: INFO: Pod "busybox-user-65534-31ce6156-c28d-4d7d-9f78-fb6c07844303": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011302215s
Jan 31 10:36:29.768: INFO: Pod "busybox-user-65534-31ce6156-c28d-4d7d-9f78-fb6c07844303": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017029673s
Jan 31 10:36:29.768: INFO: Pod "busybox-user-65534-31ce6156-c28d-4d7d-9f78-fb6c07844303" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 31 10:36:29.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4683" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":230,"skipped":4250,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:36:29.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-2271
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:36:30.394: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 31 10:36:30.395: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 31 10:36:30.395: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 31 10:36:30.395: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 31 10:36:30.395: INFO: Checking APIGroup: apps
Jan 31 10:36:30.396: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 31 10:36:30.396: INFO: Versions found [{apps/v1 v1}]
Jan 31 10:36:30.396: INFO: apps/v1 matches apps/v1
Jan 31 10:36:30.396: INFO: Checking APIGroup: events.k8s.io
Jan 31 10:36:30.397: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 31 10:36:30.397: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jan 31 10:36:30.397: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 31 10:36:30.397: INFO: Checking APIGroup: authentication.k8s.io
Jan 31 10:36:30.397: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 31 10:36:30.397: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 31 10:36:30.397: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 31 10:36:30.397: INFO: Checking APIGroup: authorization.k8s.io
Jan 31 10:36:30.398: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 31 10:36:30.398: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 31 10:36:30.398: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 31 10:36:30.398: INFO: Checking APIGroup: autoscaling
Jan 31 10:36:30.399: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 31 10:36:30.399: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jan 31 10:36:30.399: INFO: autoscaling/v2 matches autoscaling/v2
Jan 31 10:36:30.399: INFO: Checking APIGroup: batch
Jan 31 10:36:30.399: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 31 10:36:30.399: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jan 31 10:36:30.399: INFO: batch/v1 matches batch/v1
Jan 31 10:36:30.399: INFO: Checking APIGroup: certificates.k8s.io
Jan 31 10:36:30.400: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 31 10:36:30.400: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 31 10:36:30.400: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 31 10:36:30.400: INFO: Checking APIGroup: networking.k8s.io
Jan 31 10:36:30.401: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 31 10:36:30.401: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 31 10:36:30.401: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 31 10:36:30.401: INFO: Checking APIGroup: policy
Jan 31 10:36:30.401: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 31 10:36:30.401: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jan 31 10:36:30.401: INFO: policy/v1 matches policy/v1
Jan 31 10:36:30.401: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 31 10:36:30.402: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 31 10:36:30.402: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 31 10:36:30.402: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 31 10:36:30.402: INFO: Checking APIGroup: storage.k8s.io
Jan 31 10:36:30.402: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 31 10:36:30.402: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 31 10:36:30.402: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 31 10:36:30.402: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 31 10:36:30.403: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 31 10:36:30.403: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 31 10:36:30.403: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 31 10:36:30.403: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 31 10:36:30.404: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 31 10:36:30.404: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 31 10:36:30.404: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 31 10:36:30.404: INFO: Checking APIGroup: scheduling.k8s.io
Jan 31 10:36:30.404: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 31 10:36:30.404: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 31 10:36:30.404: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 31 10:36:30.404: INFO: Checking APIGroup: coordination.k8s.io
Jan 31 10:36:30.405: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 31 10:36:30.405: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 31 10:36:30.405: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 31 10:36:30.405: INFO: Checking APIGroup: node.k8s.io
Jan 31 10:36:30.405: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 31 10:36:30.405: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jan 31 10:36:30.405: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 31 10:36:30.405: INFO: Checking APIGroup: discovery.k8s.io
Jan 31 10:36:30.406: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 31 10:36:30.406: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jan 31 10:36:30.406: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 31 10:36:30.406: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 31 10:36:30.407: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 31 10:36:30.407: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 31 10:36:30.407: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 31 10:36:30.407: INFO: Checking APIGroup: crd.projectcalico.org
Jan 31 10:36:30.407: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 31 10:36:30.407: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 31 10:36:30.407: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan 31 10:36:30.407: INFO: Checking APIGroup: monitoring.coreos.com
Jan 31 10:36:30.408: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jan 31 10:36:30.408: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Jan 31 10:36:30.408: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jan 31 10:36:30.408: INFO: Checking APIGroup: logicmonitor.com
Jan 31 10:36:30.409: INFO: PreferredVersion.GroupVersion: logicmonitor.com/v1alpha2
Jan 31 10:36:30.409: INFO: Versions found [{logicmonitor.com/v1alpha2 v1alpha2} {logicmonitor.com/v1alpha1 v1alpha1}]
Jan 31 10:36:30.409: INFO: logicmonitor.com/v1alpha2 matches logicmonitor.com/v1alpha2
Jan 31 10:36:30.409: INFO: Checking APIGroup: macstadium.orka.com
Jan 31 10:36:30.410: INFO: PreferredVersion.GroupVersion: macstadium.orka.com/v1alpha1
Jan 31 10:36:30.410: INFO: Versions found [{macstadium.orka.com/v1alpha1 v1alpha1}]
Jan 31 10:36:30.410: INFO: macstadium.orka.com/v1alpha1 matches macstadium.orka.com/v1alpha1
Jan 31 10:36:30.410: INFO: Checking APIGroup: metallb.io
Jan 31 10:36:30.410: INFO: PreferredVersion.GroupVersion: metallb.io/v1beta2
Jan 31 10:36:30.410: INFO: Versions found [{metallb.io/v1beta2 v1beta2} {metallb.io/v1beta1 v1beta1} {metallb.io/v1alpha1 v1alpha1}]
Jan 31 10:36:30.410: INFO: metallb.io/v1beta2 matches metallb.io/v1beta2
Jan 31 10:36:30.410: INFO: Checking APIGroup: traefik.containo.us
Jan 31 10:36:30.411: INFO: PreferredVersion.GroupVersion: traefik.containo.us/v1alpha1
Jan 31 10:36:30.411: INFO: Versions found [{traefik.containo.us/v1alpha1 v1alpha1}]
Jan 31 10:36:30.411: INFO: traefik.containo.us/v1alpha1 matches traefik.containo.us/v1alpha1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:188
Jan 31 10:36:30.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-2271" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":356,"completed":231,"skipped":4280,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:36:30.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-2441
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:36:30.564: INFO: Endpoints addresses: [10.221.188.5 10.221.188.6 10.221.188.7] , ports: [6443]
Jan 31 10:36:30.564: INFO: EndpointSlices addresses: [10.221.188.5 10.221.188.6 10.221.188.7] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 31 10:36:30.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2441" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":356,"completed":232,"skipped":4281,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:36:30.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9200
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 31 10:36:30.703: INFO: Waiting up to 5m0s for pod "pod-6bf345d5-e787-478f-8577-b8f9f1e55f43" in namespace "emptydir-9200" to be "Succeeded or Failed"
Jan 31 10:36:30.710: INFO: Pod "pod-6bf345d5-e787-478f-8577-b8f9f1e55f43": Phase="Pending", Reason="", readiness=false. Elapsed: 7.176234ms
Jan 31 10:36:32.714: INFO: Pod "pod-6bf345d5-e787-478f-8577-b8f9f1e55f43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010907964s
Jan 31 10:36:34.720: INFO: Pod "pod-6bf345d5-e787-478f-8577-b8f9f1e55f43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016965885s
STEP: Saw pod success
Jan 31 10:36:34.720: INFO: Pod "pod-6bf345d5-e787-478f-8577-b8f9f1e55f43" satisfied condition "Succeeded or Failed"
Jan 31 10:36:34.722: INFO: Trying to get logs from node macpro-x86-2 pod pod-6bf345d5-e787-478f-8577-b8f9f1e55f43 container test-container: <nil>
STEP: delete the pod
Jan 31 10:36:34.759: INFO: Waiting for pod pod-6bf345d5-e787-478f-8577-b8f9f1e55f43 to disappear
Jan 31 10:36:34.762: INFO: Pod pod-6bf345d5-e787-478f-8577-b8f9f1e55f43 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 31 10:36:34.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9200" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":233,"skipped":4282,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:36:34.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2807
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's args
Jan 31 10:36:34.910: INFO: Waiting up to 5m0s for pod "var-expansion-527735c6-9b8e-494c-bad7-48c8a81d2330" in namespace "var-expansion-2807" to be "Succeeded or Failed"
Jan 31 10:36:34.916: INFO: Pod "var-expansion-527735c6-9b8e-494c-bad7-48c8a81d2330": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052263ms
Jan 31 10:36:36.921: INFO: Pod "var-expansion-527735c6-9b8e-494c-bad7-48c8a81d2330": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010222316s
Jan 31 10:36:38.927: INFO: Pod "var-expansion-527735c6-9b8e-494c-bad7-48c8a81d2330": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016873653s
STEP: Saw pod success
Jan 31 10:36:38.927: INFO: Pod "var-expansion-527735c6-9b8e-494c-bad7-48c8a81d2330" satisfied condition "Succeeded or Failed"
Jan 31 10:36:38.932: INFO: Trying to get logs from node macpro-x86-2 pod var-expansion-527735c6-9b8e-494c-bad7-48c8a81d2330 container dapi-container: <nil>
STEP: delete the pod
Jan 31 10:36:38.957: INFO: Waiting for pod var-expansion-527735c6-9b8e-494c-bad7-48c8a81d2330 to disappear
Jan 31 10:36:38.959: INFO: Pod var-expansion-527735c6-9b8e-494c-bad7-48c8a81d2330 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 31 10:36:38.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2807" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":356,"completed":234,"skipped":4352,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:36:38.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-444
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 10:36:39.105: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75fa3980-4428-42e8-b626-1d5364cca75d" in namespace "projected-444" to be "Succeeded or Failed"
Jan 31 10:36:39.110: INFO: Pod "downwardapi-volume-75fa3980-4428-42e8-b626-1d5364cca75d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.023217ms
Jan 31 10:36:41.116: INFO: Pod "downwardapi-volume-75fa3980-4428-42e8-b626-1d5364cca75d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010792227s
Jan 31 10:36:43.120: INFO: Pod "downwardapi-volume-75fa3980-4428-42e8-b626-1d5364cca75d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015235746s
STEP: Saw pod success
Jan 31 10:36:43.120: INFO: Pod "downwardapi-volume-75fa3980-4428-42e8-b626-1d5364cca75d" satisfied condition "Succeeded or Failed"
Jan 31 10:36:43.122: INFO: Trying to get logs from node macpro-x86-1 pod downwardapi-volume-75fa3980-4428-42e8-b626-1d5364cca75d container client-container: <nil>
STEP: delete the pod
Jan 31 10:36:43.143: INFO: Waiting for pod downwardapi-volume-75fa3980-4428-42e8-b626-1d5364cca75d to disappear
Jan 31 10:36:43.146: INFO: Pod downwardapi-volume-75fa3980-4428-42e8-b626-1d5364cca75d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 31 10:36:43.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-444" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":235,"skipped":4353,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:36:43.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5806
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with configMap that has name projected-configmap-test-upd-449b1882-356f-41fe-8df2-037e04e361a9
STEP: Creating the pod
Jan 31 10:36:43.307: INFO: The status of Pod pod-projected-configmaps-457a5144-dcdf-4cc1-9a16-3dbbdf1c9cec is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:36:45.311: INFO: The status of Pod pod-projected-configmaps-457a5144-dcdf-4cc1-9a16-3dbbdf1c9cec is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-449b1882-356f-41fe-8df2-037e04e361a9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 31 10:36:47.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5806" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":236,"skipped":4357,"failed":0}
SSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:36:47.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-6210
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jan 31 10:36:49.539: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 31 10:36:51.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6210" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":356,"completed":237,"skipped":4362,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:36:51.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-800
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-800.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-800.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-800.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-800.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 31 10:36:53.722: INFO: DNS probes using dns-800/dns-test-e84befec-2093-4a26-9a12-dcdec4e0f4d3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 31 10:36:53.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-800" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","total":356,"completed":238,"skipped":4373,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:36:53.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7053
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1412
STEP: creating an pod
Jan 31 10:36:53.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-7053 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.39 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 31 10:36:53.968: INFO: stderr: ""
Jan 31 10:36:53.968: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for log generator to start.
Jan 31 10:36:53.968: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 31 10:36:53.968: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7053" to be "running and ready, or succeeded"
Jan 31 10:36:53.986: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 18.283658ms
Jan 31 10:36:55.991: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.023101024s
Jan 31 10:36:55.991: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 31 10:36:55.991: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jan 31 10:36:55.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-7053 logs logs-generator logs-generator'
Jan 31 10:36:56.051: INFO: stderr: ""
Jan 31 10:36:56.051: INFO: stdout: "I0131 10:36:54.624865       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/pvb6 539\nI0131 10:36:54.825254       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/2mgj 494\nI0131 10:36:55.025657       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/vvxm 498\nI0131 10:36:55.224967       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/gr9 252\nI0131 10:36:55.425331       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/xdc 319\nI0131 10:36:55.625675       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/qntb 485\nI0131 10:36:55.824960       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/v95 334\nI0131 10:36:56.025289       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/c5n8 546\n"
STEP: limiting log lines
Jan 31 10:36:56.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-7053 logs logs-generator logs-generator --tail=1'
Jan 31 10:36:56.102: INFO: stderr: ""
Jan 31 10:36:56.102: INFO: stdout: "I0131 10:36:56.025289       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/c5n8 546\n"
Jan 31 10:36:56.102: INFO: got output "I0131 10:36:56.025289       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/c5n8 546\n"
STEP: limiting log bytes
Jan 31 10:36:56.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-7053 logs logs-generator logs-generator --limit-bytes=1'
Jan 31 10:36:56.153: INFO: stderr: ""
Jan 31 10:36:56.153: INFO: stdout: "I"
Jan 31 10:36:56.153: INFO: got output "I"
STEP: exposing timestamps
Jan 31 10:36:56.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-7053 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 31 10:36:56.210: INFO: stderr: ""
Jan 31 10:36:56.210: INFO: stdout: "2023-01-31T10:36:56.025381133Z I0131 10:36:56.025289       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/c5n8 546\n"
Jan 31 10:36:56.210: INFO: got output "2023-01-31T10:36:56.025381133Z I0131 10:36:56.025289       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/c5n8 546\n"
STEP: restricting to a time range
Jan 31 10:36:58.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-7053 logs logs-generator logs-generator --since=1s'
Jan 31 10:36:58.761: INFO: stderr: ""
Jan 31 10:36:58.761: INFO: stdout: "I0131 10:36:57.825198       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/w2r 423\nI0131 10:36:58.025507       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/8nz 563\nI0131 10:36:58.225815       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/4ml 207\nI0131 10:36:58.425038       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/bxf 328\nI0131 10:36:58.625414       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/6mr 295\n"
Jan 31 10:36:58.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-7053 logs logs-generator logs-generator --since=24h'
Jan 31 10:36:58.810: INFO: stderr: ""
Jan 31 10:36:58.810: INFO: stdout: "I0131 10:36:54.624865       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/pvb6 539\nI0131 10:36:54.825254       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/2mgj 494\nI0131 10:36:55.025657       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/vvxm 498\nI0131 10:36:55.224967       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/gr9 252\nI0131 10:36:55.425331       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/xdc 319\nI0131 10:36:55.625675       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/qntb 485\nI0131 10:36:55.824960       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/v95 334\nI0131 10:36:56.025289       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/c5n8 546\nI0131 10:36:56.225668       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/gnth 531\nI0131 10:36:56.424918       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/7kx 429\nI0131 10:36:56.625283       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/cd2 476\nI0131 10:36:56.825632       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/7dn 571\nI0131 10:36:57.024908       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/t7jk 427\nI0131 10:36:57.225241       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/w2s 207\nI0131 10:36:57.425588       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/hnxr 387\nI0131 10:36:57.625876       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/mbn 251\nI0131 10:36:57.825198       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/w2r 423\nI0131 10:36:58.025507       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/8nz 563\nI0131 10:36:58.225815       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/4ml 207\nI0131 10:36:58.425038       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/bxf 328\nI0131 10:36:58.625414       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/6mr 295\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1417
Jan 31 10:36:58.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-7053 delete pod logs-generator'
Jan 31 10:36:59.238: INFO: stderr: ""
Jan 31 10:36:59.238: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 10:36:59.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7053" for this suite.

• [SLOW TEST:5.463 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1409
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":356,"completed":239,"skipped":4374,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:36:59.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3423
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:36:59.374: INFO: Creating ReplicaSet my-hostname-basic-eb3b157d-8f4b-4d56-aba0-6d65d4dfdab0
Jan 31 10:36:59.383: INFO: Pod name my-hostname-basic-eb3b157d-8f4b-4d56-aba0-6d65d4dfdab0: Found 0 pods out of 1
Jan 31 10:37:04.389: INFO: Pod name my-hostname-basic-eb3b157d-8f4b-4d56-aba0-6d65d4dfdab0: Found 1 pods out of 1
Jan 31 10:37:04.389: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-eb3b157d-8f4b-4d56-aba0-6d65d4dfdab0" is running
Jan 31 10:37:04.392: INFO: Pod "my-hostname-basic-eb3b157d-8f4b-4d56-aba0-6d65d4dfdab0-2z279" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 10:36:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 10:37:00 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 10:37:00 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 10:36:59 +0000 UTC Reason: Message:}])
Jan 31 10:37:04.392: INFO: Trying to dial the pod
Jan 31 10:37:09.401: INFO: Controller my-hostname-basic-eb3b157d-8f4b-4d56-aba0-6d65d4dfdab0: Got expected result from replica 1 [my-hostname-basic-eb3b157d-8f4b-4d56-aba0-6d65d4dfdab0-2z279]: "my-hostname-basic-eb3b157d-8f4b-4d56-aba0-6d65d4dfdab0-2z279", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 31 10:37:09.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3423" for this suite.

• [SLOW TEST:10.163 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":240,"skipped":4386,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:37:09.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4696
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jan 31 10:37:09.542: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4696  5e0e3fd0-c7b8-48f7-880b-ca48d96e0448 29961 0 2023-01-31 10:37:09 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2023-01-31 10:37:09 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vkxsf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vkxsf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 10:37:09.546: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:37:11.551: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jan 31 10:37:11.551: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4696 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 10:37:11.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 10:37:11.552: INFO: ExecWithOptions: Clientset creation
Jan 31 10:37:11.552: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-4696/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod...
Jan 31 10:37:11.617: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4696 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 10:37:11.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 10:37:11.618: INFO: ExecWithOptions: Clientset creation
Jan 31 10:37:11.618: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-4696/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 31 10:37:11.671: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 31 10:37:11.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4696" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":356,"completed":241,"skipped":4400,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:37:11.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5376
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jan 31 10:37:21.875: INFO: The status of Pod kube-controller-manager-master-3 is Running (Ready = true)
Jan 31 10:37:21.936: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 31 10:37:21.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5376" for this suite.

• [SLOW TEST:10.273 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":356,"completed":242,"skipped":4419,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:37:21.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6607
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 31 10:37:22.105: INFO: Waiting up to 5m0s for pod "downward-api-3d8dac7a-efb8-4b4e-abf1-de9843af7f75" in namespace "downward-api-6607" to be "Succeeded or Failed"
Jan 31 10:37:22.112: INFO: Pod "downward-api-3d8dac7a-efb8-4b4e-abf1-de9843af7f75": Phase="Pending", Reason="", readiness=false. Elapsed: 7.121534ms
Jan 31 10:37:24.116: INFO: Pod "downward-api-3d8dac7a-efb8-4b4e-abf1-de9843af7f75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01093117s
Jan 31 10:37:26.121: INFO: Pod "downward-api-3d8dac7a-efb8-4b4e-abf1-de9843af7f75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01606395s
STEP: Saw pod success
Jan 31 10:37:26.121: INFO: Pod "downward-api-3d8dac7a-efb8-4b4e-abf1-de9843af7f75" satisfied condition "Succeeded or Failed"
Jan 31 10:37:26.124: INFO: Trying to get logs from node macpro-x86-1 pod downward-api-3d8dac7a-efb8-4b4e-abf1-de9843af7f75 container dapi-container: <nil>
STEP: delete the pod
Jan 31 10:37:26.139: INFO: Waiting for pod downward-api-3d8dac7a-efb8-4b4e-abf1-de9843af7f75 to disappear
Jan 31 10:37:26.146: INFO: Pod downward-api-3d8dac7a-efb8-4b4e-abf1-de9843af7f75 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 31 10:37:26.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6607" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":356,"completed":243,"skipped":4450,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:37:26.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8972
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Jan 31 10:37:32.397: INFO: 80 pods remaining
Jan 31 10:37:32.397: INFO: 80 pods has nil DeletionTimestamp
Jan 31 10:37:32.397: INFO: 
Jan 31 10:37:33.399: INFO: 71 pods remaining
Jan 31 10:37:33.399: INFO: 70 pods has nil DeletionTimestamp
Jan 31 10:37:33.399: INFO: 
Jan 31 10:37:34.387: INFO: 60 pods remaining
Jan 31 10:37:34.387: INFO: 60 pods has nil DeletionTimestamp
Jan 31 10:37:34.387: INFO: 
Jan 31 10:37:35.386: INFO: 40 pods remaining
Jan 31 10:37:35.386: INFO: 40 pods has nil DeletionTimestamp
Jan 31 10:37:35.386: INFO: 
Jan 31 10:37:36.411: INFO: 31 pods remaining
Jan 31 10:37:36.411: INFO: 30 pods has nil DeletionTimestamp
Jan 31 10:37:36.411: INFO: 
Jan 31 10:37:37.401: INFO: 20 pods remaining
Jan 31 10:37:37.401: INFO: 20 pods has nil DeletionTimestamp
Jan 31 10:37:37.401: INFO: 
STEP: Gathering metrics
Jan 31 10:37:38.411: INFO: The status of Pod kube-controller-manager-master-3 is Running (Ready = true)
Jan 31 10:37:38.455: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 31 10:37:38.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8972" for this suite.

• [SLOW TEST:12.281 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":356,"completed":244,"skipped":4537,"failed":0}
SSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:37:38.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6547
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 31 10:37:38.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6547" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":356,"completed":245,"skipped":4542,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:37:38.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9770
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating pod
Jan 31 10:37:38.846: INFO: The status of Pod pod-hostip-a2467463-0ed1-4b9c-93a5-8f9af5eff89d is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:37:40.851: INFO: The status of Pod pod-hostip-a2467463-0ed1-4b9c-93a5-8f9af5eff89d is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:37:42.849: INFO: The status of Pod pod-hostip-a2467463-0ed1-4b9c-93a5-8f9af5eff89d is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:37:44.851: INFO: The status of Pod pod-hostip-a2467463-0ed1-4b9c-93a5-8f9af5eff89d is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:37:46.852: INFO: The status of Pod pod-hostip-a2467463-0ed1-4b9c-93a5-8f9af5eff89d is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:37:48.852: INFO: The status of Pod pod-hostip-a2467463-0ed1-4b9c-93a5-8f9af5eff89d is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:37:50.852: INFO: The status of Pod pod-hostip-a2467463-0ed1-4b9c-93a5-8f9af5eff89d is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:37:52.849: INFO: The status of Pod pod-hostip-a2467463-0ed1-4b9c-93a5-8f9af5eff89d is Running (Ready = true)
Jan 31 10:37:52.856: INFO: Pod pod-hostip-a2467463-0ed1-4b9c-93a5-8f9af5eff89d has hostIP: 10.221.188.32
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 31 10:37:52.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9770" for this suite.

• [SLOW TEST:14.162 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":356,"completed":246,"skipped":4558,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:37:52.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5935
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-2deb5a28-ce10-4ea7-9ebd-eb95a473de88
STEP: Creating a pod to test consume secrets
Jan 31 10:37:52.996: INFO: Waiting up to 5m0s for pod "pod-secrets-5178e9ac-add2-45a6-a294-bea7055428e7" in namespace "secrets-5935" to be "Succeeded or Failed"
Jan 31 10:37:53.001: INFO: Pod "pod-secrets-5178e9ac-add2-45a6-a294-bea7055428e7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.084087ms
Jan 31 10:37:55.008: INFO: Pod "pod-secrets-5178e9ac-add2-45a6-a294-bea7055428e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011725228s
Jan 31 10:37:57.012: INFO: Pod "pod-secrets-5178e9ac-add2-45a6-a294-bea7055428e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015562318s
Jan 31 10:37:59.017: INFO: Pod "pod-secrets-5178e9ac-add2-45a6-a294-bea7055428e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020996019s
STEP: Saw pod success
Jan 31 10:37:59.017: INFO: Pod "pod-secrets-5178e9ac-add2-45a6-a294-bea7055428e7" satisfied condition "Succeeded or Failed"
Jan 31 10:37:59.020: INFO: Trying to get logs from node macpro-x86-1 pod pod-secrets-5178e9ac-add2-45a6-a294-bea7055428e7 container secret-volume-test: <nil>
STEP: delete the pod
Jan 31 10:37:59.037: INFO: Waiting for pod pod-secrets-5178e9ac-add2-45a6-a294-bea7055428e7 to disappear
Jan 31 10:37:59.039: INFO: Pod pod-secrets-5178e9ac-add2-45a6-a294-bea7055428e7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 31 10:37:59.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5935" for this suite.

• [SLOW TEST:6.185 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":247,"skipped":4569,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:37:59.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-786
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-964a8a48-5fa0-43e1-b648-1367b405155c
STEP: Creating a pod to test consume configMaps
Jan 31 10:37:59.188: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-688adf4f-4de2-4a87-8d22-3b7000512b5e" in namespace "projected-786" to be "Succeeded or Failed"
Jan 31 10:37:59.191: INFO: Pod "pod-projected-configmaps-688adf4f-4de2-4a87-8d22-3b7000512b5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.713516ms
Jan 31 10:38:01.199: INFO: Pod "pod-projected-configmaps-688adf4f-4de2-4a87-8d22-3b7000512b5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010817559s
Jan 31 10:38:03.202: INFO: Pod "pod-projected-configmaps-688adf4f-4de2-4a87-8d22-3b7000512b5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014322366s
STEP: Saw pod success
Jan 31 10:38:03.202: INFO: Pod "pod-projected-configmaps-688adf4f-4de2-4a87-8d22-3b7000512b5e" satisfied condition "Succeeded or Failed"
Jan 31 10:38:03.204: INFO: Trying to get logs from node macpro-x86-2 pod pod-projected-configmaps-688adf4f-4de2-4a87-8d22-3b7000512b5e container agnhost-container: <nil>
STEP: delete the pod
Jan 31 10:38:03.223: INFO: Waiting for pod pod-projected-configmaps-688adf4f-4de2-4a87-8d22-3b7000512b5e to disappear
Jan 31 10:38:03.225: INFO: Pod pod-projected-configmaps-688adf4f-4de2-4a87-8d22-3b7000512b5e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 31 10:38:03.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-786" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":248,"skipped":4572,"failed":0}

------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:38:03.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1256
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-1cc67d0e-26b0-4205-abf3-d6b7fba13a19-1162
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jan 31 10:38:03.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1256" for this suite.
STEP: Destroying namespace "nspatchtest-1cc67d0e-26b0-4205-abf3-d6b7fba13a19-1162" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":356,"completed":249,"skipped":4572,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:38:03.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4394
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replication controller my-hostname-basic-23407169-5235-4cd2-8c3f-02112923d30d
Jan 31 10:38:03.635: INFO: Pod name my-hostname-basic-23407169-5235-4cd2-8c3f-02112923d30d: Found 0 pods out of 1
Jan 31 10:38:08.640: INFO: Pod name my-hostname-basic-23407169-5235-4cd2-8c3f-02112923d30d: Found 1 pods out of 1
Jan 31 10:38:08.640: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-23407169-5235-4cd2-8c3f-02112923d30d" are running
Jan 31 10:38:08.647: INFO: Pod "my-hostname-basic-23407169-5235-4cd2-8c3f-02112923d30d-zkmt6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 10:38:03 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 10:38:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 10:38:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 10:38:03 +0000 UTC Reason: Message:}])
Jan 31 10:38:08.647: INFO: Trying to dial the pod
Jan 31 10:38:13.658: INFO: Controller my-hostname-basic-23407169-5235-4cd2-8c3f-02112923d30d: Got expected result from replica 1 [my-hostname-basic-23407169-5235-4cd2-8c3f-02112923d30d-zkmt6]: "my-hostname-basic-23407169-5235-4cd2-8c3f-02112923d30d-zkmt6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 31 10:38:13.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4394" for this suite.

• [SLOW TEST:10.171 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":250,"skipped":4580,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:38:13.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2398
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-f3071150-9d6e-447d-8005-a2d578de97c9
STEP: Creating a pod to test consume secrets
Jan 31 10:38:13.828: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f1507d05-6782-4250-a418-cd82b955709a" in namespace "projected-2398" to be "Succeeded or Failed"
Jan 31 10:38:13.835: INFO: Pod "pod-projected-secrets-f1507d05-6782-4250-a418-cd82b955709a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.594776ms
Jan 31 10:38:15.839: INFO: Pod "pod-projected-secrets-f1507d05-6782-4250-a418-cd82b955709a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010930479s
Jan 31 10:38:17.843: INFO: Pod "pod-projected-secrets-f1507d05-6782-4250-a418-cd82b955709a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01420349s
STEP: Saw pod success
Jan 31 10:38:17.843: INFO: Pod "pod-projected-secrets-f1507d05-6782-4250-a418-cd82b955709a" satisfied condition "Succeeded or Failed"
Jan 31 10:38:17.845: INFO: Trying to get logs from node macpro-x86-1 pod pod-projected-secrets-f1507d05-6782-4250-a418-cd82b955709a container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 31 10:38:17.864: INFO: Waiting for pod pod-projected-secrets-f1507d05-6782-4250-a418-cd82b955709a to disappear
Jan 31 10:38:17.866: INFO: Pod pod-projected-secrets-f1507d05-6782-4250-a418-cd82b955709a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 31 10:38:17.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2398" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":251,"skipped":4592,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:38:17.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-7650
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pod templates
Jan 31 10:38:18.002: INFO: created test-podtemplate-1
Jan 31 10:38:18.009: INFO: created test-podtemplate-2
Jan 31 10:38:18.013: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jan 31 10:38:18.015: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jan 31 10:38:18.026: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jan 31 10:38:18.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7650" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":356,"completed":252,"skipped":4609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:38:18.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9333
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-f0bc7711-febb-4a19-a562-7d8e1fc41961
STEP: Creating a pod to test consume secrets
Jan 31 10:38:18.171: INFO: Waiting up to 5m0s for pod "pod-secrets-2a60ee38-8d73-4af1-9ae3-cf796cd3f2f3" in namespace "secrets-9333" to be "Succeeded or Failed"
Jan 31 10:38:18.175: INFO: Pod "pod-secrets-2a60ee38-8d73-4af1-9ae3-cf796cd3f2f3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.099921ms
Jan 31 10:38:20.182: INFO: Pod "pod-secrets-2a60ee38-8d73-4af1-9ae3-cf796cd3f2f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010774516s
Jan 31 10:38:22.186: INFO: Pod "pod-secrets-2a60ee38-8d73-4af1-9ae3-cf796cd3f2f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014298848s
STEP: Saw pod success
Jan 31 10:38:22.186: INFO: Pod "pod-secrets-2a60ee38-8d73-4af1-9ae3-cf796cd3f2f3" satisfied condition "Succeeded or Failed"
Jan 31 10:38:22.188: INFO: Trying to get logs from node macpro-x86-1 pod pod-secrets-2a60ee38-8d73-4af1-9ae3-cf796cd3f2f3 container secret-volume-test: <nil>
STEP: delete the pod
Jan 31 10:38:22.208: INFO: Waiting for pod pod-secrets-2a60ee38-8d73-4af1-9ae3-cf796cd3f2f3 to disappear
Jan 31 10:38:22.210: INFO: Pod pod-secrets-2a60ee38-8d73-4af1-9ae3-cf796cd3f2f3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 31 10:38:22.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9333" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":253,"skipped":4654,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:38:22.218: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3087
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-95e8b5f1-3d12-4b96-8f68-0e43f36de73d
STEP: Creating a pod to test consume secrets
Jan 31 10:38:22.367: INFO: Waiting up to 5m0s for pod "pod-secrets-5e7575d5-cce6-4fa0-bd74-0649824c0ce7" in namespace "secrets-3087" to be "Succeeded or Failed"
Jan 31 10:38:22.370: INFO: Pod "pod-secrets-5e7575d5-cce6-4fa0-bd74-0649824c0ce7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.082953ms
Jan 31 10:38:24.376: INFO: Pod "pod-secrets-5e7575d5-cce6-4fa0-bd74-0649824c0ce7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008777372s
Jan 31 10:38:26.380: INFO: Pod "pod-secrets-5e7575d5-cce6-4fa0-bd74-0649824c0ce7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013180192s
STEP: Saw pod success
Jan 31 10:38:26.380: INFO: Pod "pod-secrets-5e7575d5-cce6-4fa0-bd74-0649824c0ce7" satisfied condition "Succeeded or Failed"
Jan 31 10:38:26.384: INFO: Trying to get logs from node macpro-x86-1 pod pod-secrets-5e7575d5-cce6-4fa0-bd74-0649824c0ce7 container secret-volume-test: <nil>
STEP: delete the pod
Jan 31 10:38:26.400: INFO: Waiting for pod pod-secrets-5e7575d5-cce6-4fa0-bd74-0649824c0ce7 to disappear
Jan 31 10:38:26.403: INFO: Pod pod-secrets-5e7575d5-cce6-4fa0-bd74-0649824c0ce7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 31 10:38:26.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3087" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":254,"skipped":4655,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:38:26.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5879
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 10:38:26.971: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 10:38:29.989: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:38:30.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5879" for this suite.
STEP: Destroying namespace "webhook-5879-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":356,"completed":255,"skipped":4669,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:38:30.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-4550
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 31 10:40:00.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4550" for this suite.

• [SLOW TEST:90.174 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":356,"completed":256,"skipped":4678,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:40:00.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7338
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 31 10:40:00.426: INFO: The status of Pod pod-update-activedeadlineseconds-69385101-c22f-41c5-b709-4de5e6799ad0 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:40:02.431: INFO: The status of Pod pod-update-activedeadlineseconds-69385101-c22f-41c5-b709-4de5e6799ad0 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 31 10:40:02.949: INFO: Successfully updated pod "pod-update-activedeadlineseconds-69385101-c22f-41c5-b709-4de5e6799ad0"
Jan 31 10:40:02.949: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-69385101-c22f-41c5-b709-4de5e6799ad0" in namespace "pods-7338" to be "terminated due to deadline exceeded"
Jan 31 10:40:02.952: INFO: Pod "pod-update-activedeadlineseconds-69385101-c22f-41c5-b709-4de5e6799ad0": Phase="Running", Reason="", readiness=true. Elapsed: 2.670926ms
Jan 31 10:40:04.959: INFO: Pod "pod-update-activedeadlineseconds-69385101-c22f-41c5-b709-4de5e6799ad0": Phase="Running", Reason="", readiness=true. Elapsed: 2.010157434s
Jan 31 10:40:06.965: INFO: Pod "pod-update-activedeadlineseconds-69385101-c22f-41c5-b709-4de5e6799ad0": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.015499766s
Jan 31 10:40:06.965: INFO: Pod "pod-update-activedeadlineseconds-69385101-c22f-41c5-b709-4de5e6799ad0" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 31 10:40:06.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7338" for this suite.

• [SLOW TEST:6.699 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":356,"completed":257,"skipped":4692,"failed":0}
SSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:40:06.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-9359
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 31 10:40:09.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9359" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":356,"completed":258,"skipped":4700,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:40:09.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-5525
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating server pod server in namespace prestop-5525
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5525
STEP: Deleting pre-stop pod
Jan 31 10:40:18.351: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:188
Jan 31 10:40:18.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5525" for this suite.

• [SLOW TEST:9.210 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":356,"completed":259,"skipped":4741,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:40:18.387: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1023
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-8154
STEP: Creating secret with name secret-test-84c07f9c-ada6-4403-a5c7-1a0e58389d4d
STEP: Creating a pod to test consume secrets
Jan 31 10:40:18.650: INFO: Waiting up to 5m0s for pod "pod-secrets-feaaa1b4-ed28-46c4-96d9-8118599335ed" in namespace "secrets-1023" to be "Succeeded or Failed"
Jan 31 10:40:18.652: INFO: Pod "pod-secrets-feaaa1b4-ed28-46c4-96d9-8118599335ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.594682ms
Jan 31 10:40:20.657: INFO: Pod "pod-secrets-feaaa1b4-ed28-46c4-96d9-8118599335ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007503647s
Jan 31 10:40:22.663: INFO: Pod "pod-secrets-feaaa1b4-ed28-46c4-96d9-8118599335ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013217327s
STEP: Saw pod success
Jan 31 10:40:22.663: INFO: Pod "pod-secrets-feaaa1b4-ed28-46c4-96d9-8118599335ed" satisfied condition "Succeeded or Failed"
Jan 31 10:40:22.666: INFO: Trying to get logs from node macpro-x86-1 pod pod-secrets-feaaa1b4-ed28-46c4-96d9-8118599335ed container secret-volume-test: <nil>
STEP: delete the pod
Jan 31 10:40:22.686: INFO: Waiting for pod pod-secrets-feaaa1b4-ed28-46c4-96d9-8118599335ed to disappear
Jan 31 10:40:22.689: INFO: Pod pod-secrets-feaaa1b4-ed28-46c4-96d9-8118599335ed no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 31 10:40:22.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1023" for this suite.
STEP: Destroying namespace "secret-namespace-8154" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":356,"completed":260,"skipped":4758,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:40:22.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4784
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 31 10:40:26.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4784" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":356,"completed":261,"skipped":4848,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:40:26.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-374
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 31 10:40:27.319: INFO: Pod name wrapped-volume-race-6942d51f-a2ef-4bb9-a353-7d3fabaeac6d: Found 2 pods out of 5
Jan 31 10:40:32.325: INFO: Pod name wrapped-volume-race-6942d51f-a2ef-4bb9-a353-7d3fabaeac6d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6942d51f-a2ef-4bb9-a353-7d3fabaeac6d in namespace emptydir-wrapper-374, will wait for the garbage collector to delete the pods
Jan 31 10:40:42.415: INFO: Deleting ReplicationController wrapped-volume-race-6942d51f-a2ef-4bb9-a353-7d3fabaeac6d took: 7.842959ms
Jan 31 10:40:42.516: INFO: Terminating ReplicationController wrapped-volume-race-6942d51f-a2ef-4bb9-a353-7d3fabaeac6d pods took: 100.987952ms
STEP: Creating RC which spawns configmap-volume pods
Jan 31 10:40:45.832: INFO: Pod name wrapped-volume-race-e5b21796-cc44-49a9-9d0c-aae6f4405c6a: Found 0 pods out of 5
Jan 31 10:40:50.840: INFO: Pod name wrapped-volume-race-e5b21796-cc44-49a9-9d0c-aae6f4405c6a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e5b21796-cc44-49a9-9d0c-aae6f4405c6a in namespace emptydir-wrapper-374, will wait for the garbage collector to delete the pods
Jan 31 10:41:02.915: INFO: Deleting ReplicationController wrapped-volume-race-e5b21796-cc44-49a9-9d0c-aae6f4405c6a took: 8.141793ms
Jan 31 10:41:03.016: INFO: Terminating ReplicationController wrapped-volume-race-e5b21796-cc44-49a9-9d0c-aae6f4405c6a pods took: 100.704728ms
STEP: Creating RC which spawns configmap-volume pods
Jan 31 10:41:06.841: INFO: Pod name wrapped-volume-race-69f64a80-d65f-49ac-b228-6f9d9212b16d: Found 0 pods out of 5
Jan 31 10:41:11.852: INFO: Pod name wrapped-volume-race-69f64a80-d65f-49ac-b228-6f9d9212b16d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-69f64a80-d65f-49ac-b228-6f9d9212b16d in namespace emptydir-wrapper-374, will wait for the garbage collector to delete the pods
Jan 31 10:41:21.941: INFO: Deleting ReplicationController wrapped-volume-race-69f64a80-d65f-49ac-b228-6f9d9212b16d took: 6.115101ms
Jan 31 10:41:22.042: INFO: Terminating ReplicationController wrapped-volume-race-69f64a80-d65f-49ac-b228-6f9d9212b16d pods took: 101.085082ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Jan 31 10:41:24.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-374" for this suite.

• [SLOW TEST:58.086 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":356,"completed":262,"skipped":4855,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:41:24.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9592
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9592.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9592.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9592.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9592.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9592.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9592.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9592.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9592.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9592.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9592.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 175.81.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.81.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.81.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.81.175_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9592.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9592.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9592.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9592.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9592.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9592.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9592.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9592.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9592.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9592.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 175.81.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.81.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.81.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.81.175_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 31 10:41:27.119: INFO: Unable to read wheezy_udp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:27.121: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:27.123: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:27.126: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:27.136: INFO: Unable to read jessie_udp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:27.138: INFO: Unable to read jessie_tcp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:27.140: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:27.142: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:27.149: INFO: Lookups using dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9 failed for: [wheezy_udp@dns-test-service.dns-9592.svc.cluster.local wheezy_tcp@dns-test-service.dns-9592.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local jessie_udp@dns-test-service.dns-9592.svc.cluster.local jessie_tcp@dns-test-service.dns-9592.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local]

Jan 31 10:41:32.155: INFO: Unable to read wheezy_udp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:32.159: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:32.161: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:32.164: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:32.176: INFO: Unable to read jessie_udp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:32.178: INFO: Unable to read jessie_tcp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:32.180: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:32.182: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:32.190: INFO: Lookups using dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9 failed for: [wheezy_udp@dns-test-service.dns-9592.svc.cluster.local wheezy_tcp@dns-test-service.dns-9592.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local jessie_udp@dns-test-service.dns-9592.svc.cluster.local jessie_tcp@dns-test-service.dns-9592.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local]

Jan 31 10:41:37.153: INFO: Unable to read wheezy_udp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:37.156: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:37.158: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:37.161: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:37.181: INFO: Unable to read jessie_udp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:37.183: INFO: Unable to read jessie_tcp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:37.185: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:37.188: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:37.201: INFO: Lookups using dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9 failed for: [wheezy_udp@dns-test-service.dns-9592.svc.cluster.local wheezy_tcp@dns-test-service.dns-9592.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local jessie_udp@dns-test-service.dns-9592.svc.cluster.local jessie_tcp@dns-test-service.dns-9592.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local]

Jan 31 10:41:42.153: INFO: Unable to read wheezy_udp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:42.156: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:42.159: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:42.161: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:42.171: INFO: Unable to read jessie_udp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:42.174: INFO: Unable to read jessie_tcp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:42.176: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:42.178: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:42.186: INFO: Lookups using dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9 failed for: [wheezy_udp@dns-test-service.dns-9592.svc.cluster.local wheezy_tcp@dns-test-service.dns-9592.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local jessie_udp@dns-test-service.dns-9592.svc.cluster.local jessie_tcp@dns-test-service.dns-9592.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local]

Jan 31 10:41:47.154: INFO: Unable to read wheezy_udp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:47.157: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:47.160: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:47.162: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:47.172: INFO: Unable to read jessie_udp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:47.174: INFO: Unable to read jessie_tcp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:47.176: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:47.178: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:47.186: INFO: Lookups using dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9 failed for: [wheezy_udp@dns-test-service.dns-9592.svc.cluster.local wheezy_tcp@dns-test-service.dns-9592.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local jessie_udp@dns-test-service.dns-9592.svc.cluster.local jessie_tcp@dns-test-service.dns-9592.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local]

Jan 31 10:41:52.156: INFO: Unable to read wheezy_udp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:52.162: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:52.165: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:52.170: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:52.184: INFO: Unable to read jessie_udp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:52.187: INFO: Unable to read jessie_tcp@dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:52.189: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:52.191: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local from pod dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9: the server could not find the requested resource (get pods dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9)
Jan 31 10:41:52.199: INFO: Lookups using dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9 failed for: [wheezy_udp@dns-test-service.dns-9592.svc.cluster.local wheezy_tcp@dns-test-service.dns-9592.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local jessie_udp@dns-test-service.dns-9592.svc.cluster.local jessie_tcp@dns-test-service.dns-9592.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9592.svc.cluster.local]

Jan 31 10:41:57.194: INFO: DNS probes using dns-9592/dns-test-6ff3cd8c-25c9-4208-b39b-33a2c481edc9 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 31 10:41:57.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9592" for this suite.

• [SLOW TEST:32.433 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":356,"completed":263,"skipped":4879,"failed":0}
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:41:57.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-509
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod test-webserver-f2bc87b3-1a50-4ab2-a8ab-fcd13d84686b in namespace container-probe-509
Jan 31 10:41:59.572: INFO: Started pod test-webserver-f2bc87b3-1a50-4ab2-a8ab-fcd13d84686b in namespace container-probe-509
STEP: checking the pod's current state and verifying that restartCount is present
Jan 31 10:41:59.574: INFO: Initial restart count of pod test-webserver-f2bc87b3-1a50-4ab2-a8ab-fcd13d84686b is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 31 10:46:00.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-509" for this suite.

• [SLOW TEST:242.873 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":264,"skipped":4879,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:46:00.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9932
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9932
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:46:00.410: INFO: Found 0 stateful pods, waiting for 1
Jan 31 10:46:10.414: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Jan 31 10:46:10.428: INFO: Found 1 stateful pods, waiting for 2
Jan 31 10:46:20.434: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 31 10:46:20.434: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 31 10:46:20.453: INFO: Deleting all statefulset in ns statefulset-9932
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 31 10:46:20.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9932" for this suite.

• [SLOW TEST:20.241 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":356,"completed":265,"skipped":4906,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:46:20.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2455
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 31 10:46:20.623: INFO: Waiting up to 5m0s for pod "pod-5e8b1d4e-1e18-427f-b743-f22082185682" in namespace "emptydir-2455" to be "Succeeded or Failed"
Jan 31 10:46:20.629: INFO: Pod "pod-5e8b1d4e-1e18-427f-b743-f22082185682": Phase="Pending", Reason="", readiness=false. Elapsed: 5.237967ms
Jan 31 10:46:22.634: INFO: Pod "pod-5e8b1d4e-1e18-427f-b743-f22082185682": Phase="Running", Reason="", readiness=false. Elapsed: 2.010615176s
Jan 31 10:46:24.641: INFO: Pod "pod-5e8b1d4e-1e18-427f-b743-f22082185682": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017246161s
STEP: Saw pod success
Jan 31 10:46:24.641: INFO: Pod "pod-5e8b1d4e-1e18-427f-b743-f22082185682" satisfied condition "Succeeded or Failed"
Jan 31 10:46:24.643: INFO: Trying to get logs from node macpro-x86-1 pod pod-5e8b1d4e-1e18-427f-b743-f22082185682 container test-container: <nil>
STEP: delete the pod
Jan 31 10:46:24.677: INFO: Waiting for pod pod-5e8b1d4e-1e18-427f-b743-f22082185682 to disappear
Jan 31 10:46:24.679: INFO: Pod pod-5e8b1d4e-1e18-427f-b743-f22082185682 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 31 10:46:24.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2455" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":266,"skipped":4920,"failed":0}
SSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:46:24.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-4762
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 31 10:46:24.827: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 31 10:46:24.830: INFO: starting watch
STEP: patching
STEP: updating
Jan 31 10:46:24.843: INFO: waiting for watch events with expected annotations
Jan 31 10:46:24.843: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 31 10:46:24.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4762" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":356,"completed":267,"skipped":4926,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:46:24.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4884
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-tsl8
STEP: Creating a pod to test atomic-volume-subpath
Jan 31 10:46:25.025: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tsl8" in namespace "subpath-4884" to be "Succeeded or Failed"
Jan 31 10:46:25.029: INFO: Pod "pod-subpath-test-configmap-tsl8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.533298ms
Jan 31 10:46:27.034: INFO: Pod "pod-subpath-test-configmap-tsl8": Phase="Running", Reason="", readiness=true. Elapsed: 2.009501762s
Jan 31 10:46:29.038: INFO: Pod "pod-subpath-test-configmap-tsl8": Phase="Running", Reason="", readiness=true. Elapsed: 4.013601356s
Jan 31 10:46:31.044: INFO: Pod "pod-subpath-test-configmap-tsl8": Phase="Running", Reason="", readiness=true. Elapsed: 6.019203074s
Jan 31 10:46:33.049: INFO: Pod "pod-subpath-test-configmap-tsl8": Phase="Running", Reason="", readiness=true. Elapsed: 8.023742666s
Jan 31 10:46:35.054: INFO: Pod "pod-subpath-test-configmap-tsl8": Phase="Running", Reason="", readiness=true. Elapsed: 10.029291295s
Jan 31 10:46:37.059: INFO: Pod "pod-subpath-test-configmap-tsl8": Phase="Running", Reason="", readiness=true. Elapsed: 12.034346484s
Jan 31 10:46:39.063: INFO: Pod "pod-subpath-test-configmap-tsl8": Phase="Running", Reason="", readiness=true. Elapsed: 14.038242018s
Jan 31 10:46:41.069: INFO: Pod "pod-subpath-test-configmap-tsl8": Phase="Running", Reason="", readiness=true. Elapsed: 16.044258532s
Jan 31 10:46:43.074: INFO: Pod "pod-subpath-test-configmap-tsl8": Phase="Running", Reason="", readiness=true. Elapsed: 18.049115862s
Jan 31 10:46:45.080: INFO: Pod "pod-subpath-test-configmap-tsl8": Phase="Running", Reason="", readiness=true. Elapsed: 20.055274154s
Jan 31 10:46:47.084: INFO: Pod "pod-subpath-test-configmap-tsl8": Phase="Running", Reason="", readiness=false. Elapsed: 22.05954045s
Jan 31 10:46:49.090: INFO: Pod "pod-subpath-test-configmap-tsl8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.065048437s
STEP: Saw pod success
Jan 31 10:46:49.090: INFO: Pod "pod-subpath-test-configmap-tsl8" satisfied condition "Succeeded or Failed"
Jan 31 10:46:49.092: INFO: Trying to get logs from node macpro-x86-1 pod pod-subpath-test-configmap-tsl8 container test-container-subpath-configmap-tsl8: <nil>
STEP: delete the pod
Jan 31 10:46:49.106: INFO: Waiting for pod pod-subpath-test-configmap-tsl8 to disappear
Jan 31 10:46:49.108: INFO: Pod pod-subpath-test-configmap-tsl8 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tsl8
Jan 31 10:46:49.108: INFO: Deleting pod "pod-subpath-test-configmap-tsl8" in namespace "subpath-4884"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 31 10:46:49.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4884" for this suite.

• [SLOW TEST:24.230 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","total":356,"completed":268,"skipped":4926,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:46:49.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4039
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 31 10:46:49.254: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4039  7eb9b0a4-04f2-465f-93b2-18072a97063d 34463 0 2023-01-31 10:46:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-31 10:46:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 31 10:46:49.254: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4039  7eb9b0a4-04f2-465f-93b2-18072a97063d 34464 0 2023-01-31 10:46:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-31 10:46:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 31 10:46:49.254: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4039  7eb9b0a4-04f2-465f-93b2-18072a97063d 34465 0 2023-01-31 10:46:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-31 10:46:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 31 10:46:59.284: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4039  7eb9b0a4-04f2-465f-93b2-18072a97063d 34503 0 2023-01-31 10:46:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-31 10:46:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 31 10:46:59.284: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4039  7eb9b0a4-04f2-465f-93b2-18072a97063d 34504 0 2023-01-31 10:46:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-31 10:46:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 31 10:46:59.284: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4039  7eb9b0a4-04f2-465f-93b2-18072a97063d 34505 0 2023-01-31 10:46:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-31 10:46:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 31 10:46:59.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4039" for this suite.

• [SLOW TEST:10.176 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":356,"completed":269,"skipped":4945,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:46:59.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9366
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 31 10:46:59.420: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 31 10:46:59.426: INFO: Waiting for terminating namespaces to be deleted...
Jan 31 10:46:59.428: INFO: 
Logging pods the apiserver thinks is on node macpro-x86-1 before test
Jan 31 10:46:59.434: INFO: calico-node-nd7tv from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 10:46:59.434: INFO: 	Container calico-node ready: true, restart count 0
Jan 31 10:46:59.434: INFO: kube-proxy-tmcdj from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 10:46:59.434: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 31 10:46:59.434: INFO: helm-charts-fluent-bit-wmmdn from logging started at 2023-01-31 10:36:05 +0000 UTC (1 container statuses recorded)
Jan 31 10:46:59.434: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 31 10:46:59.434: INFO: speaker-2t9t2 from metallb-system started at 2023-01-31 10:36:05 +0000 UTC (1 container statuses recorded)
Jan 31 10:46:59.434: INFO: 	Container speaker ready: true, restart count 0
Jan 31 10:46:59.434: INFO: kube-prometheus-stack-prometheus-node-exporter-v265d from orka-monitoring started at 2023-01-31 10:36:05 +0000 UTC (1 container statuses recorded)
Jan 31 10:46:59.434: INFO: 	Container node-exporter ready: true, restart count 0
Jan 31 10:46:59.434: INFO: sonobuoy from sonobuoy started at 2023-01-31 09:42:03 +0000 UTC (1 container statuses recorded)
Jan 31 10:46:59.434: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 31 10:46:59.434: INFO: sonobuoy-systemd-logs-daemon-set-e1cae5bb05d74403-zlhvz from sonobuoy started at 2023-01-31 09:42:03 +0000 UTC (2 container statuses recorded)
Jan 31 10:46:59.434: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 10:46:59.434: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 31 10:46:59.434: INFO: 
Logging pods the apiserver thinks is on node macpro-x86-2 before test
Jan 31 10:46:59.443: INFO: calico-node-w8d7z from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 10:46:59.443: INFO: 	Container calico-node ready: true, restart count 0
Jan 31 10:46:59.443: INFO: kube-proxy-fj2ms from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 10:46:59.443: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 31 10:46:59.443: INFO: helm-charts-fluent-bit-bwlwb from logging started at 2023-01-31 09:32:49 +0000 UTC (1 container statuses recorded)
Jan 31 10:46:59.443: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 31 10:46:59.443: INFO: speaker-7pfgg from metallb-system started at 2023-01-31 09:31:56 +0000 UTC (1 container statuses recorded)
Jan 31 10:46:59.443: INFO: 	Container speaker ready: true, restart count 0
Jan 31 10:46:59.443: INFO: kube-prometheus-stack-prometheus-node-exporter-b8d2q from orka-monitoring started at 2023-01-31 09:34:26 +0000 UTC (1 container statuses recorded)
Jan 31 10:46:59.443: INFO: 	Container node-exporter ready: true, restart count 0
Jan 31 10:46:59.443: INFO: sonobuoy-e2e-job-1083219078514678 from sonobuoy started at 2023-01-31 09:42:03 +0000 UTC (2 container statuses recorded)
Jan 31 10:46:59.443: INFO: 	Container e2e ready: true, restart count 0
Jan 31 10:46:59.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 10:46:59.443: INFO: sonobuoy-systemd-logs-daemon-set-e1cae5bb05d74403-fhxh8 from sonobuoy started at 2023-01-31 09:42:04 +0000 UTC (2 container statuses recorded)
Jan 31 10:46:59.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 10:46:59.443: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-09e813cf-cdc1-42a9-a9b8-bb4aa909c700 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.221.188.32 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-09e813cf-cdc1-42a9-a9b8-bb4aa909c700 off the node macpro-x86-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-09e813cf-cdc1-42a9-a9b8-bb4aa909c700
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 31 10:52:03.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9366" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:304.268 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":356,"completed":270,"skipped":4969,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:52:03.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7588
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:52:03.700: INFO: Creating deployment "test-recreate-deployment"
Jan 31 10:52:03.705: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 31 10:52:03.722: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 31 10:52:05.732: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 31 10:52:05.734: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 31 10:52:05.740: INFO: Updating deployment test-recreate-deployment
Jan 31 10:52:05.740: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 31 10:52:05.808: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7588  cc7a6231-4ec9-4683-8127-1c90af4b447b 35191 2 2023-01-31 10:52:03 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-01-31 10:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 10:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028e95d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-31 10:52:05 +0000 UTC,LastTransitionTime:2023-01-31 10:52:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cd8586fc7" is progressing.,LastUpdateTime:2023-01-31 10:52:05 +0000 UTC,LastTransitionTime:2023-01-31 10:52:03 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 31 10:52:05.810: INFO: New ReplicaSet "test-recreate-deployment-cd8586fc7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cd8586fc7  deployment-7588  e431195e-3ab7-41ab-910e-74485ad4c0fa 35189 1 2023-01-31 10:52:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment cc7a6231-4ec9-4683-8127-1c90af4b447b 0xc00469cb90 0xc00469cb91}] []  [{kube-controller-manager Update apps/v1 2023-01-31 10:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc7a6231-4ec9-4683-8127-1c90af4b447b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 10:52:05 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cd8586fc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00469cc28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 31 10:52:05.810: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 31 10:52:05.810: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-845d658455  deployment-7588  ebbed5cc-5958-47e0-97cf-a3d1a48eb9d8 35180 2 2023-01-31 10:52:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment cc7a6231-4ec9-4683-8127-1c90af4b447b 0xc00469ca77 0xc00469ca78}] []  [{kube-controller-manager Update apps/v1 2023-01-31 10:52:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc7a6231-4ec9-4683-8127-1c90af4b447b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 10:52:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 845d658455,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00469cb28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 31 10:52:05.815: INFO: Pod "test-recreate-deployment-cd8586fc7-n28l8" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cd8586fc7-n28l8 test-recreate-deployment-cd8586fc7- deployment-7588  0cd56fd9-0f76-4c70-89a9-aa0f5805fd34 35187 0 2023-01-31 10:52:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-cd8586fc7 e431195e-3ab7-41ab-910e-74485ad4c0fa 0xc00469d0d0 0xc00469d0d1}] []  [{kube-controller-manager Update v1 2023-01-31 10:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e431195e-3ab7-41ab-910e-74485ad4c0fa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k9zf2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k9zf2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 10:52:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 31 10:52:05.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7588" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":271,"skipped":4970,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:52:05.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3855
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 10:52:06.262: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 10:52:09.282: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:52:09.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3855" for this suite.
STEP: Destroying namespace "webhook-3855-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":356,"completed":272,"skipped":4981,"failed":0}

------------------------------
[sig-apps] Job 
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:52:09.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-2782
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Indexed job
STEP: Ensuring job reaches completions
STEP: Ensuring pods with index for job exist
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 31 10:52:17.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2782" for this suite.

• [SLOW TEST:8.151 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","total":356,"completed":273,"skipped":4981,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:52:17.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-4673
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jan 31 10:52:17.707: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jan 31 10:52:17.714: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 31 10:52:17.714: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jan 31 10:52:17.740: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 31 10:52:17.740: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jan 31 10:52:17.779: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 31 10:52:17.779: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jan 31 10:52:24.834: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:188
Jan 31 10:52:24.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4673" for this suite.

• [SLOW TEST:7.284 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":356,"completed":274,"skipped":5010,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:52:24.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3612
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 31 10:52:36.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3612" for this suite.

• [SLOW TEST:11.191 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":356,"completed":275,"skipped":5018,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:52:36.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5187
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:188
Jan 31 10:52:36.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5187" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":356,"completed":276,"skipped":5036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:52:36.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2821
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-b36f951e-9ea3-4490-b4a1-f6124e3f60fe
STEP: Creating a pod to test consume configMaps
Jan 31 10:52:36.352: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-85cfb7f3-beff-420f-8bc4-e13d4fcd10ff" in namespace "projected-2821" to be "Succeeded or Failed"
Jan 31 10:52:36.356: INFO: Pod "pod-projected-configmaps-85cfb7f3-beff-420f-8bc4-e13d4fcd10ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.460813ms
Jan 31 10:52:38.360: INFO: Pod "pod-projected-configmaps-85cfb7f3-beff-420f-8bc4-e13d4fcd10ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008455469s
Jan 31 10:52:40.366: INFO: Pod "pod-projected-configmaps-85cfb7f3-beff-420f-8bc4-e13d4fcd10ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013907119s
STEP: Saw pod success
Jan 31 10:52:40.366: INFO: Pod "pod-projected-configmaps-85cfb7f3-beff-420f-8bc4-e13d4fcd10ff" satisfied condition "Succeeded or Failed"
Jan 31 10:52:40.368: INFO: Trying to get logs from node macpro-x86-1 pod pod-projected-configmaps-85cfb7f3-beff-420f-8bc4-e13d4fcd10ff container agnhost-container: <nil>
STEP: delete the pod
Jan 31 10:52:40.397: INFO: Waiting for pod pod-projected-configmaps-85cfb7f3-beff-420f-8bc4-e13d4fcd10ff to disappear
Jan 31 10:52:40.400: INFO: Pod pod-projected-configmaps-85cfb7f3-beff-420f-8bc4-e13d4fcd10ff no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 31 10:52:40.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2821" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":277,"skipped":5111,"failed":0}
SSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:52:40.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-3069
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 31 10:52:42.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3069" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","total":356,"completed":278,"skipped":5116,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:52:42.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6113
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 10:52:42.724: INFO: Waiting up to 5m0s for pod "downwardapi-volume-62245f86-68cf-4a36-99d2-d057a89467a3" in namespace "downward-api-6113" to be "Succeeded or Failed"
Jan 31 10:52:42.728: INFO: Pod "downwardapi-volume-62245f86-68cf-4a36-99d2-d057a89467a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.355554ms
Jan 31 10:52:44.735: INFO: Pod "downwardapi-volume-62245f86-68cf-4a36-99d2-d057a89467a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011052045s
Jan 31 10:52:46.739: INFO: Pod "downwardapi-volume-62245f86-68cf-4a36-99d2-d057a89467a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015269778s
STEP: Saw pod success
Jan 31 10:52:46.739: INFO: Pod "downwardapi-volume-62245f86-68cf-4a36-99d2-d057a89467a3" satisfied condition "Succeeded or Failed"
Jan 31 10:52:46.742: INFO: Trying to get logs from node macpro-x86-1 pod downwardapi-volume-62245f86-68cf-4a36-99d2-d057a89467a3 container client-container: <nil>
STEP: delete the pod
Jan 31 10:52:46.755: INFO: Waiting for pod downwardapi-volume-62245f86-68cf-4a36-99d2-d057a89467a3 to disappear
Jan 31 10:52:46.758: INFO: Pod downwardapi-volume-62245f86-68cf-4a36-99d2-d057a89467a3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 31 10:52:46.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6113" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":279,"skipped":5116,"failed":0}

------------------------------
[sig-node] Containers 
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:52:46.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4527
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override command
Jan 31 10:52:46.895: INFO: Waiting up to 5m0s for pod "client-containers-edcfa919-a854-410a-9d1a-e2db3df6d486" in namespace "containers-4527" to be "Succeeded or Failed"
Jan 31 10:52:46.901: INFO: Pod "client-containers-edcfa919-a854-410a-9d1a-e2db3df6d486": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764176ms
Jan 31 10:52:48.906: INFO: Pod "client-containers-edcfa919-a854-410a-9d1a-e2db3df6d486": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010820067s
Jan 31 10:52:50.911: INFO: Pod "client-containers-edcfa919-a854-410a-9d1a-e2db3df6d486": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015577393s
STEP: Saw pod success
Jan 31 10:52:50.911: INFO: Pod "client-containers-edcfa919-a854-410a-9d1a-e2db3df6d486" satisfied condition "Succeeded or Failed"
Jan 31 10:52:50.913: INFO: Trying to get logs from node macpro-x86-1 pod client-containers-edcfa919-a854-410a-9d1a-e2db3df6d486 container agnhost-container: <nil>
STEP: delete the pod
Jan 31 10:52:50.925: INFO: Waiting for pod client-containers-edcfa919-a854-410a-9d1a-e2db3df6d486 to disappear
Jan 31 10:52:50.928: INFO: Pod client-containers-edcfa919-a854-410a-9d1a-e2db3df6d486 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 31 10:52:50.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4527" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","total":356,"completed":280,"skipped":5116,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:52:50.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4059
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:52:51.074: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 31 10:52:56.081: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jan 31 10:52:56.087: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jan 31 10:52:56.104: INFO: observed ReplicaSet test-rs in namespace replicaset-4059 with ReadyReplicas 1, AvailableReplicas 1
Jan 31 10:52:56.115: INFO: observed ReplicaSet test-rs in namespace replicaset-4059 with ReadyReplicas 1, AvailableReplicas 1
Jan 31 10:52:56.128: INFO: observed ReplicaSet test-rs in namespace replicaset-4059 with ReadyReplicas 1, AvailableReplicas 1
Jan 31 10:52:56.138: INFO: observed ReplicaSet test-rs in namespace replicaset-4059 with ReadyReplicas 1, AvailableReplicas 1
Jan 31 10:52:57.626: INFO: observed ReplicaSet test-rs in namespace replicaset-4059 with ReadyReplicas 2, AvailableReplicas 2
Jan 31 10:52:57.794: INFO: observed Replicaset test-rs in namespace replicaset-4059 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 31 10:52:57.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4059" for this suite.

• [SLOW TEST:6.869 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":356,"completed":281,"skipped":5136,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:52:57.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3998
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating all guestbook components
Jan 31 10:52:57.927: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 31 10:52:57.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-3998 create -f -'
Jan 31 10:52:59.121: INFO: stderr: ""
Jan 31 10:52:59.121: INFO: stdout: "service/agnhost-replica created\n"
Jan 31 10:52:59.121: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 31 10:52:59.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-3998 create -f -'
Jan 31 10:53:00.376: INFO: stderr: ""
Jan 31 10:53:00.376: INFO: stdout: "service/agnhost-primary created\n"
Jan 31 10:53:00.376: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 31 10:53:00.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-3998 create -f -'
Jan 31 10:53:00.555: INFO: stderr: ""
Jan 31 10:53:00.555: INFO: stdout: "service/frontend created\n"
Jan 31 10:53:00.555: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 31 10:53:00.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-3998 create -f -'
Jan 31 10:53:01.675: INFO: stderr: ""
Jan 31 10:53:01.675: INFO: stdout: "deployment.apps/frontend created\n"
Jan 31 10:53:01.675: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 31 10:53:01.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-3998 create -f -'
Jan 31 10:53:01.853: INFO: stderr: ""
Jan 31 10:53:01.853: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 31 10:53:01.853: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 31 10:53:01.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-3998 create -f -'
Jan 31 10:53:02.023: INFO: stderr: ""
Jan 31 10:53:02.023: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jan 31 10:53:02.023: INFO: Waiting for all frontend pods to be Running.
Jan 31 10:53:07.077: INFO: Waiting for frontend to serve content.
Jan 31 10:53:07.083: INFO: Trying to add a new entry to the guestbook.
Jan 31 10:53:07.089: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan 31 10:53:07.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-3998 delete --grace-period=0 --force -f -'
Jan 31 10:53:07.166: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 31 10:53:07.166: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jan 31 10:53:07.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-3998 delete --grace-period=0 --force -f -'
Jan 31 10:53:07.247: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 31 10:53:07.247: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 31 10:53:07.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-3998 delete --grace-period=0 --force -f -'
Jan 31 10:53:07.336: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 31 10:53:07.336: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 31 10:53:07.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-3998 delete --grace-period=0 --force -f -'
Jan 31 10:53:07.383: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 31 10:53:07.383: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 31 10:53:07.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-3998 delete --grace-period=0 --force -f -'
Jan 31 10:53:07.460: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 31 10:53:07.460: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 31 10:53:07.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-3998 delete --grace-period=0 --force -f -'
Jan 31 10:53:07.545: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 31 10:53:07.545: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 10:53:07.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3998" for this suite.

• [SLOW TEST:9.781 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:340
    should create and stop a working application  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":356,"completed":282,"skipped":5146,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:53:07.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-8290
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a pod template
STEP: Replace a pod template
Jan 31 10:53:07.777: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jan 31 10:53:07.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8290" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","total":356,"completed":283,"skipped":5161,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:53:07.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8425
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:53:07.929: INFO: created pod
Jan 31 10:53:07.929: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8425" to be "Succeeded or Failed"
Jan 31 10:53:07.932: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.286898ms
Jan 31 10:53:09.939: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009992147s
Jan 31 10:53:11.947: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018055552s
STEP: Saw pod success
Jan 31 10:53:11.947: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 31 10:53:41.950: INFO: polling logs
Jan 31 10:53:41.956: INFO: Pod logs: 
I0131 10:53:09.196283       1 log.go:195] OK: Got token
I0131 10:53:09.196306       1 log.go:195] validating with in-cluster discovery
I0131 10:53:09.196586       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0131 10:53:09.196608       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8425:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1675162988, NotBefore:1675162388, IssuedAt:1675162388, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8425", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cb111e9d-a2ac-4843-9fd5-81dba50e27c5"}}}
I0131 10:53:09.206247       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0131 10:53:09.210927       1 log.go:195] OK: Validated signature on JWT
I0131 10:53:09.210999       1 log.go:195] OK: Got valid claims from token!
I0131 10:53:09.211018       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8425:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1675162988, NotBefore:1675162388, IssuedAt:1675162388, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8425", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cb111e9d-a2ac-4843-9fd5-81dba50e27c5"}}}

Jan 31 10:53:41.956: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 31 10:53:41.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8425" for this suite.

• [SLOW TEST:34.183 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":356,"completed":284,"skipped":5174,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:53:41.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4805
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-1ac3297a-f3c5-498c-804e-9f9a27f9b5d0 in namespace container-probe-4805
Jan 31 10:53:44.111: INFO: Started pod busybox-1ac3297a-f3c5-498c-804e-9f9a27f9b5d0 in namespace container-probe-4805
STEP: checking the pod's current state and verifying that restartCount is present
Jan 31 10:53:44.113: INFO: Initial restart count of pod busybox-1ac3297a-f3c5-498c-804e-9f9a27f9b5d0 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 31 10:57:44.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4805" for this suite.

• [SLOW TEST:242.841 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":285,"skipped":5182,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:57:44.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7812
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 10:57:45.477: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 10:57:48.497: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:57:48.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8510-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:57:51.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7812" for this suite.
STEP: Destroying namespace "webhook-7812-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.959 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":356,"completed":286,"skipped":5201,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:57:51.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9697
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jan 31 10:57:51.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 10:57:57.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:58:16.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9697" for this suite.

• [SLOW TEST:24.778 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":356,"completed":287,"skipped":5257,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:58:16.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6326
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 10:58:17.146: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 10:58:20.169: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
Jan 31 10:58:21.210: INFO: Waiting for webhook configuration to be ready...
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 10:58:32.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6326" for this suite.
STEP: Destroying namespace "webhook-6326-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:15.924 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":356,"completed":288,"skipped":5368,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:58:32.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8347
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 31 10:58:43.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8347" for this suite.

• [SLOW TEST:11.296 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":356,"completed":289,"skipped":5388,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:58:43.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5817
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 31 10:58:43.897: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 31 10:58:43.903: INFO: Waiting for terminating namespaces to be deleted...
Jan 31 10:58:43.905: INFO: 
Logging pods the apiserver thinks is on node macpro-x86-1 before test
Jan 31 10:58:43.912: INFO: calico-node-nd7tv from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 10:58:43.912: INFO: 	Container calico-node ready: true, restart count 0
Jan 31 10:58:43.912: INFO: kube-proxy-tmcdj from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 10:58:43.912: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 31 10:58:43.912: INFO: helm-charts-fluent-bit-wmmdn from logging started at 2023-01-31 10:36:05 +0000 UTC (1 container statuses recorded)
Jan 31 10:58:43.912: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 31 10:58:43.912: INFO: speaker-2t9t2 from metallb-system started at 2023-01-31 10:36:05 +0000 UTC (1 container statuses recorded)
Jan 31 10:58:43.912: INFO: 	Container speaker ready: true, restart count 0
Jan 31 10:58:43.912: INFO: kube-prometheus-stack-prometheus-node-exporter-v265d from orka-monitoring started at 2023-01-31 10:36:05 +0000 UTC (1 container statuses recorded)
Jan 31 10:58:43.912: INFO: 	Container node-exporter ready: true, restart count 0
Jan 31 10:58:43.912: INFO: sonobuoy from sonobuoy started at 2023-01-31 09:42:03 +0000 UTC (1 container statuses recorded)
Jan 31 10:58:43.912: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 31 10:58:43.912: INFO: sonobuoy-systemd-logs-daemon-set-e1cae5bb05d74403-zlhvz from sonobuoy started at 2023-01-31 09:42:03 +0000 UTC (2 container statuses recorded)
Jan 31 10:58:43.912: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 10:58:43.912: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 31 10:58:43.912: INFO: 
Logging pods the apiserver thinks is on node macpro-x86-2 before test
Jan 31 10:58:43.918: INFO: calico-node-w8d7z from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 10:58:43.918: INFO: 	Container calico-node ready: true, restart count 0
Jan 31 10:58:43.918: INFO: kube-proxy-fj2ms from kube-system started at 2023-01-31 09:31:46 +0000 UTC (1 container statuses recorded)
Jan 31 10:58:43.918: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 31 10:58:43.918: INFO: helm-charts-fluent-bit-bwlwb from logging started at 2023-01-31 09:32:49 +0000 UTC (1 container statuses recorded)
Jan 31 10:58:43.918: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 31 10:58:43.918: INFO: speaker-7pfgg from metallb-system started at 2023-01-31 09:31:56 +0000 UTC (1 container statuses recorded)
Jan 31 10:58:43.918: INFO: 	Container speaker ready: true, restart count 0
Jan 31 10:58:43.918: INFO: kube-prometheus-stack-prometheus-node-exporter-b8d2q from orka-monitoring started at 2023-01-31 09:34:26 +0000 UTC (1 container statuses recorded)
Jan 31 10:58:43.918: INFO: 	Container node-exporter ready: true, restart count 0
Jan 31 10:58:43.918: INFO: sonobuoy-e2e-job-1083219078514678 from sonobuoy started at 2023-01-31 09:42:03 +0000 UTC (2 container statuses recorded)
Jan 31 10:58:43.918: INFO: 	Container e2e ready: true, restart count 0
Jan 31 10:58:43.918: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 10:58:43.918: INFO: sonobuoy-systemd-logs-daemon-set-e1cae5bb05d74403-fhxh8 from sonobuoy started at 2023-01-31 09:42:04 +0000 UTC (2 container statuses recorded)
Jan 31 10:58:43.918: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 10:58:43.918: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-593c3628-c82a-459f-a70b-80c9f14db5a4 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-593c3628-c82a-459f-a70b-80c9f14db5a4 off the node macpro-x86-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-593c3628-c82a-459f-a70b-80c9f14db5a4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 31 10:58:48.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5817" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":356,"completed":290,"skipped":5430,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:58:48.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8371
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8371
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8371
I0131 10:58:48.219215      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8371, replica count: 2
Jan 31 10:58:51.270: INFO: Creating new exec pod
I0131 10:58:51.270388      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 31 10:58:54.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-8371 exec execpoddc7t4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 31 10:58:54.400: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 31 10:58:54.400: INFO: stdout: "externalname-service-fkj5q"
Jan 31 10:58:54.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-8371 exec execpoddc7t4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.227.4 80'
Jan 31 10:58:54.485: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.227.4 80\nConnection to 10.108.227.4 80 port [tcp/http] succeeded!\n"
Jan 31 10:58:54.485: INFO: stdout: "externalname-service-fkj5q"
Jan 31 10:58:54.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-8371 exec execpoddc7t4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.31 32314'
Jan 31 10:58:54.574: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.31 32314\nConnection to 10.221.188.31 32314 port [tcp/*] succeeded!\n"
Jan 31 10:58:54.574: INFO: stdout: "externalname-service-tjxxc"
Jan 31 10:58:54.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-8371 exec execpoddc7t4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.32 32314'
Jan 31 10:58:54.656: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.32 32314\nConnection to 10.221.188.32 32314 port [tcp/*] succeeded!\n"
Jan 31 10:58:54.656: INFO: stdout: ""
Jan 31 10:58:55.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-8371 exec execpoddc7t4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.32 32314'
Jan 31 10:58:55.757: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.32 32314\nConnection to 10.221.188.32 32314 port [tcp/*] succeeded!\n"
Jan 31 10:58:55.757: INFO: stdout: ""
Jan 31 10:58:56.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-8371 exec execpoddc7t4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.32 32314'
Jan 31 10:58:56.750: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.32 32314\nConnection to 10.221.188.32 32314 port [tcp/*] succeeded!\n"
Jan 31 10:58:56.750: INFO: stdout: "externalname-service-tjxxc"
Jan 31 10:58:56.750: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 10:58:56.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8371" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:8.731 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":356,"completed":291,"skipped":5436,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:58:56.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-1346
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 31 10:58:56.932: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 31 10:58:56.937: INFO: starting watch
STEP: patching
STEP: updating
Jan 31 10:58:56.949: INFO: waiting for watch events with expected annotations
Jan 31 10:58:56.949: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 31 10:58:56.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1346" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":356,"completed":292,"skipped":5437,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:58:56.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5704
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 10:58:57.107: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8233155c-f0db-4389-9dfb-be4259a7606b" in namespace "projected-5704" to be "Succeeded or Failed"
Jan 31 10:58:57.113: INFO: Pod "downwardapi-volume-8233155c-f0db-4389-9dfb-be4259a7606b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.396557ms
Jan 31 10:58:59.117: INFO: Pod "downwardapi-volume-8233155c-f0db-4389-9dfb-be4259a7606b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009146864s
Jan 31 10:59:01.124: INFO: Pod "downwardapi-volume-8233155c-f0db-4389-9dfb-be4259a7606b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016552001s
STEP: Saw pod success
Jan 31 10:59:01.124: INFO: Pod "downwardapi-volume-8233155c-f0db-4389-9dfb-be4259a7606b" satisfied condition "Succeeded or Failed"
Jan 31 10:59:01.126: INFO: Trying to get logs from node macpro-x86-1 pod downwardapi-volume-8233155c-f0db-4389-9dfb-be4259a7606b container client-container: <nil>
STEP: delete the pod
Jan 31 10:59:01.149: INFO: Waiting for pod downwardapi-volume-8233155c-f0db-4389-9dfb-be4259a7606b to disappear
Jan 31 10:59:01.152: INFO: Pod downwardapi-volume-8233155c-f0db-4389-9dfb-be4259a7606b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 31 10:59:01.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5704" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":293,"skipped":5454,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:59:01.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3774
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-3774
Jan 31 10:59:01.293: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 31 10:59:03.303: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 31 10:59:03.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-3774 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 31 10:59:03.400: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 31 10:59:03.400: INFO: stdout: "iptables"
Jan 31 10:59:03.400: INFO: proxyMode: iptables
Jan 31 10:59:03.407: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 31 10:59:03.410: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-3774
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3774
I0131 10:59:03.444792      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3774, replica count: 3
I0131 10:59:06.496887      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 31 10:59:06.505: INFO: Creating new exec pod
Jan 31 10:59:09.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-3774 exec execpod-affinity85mjf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 31 10:59:09.627: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 31 10:59:09.627: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:59:09.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-3774 exec execpod-affinity85mjf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.137.178 80'
Jan 31 10:59:09.711: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.137.178 80\nConnection to 10.99.137.178 80 port [tcp/http] succeeded!\n"
Jan 31 10:59:09.711: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:59:09.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-3774 exec execpod-affinity85mjf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.31 31353'
Jan 31 10:59:09.794: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.31 31353\nConnection to 10.221.188.31 31353 port [tcp/*] succeeded!\n"
Jan 31 10:59:09.794: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:59:09.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-3774 exec execpod-affinity85mjf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.32 31353'
Jan 31 10:59:09.874: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.32 31353\nConnection to 10.221.188.32 31353 port [tcp/*] succeeded!\n"
Jan 31 10:59:09.874: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:59:09.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-3774 exec execpod-affinity85mjf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.221.188.31:31353/ ; done'
Jan 31 10:59:10.008: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n"
Jan 31 10:59:10.008: INFO: stdout: "\naffinity-nodeport-timeout-cz2hd\naffinity-nodeport-timeout-cz2hd\naffinity-nodeport-timeout-cz2hd\naffinity-nodeport-timeout-cz2hd\naffinity-nodeport-timeout-cz2hd\naffinity-nodeport-timeout-cz2hd\naffinity-nodeport-timeout-cz2hd\naffinity-nodeport-timeout-cz2hd\naffinity-nodeport-timeout-cz2hd\naffinity-nodeport-timeout-cz2hd\naffinity-nodeport-timeout-cz2hd\naffinity-nodeport-timeout-cz2hd\naffinity-nodeport-timeout-cz2hd\naffinity-nodeport-timeout-cz2hd\naffinity-nodeport-timeout-cz2hd\naffinity-nodeport-timeout-cz2hd"
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.008: INFO: Received response from host: affinity-nodeport-timeout-cz2hd
Jan 31 10:59:10.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-3774 exec execpod-affinity85mjf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.221.188.31:31353/'
Jan 31 10:59:10.096: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n"
Jan 31 10:59:10.096: INFO: stdout: "affinity-nodeport-timeout-cz2hd"
Jan 31 10:59:30.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-3774 exec execpod-affinity85mjf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.221.188.31:31353/'
Jan 31 10:59:30.193: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.221.188.31:31353/\n"
Jan 31 10:59:30.193: INFO: stdout: "affinity-nodeport-timeout-9wrv6"
Jan 31 10:59:30.193: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3774, will wait for the garbage collector to delete the pods
Jan 31 10:59:30.269: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.051893ms
Jan 31 10:59:30.370: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.088067ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 10:59:32.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3774" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:31.358 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":294,"skipped":5488,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:59:32.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6390
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-6390
STEP: creating service affinity-clusterip-transition in namespace services-6390
STEP: creating replication controller affinity-clusterip-transition in namespace services-6390
I0131 10:59:32.664955      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-6390, replica count: 3
I0131 10:59:35.716541      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 31 10:59:35.723: INFO: Creating new exec pod
Jan 31 10:59:38.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-6390 exec execpod-affinityjb2dl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 31 10:59:38.831: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 31 10:59:38.831: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:59:38.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-6390 exec execpod-affinityjb2dl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.184.172 80'
Jan 31 10:59:38.923: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.184.172 80\nConnection to 10.104.184.172 80 port [tcp/http] succeeded!\n"
Jan 31 10:59:38.923: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 10:59:38.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-6390 exec execpod-affinityjb2dl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.104.184.172:80/ ; done'
Jan 31 10:59:39.066: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n"
Jan 31 10:59:39.066: INFO: stdout: "\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-8n27d\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-ghb7r\naffinity-clusterip-transition-ghb7r\naffinity-clusterip-transition-ghb7r\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-8n27d\naffinity-clusterip-transition-8n27d\naffinity-clusterip-transition-ghb7r\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-8n27d\naffinity-clusterip-transition-7c5js"
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-8n27d
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-ghb7r
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-ghb7r
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-ghb7r
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-8n27d
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-8n27d
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-ghb7r
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-8n27d
Jan 31 10:59:39.066: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-6390 exec execpod-affinityjb2dl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.104.184.172:80/ ; done'
Jan 31 10:59:39.208: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.184.172:80/\n"
Jan 31 10:59:39.208: INFO: stdout: "\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js\naffinity-clusterip-transition-7c5js"
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Received response from host: affinity-clusterip-transition-7c5js
Jan 31 10:59:39.208: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6390, will wait for the garbage collector to delete the pods
Jan 31 10:59:39.276: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.131937ms
Jan 31 10:59:39.377: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.757979ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 10:59:41.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6390" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:8.998 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":295,"skipped":5495,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:59:41.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3536
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 10:59:41.651: INFO: Got root ca configmap in namespace "svcaccounts-3536"
Jan 31 10:59:41.656: INFO: Deleted root ca configmap in namespace "svcaccounts-3536"
STEP: waiting for a new root ca configmap created
Jan 31 10:59:42.160: INFO: Recreated root ca configmap in namespace "svcaccounts-3536"
Jan 31 10:59:42.165: INFO: Updated root ca configmap in namespace "svcaccounts-3536"
STEP: waiting for the root ca configmap reconciled
Jan 31 10:59:42.669: INFO: Reconciled root ca configmap in namespace "svcaccounts-3536"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 31 10:59:42.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3536" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":356,"completed":296,"skipped":5542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:59:42.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-9562
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jan 31 10:59:42.814: INFO: Waiting up to 5m0s for pod "security-context-f4e03c64-71c7-4e0d-a1b8-a026e2c7ced0" in namespace "security-context-9562" to be "Succeeded or Failed"
Jan 31 10:59:42.819: INFO: Pod "security-context-f4e03c64-71c7-4e0d-a1b8-a026e2c7ced0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.489876ms
Jan 31 10:59:44.824: INFO: Pod "security-context-f4e03c64-71c7-4e0d-a1b8-a026e2c7ced0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010447466s
Jan 31 10:59:46.831: INFO: Pod "security-context-f4e03c64-71c7-4e0d-a1b8-a026e2c7ced0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01695423s
STEP: Saw pod success
Jan 31 10:59:46.831: INFO: Pod "security-context-f4e03c64-71c7-4e0d-a1b8-a026e2c7ced0" satisfied condition "Succeeded or Failed"
Jan 31 10:59:46.836: INFO: Trying to get logs from node macpro-x86-2 pod security-context-f4e03c64-71c7-4e0d-a1b8-a026e2c7ced0 container test-container: <nil>
STEP: delete the pod
Jan 31 10:59:46.863: INFO: Waiting for pod security-context-f4e03c64-71c7-4e0d-a1b8-a026e2c7ced0 to disappear
Jan 31 10:59:46.865: INFO: Pod security-context-f4e03c64-71c7-4e0d-a1b8-a026e2c7ced0 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 31 10:59:46.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-9562" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":297,"skipped":5582,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:59:46.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3265
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 31 10:59:46.995: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 31 10:59:52.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3265" for this suite.

• [SLOW TEST:5.522 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":356,"completed":298,"skipped":5609,"failed":0}
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:59:52.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8017
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 31 10:59:52.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8017" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":356,"completed":299,"skipped":5614,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 10:59:52.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-6974
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 31 10:59:52.694: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 31 11:00:52.743: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:00:52.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-1149
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jan 31 11:00:54.911: INFO: found a healthy node: macpro-x86-1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 11:01:03.010: INFO: pods created so far: [1 1 1]
Jan 31 11:01:03.011: INFO: length of pods created so far: 3
Jan 31 11:01:05.024: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:188
Jan 31 11:01:12.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1149" for this suite.
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 31 11:01:12.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6974" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:79.541 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":356,"completed":300,"skipped":5621,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:01:12.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2283
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-a557541b-5156-4352-a21f-41c6bebe714a
STEP: Creating a pod to test consume configMaps
Jan 31 11:01:12.232: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bd4add85-8871-48b9-bd4c-e6ca2f586d0a" in namespace "projected-2283" to be "Succeeded or Failed"
Jan 31 11:01:12.234: INFO: Pod "pod-projected-configmaps-bd4add85-8871-48b9-bd4c-e6ca2f586d0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.780975ms
Jan 31 11:01:14.242: INFO: Pod "pod-projected-configmaps-bd4add85-8871-48b9-bd4c-e6ca2f586d0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010205732s
Jan 31 11:01:16.247: INFO: Pod "pod-projected-configmaps-bd4add85-8871-48b9-bd4c-e6ca2f586d0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015756363s
STEP: Saw pod success
Jan 31 11:01:16.247: INFO: Pod "pod-projected-configmaps-bd4add85-8871-48b9-bd4c-e6ca2f586d0a" satisfied condition "Succeeded or Failed"
Jan 31 11:01:16.250: INFO: Trying to get logs from node macpro-x86-2 pod pod-projected-configmaps-bd4add85-8871-48b9-bd4c-e6ca2f586d0a container agnhost-container: <nil>
STEP: delete the pod
Jan 31 11:01:16.268: INFO: Waiting for pod pod-projected-configmaps-bd4add85-8871-48b9-bd4c-e6ca2f586d0a to disappear
Jan 31 11:01:16.271: INFO: Pod pod-projected-configmaps-bd4add85-8871-48b9-bd4c-e6ca2f586d0a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 31 11:01:16.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2283" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":301,"skipped":5624,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:01:16.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2547
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 31 11:01:16.413: INFO: Waiting up to 5m0s for pod "downward-api-8b88beb8-1ce7-40ec-822e-200b881c889c" in namespace "downward-api-2547" to be "Succeeded or Failed"
Jan 31 11:01:16.419: INFO: Pod "downward-api-8b88beb8-1ce7-40ec-822e-200b881c889c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.595253ms
Jan 31 11:01:18.424: INFO: Pod "downward-api-8b88beb8-1ce7-40ec-822e-200b881c889c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011767225s
Jan 31 11:01:20.434: INFO: Pod "downward-api-8b88beb8-1ce7-40ec-822e-200b881c889c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020985506s
STEP: Saw pod success
Jan 31 11:01:20.434: INFO: Pod "downward-api-8b88beb8-1ce7-40ec-822e-200b881c889c" satisfied condition "Succeeded or Failed"
Jan 31 11:01:20.436: INFO: Trying to get logs from node macpro-x86-2 pod downward-api-8b88beb8-1ce7-40ec-822e-200b881c889c container dapi-container: <nil>
STEP: delete the pod
Jan 31 11:01:20.448: INFO: Waiting for pod downward-api-8b88beb8-1ce7-40ec-822e-200b881c889c to disappear
Jan 31 11:01:20.451: INFO: Pod downward-api-8b88beb8-1ce7-40ec-822e-200b881c889c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 31 11:01:20.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2547" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":356,"completed":302,"skipped":5626,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:01:20.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2513
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Jan 31 11:01:22.610: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 31 11:01:28.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2513" for this suite.

• [SLOW TEST:8.293 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":356,"completed":303,"skipped":5639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:01:28.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2302
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 31 11:01:32.942: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 31 11:01:32.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2302" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":304,"skipped":5683,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:01:32.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9310
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-7fd7ac51-bdb0-43c4-9eb9-cffc2632f3cf
STEP: Creating a pod to test consume secrets
Jan 31 11:01:33.108: INFO: Waiting up to 5m0s for pod "pod-secrets-662d4a6b-bc2a-4c41-aca7-a37840b5d50b" in namespace "secrets-9310" to be "Succeeded or Failed"
Jan 31 11:01:33.114: INFO: Pod "pod-secrets-662d4a6b-bc2a-4c41-aca7-a37840b5d50b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.652704ms
Jan 31 11:01:35.123: INFO: Pod "pod-secrets-662d4a6b-bc2a-4c41-aca7-a37840b5d50b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015228427s
Jan 31 11:01:37.126: INFO: Pod "pod-secrets-662d4a6b-bc2a-4c41-aca7-a37840b5d50b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018430964s
STEP: Saw pod success
Jan 31 11:01:37.126: INFO: Pod "pod-secrets-662d4a6b-bc2a-4c41-aca7-a37840b5d50b" satisfied condition "Succeeded or Failed"
Jan 31 11:01:37.128: INFO: Trying to get logs from node macpro-x86-1 pod pod-secrets-662d4a6b-bc2a-4c41-aca7-a37840b5d50b container secret-volume-test: <nil>
STEP: delete the pod
Jan 31 11:01:37.149: INFO: Waiting for pod pod-secrets-662d4a6b-bc2a-4c41-aca7-a37840b5d50b to disappear
Jan 31 11:01:37.151: INFO: Pod pod-secrets-662d4a6b-bc2a-4c41-aca7-a37840b5d50b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 31 11:01:37.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9310" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":305,"skipped":5685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:01:37.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1578
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1578
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1578
STEP: creating replication controller externalsvc in namespace services-1578
I0131 11:01:37.335363      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1578, replica count: 2
I0131 11:01:40.386943      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jan 31 11:01:40.408: INFO: Creating new exec pod
Jan 31 11:01:42.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-1578 exec execpodgxx8f -- /bin/sh -x -c nslookup nodeport-service.services-1578.svc.cluster.local'
Jan 31 11:01:42.527: INFO: stderr: "+ nslookup nodeport-service.services-1578.svc.cluster.local\n"
Jan 31 11:01:42.527: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nName:\tnodeport-service.services-1578.svc.cluster.local\nAddress: 10.97.114.58\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1578, will wait for the garbage collector to delete the pods
Jan 31 11:01:42.586: INFO: Deleting ReplicationController externalsvc took: 5.424533ms
Jan 31 11:01:42.687: INFO: Terminating ReplicationController externalsvc pods took: 100.690767ms
Jan 31 11:01:44.620: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 11:01:44.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1578" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:7.487 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":356,"completed":306,"skipped":5707,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:01:44.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8751
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 31 11:01:45.160: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 31 11:01:48.186: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 11:01:49.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8751" for this suite.
STEP: Destroying namespace "webhook-8751-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":356,"completed":307,"skipped":5718,"failed":0}
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:01:49.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4647
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 31 11:01:49.551: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 31 11:01:51.555: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 31 11:01:51.571: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 31 11:01:53.575: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 31 11:01:53.588: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 31 11:01:53.590: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 31 11:01:55.591: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 31 11:01:55.595: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 31 11:01:55.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4647" for this suite.

• [SLOW TEST:6.207 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":356,"completed":308,"skipped":5721,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:01:55.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7459
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 11:01:55.757: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a200417-4c00-4cee-8b4c-dee22a63578f" in namespace "projected-7459" to be "Succeeded or Failed"
Jan 31 11:01:55.763: INFO: Pod "downwardapi-volume-0a200417-4c00-4cee-8b4c-dee22a63578f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.218536ms
Jan 31 11:01:57.767: INFO: Pod "downwardapi-volume-0a200417-4c00-4cee-8b4c-dee22a63578f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010187789s
Jan 31 11:01:59.774: INFO: Pod "downwardapi-volume-0a200417-4c00-4cee-8b4c-dee22a63578f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017761262s
STEP: Saw pod success
Jan 31 11:01:59.774: INFO: Pod "downwardapi-volume-0a200417-4c00-4cee-8b4c-dee22a63578f" satisfied condition "Succeeded or Failed"
Jan 31 11:01:59.776: INFO: Trying to get logs from node macpro-x86-1 pod downwardapi-volume-0a200417-4c00-4cee-8b4c-dee22a63578f container client-container: <nil>
STEP: delete the pod
Jan 31 11:01:59.792: INFO: Waiting for pod downwardapi-volume-0a200417-4c00-4cee-8b4c-dee22a63578f to disappear
Jan 31 11:01:59.794: INFO: Pod downwardapi-volume-0a200417-4c00-4cee-8b4c-dee22a63578f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 31 11:01:59.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7459" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":309,"skipped":5730,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:01:59.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5262
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jan 31 11:01:59.936: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 11:01:59.936: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 11:01:59.945: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 11:01:59.945: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 11:01:59.963: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 11:01:59.963: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 11:02:00.003: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 11:02:00.003: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 11:02:01.378: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 31 11:02:01.378: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 31 11:02:01.603: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jan 31 11:02:01.620: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jan 31 11:02:01.621: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0
Jan 31 11:02:01.621: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0
Jan 31 11:02:01.621: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0
Jan 31 11:02:01.621: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0
Jan 31 11:02:01.621: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0
Jan 31 11:02:01.621: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0
Jan 31 11:02:01.621: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0
Jan 31 11:02:01.621: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 0
Jan 31 11:02:01.621: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1
Jan 31 11:02:01.621: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1
Jan 31 11:02:01.621: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2
Jan 31 11:02:01.621: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2
Jan 31 11:02:01.621: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2
Jan 31 11:02:01.621: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2
Jan 31 11:02:01.632: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2
Jan 31 11:02:01.632: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2
Jan 31 11:02:01.664: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2
Jan 31 11:02:01.664: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2
Jan 31 11:02:01.675: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1
Jan 31 11:02:01.675: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1
Jan 31 11:02:01.692: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1
Jan 31 11:02:01.692: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1
Jan 31 11:02:02.609: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2
Jan 31 11:02:02.609: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2
Jan 31 11:02:02.631: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1
STEP: listing Deployments
Jan 31 11:02:02.636: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jan 31 11:02:02.647: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jan 31 11:02:02.656: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 11:02:02.661: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 11:02:02.693: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 11:02:02.709: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 11:02:02.723: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 11:02:02.731: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 11:02:03.391: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 11:02:03.618: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 11:02:03.660: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 11:02:03.677: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 11:02:04.399: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jan 31 11:02:04.451: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1
Jan 31 11:02:04.452: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1
Jan 31 11:02:04.452: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1
Jan 31 11:02:04.452: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1
Jan 31 11:02:04.452: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1
Jan 31 11:02:04.452: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 1
Jan 31 11:02:04.452: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2
Jan 31 11:02:04.452: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 3
Jan 31 11:02:04.452: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2
Jan 31 11:02:04.452: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 2
Jan 31 11:02:04.452: INFO: observed Deployment test-deployment in namespace deployment-5262 with ReadyReplicas 3
STEP: deleting the Deployment
Jan 31 11:02:04.460: INFO: observed event type MODIFIED
Jan 31 11:02:04.460: INFO: observed event type MODIFIED
Jan 31 11:02:04.460: INFO: observed event type MODIFIED
Jan 31 11:02:04.460: INFO: observed event type MODIFIED
Jan 31 11:02:04.460: INFO: observed event type MODIFIED
Jan 31 11:02:04.460: INFO: observed event type MODIFIED
Jan 31 11:02:04.460: INFO: observed event type MODIFIED
Jan 31 11:02:04.460: INFO: observed event type MODIFIED
Jan 31 11:02:04.461: INFO: observed event type MODIFIED
Jan 31 11:02:04.461: INFO: observed event type MODIFIED
Jan 31 11:02:04.461: INFO: observed event type MODIFIED
Jan 31 11:02:04.461: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 31 11:02:04.474: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 31 11:02:04.482: INFO: ReplicaSet "test-deployment-6b48c869b6":
&ReplicaSet{ObjectMeta:{test-deployment-6b48c869b6  deployment-5262  b5e33ed4-eb9b-4317-9c64-7506d4b36a6a 38998 3 2023-01-31 11:01:59 +0000 UTC <nil> <nil> map[pod-template-hash:6b48c869b6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment dc5d3d08-3bd2-4d5e-8c75-c3f4ac578984 0xc0042ec9f7 0xc0042ec9f8}] []  [{kube-controller-manager Update apps/v1 2023-01-31 11:01:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc5d3d08-3bd2-4d5e-8c75-c3f4ac578984\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 11:02:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6b48c869b6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6b48c869b6 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042eca80 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 31 11:02:04.487: INFO: pod: "test-deployment-6b48c869b6-svj8n":
&Pod{ObjectMeta:{test-deployment-6b48c869b6-svj8n test-deployment-6b48c869b6- deployment-5262  c644ddf1-1882-48ea-b149-80f02fbfc251 39068 0 2023-01-31 11:01:59 +0000 UTC 2023-01-31 11:02:02 +0000 UTC 0xc0042ed078 map[pod-template-hash:6b48c869b6 test-deployment-static:true] map[cni.projectcalico.org/containerID:2e14ab1fc5b14b0a764abf5488702a9be41c0dba0ba2ffd5fd5d60fac32e2d7a cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs: kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-6b48c869b6 b5e33ed4-eb9b-4317-9c64-7506d4b36a6a 0xc0042ed0a7 0xc0042ed0a8}] []  [{kube-controller-manager Update v1 2023-01-31 11:01:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5e33ed4-eb9b-4317-9c64-7506d4b36a6a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:02:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:02:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.75.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8fw8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8fw8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:01:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:01:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.31,PodIP:192.168.75.52,StartTime:2023-01-31 11:01:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 11:02:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://819dbf4e331a7668e40cf1f463b4e6f4fa27391d0af42fcac866fb6368bc2fbe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.75.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 31 11:02:04.487: INFO: ReplicaSet "test-deployment-74c6dd549b":
&ReplicaSet{ObjectMeta:{test-deployment-74c6dd549b  deployment-5262  668d38be-580c-4421-bf2f-7273ecca3fa3 39079 2 2023-01-31 11:02:02 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment dc5d3d08-3bd2-4d5e-8c75-c3f4ac578984 0xc0042ecae7 0xc0042ecae8}] []  [{kube-controller-manager Update apps/v1 2023-01-31 11:02:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc5d3d08-3bd2-4d5e-8c75-c3f4ac578984\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 11:02:03 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 74c6dd549b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042ecc50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 31 11:02:04.493: INFO: pod: "test-deployment-74c6dd549b-jjxmz":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-jjxmz test-deployment-74c6dd549b- deployment-5262  a79fc50e-148e-4780-9839-7e6bda5ad91e 39048 0 2023-01-31 11:02:02 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[cni.projectcalico.org/containerID:5248d5249fa8de3240de6353238c39395849fc351882277bd295bd2836040a4d cni.projectcalico.org/podIP:192.168.75.23/32 cni.projectcalico.org/podIPs:192.168.75.23/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-74c6dd549b 668d38be-580c-4421-bf2f-7273ecca3fa3 0xc0042edd07 0xc0042edd08}] []  [{kube-controller-manager Update v1 2023-01-31 11:02:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"668d38be-580c-4421-bf2f-7273ecca3fa3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:02:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:02:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.75.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gf5bt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gf5bt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.31,PodIP:192.168.75.23,StartTime:2023-01-31 11:02:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 11:02:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://53c77d5bedc27cca823f13aac2f54a5117aba49986ae997edacc43f440a62062,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.75.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 31 11:02:04.493: INFO: pod: "test-deployment-74c6dd549b-wprpf":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-wprpf test-deployment-74c6dd549b- deployment-5262  2cd2012c-8086-434d-80b5-a35d93b5d993 39078 0 2023-01-31 11:02:03 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[cni.projectcalico.org/containerID:ad4958af8910e6060ed24a9e46cc69eced7a9168b152e923775f24c6ea10afbf cni.projectcalico.org/podIP:192.168.38.245/32 cni.projectcalico.org/podIPs:192.168.38.245/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-74c6dd549b 668d38be-580c-4421-bf2f-7273ecca3fa3 0xc0042edf17 0xc0042edf18}] []  [{kube-controller-manager Update v1 2023-01-31 11:02:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"668d38be-580c-4421-bf2f-7273ecca3fa3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:02:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:02:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.38.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4ns74,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4ns74,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.32,PodIP:192.168.38.245,StartTime:2023-01-31 11:02:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 11:02:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c6c33eda1e0c1ba80784f1177d2d2c445d5964c06bcc03e8491d57049cd1ac24,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.38.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 31 11:02:04.493: INFO: ReplicaSet "test-deployment-84b949bdfc":
&ReplicaSet{ObjectMeta:{test-deployment-84b949bdfc  deployment-5262  ef1ca996-9c02-4afa-9810-e977dbb083e2 39089 4 2023-01-31 11:02:01 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment dc5d3d08-3bd2-4d5e-8c75-c3f4ac578984 0xc0042ecd37 0xc0042ecd38}] []  [{kube-controller-manager Update apps/v1 2023-01-31 11:02:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc5d3d08-3bd2-4d5e-8c75-c3f4ac578984\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 11:02:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 84b949bdfc,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.7 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042ecdc0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 31 11:02:04.500: INFO: pod: "test-deployment-84b949bdfc-bj5z6":
&Pod{ObjectMeta:{test-deployment-84b949bdfc-bj5z6 test-deployment-84b949bdfc- deployment-5262  be093b62-cd24-4638-a3e6-06e0d12efd95 39053 0 2023-01-31 11:02:01 +0000 UTC 2023-01-31 11:02:04 +0000 UTC 0xc00314f1a8 map[pod-template-hash:84b949bdfc test-deployment-static:true] map[cni.projectcalico.org/containerID:cd2371844f212eb5c8f5c9edfcad326b38dd5ef03158b67bf5cbd1a6acf00b97 cni.projectcalico.org/podIP:192.168.75.57/32 cni.projectcalico.org/podIPs:192.168.75.57/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-84b949bdfc ef1ca996-9c02-4afa-9810-e977dbb083e2 0xc00314f377 0xc00314f378}] []  [{kube-controller-manager Update v1 2023-01-31 11:02:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef1ca996-9c02-4afa-9810-e977dbb083e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:02:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:02:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.75.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cq2pj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.7,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cq2pj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.31,PodIP:192.168.75.57,StartTime:2023-01-31 11:02:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 11:02:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.7,ImageID:k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c,ContainerID:containerd://af0c03a365f27f2388fee68fcd1b60905d65f49347b5cb4ca91f3c556734cdb6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.75.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 31 11:02:04.500: INFO: pod: "test-deployment-84b949bdfc-qj8hm":
&Pod{ObjectMeta:{test-deployment-84b949bdfc-qj8hm test-deployment-84b949bdfc- deployment-5262  f0755258-7f98-4fe8-b2a5-b200b08d13ac 39084 0 2023-01-31 11:02:02 +0000 UTC 2023-01-31 11:02:05 +0000 UTC 0xc00314f640 map[pod-template-hash:84b949bdfc test-deployment-static:true] map[cni.projectcalico.org/containerID:21740d3dc7d3b5d72d3d05e6f3c5bf02eba13517e614651927a05e15b8333527 cni.projectcalico.org/podIP:192.168.38.218/32 cni.projectcalico.org/podIPs:192.168.38.218/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-84b949bdfc ef1ca996-9c02-4afa-9810-e977dbb083e2 0xc00314f677 0xc00314f678}] []  [{kube-controller-manager Update v1 2023-01-31 11:02:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef1ca996-9c02-4afa-9810-e977dbb083e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:02:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:02:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.38.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kd6hx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.7,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kd6hx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:02:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.32,PodIP:192.168.38.218,StartTime:2023-01-31 11:02:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 11:02:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.7,ImageID:k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c,ContainerID:containerd://11db3c664de689a1345625e00be3d0c7684a5f9461e56a791514e6c9de22c44a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.38.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 31 11:02:04.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5262" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":356,"completed":310,"skipped":5742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:02:04.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8622
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-391b258a-6411-4244-ade4-04b4ad0a3dd8
STEP: Creating a pod to test consume secrets
Jan 31 11:02:04.680: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9f525f18-7302-477e-881c-e409c08d54a1" in namespace "projected-8622" to be "Succeeded or Failed"
Jan 31 11:02:04.684: INFO: Pod "pod-projected-secrets-9f525f18-7302-477e-881c-e409c08d54a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.434542ms
Jan 31 11:02:06.690: INFO: Pod "pod-projected-secrets-9f525f18-7302-477e-881c-e409c08d54a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009794494s
Jan 31 11:02:08.695: INFO: Pod "pod-projected-secrets-9f525f18-7302-477e-881c-e409c08d54a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014960909s
STEP: Saw pod success
Jan 31 11:02:08.695: INFO: Pod "pod-projected-secrets-9f525f18-7302-477e-881c-e409c08d54a1" satisfied condition "Succeeded or Failed"
Jan 31 11:02:08.697: INFO: Trying to get logs from node macpro-x86-2 pod pod-projected-secrets-9f525f18-7302-477e-881c-e409c08d54a1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 31 11:02:08.710: INFO: Waiting for pod pod-projected-secrets-9f525f18-7302-477e-881c-e409c08d54a1 to disappear
Jan 31 11:02:08.713: INFO: Pod pod-projected-secrets-9f525f18-7302-477e-881c-e409c08d54a1 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 31 11:02:08.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8622" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":311,"skipped":5781,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:02:08.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslicemirroring-8921
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
STEP: mirroring a new custom Endpoint
Jan 31 11:02:08.876: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
STEP: mirroring deletion of a custom Endpoint
Jan 31 11:02:10.902: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:188
Jan 31 11:02:12.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-8921" for this suite.
•{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":356,"completed":312,"skipped":5791,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:02:12.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4971
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 11:02:13.053: INFO: The status of Pod busybox-scheduling-22ab3338-4702-49fa-80cf-7dc28188a159 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 11:02:15.056: INFO: The status of Pod busybox-scheduling-22ab3338-4702-49fa-80cf-7dc28188a159 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 31 11:02:15.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4971" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":356,"completed":313,"skipped":5816,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:02:15.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9866
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 31 11:02:15.206: INFO: Waiting up to 5m0s for pod "downward-api-a11714ab-9bc4-499f-9eb2-7e4206953696" in namespace "downward-api-9866" to be "Succeeded or Failed"
Jan 31 11:02:15.210: INFO: Pod "downward-api-a11714ab-9bc4-499f-9eb2-7e4206953696": Phase="Pending", Reason="", readiness=false. Elapsed: 3.970674ms
Jan 31 11:02:17.216: INFO: Pod "downward-api-a11714ab-9bc4-499f-9eb2-7e4206953696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009924402s
Jan 31 11:02:19.220: INFO: Pod "downward-api-a11714ab-9bc4-499f-9eb2-7e4206953696": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013597864s
STEP: Saw pod success
Jan 31 11:02:19.220: INFO: Pod "downward-api-a11714ab-9bc4-499f-9eb2-7e4206953696" satisfied condition "Succeeded or Failed"
Jan 31 11:02:19.223: INFO: Trying to get logs from node macpro-x86-1 pod downward-api-a11714ab-9bc4-499f-9eb2-7e4206953696 container dapi-container: <nil>
STEP: delete the pod
Jan 31 11:02:19.237: INFO: Waiting for pod downward-api-a11714ab-9bc4-499f-9eb2-7e4206953696 to disappear
Jan 31 11:02:19.240: INFO: Pod downward-api-a11714ab-9bc4-499f-9eb2-7e4206953696 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 31 11:02:19.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9866" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":356,"completed":314,"skipped":5861,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:02:19.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6560
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-a208b5e5-f34d-491a-a480-bfe48407b413
STEP: Creating a pod to test consume secrets
Jan 31 11:02:19.387: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ad5e6a7b-20ce-43b7-9103-bb0b94d9a626" in namespace "projected-6560" to be "Succeeded or Failed"
Jan 31 11:02:19.392: INFO: Pod "pod-projected-secrets-ad5e6a7b-20ce-43b7-9103-bb0b94d9a626": Phase="Pending", Reason="", readiness=false. Elapsed: 4.972466ms
Jan 31 11:02:21.399: INFO: Pod "pod-projected-secrets-ad5e6a7b-20ce-43b7-9103-bb0b94d9a626": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011980291s
Jan 31 11:02:23.403: INFO: Pod "pod-projected-secrets-ad5e6a7b-20ce-43b7-9103-bb0b94d9a626": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016006877s
STEP: Saw pod success
Jan 31 11:02:23.404: INFO: Pod "pod-projected-secrets-ad5e6a7b-20ce-43b7-9103-bb0b94d9a626" satisfied condition "Succeeded or Failed"
Jan 31 11:02:23.406: INFO: Trying to get logs from node macpro-x86-1 pod pod-projected-secrets-ad5e6a7b-20ce-43b7-9103-bb0b94d9a626 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 31 11:02:23.424: INFO: Waiting for pod pod-projected-secrets-ad5e6a7b-20ce-43b7-9103-bb0b94d9a626 to disappear
Jan 31 11:02:23.426: INFO: Pod pod-projected-secrets-ad5e6a7b-20ce-43b7-9103-bb0b94d9a626 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 31 11:02:23.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6560" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":315,"skipped":5864,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:02:23.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7222
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 31 11:02:23.564: INFO: Waiting up to 5m0s for pod "pod-ff7b4079-15f9-44b4-ad86-586f33f0bde2" in namespace "emptydir-7222" to be "Succeeded or Failed"
Jan 31 11:02:23.569: INFO: Pod "pod-ff7b4079-15f9-44b4-ad86-586f33f0bde2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.679355ms
Jan 31 11:02:25.574: INFO: Pod "pod-ff7b4079-15f9-44b4-ad86-586f33f0bde2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010038351s
Jan 31 11:02:27.578: INFO: Pod "pod-ff7b4079-15f9-44b4-ad86-586f33f0bde2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014271492s
STEP: Saw pod success
Jan 31 11:02:27.578: INFO: Pod "pod-ff7b4079-15f9-44b4-ad86-586f33f0bde2" satisfied condition "Succeeded or Failed"
Jan 31 11:02:27.581: INFO: Trying to get logs from node macpro-x86-1 pod pod-ff7b4079-15f9-44b4-ad86-586f33f0bde2 container test-container: <nil>
STEP: delete the pod
Jan 31 11:02:27.594: INFO: Waiting for pod pod-ff7b4079-15f9-44b4-ad86-586f33f0bde2 to disappear
Jan 31 11:02:27.597: INFO: Pod pod-ff7b4079-15f9-44b4-ad86-586f33f0bde2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 31 11:02:27.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7222" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":316,"skipped":5865,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:02:27.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5526
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 31 11:02:55.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5526" for this suite.

• [SLOW TEST:28.181 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":356,"completed":317,"skipped":5865,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:02:55.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1748
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 31 11:02:55.914: INFO: Waiting up to 5m0s for pod "pod-af0daf38-bad9-4b28-baa9-a1341b7b95a5" in namespace "emptydir-1748" to be "Succeeded or Failed"
Jan 31 11:02:55.918: INFO: Pod "pod-af0daf38-bad9-4b28-baa9-a1341b7b95a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.525508ms
Jan 31 11:02:57.921: INFO: Pod "pod-af0daf38-bad9-4b28-baa9-a1341b7b95a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006740286s
Jan 31 11:02:59.927: INFO: Pod "pod-af0daf38-bad9-4b28-baa9-a1341b7b95a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012907933s
STEP: Saw pod success
Jan 31 11:02:59.927: INFO: Pod "pod-af0daf38-bad9-4b28-baa9-a1341b7b95a5" satisfied condition "Succeeded or Failed"
Jan 31 11:02:59.930: INFO: Trying to get logs from node macpro-x86-1 pod pod-af0daf38-bad9-4b28-baa9-a1341b7b95a5 container test-container: <nil>
STEP: delete the pod
Jan 31 11:02:59.943: INFO: Waiting for pod pod-af0daf38-bad9-4b28-baa9-a1341b7b95a5 to disappear
Jan 31 11:02:59.946: INFO: Pod pod-af0daf38-bad9-4b28-baa9-a1341b7b95a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 31 11:02:59.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1748" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":318,"skipped":5873,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:02:59.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7214
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jan 31 11:03:00.085: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 31 11:03:02.089: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 31 11:03:03.105: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 31 11:03:04.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7214" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":356,"completed":319,"skipped":5892,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:03:04.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7508
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test env composition
Jan 31 11:03:04.275: INFO: Waiting up to 5m0s for pod "var-expansion-393ca1e3-ed00-4020-bb9f-21a261d70610" in namespace "var-expansion-7508" to be "Succeeded or Failed"
Jan 31 11:03:04.278: INFO: Pod "var-expansion-393ca1e3-ed00-4020-bb9f-21a261d70610": Phase="Pending", Reason="", readiness=false. Elapsed: 2.995906ms
Jan 31 11:03:06.282: INFO: Pod "var-expansion-393ca1e3-ed00-4020-bb9f-21a261d70610": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006405745s
Jan 31 11:03:08.285: INFO: Pod "var-expansion-393ca1e3-ed00-4020-bb9f-21a261d70610": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009930719s
STEP: Saw pod success
Jan 31 11:03:08.285: INFO: Pod "var-expansion-393ca1e3-ed00-4020-bb9f-21a261d70610" satisfied condition "Succeeded or Failed"
Jan 31 11:03:08.287: INFO: Trying to get logs from node macpro-x86-2 pod var-expansion-393ca1e3-ed00-4020-bb9f-21a261d70610 container dapi-container: <nil>
STEP: delete the pod
Jan 31 11:03:08.299: INFO: Waiting for pod var-expansion-393ca1e3-ed00-4020-bb9f-21a261d70610 to disappear
Jan 31 11:03:08.302: INFO: Pod var-expansion-393ca1e3-ed00-4020-bb9f-21a261d70610 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 31 11:03:08.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7508" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":356,"completed":320,"skipped":5893,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:03:08.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1724
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 31 11:03:08.448: INFO: Waiting up to 5m0s for pod "pod-04e0ebb0-553f-42cf-bea7-9cb4f8959584" in namespace "emptydir-1724" to be "Succeeded or Failed"
Jan 31 11:03:08.452: INFO: Pod "pod-04e0ebb0-553f-42cf-bea7-9cb4f8959584": Phase="Pending", Reason="", readiness=false. Elapsed: 3.567957ms
Jan 31 11:03:10.456: INFO: Pod "pod-04e0ebb0-553f-42cf-bea7-9cb4f8959584": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00777713s
Jan 31 11:03:12.460: INFO: Pod "pod-04e0ebb0-553f-42cf-bea7-9cb4f8959584": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012031943s
STEP: Saw pod success
Jan 31 11:03:12.460: INFO: Pod "pod-04e0ebb0-553f-42cf-bea7-9cb4f8959584" satisfied condition "Succeeded or Failed"
Jan 31 11:03:12.462: INFO: Trying to get logs from node macpro-x86-2 pod pod-04e0ebb0-553f-42cf-bea7-9cb4f8959584 container test-container: <nil>
STEP: delete the pod
Jan 31 11:03:12.474: INFO: Waiting for pod pod-04e0ebb0-553f-42cf-bea7-9cb4f8959584 to disappear
Jan 31 11:03:12.476: INFO: Pod pod-04e0ebb0-553f-42cf-bea7-9cb4f8959584 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 31 11:03:12.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1724" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":321,"skipped":5978,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:03:12.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4396
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Jan 31 11:03:12.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 11:03:34.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4396" for this suite.

• [SLOW TEST:21.974 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":356,"completed":322,"skipped":5990,"failed":0}
SSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:03:34.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9507
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 11:03:34.595: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-43bd8a7d-bcb2-4dad-8d34-5c9e66e2a936" in namespace "security-context-test-9507" to be "Succeeded or Failed"
Jan 31 11:03:34.604: INFO: Pod "busybox-readonly-false-43bd8a7d-bcb2-4dad-8d34-5c9e66e2a936": Phase="Pending", Reason="", readiness=false. Elapsed: 8.97554ms
Jan 31 11:03:36.618: INFO: Pod "busybox-readonly-false-43bd8a7d-bcb2-4dad-8d34-5c9e66e2a936": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023372589s
Jan 31 11:03:38.625: INFO: Pod "busybox-readonly-false-43bd8a7d-bcb2-4dad-8d34-5c9e66e2a936": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030611573s
Jan 31 11:03:38.625: INFO: Pod "busybox-readonly-false-43bd8a7d-bcb2-4dad-8d34-5c9e66e2a936" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 31 11:03:38.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9507" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":356,"completed":323,"skipped":5993,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:03:38.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7782
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Jan 31 11:03:38.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 11:04:03.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7782" for this suite.

• [SLOW TEST:24.966 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":356,"completed":324,"skipped":5995,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:04:03.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4703
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jan 31 11:04:05.758: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4703 PodName:pod-sharedvolume-0e868152-9505-4124-8fd6-da92caa1230e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 11:04:05.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 11:04:05.758: INFO: ExecWithOptions: Clientset creation
Jan 31 11:04:05.758: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-4703/pods/pod-sharedvolume-0e868152-9505-4124-8fd6-da92caa1230e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 31 11:04:05.822: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 31 11:04:05.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4703" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":356,"completed":325,"skipped":5998,"failed":0}
S
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:04:05.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9500
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-454f2b56-9131-49df-be83-568eac73b654 in namespace container-probe-9500
Jan 31 11:04:07.998: INFO: Started pod liveness-454f2b56-9131-49df-be83-568eac73b654 in namespace container-probe-9500
STEP: checking the pod's current state and verifying that restartCount is present
Jan 31 11:04:08.001: INFO: Initial restart count of pod liveness-454f2b56-9131-49df-be83-568eac73b654 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 31 11:08:08.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9500" for this suite.

• [SLOW TEST:243.180 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":356,"completed":326,"skipped":5999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:08:09.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-5614
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 31 11:13:09.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5614" for this suite.

• [SLOW TEST:300.168 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":356,"completed":327,"skipped":6025,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:13:09.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6426
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 31 11:13:22.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6426" for this suite.

• [SLOW TEST:13.237 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":356,"completed":328,"skipped":6030,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:13:22.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8489
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 11:13:22.547: INFO: Creating deployment "webserver-deployment"
Jan 31 11:13:22.552: INFO: Waiting for observed generation 1
Jan 31 11:13:24.563: INFO: Waiting for all required pods to come up
Jan 31 11:13:24.572: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 31 11:13:26.586: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 31 11:13:26.590: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 31 11:13:26.598: INFO: Updating deployment webserver-deployment
Jan 31 11:13:26.598: INFO: Waiting for observed generation 2
Jan 31 11:13:28.611: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 31 11:13:28.613: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 31 11:13:28.616: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 31 11:13:28.622: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 31 11:13:28.622: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 31 11:13:28.624: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 31 11:13:28.627: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 31 11:13:28.627: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 31 11:13:28.634: INFO: Updating deployment webserver-deployment
Jan 31 11:13:28.634: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 31 11:13:28.638: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 31 11:13:28.640: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 31 11:13:30.654: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8489  4d50124d-ca6c-4b52-a7a1-0214a8255e8a 41349 3 2023-01-31 11:13:22 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-01-31 11:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 11:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004225bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-31 11:13:28 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-57ccb67bb8" is progressing.,LastUpdateTime:2023-01-31 11:13:28 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 31 11:13:30.657: INFO: New ReplicaSet "webserver-deployment-57ccb67bb8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-57ccb67bb8  deployment-8489  722d15c0-31b5-4d2b-bd9f-eded81278565 41345 3 2023-01-31 11:13:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 4d50124d-ca6c-4b52-a7a1-0214a8255e8a 0xc00459c107 0xc00459c108}] []  [{kube-controller-manager Update apps/v1 2023-01-31 11:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d50124d-ca6c-4b52-a7a1-0214a8255e8a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 11:13:26 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 57ccb67bb8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00459c1a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 31 11:13:30.657: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 31 11:13:30.657: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-55df494869  deployment-8489  8aa400cf-5d84-40b9-b016-d5bd8dadff8c 41346 3 2023-01-31 11:13:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 4d50124d-ca6c-4b52-a7a1-0214a8255e8a 0xc00459c017 0xc00459c018}] []  [{kube-controller-manager Update apps/v1 2023-01-31 11:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d50124d-ca6c-4b52-a7a1-0214a8255e8a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 11:13:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00459c0a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 31 11:13:30.662: INFO: Pod "webserver-deployment-55df494869-27fvt" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-27fvt webserver-deployment-55df494869- deployment-8489  1ae764aa-7973-4bac-a7bf-aa22efd91fe1 41399 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:f08ed7bb400dc695ca16a28747a04935cd9f739f61550f733e41023db973b544 cni.projectcalico.org/podIP:192.168.75.44/32 cni.projectcalico.org/podIPs:192.168.75.44/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004d72607 0xc004d72608}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v9x6f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9x6f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.662: INFO: Pod "webserver-deployment-55df494869-2gpv4" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-2gpv4 webserver-deployment-55df494869- deployment-8489  59d3bec4-3987-4a26-8f43-18bc693872d2 41153 0 2023-01-31 11:13:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:cafd7005d13238040ccf8c21c517241c9abe53694b404986b28fdc33b7dbe802 cni.projectcalico.org/podIP:192.168.38.239/32 cni.projectcalico.org/podIPs:192.168.38.239/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004d727c0 0xc004d727c1}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:13:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.38.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zs6kz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zs6kz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.32,PodIP:192.168.38.239,StartTime:2023-01-31 11:13:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 11:13:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f428c8b15cc10b3ce5d81ea4113f64e19b16eea265ffe3ef7933042ad6403c8d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.38.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.663: INFO: Pod "webserver-deployment-55df494869-4gpj7" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-4gpj7 webserver-deployment-55df494869- deployment-8489  9848b804-fb84-40b5-8dcc-d24611900148 41405 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:8764a724f8e472653e0984bd6e443bea3808c21f562a5d4a67be2e1858cba30e cni.projectcalico.org/podIP:192.168.38.219/32 cni.projectcalico.org/podIPs:192.168.38.219/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004d729c7 0xc004d729c8}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vmvn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vmvn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.663: INFO: Pod "webserver-deployment-55df494869-5nbf5" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-5nbf5 webserver-deployment-55df494869- deployment-8489  8ce84c78-23bc-4f6f-aed3-6af013e5e044 41156 0 2023-01-31 11:13:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:e7db1cb07560c4825f1b5da590b77070b108cd53fddab5f0f4a63cba4bde8dbf cni.projectcalico.org/podIP:192.168.75.15/32 cni.projectcalico.org/podIPs:192.168.75.15/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004d72b70 0xc004d72b71}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:13:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.75.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q56wd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q56wd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.31,PodIP:192.168.75.15,StartTime:2023-01-31 11:13:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 11:13:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://42bfd5c663ee44800d72d05a1d9ad4e21497b3731e182b91e66c9477ac6b5e7d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.75.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.663: INFO: Pod "webserver-deployment-55df494869-6ln4l" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-6ln4l webserver-deployment-55df494869- deployment-8489  53ad92a5-9954-42aa-ad85-422a82dd8639 41341 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004d72d77 0xc004d72d78}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttx26,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttx26,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.663: INFO: Pod "webserver-deployment-55df494869-7sgkk" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-7sgkk webserver-deployment-55df494869- deployment-8489  e0568339-a0d5-425e-a64d-0369c0b5af30 41422 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:0fac812cc3cd9821f8408200d3292bdf2978e3b3924cf7347301a5d1eb95788c cni.projectcalico.org/podIP:192.168.75.25/32 cni.projectcalico.org/podIPs:192.168.75.25/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004d72f00 0xc004d72f01}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ngkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ngkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.663: INFO: Pod "webserver-deployment-55df494869-7wnt7" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-7wnt7 webserver-deployment-55df494869- deployment-8489  4634f936-e836-43c8-81f2-d720842cc086 41160 0 2023-01-31 11:13:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:998b31adbe3e87b6b9ae30de87e6002d4d3d52a3789d5ba6052205eeb3d9682a cni.projectcalico.org/podIP:192.168.38.212/32 cni.projectcalico.org/podIPs:192.168.38.212/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004d73080 0xc004d73081}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:13:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.38.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qft5w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qft5w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.32,PodIP:192.168.38.212,StartTime:2023-01-31 11:13:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 11:13:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d96abe0ffe78012402c0ea58d2a9b5351f4d384db0f27a86e74c2c800d83db0b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.38.212,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.663: INFO: Pod "webserver-deployment-55df494869-ct2dt" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-ct2dt webserver-deployment-55df494869- deployment-8489  74de42b7-60cb-47fc-a4ea-2398d0f43859 41441 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:e37178e32a8c1a83880dbae9b2372242f211dd78cb7a08c67ee848915d0ba0eb cni.projectcalico.org/podIP:192.168.75.55/32 cni.projectcalico.org/podIPs:192.168.75.55/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004d732a7 0xc004d732a8}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jks68,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jks68,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.663: INFO: Pod "webserver-deployment-55df494869-fxlqd" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-fxlqd webserver-deployment-55df494869- deployment-8489  2cb2cad9-787a-44bb-95f1-7d72837e17e9 41326 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004d73430 0xc004d73431}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vj977,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vj977,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.663: INFO: Pod "webserver-deployment-55df494869-h9fcd" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-h9fcd webserver-deployment-55df494869- deployment-8489  babcdae4-3506-46c7-83fa-0d1cca754617 41342 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004d73590 0xc004d73591}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x2257,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x2257,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.663: INFO: Pod "webserver-deployment-55df494869-kbj8t" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-kbj8t webserver-deployment-55df494869- deployment-8489  1706fbe4-f538-4ea3-b1f3-4afc0d991610 41167 0 2023-01-31 11:13:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:d0153bc50adcc62948ab0422c51a452561269cad7b2cbb184dbf47c0d9c0d2c5 cni.projectcalico.org/podIP:192.168.38.230/32 cni.projectcalico.org/podIPs:192.168.38.230/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004d736f0 0xc004d736f1}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:13:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.38.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7c4x5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7c4x5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.32,PodIP:192.168.38.230,StartTime:2023-01-31 11:13:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 11:13:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8530679a8fa646e018dd076afecb546d89e912d65288405ee4f7a8c9d169ab1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.38.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.663: INFO: Pod "webserver-deployment-55df494869-prfp6" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-prfp6 webserver-deployment-55df494869- deployment-8489  bb3690e1-ad53-47b5-ade0-e6b7b1ba2ac7 41330 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004d738f7 0xc004d738f8}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ns9zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ns9zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.664: INFO: Pod "webserver-deployment-55df494869-rmt7z" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-rmt7z webserver-deployment-55df494869- deployment-8489  e4c71856-c222-4c38-82b1-662b45d5e6fa 41147 0 2023-01-31 11:13:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:d1037620a556b64110ce99030d36b461d2898ca6adaa2e1c81125601fbc99238 cni.projectcalico.org/podIP:192.168.38.222/32 cni.projectcalico.org/podIPs:192.168.38.222/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004d73a60 0xc004d73a61}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:13:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.38.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jtt8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jtt8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.32,PodIP:192.168.38.222,StartTime:2023-01-31 11:13:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 11:13:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7928d82b4a3cd2bc10c2397fabffc52c01234036471d4198aa75f71f5049c413,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.38.222,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.664: INFO: Pod "webserver-deployment-55df494869-rzhdz" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-rzhdz webserver-deployment-55df494869- deployment-8489  2925d42b-1b52-461b-9050-c5b24482b617 41409 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:49d78c61d041b70e1ecba91dbee261729bd5c8aafbeece1ebceaeced4280fb50 cni.projectcalico.org/podIP:192.168.38.207/32 cni.projectcalico.org/podIPs:192.168.38.207/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004d73c67 0xc004d73c68}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vmhmj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vmhmj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.664: INFO: Pod "webserver-deployment-55df494869-t7rx6" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-t7rx6 webserver-deployment-55df494869- deployment-8489  2ee5a247-e3d7-4535-962e-59597903f5a7 41163 0 2023-01-31 11:13:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:85c73c4f7bbd2ff16047182ca6b9cb594e3dcc94e889f3a0ef79f4689642c1bc cni.projectcalico.org/podIP:192.168.75.45/32 cni.projectcalico.org/podIPs:192.168.75.45/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004d73e10 0xc004d73e11}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:13:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.75.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zxfdh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zxfdh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.31,PodIP:192.168.75.45,StartTime:2023-01-31 11:13:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 11:13:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4565956b952fd8d278950f092c5138083ce0ac879b1bb51fb960d0d9a8fbfa3b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.75.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.664: INFO: Pod "webserver-deployment-55df494869-vlj5h" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-vlj5h webserver-deployment-55df494869- deployment-8489  18c0b385-6819-40c1-9703-09d5b17b6ddb 41318 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004910017 0xc004910018}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-npnbw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-npnbw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.32,PodIP:,StartTime:2023-01-31 11:13:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.664: INFO: Pod "webserver-deployment-55df494869-vmtbs" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-vmtbs webserver-deployment-55df494869- deployment-8489  5e89c0f2-b1c6-4d3a-a6ec-2dd60954968b 41386 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:0f9261c1ff35a347594f3663488d266f72249d1c1e8ab843bcfb5348c5501256 cni.projectcalico.org/podIP:192.168.75.27/32 cni.projectcalico.org/podIPs:192.168.75.27/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc0049102c7 0xc0049102c8}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h6rsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h6rsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.664: INFO: Pod "webserver-deployment-55df494869-vsqdx" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-vsqdx webserver-deployment-55df494869- deployment-8489  a9976939-da59-4aaa-9240-e02e4b8be772 41135 0 2023-01-31 11:13:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:f8335f52cb6b691cf4c538fe00c6dd3f4a0c270194831a5f0e805d080566f169 cni.projectcalico.org/podIP:192.168.38.200/32 cni.projectcalico.org/podIPs:192.168.38.200/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc0049104d0 0xc0049104d1}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:13:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.38.200\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5n95g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5n95g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.32,PodIP:192.168.38.200,StartTime:2023-01-31 11:13:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 11:13:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6643f4b4eac1a2403fcf338958a4921b8590e1d45a55af154febc670efebf814,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.38.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.664: INFO: Pod "webserver-deployment-55df494869-wjvzj" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-wjvzj webserver-deployment-55df494869- deployment-8489  df1cadb5-8491-491a-a1e2-879dc4857626 41339 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc0049108d7 0xc0049108d8}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g9gj8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g9gj8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.664: INFO: Pod "webserver-deployment-55df494869-xf8vt" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-xf8vt webserver-deployment-55df494869- deployment-8489  a1fe55ae-4a24-439b-8cd0-23dfdff54ffe 41170 0 2023-01-31 11:13:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:41c2999f6bf25ca8ad7f2c9d3465f9ed4783e17540805567463f58ec143e37cb cni.projectcalico.org/podIP:192.168.75.34/32 cni.projectcalico.org/podIPs:192.168.75.34/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 8aa400cf-5d84-40b9-b016-d5bd8dadff8c 0xc004910c30 0xc004910c31}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aa400cf-5d84-40b9-b016-d5bd8dadff8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:13:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.75.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g8wpp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g8wpp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.31,PodIP:192.168.75.34,StartTime:2023-01-31 11:13:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 11:13:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5d219703643fbe9657e0a5fc2f7da0f840f0a748a657186cc5a93889f22f9a5d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.75.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.664: INFO: Pod "webserver-deployment-57ccb67bb8-2vsj4" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-2vsj4 webserver-deployment-57ccb67bb8- deployment-8489  34868c94-a320-4b36-9ef9-5bc3a2a5fdea 41243 0 2023-01-31 11:13:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:f84da58ab2a734c35f29a627231f8669d96cee11cb83c47c1a3b0c3362341ce7 cni.projectcalico.org/podIP:192.168.75.3/32 cni.projectcalico.org/podIPs:192.168.75.3/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 722d15c0-31b5-4d2b-bd9f-eded81278565 0xc004910e57 0xc004910e58}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"722d15c0-31b5-4d2b-bd9f-eded81278565\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 11:13:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-31 11:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vm584,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vm584,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.31,PodIP:,StartTime:2023-01-31 11:13:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.665: INFO: Pod "webserver-deployment-57ccb67bb8-4hpkq" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-4hpkq webserver-deployment-57ccb67bb8- deployment-8489  bd34c784-810d-409c-a38a-5d7946d80ada 41338 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 722d15c0-31b5-4d2b-bd9f-eded81278565 0xc004911067 0xc004911068}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"722d15c0-31b5-4d2b-bd9f-eded81278565\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m9fr6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m9fr6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.665: INFO: Pod "webserver-deployment-57ccb67bb8-4z798" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-4z798 webserver-deployment-57ccb67bb8- deployment-8489  3880a492-9ee1-4122-8912-ced1df1465a0 41260 0 2023-01-31 11:13:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:9574cbc7383002aedb5836aa5796d1a3d2526bb4957719af1f2cfc5deaa89c00 cni.projectcalico.org/podIP:192.168.75.63/32 cni.projectcalico.org/podIPs:192.168.75.63/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 722d15c0-31b5-4d2b-bd9f-eded81278565 0xc004911200 0xc004911201}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"722d15c0-31b5-4d2b-bd9f-eded81278565\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 11:13:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-31 11:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ghrhc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ghrhc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.31,PodIP:,StartTime:2023-01-31 11:13:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.665: INFO: Pod "webserver-deployment-57ccb67bb8-5khn9" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-5khn9 webserver-deployment-57ccb67bb8- deployment-8489  d15aa3bb-c892-48a6-8973-7e8df230ad48 41272 0 2023-01-31 11:13:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:8a92da6e319470a54f3af274f1bee69a760219b8e3f3eb7770e709d74d2a1114 cni.projectcalico.org/podIP:192.168.38.202/32 cni.projectcalico.org/podIPs:192.168.38.202/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 722d15c0-31b5-4d2b-bd9f-eded81278565 0xc004911407 0xc004911408}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"722d15c0-31b5-4d2b-bd9f-eded81278565\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.38.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p6rm9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p6rm9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.32,PodIP:192.168.38.202,StartTime:2023-01-31 11:13:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.38.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.665: INFO: Pod "webserver-deployment-57ccb67bb8-cl2nh" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-cl2nh webserver-deployment-57ccb67bb8- deployment-8489  4b03b34c-1b9c-472d-bb41-496c49cf77ac 41421 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:b912410080ebfadcbc59f85f66c044e5741295df080ffbbc28849d14483ef056 cni.projectcalico.org/podIP:192.168.38.201/32 cni.projectcalico.org/podIPs:192.168.38.201/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 722d15c0-31b5-4d2b-bd9f-eded81278565 0xc004911647 0xc004911648}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"722d15c0-31b5-4d2b-bd9f-eded81278565\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dtvd8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dtvd8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.665: INFO: Pod "webserver-deployment-57ccb67bb8-dw2dd" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-dw2dd webserver-deployment-57ccb67bb8- deployment-8489  deb5727e-0dec-4c42-906b-d98f6c72b10b 41275 0 2023-01-31 11:13:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:75b6175126198d53d0f939385a4b6b74ce2e5fad783fa6187bf68fd331c89f41 cni.projectcalico.org/podIP:192.168.38.223/32 cni.projectcalico.org/podIPs:192.168.38.223/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 722d15c0-31b5-4d2b-bd9f-eded81278565 0xc0049117e0 0xc0049117e1}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"722d15c0-31b5-4d2b-bd9f-eded81278565\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.38.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x95zl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x95zl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.32,PodIP:192.168.38.223,StartTime:2023-01-31 11:13:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.38.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.665: INFO: Pod "webserver-deployment-57ccb67bb8-ghxg2" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-ghxg2 webserver-deployment-57ccb67bb8- deployment-8489  27203c51-b50e-4c2d-910f-fd43d0a122b6 41403 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:31c8939fc8dc8e91002f26f33652d354a0f91ca1b150b094b552ef3432fb6a7c cni.projectcalico.org/podIP:192.168.75.24/32 cni.projectcalico.org/podIPs:192.168.75.24/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 722d15c0-31b5-4d2b-bd9f-eded81278565 0xc004911a37 0xc004911a38}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"722d15c0-31b5-4d2b-bd9f-eded81278565\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wxb5t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wxb5t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.665: INFO: Pod "webserver-deployment-57ccb67bb8-lcnn7" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-lcnn7 webserver-deployment-57ccb67bb8- deployment-8489  6a4fd8b0-7344-4e15-882e-3378a0071567 41268 0 2023-01-31 11:13:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:7772cb0d924f4e0da42a375fc9f2691beab99ddc7c83de6ab9ed528f1ce8f7cc cni.projectcalico.org/podIP:192.168.75.48/32 cni.projectcalico.org/podIPs:192.168.75.48/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 722d15c0-31b5-4d2b-bd9f-eded81278565 0xc004911bf0 0xc004911bf1}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"722d15c0-31b5-4d2b-bd9f-eded81278565\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-95gnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-95gnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.31,PodIP:,StartTime:2023-01-31 11:13:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.665: INFO: Pod "webserver-deployment-57ccb67bb8-mvqr9" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-mvqr9 webserver-deployment-57ccb67bb8- deployment-8489  e48560de-500b-4217-86f6-2cb71decf972 41436 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:c24145685120f1757441407e194dd354969eb3702e28a64da7feb562b4570df3 cni.projectcalico.org/podIP:192.168.38.232/32 cni.projectcalico.org/podIPs:192.168.38.232/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 722d15c0-31b5-4d2b-bd9f-eded81278565 0xc004911df7 0xc004911df8}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"722d15c0-31b5-4d2b-bd9f-eded81278565\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z4lmn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z4lmn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.665: INFO: Pod "webserver-deployment-57ccb67bb8-njh6m" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-njh6m webserver-deployment-57ccb67bb8- deployment-8489  5cef21ce-3287-47cb-a071-a16219f19ca0 41437 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:f2874c3c1d6f7e3c1fbf74b2ff0f5d5e0a6a3fc2de07d899cc26a436873d2626 cni.projectcalico.org/podIP:192.168.75.16/32 cni.projectcalico.org/podIPs:192.168.75.16/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 722d15c0-31b5-4d2b-bd9f-eded81278565 0xc004911fb0 0xc004911fb1}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"722d15c0-31b5-4d2b-bd9f-eded81278565\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 11:13:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mt59b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mt59b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.666: INFO: Pod "webserver-deployment-57ccb67bb8-r5tcd" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-r5tcd webserver-deployment-57ccb67bb8- deployment-8489  68e0746e-794e-4746-b6c4-d742fd76b316 41383 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:14d521ff980c8623bbd9228c4b529f953de1917a27abe8d85173412223b5773f cni.projectcalico.org/podIP:192.168.38.217/32 cni.projectcalico.org/podIPs:192.168.38.217/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 722d15c0-31b5-4d2b-bd9f-eded81278565 0xc004836140 0xc004836141}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"722d15c0-31b5-4d2b-bd9f-eded81278565\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-31 11:13:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8df65,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8df65,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.32,PodIP:,StartTime:2023-01-31 11:13:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.666: INFO: Pod "webserver-deployment-57ccb67bb8-ws49g" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-ws49g webserver-deployment-57ccb67bb8- deployment-8489  c73c5512-e47d-472d-a8e0-55f91f67b8ce 41340 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 722d15c0-31b5-4d2b-bd9f-eded81278565 0xc004836347 0xc004836348}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"722d15c0-31b5-4d2b-bd9f-eded81278565\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bmt85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bmt85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 11:13:30.666: INFO: Pod "webserver-deployment-57ccb67bb8-xqntr" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-xqntr webserver-deployment-57ccb67bb8- deployment-8489  38dcaba0-b280-40dc-b566-c40b348b3206 41430 0 2023-01-31 11:13:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:f70bfabe7aaa588bbc14cca9d60cd995d79c8c1624cf32f60bad093a6b89280f cni.projectcalico.org/podIP:192.168.38.233/32 cni.projectcalico.org/podIPs:192.168.38.233/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 722d15c0-31b5-4d2b-bd9f-eded81278565 0xc0048364f0 0xc0048364f1}] []  [{kube-controller-manager Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"722d15c0-31b5-4d2b-bd9f-eded81278565\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 11:13:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-31 11:13:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqbmj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqbmj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:13:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.32,PodIP:,StartTime:2023-01-31 11:13:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 31 11:13:30.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8489" for this suite.

• [SLOW TEST:8.258 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":356,"completed":329,"skipped":6031,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:13:30.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5240
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jan 31 11:13:30.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 11:13:35.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 11:13:54.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5240" for this suite.

• [SLOW TEST:24.102 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":356,"completed":330,"skipped":6053,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:13:54.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1536
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service endpoint-test2 in namespace services-1536
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1536 to expose endpoints map[]
Jan 31 11:13:54.929: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 31 11:13:55.937: INFO: successfully validated that service endpoint-test2 in namespace services-1536 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1536
Jan 31 11:13:55.953: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 11:13:57.961: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1536 to expose endpoints map[pod1:[80]]
Jan 31 11:13:57.971: INFO: successfully validated that service endpoint-test2 in namespace services-1536 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Jan 31 11:13:57.971: INFO: Creating new exec pod
Jan 31 11:14:00.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-1536 exec execpodbsqdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 31 11:14:01.102: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 31 11:14:01.102: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 11:14:01.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-1536 exec execpodbsqdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.26.40 80'
Jan 31 11:14:01.186: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.26.40 80\nConnection to 10.96.26.40 80 port [tcp/http] succeeded!\n"
Jan 31 11:14:01.186: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-1536
Jan 31 11:14:01.205: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 11:14:03.215: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1536 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 31 11:14:03.226: INFO: successfully validated that service endpoint-test2 in namespace services-1536 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Jan 31 11:14:04.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-1536 exec execpodbsqdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 31 11:14:04.337: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 31 11:14:04.337: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 11:14:04.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-1536 exec execpodbsqdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.26.40 80'
Jan 31 11:14:04.425: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.26.40 80\nConnection to 10.96.26.40 80 port [tcp/http] succeeded!\n"
Jan 31 11:14:04.425: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-1536
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1536 to expose endpoints map[pod2:[80]]
Jan 31 11:14:04.462: INFO: successfully validated that service endpoint-test2 in namespace services-1536 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Jan 31 11:14:05.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-1536 exec execpodbsqdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 31 11:14:05.570: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 31 11:14:05.570: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 31 11:14:05.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-1536 exec execpodbsqdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.26.40 80'
Jan 31 11:14:05.654: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.26.40 80\nConnection to 10.96.26.40 80 port [tcp/http] succeeded!\n"
Jan 31 11:14:05.654: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-1536
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1536 to expose endpoints map[]
Jan 31 11:14:06.687: INFO: successfully validated that service endpoint-test2 in namespace services-1536 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 11:14:06.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1536" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:11.953 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":356,"completed":331,"skipped":6078,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:14:06.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5002
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 31 11:14:06.866: INFO: Waiting up to 5m0s for pod "pod-baed9978-5853-4495-bc9e-75ff9a4c799a" in namespace "emptydir-5002" to be "Succeeded or Failed"
Jan 31 11:14:06.870: INFO: Pod "pod-baed9978-5853-4495-bc9e-75ff9a4c799a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.466802ms
Jan 31 11:14:08.880: INFO: Pod "pod-baed9978-5853-4495-bc9e-75ff9a4c799a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014247859s
Jan 31 11:14:10.884: INFO: Pod "pod-baed9978-5853-4495-bc9e-75ff9a4c799a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018684722s
STEP: Saw pod success
Jan 31 11:14:10.884: INFO: Pod "pod-baed9978-5853-4495-bc9e-75ff9a4c799a" satisfied condition "Succeeded or Failed"
Jan 31 11:14:10.887: INFO: Trying to get logs from node macpro-x86-1 pod pod-baed9978-5853-4495-bc9e-75ff9a4c799a container test-container: <nil>
STEP: delete the pod
Jan 31 11:14:10.916: INFO: Waiting for pod pod-baed9978-5853-4495-bc9e-75ff9a4c799a to disappear
Jan 31 11:14:10.919: INFO: Pod pod-baed9978-5853-4495-bc9e-75ff9a4c799a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 31 11:14:10.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5002" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":332,"skipped":6149,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:14:10.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9541
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 11:14:11.061: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e8874f54-3761-41cb-b91f-e5ca07eec4cb" in namespace "downward-api-9541" to be "Succeeded or Failed"
Jan 31 11:14:11.064: INFO: Pod "downwardapi-volume-e8874f54-3761-41cb-b91f-e5ca07eec4cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.872811ms
Jan 31 11:14:13.071: INFO: Pod "downwardapi-volume-e8874f54-3761-41cb-b91f-e5ca07eec4cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010180093s
Jan 31 11:14:15.079: INFO: Pod "downwardapi-volume-e8874f54-3761-41cb-b91f-e5ca07eec4cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018666984s
STEP: Saw pod success
Jan 31 11:14:15.079: INFO: Pod "downwardapi-volume-e8874f54-3761-41cb-b91f-e5ca07eec4cb" satisfied condition "Succeeded or Failed"
Jan 31 11:14:15.086: INFO: Trying to get logs from node macpro-x86-1 pod downwardapi-volume-e8874f54-3761-41cb-b91f-e5ca07eec4cb container client-container: <nil>
STEP: delete the pod
Jan 31 11:14:15.113: INFO: Waiting for pod downwardapi-volume-e8874f54-3761-41cb-b91f-e5ca07eec4cb to disappear
Jan 31 11:14:15.116: INFO: Pod downwardapi-volume-e8874f54-3761-41cb-b91f-e5ca07eec4cb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 31 11:14:15.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9541" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":333,"skipped":6150,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:14:15.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9229
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-107
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5588
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jan 31 11:14:21.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9229" for this suite.
STEP: Destroying namespace "nsdeletetest-107" for this suite.
Jan 31 11:14:21.559: INFO: Namespace nsdeletetest-107 was already deleted
STEP: Destroying namespace "nsdeletetest-5588" for this suite.

• [SLOW TEST:6.433 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":356,"completed":334,"skipped":6154,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:14:21.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-603
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-54063914-5d2b-4c91-96e1-9958a30df8b4
STEP: Creating configMap with name cm-test-opt-upd-8bf983fa-ca2e-4188-90c6-a72ea7a9d25b
STEP: Creating the pod
Jan 31 11:14:21.716: INFO: The status of Pod pod-configmaps-85484b7e-62ed-4585-8f20-1ffc43df7362 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 11:14:23.725: INFO: The status of Pod pod-configmaps-85484b7e-62ed-4585-8f20-1ffc43df7362 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-54063914-5d2b-4c91-96e1-9958a30df8b4
STEP: Updating configmap cm-test-opt-upd-8bf983fa-ca2e-4188-90c6-a72ea7a9d25b
STEP: Creating configMap with name cm-test-opt-create-c91262c6-c4e9-494e-9d0a-baf74f1745a8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 31 11:15:40.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-603" for this suite.

• [SLOW TEST:78.495 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":335,"skipped":6159,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:15:40.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9250
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 1 pods
STEP: Gathering metrics
Jan 31 11:15:40.775: INFO: The status of Pod kube-controller-manager-master-3 is Running (Ready = true)
Jan 31 11:15:40.819: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 31 11:15:40.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9250" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":356,"completed":336,"skipped":6174,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:15:40.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7470
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 31 11:15:43.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7470" for this suite.
•{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":356,"completed":337,"skipped":6195,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:15:43.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7071
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 11:15:43.155: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 31 11:15:43.162: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 31 11:15:48.172: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 31 11:15:48.172: INFO: Creating deployment "test-rolling-update-deployment"
Jan 31 11:15:48.181: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 31 11:15:48.188: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 31 11:15:50.198: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 31 11:15:50.201: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 31 11:15:50.209: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7071  0ad74fb2-89d1-4997-8767-9191bcd94c7b 42513 1 2023-01-31 11:15:48 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2023-01-31 11:15:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 11:15:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008417c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-31 11:15:48 +0000 UTC,LastTransitionTime:2023-01-31 11:15:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67c8f74c6c" has successfully progressed.,LastUpdateTime:2023-01-31 11:15:49 +0000 UTC,LastTransitionTime:2023-01-31 11:15:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 31 11:15:50.211: INFO: New ReplicaSet "test-rolling-update-deployment-67c8f74c6c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c  deployment-7071  25473944-6119-49a6-9118-bd2b476b1d3c 42504 1 2023-01-31 11:15:48 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 0ad74fb2-89d1-4997-8767-9191bcd94c7b 0xc0029b9d87 0xc0029b9d88}] []  [{kube-controller-manager Update apps/v1 2023-01-31 11:15:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ad74fb2-89d1-4997-8767-9191bcd94c7b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 11:15:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67c8f74c6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0029b9e38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 31 11:15:50.211: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 31 11:15:50.211: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7071  cff75952-7943-4749-8223-74e53f596f43 42512 2 2023-01-31 11:15:43 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 0ad74fb2-89d1-4997-8767-9191bcd94c7b 0xc0029b9c57 0xc0029b9c58}] []  [{e2e.test Update apps/v1 2023-01-31 11:15:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 11:15:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ad74fb2-89d1-4997-8767-9191bcd94c7b\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-31 11:15:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0029b9d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 31 11:15:50.213: INFO: Pod "test-rolling-update-deployment-67c8f74c6c-f6xt4" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c-f6xt4 test-rolling-update-deployment-67c8f74c6c- deployment-7071  095c042b-2471-4142-931b-ddb8d64eb976 42503 0 2023-01-31 11:15:48 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[cni.projectcalico.org/containerID:015faafaf0c233f59d4d0b56a3cda6ed986d44522d0145531b5e38e18e67db46 cni.projectcalico.org/podIP:192.168.75.62/32 cni.projectcalico.org/podIPs:192.168.75.62/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-67c8f74c6c 25473944-6119-49a6-9118-bd2b476b1d3c 0xc0027361b7 0xc0027361b8}] []  [{calico Update v1 2023-01-31 11:15:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-31 11:15:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25473944-6119-49a6-9118-bd2b476b1d3c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 11:15:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.75.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-29zvs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-29zvs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-x86-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:15:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:15:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:15:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 11:15:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.31,PodIP:192.168.75.62,StartTime:2023-01-31 11:15:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 11:15:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://88342668cd1e408c21613436880a727122c57b8de08683835a4c71b68ce5624c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.75.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 31 11:15:50.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7071" for this suite.

• [SLOW TEST:7.194 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":338,"skipped":6201,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:15:50.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9786
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 31 11:15:50.365: INFO: The status of Pod annotationupdate0d71a0e8-e508-484b-a229-c09ef8ef32f9 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 11:15:52.374: INFO: The status of Pod annotationupdate0d71a0e8-e508-484b-a229-c09ef8ef32f9 is Running (Ready = true)
Jan 31 11:15:52.899: INFO: Successfully updated pod "annotationupdate0d71a0e8-e508-484b-a229-c09ef8ef32f9"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 31 11:15:56.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9786" for this suite.

• [SLOW TEST:6.714 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":339,"skipped":6249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:15:56.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-7309
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:188
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 31 11:15:57.078: INFO: starting watch
STEP: patching
STEP: updating
Jan 31 11:15:57.087: INFO: waiting for watch events with expected annotations
Jan 31 11:15:57.087: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:188
Jan 31 11:15:57.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7309" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":356,"completed":340,"skipped":6275,"failed":0}
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:15:57.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3282
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-9ssgp in namespace proxy-3282
I0131 11:15:57.264012      22 runners.go:193] Created replication controller with name: proxy-service-9ssgp, namespace: proxy-3282, replica count: 1
I0131 11:15:58.316205      22 runners.go:193] proxy-service-9ssgp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0131 11:15:59.317274      22 runners.go:193] proxy-service-9ssgp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 31 11:15:59.324: INFO: setup took 2.079689837s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 31 11:15:59.335: INFO: (0) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 10.775974ms)
Jan 31 11:15:59.339: INFO: (0) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 15.121681ms)
Jan 31 11:15:59.339: INFO: (0) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 15.232851ms)
Jan 31 11:15:59.339: INFO: (0) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 15.229014ms)
Jan 31 11:15:59.340: INFO: (0) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 15.382934ms)
Jan 31 11:15:59.340: INFO: (0) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 15.425912ms)
Jan 31 11:15:59.340: INFO: (0) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 15.476922ms)
Jan 31 11:15:59.340: INFO: (0) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 16.048466ms)
Jan 31 11:15:59.340: INFO: (0) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 16.162223ms)
Jan 31 11:15:59.341: INFO: (0) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 17.236119ms)
Jan 31 11:15:59.341: INFO: (0) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 17.296887ms)
Jan 31 11:15:59.341: INFO: (0) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 17.274401ms)
Jan 31 11:15:59.342: INFO: (0) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 17.626219ms)
Jan 31 11:15:59.342: INFO: (0) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 17.605186ms)
Jan 31 11:15:59.345: INFO: (0) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 20.395099ms)
Jan 31 11:15:59.345: INFO: (0) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 20.484884ms)
Jan 31 11:15:59.351: INFO: (1) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 6.639355ms)
Jan 31 11:15:59.355: INFO: (1) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 10.408804ms)
Jan 31 11:15:59.355: INFO: (1) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 10.505263ms)
Jan 31 11:15:59.355: INFO: (1) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 10.518687ms)
Jan 31 11:15:59.355: INFO: (1) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 10.624285ms)
Jan 31 11:15:59.355: INFO: (1) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 10.693659ms)
Jan 31 11:15:59.355: INFO: (1) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 10.713372ms)
Jan 31 11:15:59.355: INFO: (1) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 10.79322ms)
Jan 31 11:15:59.356: INFO: (1) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 10.81259ms)
Jan 31 11:15:59.356: INFO: (1) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 11.014886ms)
Jan 31 11:15:59.358: INFO: (1) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 13.391247ms)
Jan 31 11:15:59.359: INFO: (1) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 14.169102ms)
Jan 31 11:15:59.359: INFO: (1) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 14.150032ms)
Jan 31 11:15:59.359: INFO: (1) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 14.283519ms)
Jan 31 11:15:59.359: INFO: (1) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 14.261187ms)
Jan 31 11:15:59.359: INFO: (1) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 14.323243ms)
Jan 31 11:15:59.364: INFO: (2) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 4.832425ms)
Jan 31 11:15:59.368: INFO: (2) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 9.363045ms)
Jan 31 11:15:59.369: INFO: (2) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 9.637196ms)
Jan 31 11:15:59.369: INFO: (2) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 9.747732ms)
Jan 31 11:15:59.369: INFO: (2) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 9.712922ms)
Jan 31 11:15:59.369: INFO: (2) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 9.793251ms)
Jan 31 11:15:59.369: INFO: (2) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 9.79208ms)
Jan 31 11:15:59.369: INFO: (2) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 9.954885ms)
Jan 31 11:15:59.369: INFO: (2) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 9.989757ms)
Jan 31 11:15:59.369: INFO: (2) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 10.125491ms)
Jan 31 11:15:59.370: INFO: (2) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 10.859973ms)
Jan 31 11:15:59.371: INFO: (2) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 11.864313ms)
Jan 31 11:15:59.372: INFO: (2) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 12.797034ms)
Jan 31 11:15:59.372: INFO: (2) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 12.794066ms)
Jan 31 11:15:59.372: INFO: (2) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 13.001446ms)
Jan 31 11:15:59.372: INFO: (2) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 13.038422ms)
Jan 31 11:15:59.377: INFO: (3) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 4.895283ms)
Jan 31 11:15:59.378: INFO: (3) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 6.184872ms)
Jan 31 11:15:59.382: INFO: (3) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 9.955522ms)
Jan 31 11:15:59.382: INFO: (3) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 10.106385ms)
Jan 31 11:15:59.382: INFO: (3) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 10.058757ms)
Jan 31 11:15:59.382: INFO: (3) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 10.103456ms)
Jan 31 11:15:59.383: INFO: (3) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 10.305357ms)
Jan 31 11:15:59.383: INFO: (3) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 10.875539ms)
Jan 31 11:15:59.383: INFO: (3) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 10.970409ms)
Jan 31 11:15:59.384: INFO: (3) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 11.509684ms)
Jan 31 11:15:59.384: INFO: (3) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 11.513265ms)
Jan 31 11:15:59.384: INFO: (3) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 11.932378ms)
Jan 31 11:15:59.384: INFO: (3) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 12.014702ms)
Jan 31 11:15:59.384: INFO: (3) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 11.985393ms)
Jan 31 11:15:59.384: INFO: (3) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 12.132993ms)
Jan 31 11:15:59.384: INFO: (3) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 12.197975ms)
Jan 31 11:15:59.389: INFO: (4) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 4.72373ms)
Jan 31 11:15:59.396: INFO: (4) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 11.12929ms)
Jan 31 11:15:59.396: INFO: (4) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 11.186503ms)
Jan 31 11:15:59.396: INFO: (4) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 11.180759ms)
Jan 31 11:15:59.396: INFO: (4) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 11.442396ms)
Jan 31 11:15:59.397: INFO: (4) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 12.723564ms)
Jan 31 11:15:59.397: INFO: (4) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 12.762397ms)
Jan 31 11:15:59.397: INFO: (4) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 13.030633ms)
Jan 31 11:15:59.399: INFO: (4) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 14.098507ms)
Jan 31 11:15:59.399: INFO: (4) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 14.371886ms)
Jan 31 11:15:59.399: INFO: (4) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 14.60936ms)
Jan 31 11:15:59.400: INFO: (4) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 15.459283ms)
Jan 31 11:15:59.400: INFO: (4) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 15.433648ms)
Jan 31 11:15:59.400: INFO: (4) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 15.620434ms)
Jan 31 11:15:59.400: INFO: (4) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 15.673962ms)
Jan 31 11:15:59.400: INFO: (4) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 15.801326ms)
Jan 31 11:15:59.411: INFO: (5) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 10.63183ms)
Jan 31 11:15:59.411: INFO: (5) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 10.611713ms)
Jan 31 11:15:59.411: INFO: (5) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 10.66249ms)
Jan 31 11:15:59.411: INFO: (5) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 10.679432ms)
Jan 31 11:15:59.411: INFO: (5) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 10.636264ms)
Jan 31 11:15:59.411: INFO: (5) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 10.864538ms)
Jan 31 11:15:59.411: INFO: (5) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 10.912099ms)
Jan 31 11:15:59.412: INFO: (5) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 11.71211ms)
Jan 31 11:15:59.412: INFO: (5) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 12.105097ms)
Jan 31 11:15:59.413: INFO: (5) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 12.431427ms)
Jan 31 11:15:59.413: INFO: (5) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 12.425255ms)
Jan 31 11:15:59.413: INFO: (5) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 12.558048ms)
Jan 31 11:15:59.413: INFO: (5) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 12.616151ms)
Jan 31 11:15:59.413: INFO: (5) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 12.600894ms)
Jan 31 11:15:59.413: INFO: (5) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 12.624563ms)
Jan 31 11:15:59.413: INFO: (5) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 12.749236ms)
Jan 31 11:15:59.420: INFO: (6) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 6.918056ms)
Jan 31 11:15:59.420: INFO: (6) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 6.975302ms)
Jan 31 11:15:59.420: INFO: (6) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 7.182175ms)
Jan 31 11:15:59.422: INFO: (6) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 8.510225ms)
Jan 31 11:15:59.422: INFO: (6) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 8.555263ms)
Jan 31 11:15:59.423: INFO: (6) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 9.925309ms)
Jan 31 11:15:59.423: INFO: (6) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 9.998807ms)
Jan 31 11:15:59.423: INFO: (6) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 10.219057ms)
Jan 31 11:15:59.424: INFO: (6) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 10.440939ms)
Jan 31 11:15:59.424: INFO: (6) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 10.464954ms)
Jan 31 11:15:59.424: INFO: (6) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 10.678521ms)
Jan 31 11:15:59.424: INFO: (6) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 10.825494ms)
Jan 31 11:15:59.424: INFO: (6) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 10.860834ms)
Jan 31 11:15:59.424: INFO: (6) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 10.937097ms)
Jan 31 11:15:59.424: INFO: (6) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 11.232179ms)
Jan 31 11:15:59.425: INFO: (6) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 11.355895ms)
Jan 31 11:15:59.431: INFO: (7) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 6.159234ms)
Jan 31 11:15:59.435: INFO: (7) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 10.043422ms)
Jan 31 11:15:59.435: INFO: (7) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 10.001581ms)
Jan 31 11:15:59.436: INFO: (7) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 11.694331ms)
Jan 31 11:15:59.436: INFO: (7) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 11.730488ms)
Jan 31 11:15:59.437: INFO: (7) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 11.866736ms)
Jan 31 11:15:59.437: INFO: (7) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 11.890031ms)
Jan 31 11:15:59.437: INFO: (7) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 11.993445ms)
Jan 31 11:15:59.437: INFO: (7) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 12.102387ms)
Jan 31 11:15:59.437: INFO: (7) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 12.308673ms)
Jan 31 11:15:59.437: INFO: (7) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 12.561166ms)
Jan 31 11:15:59.438: INFO: (7) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 13.09928ms)
Jan 31 11:15:59.439: INFO: (7) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 14.686124ms)
Jan 31 11:15:59.439: INFO: (7) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 14.815475ms)
Jan 31 11:15:59.439: INFO: (7) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 14.917366ms)
Jan 31 11:15:59.440: INFO: (7) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 15.007287ms)
Jan 31 11:15:59.445: INFO: (8) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 5.187577ms)
Jan 31 11:15:59.449: INFO: (8) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 9.260709ms)
Jan 31 11:15:59.449: INFO: (8) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 9.548245ms)
Jan 31 11:15:59.449: INFO: (8) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 9.512773ms)
Jan 31 11:15:59.449: INFO: (8) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 9.643196ms)
Jan 31 11:15:59.450: INFO: (8) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 9.892394ms)
Jan 31 11:15:59.450: INFO: (8) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 9.920266ms)
Jan 31 11:15:59.450: INFO: (8) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 9.907755ms)
Jan 31 11:15:59.450: INFO: (8) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 9.963918ms)
Jan 31 11:15:59.451: INFO: (8) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 11.101802ms)
Jan 31 11:15:59.452: INFO: (8) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 12.487889ms)
Jan 31 11:15:59.453: INFO: (8) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 12.895288ms)
Jan 31 11:15:59.453: INFO: (8) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 13.26306ms)
Jan 31 11:15:59.453: INFO: (8) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 13.231128ms)
Jan 31 11:15:59.453: INFO: (8) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 13.288682ms)
Jan 31 11:15:59.453: INFO: (8) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 13.351243ms)
Jan 31 11:15:59.458: INFO: (9) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 4.44401ms)
Jan 31 11:15:59.459: INFO: (9) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 5.703439ms)
Jan 31 11:15:59.462: INFO: (9) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 9.249661ms)
Jan 31 11:15:59.463: INFO: (9) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 9.356871ms)
Jan 31 11:15:59.463: INFO: (9) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 9.831836ms)
Jan 31 11:15:59.463: INFO: (9) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 9.941215ms)
Jan 31 11:15:59.464: INFO: (9) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 10.467575ms)
Jan 31 11:15:59.464: INFO: (9) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 10.538274ms)
Jan 31 11:15:59.464: INFO: (9) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 10.573465ms)
Jan 31 11:15:59.464: INFO: (9) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 10.570507ms)
Jan 31 11:15:59.464: INFO: (9) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 11.160807ms)
Jan 31 11:15:59.465: INFO: (9) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 11.788757ms)
Jan 31 11:15:59.465: INFO: (9) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 11.781081ms)
Jan 31 11:15:59.465: INFO: (9) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 11.768552ms)
Jan 31 11:15:59.465: INFO: (9) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 11.840797ms)
Jan 31 11:15:59.465: INFO: (9) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 11.857527ms)
Jan 31 11:15:59.471: INFO: (10) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 5.956772ms)
Jan 31 11:15:59.473: INFO: (10) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 8.250084ms)
Jan 31 11:15:59.473: INFO: (10) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 8.296549ms)
Jan 31 11:15:59.475: INFO: (10) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 9.94169ms)
Jan 31 11:15:59.475: INFO: (10) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 9.995613ms)
Jan 31 11:15:59.475: INFO: (10) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 10.003247ms)
Jan 31 11:15:59.475: INFO: (10) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 10.017869ms)
Jan 31 11:15:59.475: INFO: (10) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 10.036836ms)
Jan 31 11:15:59.475: INFO: (10) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 10.155241ms)
Jan 31 11:15:59.475: INFO: (10) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 10.168553ms)
Jan 31 11:15:59.475: INFO: (10) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 10.182113ms)
Jan 31 11:15:59.475: INFO: (10) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 10.321458ms)
Jan 31 11:15:59.475: INFO: (10) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 10.398213ms)
Jan 31 11:15:59.476: INFO: (10) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 10.672864ms)
Jan 31 11:15:59.476: INFO: (10) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 10.810134ms)
Jan 31 11:15:59.476: INFO: (10) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 10.893648ms)
Jan 31 11:15:59.487: INFO: (11) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 10.783949ms)
Jan 31 11:15:59.487: INFO: (11) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 10.893955ms)
Jan 31 11:15:59.487: INFO: (11) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 10.862515ms)
Jan 31 11:15:59.487: INFO: (11) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 10.893863ms)
Jan 31 11:15:59.487: INFO: (11) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 10.982715ms)
Jan 31 11:15:59.487: INFO: (11) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 10.966091ms)
Jan 31 11:15:59.487: INFO: (11) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 10.998213ms)
Jan 31 11:15:59.487: INFO: (11) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 11.025656ms)
Jan 31 11:15:59.487: INFO: (11) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 11.058103ms)
Jan 31 11:15:59.487: INFO: (11) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 11.253421ms)
Jan 31 11:15:59.487: INFO: (11) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 11.21784ms)
Jan 31 11:15:59.487: INFO: (11) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 11.371034ms)
Jan 31 11:15:59.487: INFO: (11) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 11.36443ms)
Jan 31 11:15:59.487: INFO: (11) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 11.401317ms)
Jan 31 11:15:59.488: INFO: (11) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 11.94155ms)
Jan 31 11:15:59.488: INFO: (11) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 11.934205ms)
Jan 31 11:15:59.493: INFO: (12) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 5.060209ms)
Jan 31 11:15:59.495: INFO: (12) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 6.533805ms)
Jan 31 11:15:59.495: INFO: (12) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 7.228623ms)
Jan 31 11:15:59.495: INFO: (12) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 7.215003ms)
Jan 31 11:15:59.496: INFO: (12) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 7.93845ms)
Jan 31 11:15:59.496: INFO: (12) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 8.128608ms)
Jan 31 11:15:59.496: INFO: (12) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 8.047629ms)
Jan 31 11:15:59.496: INFO: (12) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 8.230815ms)
Jan 31 11:15:59.496: INFO: (12) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 8.450083ms)
Jan 31 11:15:59.497: INFO: (12) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 8.807873ms)
Jan 31 11:15:59.497: INFO: (12) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 9.09978ms)
Jan 31 11:15:59.497: INFO: (12) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 9.246775ms)
Jan 31 11:15:59.498: INFO: (12) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 10.235484ms)
Jan 31 11:15:59.499: INFO: (12) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 11.161261ms)
Jan 31 11:15:59.500: INFO: (12) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 12.455353ms)
Jan 31 11:15:59.501: INFO: (12) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 13.204808ms)
Jan 31 11:15:59.507: INFO: (13) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 6.067375ms)
Jan 31 11:15:59.509: INFO: (13) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 7.897235ms)
Jan 31 11:15:59.509: INFO: (13) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 7.985435ms)
Jan 31 11:15:59.510: INFO: (13) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 8.578053ms)
Jan 31 11:15:59.510: INFO: (13) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 8.624556ms)
Jan 31 11:15:59.510: INFO: (13) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 8.77921ms)
Jan 31 11:15:59.510: INFO: (13) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 8.667614ms)
Jan 31 11:15:59.510: INFO: (13) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 8.685531ms)
Jan 31 11:15:59.511: INFO: (13) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 10.0532ms)
Jan 31 11:15:59.512: INFO: (13) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 10.17344ms)
Jan 31 11:15:59.512: INFO: (13) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 10.335963ms)
Jan 31 11:15:59.516: INFO: (13) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 14.343829ms)
Jan 31 11:15:59.516: INFO: (13) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 14.432118ms)
Jan 31 11:15:59.516: INFO: (13) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 14.435225ms)
Jan 31 11:15:59.516: INFO: (13) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 14.505854ms)
Jan 31 11:15:59.516: INFO: (13) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 14.536888ms)
Jan 31 11:15:59.526: INFO: (14) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 9.665581ms)
Jan 31 11:15:59.528: INFO: (14) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 11.720576ms)
Jan 31 11:15:59.528: INFO: (14) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 11.891828ms)
Jan 31 11:15:59.528: INFO: (14) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 11.973053ms)
Jan 31 11:15:59.528: INFO: (14) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 11.974043ms)
Jan 31 11:15:59.528: INFO: (14) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 12.154816ms)
Jan 31 11:15:59.528: INFO: (14) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 12.181982ms)
Jan 31 11:15:59.528: INFO: (14) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 12.146877ms)
Jan 31 11:15:59.528: INFO: (14) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 12.172464ms)
Jan 31 11:15:59.528: INFO: (14) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 12.210097ms)
Jan 31 11:15:59.534: INFO: (14) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 17.792534ms)
Jan 31 11:15:59.534: INFO: (14) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 18.092698ms)
Jan 31 11:15:59.534: INFO: (14) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 18.068881ms)
Jan 31 11:15:59.534: INFO: (14) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 18.292747ms)
Jan 31 11:15:59.534: INFO: (14) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 18.294251ms)
Jan 31 11:15:59.534: INFO: (14) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 18.332302ms)
Jan 31 11:15:59.544: INFO: (15) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 9.564519ms)
Jan 31 11:15:59.544: INFO: (15) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 9.749558ms)
Jan 31 11:15:59.544: INFO: (15) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 9.798297ms)
Jan 31 11:15:59.545: INFO: (15) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 10.658093ms)
Jan 31 11:15:59.545: INFO: (15) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 10.653857ms)
Jan 31 11:15:59.545: INFO: (15) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 10.704713ms)
Jan 31 11:15:59.545: INFO: (15) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 10.748134ms)
Jan 31 11:15:59.545: INFO: (15) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 10.762894ms)
Jan 31 11:15:59.545: INFO: (15) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 10.812312ms)
Jan 31 11:15:59.546: INFO: (15) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 11.301632ms)
Jan 31 11:15:59.546: INFO: (15) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 11.403193ms)
Jan 31 11:15:59.546: INFO: (15) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 11.970128ms)
Jan 31 11:15:59.547: INFO: (15) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 12.722427ms)
Jan 31 11:15:59.548: INFO: (15) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 13.261551ms)
Jan 31 11:15:59.548: INFO: (15) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 13.26024ms)
Jan 31 11:15:59.548: INFO: (15) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 13.259557ms)
Jan 31 11:15:59.556: INFO: (16) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 8.492038ms)
Jan 31 11:15:59.556: INFO: (16) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 8.559595ms)
Jan 31 11:15:59.556: INFO: (16) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 8.614984ms)
Jan 31 11:15:59.557: INFO: (16) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 8.968529ms)
Jan 31 11:15:59.557: INFO: (16) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 9.064118ms)
Jan 31 11:15:59.557: INFO: (16) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 9.058121ms)
Jan 31 11:15:59.557: INFO: (16) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 9.147186ms)
Jan 31 11:15:59.557: INFO: (16) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 9.135042ms)
Jan 31 11:15:59.557: INFO: (16) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 9.145663ms)
Jan 31 11:15:59.557: INFO: (16) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 9.109353ms)
Jan 31 11:15:59.559: INFO: (16) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 10.898704ms)
Jan 31 11:15:59.560: INFO: (16) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 11.969753ms)
Jan 31 11:15:59.561: INFO: (16) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 12.786624ms)
Jan 31 11:15:59.561: INFO: (16) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 12.885917ms)
Jan 31 11:15:59.561: INFO: (16) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 12.926765ms)
Jan 31 11:15:59.561: INFO: (16) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 12.922821ms)
Jan 31 11:15:59.570: INFO: (17) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 9.067382ms)
Jan 31 11:15:59.574: INFO: (17) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 13.321903ms)
Jan 31 11:15:59.574: INFO: (17) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 13.321921ms)
Jan 31 11:15:59.574: INFO: (17) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 13.3154ms)
Jan 31 11:15:59.574: INFO: (17) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 13.361149ms)
Jan 31 11:15:59.574: INFO: (17) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 13.332443ms)
Jan 31 11:15:59.574: INFO: (17) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 13.392288ms)
Jan 31 11:15:59.574: INFO: (17) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 13.369934ms)
Jan 31 11:15:59.574: INFO: (17) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 13.366251ms)
Jan 31 11:15:59.574: INFO: (17) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 13.352198ms)
Jan 31 11:15:59.576: INFO: (17) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 15.349543ms)
Jan 31 11:15:59.576: INFO: (17) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 15.55277ms)
Jan 31 11:15:59.577: INFO: (17) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 15.764779ms)
Jan 31 11:15:59.577: INFO: (17) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 15.741729ms)
Jan 31 11:15:59.577: INFO: (17) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 15.921063ms)
Jan 31 11:15:59.577: INFO: (17) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 16.021865ms)
Jan 31 11:15:59.583: INFO: (18) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 6.549775ms)
Jan 31 11:15:59.588: INFO: (18) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 11.448073ms)
Jan 31 11:15:59.588: INFO: (18) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 11.505725ms)
Jan 31 11:15:59.588: INFO: (18) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 11.551014ms)
Jan 31 11:15:59.588: INFO: (18) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 11.490975ms)
Jan 31 11:15:59.588: INFO: (18) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 11.531685ms)
Jan 31 11:15:59.588: INFO: (18) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 11.517031ms)
Jan 31 11:15:59.588: INFO: (18) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 11.527051ms)
Jan 31 11:15:59.588: INFO: (18) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 11.53414ms)
Jan 31 11:15:59.589: INFO: (18) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 11.564182ms)
Jan 31 11:15:59.592: INFO: (18) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 14.928686ms)
Jan 31 11:15:59.594: INFO: (18) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 16.740277ms)
Jan 31 11:15:59.594: INFO: (18) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 16.892117ms)
Jan 31 11:15:59.594: INFO: (18) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 17.039353ms)
Jan 31 11:15:59.594: INFO: (18) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 17.204677ms)
Jan 31 11:15:59.594: INFO: (18) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 17.526272ms)
Jan 31 11:15:59.605: INFO: (19) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:162/proxy/: bar (200; 9.982002ms)
Jan 31 11:15:59.605: INFO: (19) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:1080/proxy/rewriteme">test<... (200; 10.08222ms)
Jan 31 11:15:59.605: INFO: (19) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:1080/proxy/rewriteme">... (200; 10.026884ms)
Jan 31 11:15:59.605: INFO: (19) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:162/proxy/: bar (200; 10.083978ms)
Jan 31 11:15:59.605: INFO: (19) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx/proxy/rewriteme">test</a> (200; 10.120877ms)
Jan 31 11:15:59.606: INFO: (19) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:460/proxy/: tls baz (200; 10.948629ms)
Jan 31 11:15:59.606: INFO: (19) /api/v1/namespaces/proxy-3282/pods/proxy-service-9ssgp-649sx:160/proxy/: foo (200; 11.65404ms)
Jan 31 11:15:59.606: INFO: (19) /api/v1/namespaces/proxy-3282/pods/http:proxy-service-9ssgp-649sx:160/proxy/: foo (200; 11.691782ms)
Jan 31 11:15:59.606: INFO: (19) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:462/proxy/: tls qux (200; 11.732546ms)
Jan 31 11:15:59.606: INFO: (19) /api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/: <a href="/api/v1/namespaces/proxy-3282/pods/https:proxy-service-9ssgp-649sx:443/proxy/tlsrewritem... (200; 11.79366ms)
Jan 31 11:15:59.606: INFO: (19) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname1/proxy/: tls baz (200; 11.810333ms)
Jan 31 11:15:59.607: INFO: (19) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname2/proxy/: bar (200; 12.074799ms)
Jan 31 11:15:59.607: INFO: (19) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname1/proxy/: foo (200; 12.191414ms)
Jan 31 11:15:59.609: INFO: (19) /api/v1/namespaces/proxy-3282/services/https:proxy-service-9ssgp:tlsportname2/proxy/: tls qux (200; 14.112495ms)
Jan 31 11:15:59.609: INFO: (19) /api/v1/namespaces/proxy-3282/services/proxy-service-9ssgp:portname2/proxy/: bar (200; 14.751718ms)
Jan 31 11:15:59.609: INFO: (19) /api/v1/namespaces/proxy-3282/services/http:proxy-service-9ssgp:portname1/proxy/: foo (200; 14.79161ms)
STEP: deleting ReplicationController proxy-service-9ssgp in namespace proxy-3282, will wait for the garbage collector to delete the pods
Jan 31 11:15:59.676: INFO: Deleting ReplicationController proxy-service-9ssgp took: 11.134315ms
Jan 31 11:15:59.776: INFO: Terminating ReplicationController proxy-service-9ssgp pods took: 100.916373ms
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jan 31 11:16:01.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3282" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":356,"completed":341,"skipped":6278,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:16:01.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6509
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap that has name configmap-test-emptyKey-6969b0aa-ed48-4df5-9025-bc2aebef38a2
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 31 11:16:01.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6509" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":356,"completed":342,"skipped":6282,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:16:01.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1854
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 11:16:04.090: INFO: Deleting pod "var-expansion-b02501e9-4602-447e-a3f4-d9ff67e20aeb" in namespace "var-expansion-1854"
Jan 31 11:16:04.097: INFO: Wait up to 5m0s for pod "var-expansion-b02501e9-4602-447e-a3f4-d9ff67e20aeb" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 31 11:16:06.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1854" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":356,"completed":343,"skipped":6284,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:16:06.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-541
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 11:16:06.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-541 version'
Jan 31 11:16:06.316: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 31 11:16:06.316: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.6\", GitCommit:\"b39bf148cd654599a52e867485c02c4f9d28b312\", GitTreeState:\"clean\", BuildDate:\"2022-09-21T13:19:24Z\", GoVersion:\"go1.18.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.4\nServer Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.6\", GitCommit:\"b39bf148cd654599a52e867485c02c4f9d28b312\", GitTreeState:\"clean\", BuildDate:\"2022-09-21T13:12:04Z\", GoVersion:\"go1.18.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 11:16:06.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-541" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":356,"completed":344,"skipped":6332,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:16:06.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-313
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1334
STEP: creating the pod
Jan 31 11:16:06.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-313 create -f -'
Jan 31 11:16:07.286: INFO: stderr: ""
Jan 31 11:16:07.286: INFO: stdout: "pod/pause created\n"
Jan 31 11:16:07.286: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 31 11:16:07.286: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-313" to be "running and ready"
Jan 31 11:16:07.295: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.94998ms
Jan 31 11:16:09.305: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.018892998s
Jan 31 11:16:09.305: INFO: Pod "pause" satisfied condition "running and ready"
Jan 31 11:16:09.305: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 31 11:16:09.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-313 label pods pause testing-label=testing-label-value'
Jan 31 11:16:09.364: INFO: stderr: ""
Jan 31 11:16:09.364: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 31 11:16:09.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-313 get pod pause -L testing-label'
Jan 31 11:16:09.406: INFO: stderr: ""
Jan 31 11:16:09.406: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 31 11:16:09.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-313 label pods pause testing-label-'
Jan 31 11:16:09.469: INFO: stderr: ""
Jan 31 11:16:09.469: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 31 11:16:09.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-313 get pod pause -L testing-label'
Jan 31 11:16:09.511: INFO: stderr: ""
Jan 31 11:16:09.511: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Jan 31 11:16:09.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-313 delete --grace-period=0 --force -f -'
Jan 31 11:16:09.564: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 31 11:16:09.564: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 31 11:16:09.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-313 get rc,svc -l name=pause --no-headers'
Jan 31 11:16:09.612: INFO: stderr: "No resources found in kubectl-313 namespace.\n"
Jan 31 11:16:09.612: INFO: stdout: ""
Jan 31 11:16:09.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=kubectl-313 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 31 11:16:09.655: INFO: stderr: ""
Jan 31 11:16:09.655: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 31 11:16:09.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-313" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":356,"completed":345,"skipped":6363,"failed":0}
S
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:16:09.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-5341
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jan 31 11:16:09.802: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jan 31 11:16:09.819: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 31 11:16:09.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5341" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":356,"completed":346,"skipped":6364,"failed":0}
SS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:16:09.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5090
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Jan 31 11:16:10.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5090" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":347,"skipped":6366,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:16:10.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-63
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 31 11:16:10.152: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82d95063-8e2a-473e-9ba6-6d7677e4d948" in namespace "projected-63" to be "Succeeded or Failed"
Jan 31 11:16:10.154: INFO: Pod "downwardapi-volume-82d95063-8e2a-473e-9ba6-6d7677e4d948": Phase="Pending", Reason="", readiness=false. Elapsed: 2.699843ms
Jan 31 11:16:12.162: INFO: Pod "downwardapi-volume-82d95063-8e2a-473e-9ba6-6d7677e4d948": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009785803s
Jan 31 11:16:14.170: INFO: Pod "downwardapi-volume-82d95063-8e2a-473e-9ba6-6d7677e4d948": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018624252s
STEP: Saw pod success
Jan 31 11:16:14.170: INFO: Pod "downwardapi-volume-82d95063-8e2a-473e-9ba6-6d7677e4d948" satisfied condition "Succeeded or Failed"
Jan 31 11:16:14.173: INFO: Trying to get logs from node macpro-x86-1 pod downwardapi-volume-82d95063-8e2a-473e-9ba6-6d7677e4d948 container client-container: <nil>
STEP: delete the pod
Jan 31 11:16:14.188: INFO: Waiting for pod downwardapi-volume-82d95063-8e2a-473e-9ba6-6d7677e4d948 to disappear
Jan 31 11:16:14.190: INFO: Pod downwardapi-volume-82d95063-8e2a-473e-9ba6-6d7677e4d948 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 31 11:16:14.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-63" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":348,"skipped":6383,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:16:14.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9594
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service nodeport-test with type=NodePort in namespace services-9594
STEP: creating replication controller nodeport-test in namespace services-9594
I0131 11:16:14.354858      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9594, replica count: 2
I0131 11:16:17.405944      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 31 11:16:17.405: INFO: Creating new exec pod
Jan 31 11:16:20.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-9594 exec execpodmddkh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 31 11:16:20.539: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 31 11:16:20.539: INFO: stdout: "nodeport-test-cjp2b"
Jan 31 11:16:20.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-9594 exec execpodmddkh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.159.44 80'
Jan 31 11:16:20.636: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.159.44 80\nConnection to 10.111.159.44 80 port [tcp/http] succeeded!\n"
Jan 31 11:16:20.636: INFO: stdout: "nodeport-test-9ht8z"
Jan 31 11:16:20.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-9594 exec execpodmddkh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.31 30133'
Jan 31 11:16:20.716: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.31 30133\nConnection to 10.221.188.31 30133 port [tcp/*] succeeded!\n"
Jan 31 11:16:20.716: INFO: stdout: "nodeport-test-9ht8z"
Jan 31 11:16:20.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=services-9594 exec execpodmddkh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.32 30133'
Jan 31 11:16:20.794: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.32 30133\nConnection to 10.221.188.32 30133 port [tcp/*] succeeded!\n"
Jan 31 11:16:20.794: INFO: stdout: "nodeport-test-9ht8z"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 31 11:16:20.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9594" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:6.609 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":356,"completed":349,"skipped":6459,"failed":0}
SSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:16:20.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9745
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name secret-emptykey-test-a2a0c34b-d2a2-4409-9380-1c96d3b95f5b
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 31 11:16:20.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9745" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":356,"completed":350,"skipped":6464,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:16:20.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4733
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jan 31 11:16:21.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jan 31 11:16:37.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 11:16:42.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 11:16:59.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4733" for this suite.

• [SLOW TEST:38.966 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":356,"completed":351,"skipped":6496,"failed":0}
SSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:16:59.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1758
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 11:17:02.073: INFO: Deleting pod "var-expansion-e0ed81fb-556e-4703-b922-9f79227c4b01" in namespace "var-expansion-1758"
Jan 31 11:17:02.080: INFO: Wait up to 5m0s for pod "var-expansion-e0ed81fb-556e-4703-b922-9f79227c4b01" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 31 11:17:04.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1758" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":356,"completed":352,"skipped":6501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:17:04.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-695
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
Jan 31 11:17:04.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties
Jan 31 11:17:09.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-695 --namespace=crd-publish-openapi-695 create -f -'
Jan 31 11:17:10.266: INFO: stderr: ""
Jan 31 11:17:10.266: INFO: stdout: "e2e-test-crd-publish-openapi-6915-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 31 11:17:10.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-695 --namespace=crd-publish-openapi-695 delete e2e-test-crd-publish-openapi-6915-crds test-foo'
Jan 31 11:17:10.354: INFO: stderr: ""
Jan 31 11:17:10.354: INFO: stdout: "e2e-test-crd-publish-openapi-6915-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 31 11:17:10.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-695 --namespace=crd-publish-openapi-695 apply -f -'
Jan 31 11:17:10.489: INFO: stderr: ""
Jan 31 11:17:10.489: INFO: stdout: "e2e-test-crd-publish-openapi-6915-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 31 11:17:10.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-695 --namespace=crd-publish-openapi-695 delete e2e-test-crd-publish-openapi-6915-crds test-foo'
Jan 31 11:17:10.540: INFO: stderr: ""
Jan 31 11:17:10.540: INFO: stdout: "e2e-test-crd-publish-openapi-6915-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values
Jan 31 11:17:10.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-695 --namespace=crd-publish-openapi-695 create -f -'
Jan 31 11:17:11.172: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jan 31 11:17:11.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-695 --namespace=crd-publish-openapi-695 create -f -'
Jan 31 11:17:11.307: INFO: rc: 1
Jan 31 11:17:11.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-695 --namespace=crd-publish-openapi-695 apply -f -'
Jan 31 11:17:11.434: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties
Jan 31 11:17:11.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-695 --namespace=crd-publish-openapi-695 create -f -'
Jan 31 11:17:11.563: INFO: rc: 1
Jan 31 11:17:11.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-695 --namespace=crd-publish-openapi-695 apply -f -'
Jan 31 11:17:11.688: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jan 31 11:17:11.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-695 explain e2e-test-crd-publish-openapi-6915-crds'
Jan 31 11:17:12.306: INFO: stderr: ""
Jan 31 11:17:12.306: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6915-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jan 31 11:17:12.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-695 explain e2e-test-crd-publish-openapi-6915-crds.metadata'
Jan 31 11:17:12.433: INFO: stderr: ""
Jan 31 11:17:12.433: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6915-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     Deprecated: ClusterName is a legacy field that was always cleared by the\n     system and never used; it will be removed completely in 1.25.\n\n     The name in the go struct is changed to help clients detect accidental use.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 31 11:17:12.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-695 explain e2e-test-crd-publish-openapi-6915-crds.spec'
Jan 31 11:17:12.565: INFO: stderr: ""
Jan 31 11:17:12.565: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6915-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 31 11:17:12.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-695 explain e2e-test-crd-publish-openapi-6915-crds.spec.bars'
Jan 31 11:17:12.691: INFO: stderr: ""
Jan 31 11:17:12.691: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6915-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jan 31 11:17:12.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2777931143 --namespace=crd-publish-openapi-695 explain e2e-test-crd-publish-openapi-6915-crds.spec.bars2'
Jan 31 11:17:12.815: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 31 11:17:16.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-695" for this suite.

• [SLOW TEST:12.306 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":356,"completed":353,"skipped":6533,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:17:16.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8296
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-828ce195-cf0a-402c-b800-7376dae71d77
STEP: Creating a pod to test consume secrets
Jan 31 11:17:16.562: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1f0cddad-1784-4e88-a174-d88ed18a6e10" in namespace "projected-8296" to be "Succeeded or Failed"
Jan 31 11:17:16.582: INFO: Pod "pod-projected-secrets-1f0cddad-1784-4e88-a174-d88ed18a6e10": Phase="Pending", Reason="", readiness=false. Elapsed: 20.817705ms
Jan 31 11:17:18.591: INFO: Pod "pod-projected-secrets-1f0cddad-1784-4e88-a174-d88ed18a6e10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029066197s
Jan 31 11:17:20.598: INFO: Pod "pod-projected-secrets-1f0cddad-1784-4e88-a174-d88ed18a6e10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036829819s
STEP: Saw pod success
Jan 31 11:17:20.599: INFO: Pod "pod-projected-secrets-1f0cddad-1784-4e88-a174-d88ed18a6e10" satisfied condition "Succeeded or Failed"
Jan 31 11:17:20.601: INFO: Trying to get logs from node macpro-x86-2 pod pod-projected-secrets-1f0cddad-1784-4e88-a174-d88ed18a6e10 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 31 11:17:20.614: INFO: Waiting for pod pod-projected-secrets-1f0cddad-1784-4e88-a174-d88ed18a6e10 to disappear
Jan 31 11:17:20.616: INFO: Pod pod-projected-secrets-1f0cddad-1784-4e88-a174-d88ed18a6e10 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 31 11:17:20.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8296" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":354,"skipped":6547,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:17:20.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename hostport
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostport-5423
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jan 31 11:17:20.768: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 11:17:22.775: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.221.188.31 on the node which pod1 resides and expect scheduled
Jan 31 11:17:22.787: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 11:17:24.796: INFO: The status of Pod pod2 is Running (Ready = false)
Jan 31 11:17:26.792: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.221.188.31 but use UDP protocol on the node which pod2 resides
Jan 31 11:17:26.804: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 11:17:28.812: INFO: The status of Pod pod3 is Running (Ready = true)
Jan 31 11:17:28.821: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 31 11:17:30.831: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jan 31 11:17:30.834: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.221.188.31 http://127.0.0.1:54323/hostname] Namespace:hostport-5423 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 11:17:30.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 11:17:30.835: INFO: ExecWithOptions: Clientset creation
Jan 31 11:17:30.835: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5423/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.221.188.31+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.221.188.31, port: 54323
Jan 31 11:17:30.891: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.221.188.31:54323/hostname] Namespace:hostport-5423 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 11:17:30.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 11:17:30.891: INFO: ExecWithOptions: Clientset creation
Jan 31 11:17:30.891: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5423/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.221.188.31%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.221.188.31, port: 54323 UDP
Jan 31 11:17:30.938: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.221.188.31 54323] Namespace:hostport-5423 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 11:17:30.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
Jan 31 11:17:30.938: INFO: ExecWithOptions: Clientset creation
Jan 31 11:17:30.938: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5423/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+10.221.188.31+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:188
Jan 31 11:17:35.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-5423" for this suite.

• [SLOW TEST:15.376 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":356,"completed":355,"skipped":6562,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 31 11:17:36.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2777931143
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5370
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-4a917374-962b-4d7e-96c5-33bdd273e8e6
STEP: Creating a pod to test consume configMaps
Jan 31 11:17:36.140: INFO: Waiting up to 5m0s for pod "pod-configmaps-fe07eeba-371d-4483-98f2-76968fe53022" in namespace "configmap-5370" to be "Succeeded or Failed"
Jan 31 11:17:36.142: INFO: Pod "pod-configmaps-fe07eeba-371d-4483-98f2-76968fe53022": Phase="Pending", Reason="", readiness=false. Elapsed: 2.475486ms
Jan 31 11:17:38.148: INFO: Pod "pod-configmaps-fe07eeba-371d-4483-98f2-76968fe53022": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008486948s
Jan 31 11:17:40.160: INFO: Pod "pod-configmaps-fe07eeba-371d-4483-98f2-76968fe53022": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020236036s
STEP: Saw pod success
Jan 31 11:17:40.160: INFO: Pod "pod-configmaps-fe07eeba-371d-4483-98f2-76968fe53022" satisfied condition "Succeeded or Failed"
Jan 31 11:17:40.162: INFO: Trying to get logs from node macpro-x86-2 pod pod-configmaps-fe07eeba-371d-4483-98f2-76968fe53022 container agnhost-container: <nil>
STEP: delete the pod
Jan 31 11:17:40.177: INFO: Waiting for pod pod-configmaps-fe07eeba-371d-4483-98f2-76968fe53022 to disappear
Jan 31 11:17:40.179: INFO: Pod pod-configmaps-fe07eeba-371d-4483-98f2-76968fe53022 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 31 11:17:40.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5370" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":356,"skipped":6606,"failed":0}
SSSSSSSSSSSJan 31 11:17:40.187: INFO: Running AfterSuite actions on all nodes
Jan 31 11:17:40.187: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func19.2
Jan 31 11:17:40.187: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan 31 11:17:40.187: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Jan 31 11:17:40.187: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 31 11:17:40.187: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 31 11:17:40.187: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 31 11:17:40.187: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Jan 31 11:17:40.187: INFO: Running AfterSuite actions on node 1
Jan 31 11:17:40.187: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":356,"completed":356,"skipped":6617,"failed":0}

Ran 356 of 6973 Specs in 5731.046 seconds
SUCCESS! -- 356 Passed | 0 Failed | 0 Pending | 6617 Skipped
PASS

Ginkgo ran 1 suite in 1h35m32.509954368s
Test Suite Passed
