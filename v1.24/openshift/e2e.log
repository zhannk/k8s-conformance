I0819 14:57:39.413192      22 e2e.go:129] Starting e2e run "19c9de2a-8e2e-4851-a822-3738a266eea5" on Ginkgo node 1
{"msg":"Test Suite starting","total":356,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1660921059 - Will randomize all specs
Will run 356 of 6965 specs

Aug 19 14:57:40.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 14:57:40.484: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 19 14:57:40.498: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 19 14:57:40.507: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 19 14:57:40.507: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Aug 19 14:57:40.507: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 19 14:57:40.510: INFO: e2e test version: v1.24.0
Aug 19 14:57:40.511: INFO: kube-apiserver version: v1.24.0+4f0dd4d
Aug 19 14:57:40.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 14:57:40.514: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 14:57:40.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename deployment
W0819 14:57:40.567072      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Aug 19 14:57:40.567: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 14:57:40.572: INFO: Creating deployment "test-recreate-deployment"
Aug 19 14:57:40.579: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 19 14:57:40.627: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 19 14:57:42.636: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 19 14:57:42.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 14, 57, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 14, 57, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 14, 57, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 14, 57, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-848969dbcd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 19 14:57:44.644: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 19 14:57:44.654: INFO: Updating deployment test-recreate-deployment
Aug 19 14:57:44.654: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 19 14:57:44.732: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5011  b2e6e8dd-7c56-4cc7-ae85-04461dd6af4f 29830 2 2022-08-19 14:57:40 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-08-19 14:57:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 14:57:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e409e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-08-19 14:57:44 +0000 UTC,LastTransitionTime:2022-08-19 14:57:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cd8586fc7" is progressing.,LastUpdateTime:2022-08-19 14:57:44 +0000 UTC,LastTransitionTime:2022-08-19 14:57:40 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 19 14:57:44.735: INFO: New ReplicaSet "test-recreate-deployment-cd8586fc7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cd8586fc7  deployment-5011  927e23d2-e9ce-4d99-82df-de0ca01fe990 29828 1 2022-08-19 14:57:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b2e6e8dd-7c56-4cc7-ae85-04461dd6af4f 0xc000e40e80 0xc000e40e81}] []  [{kube-controller-manager Update apps/v1 2022-08-19 14:57:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2e6e8dd-7c56-4cc7-ae85-04461dd6af4f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 14:57:44 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cd8586fc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e40f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 19 14:57:44.735: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 19 14:57:44.735: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-848969dbcd  deployment-5011  10cb67fc-f431-4523-a49e-8e03a7d73be2 29819 2 2022-08-19 14:57:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:848969dbcd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b2e6e8dd-7c56-4cc7-ae85-04461dd6af4f 0xc000e40d67 0xc000e40d68}] []  [{kube-controller-manager Update apps/v1 2022-08-19 14:57:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2e6e8dd-7c56-4cc7-ae85-04461dd6af4f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 14:57:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 848969dbcd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:848969dbcd] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e40e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 19 14:57:44.738: INFO: Pod "test-recreate-deployment-cd8586fc7-kqr2r" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cd8586fc7-kqr2r test-recreate-deployment-cd8586fc7- deployment-5011  b61cb8ff-c791-44ba-9258-66829f7b40fe 29831 0 2022-08-19 14:57:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-cd8586fc7 927e23d2-e9ce-4d99-82df-de0ca01fe990 0xc000cd1457 0xc000cd1458}] []  [{kube-controller-manager Update v1 2022-08-19 14:57:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"927e23d2-e9ce-4d99-82df-de0ca01fe990\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-19 14:57:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bg9dl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bg9dl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-164-144.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c26,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-222pm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 14:57:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 14:57:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 14:57:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 14:57:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.164.144,PodIP:,StartTime:2022-08-19 14:57:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 19 14:57:44.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5011" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":1,"skipped":33,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 14:57:44.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:79
Aug 19 14:57:44.787: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the sample API server.
Aug 19 14:57:45.038: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Aug 19 14:57:47.119: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 19 14:57:49.125: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 19 14:57:51.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 19 14:57:53.125: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 14, 57, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 19 14:57:55.271: INFO: Waited 140.423219ms for the sample-apiserver to be ready to handle requests.
I0819 14:57:56.305716      22 request.go:601] Waited for 1.006895934s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/whereabouts.cni.cncf.io/v1alpha1
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Aug 19 14:57:57.363: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:69
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:188
Aug 19 14:57:58.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-701" for this suite.

• [SLOW TEST:13.510 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":356,"completed":2,"skipped":41,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 14:57:58.260: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 19 14:57:58.319: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1efd92ec-a192-4fef-b17a-800db7ce0567" in namespace "downward-api-8991" to be "Succeeded or Failed"
Aug 19 14:57:58.324: INFO: Pod "downwardapi-volume-1efd92ec-a192-4fef-b17a-800db7ce0567": Phase="Pending", Reason="", readiness=false. Elapsed: 4.092013ms
Aug 19 14:58:00.328: INFO: Pod "downwardapi-volume-1efd92ec-a192-4fef-b17a-800db7ce0567": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008316994s
Aug 19 14:58:02.333: INFO: Pod "downwardapi-volume-1efd92ec-a192-4fef-b17a-800db7ce0567": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01316642s
Aug 19 14:58:04.340: INFO: Pod "downwardapi-volume-1efd92ec-a192-4fef-b17a-800db7ce0567": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020618139s
Aug 19 14:58:06.345: INFO: Pod "downwardapi-volume-1efd92ec-a192-4fef-b17a-800db7ce0567": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02501881s
STEP: Saw pod success
Aug 19 14:58:06.345: INFO: Pod "downwardapi-volume-1efd92ec-a192-4fef-b17a-800db7ce0567" satisfied condition "Succeeded or Failed"
Aug 19 14:58:06.347: INFO: Trying to get logs from node ip-10-0-157-99.ec2.internal pod downwardapi-volume-1efd92ec-a192-4fef-b17a-800db7ce0567 container client-container: <nil>
STEP: delete the pod
Aug 19 14:58:06.376: INFO: Waiting for pod downwardapi-volume-1efd92ec-a192-4fef-b17a-800db7ce0567 to disappear
Aug 19 14:58:06.380: INFO: Pod downwardapi-volume-1efd92ec-a192-4fef-b17a-800db7ce0567 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 19 14:58:06.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8991" for this suite.

• [SLOW TEST:8.128 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":3,"skipped":43,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 14:58:06.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-1325
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 19 14:58:06.423: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 19 14:58:06.524: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 14:58:08.528: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 14:58:10.535: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 14:58:12.528: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 14:58:14.530: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 14:58:16.528: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 14:58:18.533: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 14:58:20.535: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 14:58:22.530: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 14:58:24.530: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 14:58:26.530: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 14:58:28.532: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 19 14:58:28.536: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 19 14:58:28.542: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 19 14:58:32.586: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 19 14:58:32.586: INFO: Going to poll 10.128.2.14 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 19 14:58:32.587: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.2.14 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1325 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 14:58:32.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 14:58:32.588: INFO: ExecWithOptions: Clientset creation
Aug 19 14:58:32.588: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1325/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.2.14+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 19 14:58:33.670: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 19 14:58:33.670: INFO: Going to poll 10.129.2.18 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 19 14:58:33.675: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.129.2.18 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1325 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 14:58:33.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 14:58:33.675: INFO: ExecWithOptions: Clientset creation
Aug 19 14:58:33.675: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1325/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.129.2.18+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 19 14:58:34.754: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 19 14:58:34.754: INFO: Going to poll 10.131.0.17 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 19 14:58:34.759: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.131.0.17 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1325 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 14:58:34.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 14:58:34.759: INFO: ExecWithOptions: Clientset creation
Aug 19 14:58:34.759: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1325/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.131.0.17+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 19 14:58:35.859: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Aug 19 14:58:35.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1325" for this suite.

• [SLOW TEST:29.485 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":4,"skipped":56,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 14:58:35.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
W0819 14:58:35.923992      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Aug 19 14:58:40.446: INFO: Successfully updated pod "adopt-release-7hgpq"
STEP: Checking that the Job readopts the Pod
Aug 19 14:58:40.446: INFO: Waiting up to 15m0s for pod "adopt-release-7hgpq" in namespace "job-2690" to be "adopted"
Aug 19 14:58:40.449: INFO: Pod "adopt-release-7hgpq": Phase="Running", Reason="", readiness=true. Elapsed: 2.803604ms
Aug 19 14:58:42.453: INFO: Pod "adopt-release-7hgpq": Phase="Running", Reason="", readiness=true. Elapsed: 2.00698893s
Aug 19 14:58:42.453: INFO: Pod "adopt-release-7hgpq" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Aug 19 14:58:42.972: INFO: Successfully updated pod "adopt-release-7hgpq"
STEP: Checking that the Job releases the Pod
Aug 19 14:58:42.972: INFO: Waiting up to 15m0s for pod "adopt-release-7hgpq" in namespace "job-2690" to be "released"
Aug 19 14:58:42.974: INFO: Pod "adopt-release-7hgpq": Phase="Running", Reason="", readiness=true. Elapsed: 2.32413ms
Aug 19 14:58:44.981: INFO: Pod "adopt-release-7hgpq": Phase="Running", Reason="", readiness=true. Elapsed: 2.008636466s
Aug 19 14:58:44.981: INFO: Pod "adopt-release-7hgpq" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Aug 19 14:58:44.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2690" for this suite.

• [SLOW TEST:9.119 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":356,"completed":5,"skipped":58,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 14:58:44.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 14:58:45.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-8934 create -f -'
Aug 19 14:58:46.593: INFO: stderr: ""
Aug 19 14:58:46.593: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 19 14:58:46.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-8934 create -f -'
Aug 19 14:58:46.772: INFO: stderr: ""
Aug 19 14:58:46.773: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 19 14:58:47.778: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 19 14:58:47.778: INFO: Found 0 / 1
Aug 19 14:58:48.777: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 19 14:58:48.777: INFO: Found 0 / 1
Aug 19 14:58:49.778: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 19 14:58:49.778: INFO: Found 1 / 1
Aug 19 14:58:49.778: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 19 14:58:49.780: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 19 14:58:49.780: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 19 14:58:49.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-8934 describe pod agnhost-primary-rbshj'
Aug 19 14:58:49.830: INFO: stderr: ""
Aug 19 14:58:49.830: INFO: stdout: "Name:         agnhost-primary-rbshj\nNamespace:    kubectl-8934\nPriority:     0\nNode:         ip-10-0-131-169.ec2.internal/10.0.131.169\nStart Time:   Fri, 19 Aug 2022 14:58:46 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"openshift-sdn\",\n                    \"interface\": \"eth0\",\n                    \"ips\": [\n                        \"10.128.2.15\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"openshift-sdn\",\n                    \"interface\": \"eth0\",\n                    \"ips\": [\n                        \"10.128.2.15\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           10.128.2.15\nIPs:\n  IP:           10.128.2.15\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://a33a4e4c49388343a518434884343e32063b54baaea660a25eb7c65261e72b49\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.36\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 19 Aug 2022 14:58:48 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kg8zs (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-kg8zs:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       3s    default-scheduler  Successfully assigned kubectl-8934/agnhost-primary-rbshj to ip-10-0-131-169.ec2.internal by ip-10-0-155-87\n  Normal  AddedInterface  1s    multus             Add eth0 [10.128.2.15/23] from openshift-sdn\n  Normal  Pulled          1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.36\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Aug 19 14:58:49.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-8934 describe rc agnhost-primary'
Aug 19 14:58:49.878: INFO: stderr: ""
Aug 19 14:58:49.878: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8934\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.36\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-rbshj\n"
Aug 19 14:58:49.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-8934 describe service agnhost-primary'
Aug 19 14:58:49.923: INFO: stderr: ""
Aug 19 14:58:49.923: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8934\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.30.50.46\nIPs:               172.30.50.46\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.128.2.15:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 19 14:58:49.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-8934 describe node ip-10-0-131-169.ec2.internal'
Aug 19 14:58:50.203: INFO: stderr: ""
Aug 19 14:58:50.203: INFO: stdout: "Name:               ip-10-0-131-169.ec2.internal\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m6i.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-131-169.ec2.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=m6i.xlarge\n                    node.openshift.io/os_id=rhcos\n                    topology.ebs.csi.aws.com/zone=us-east-1a\n                    topology.kubernetes.io/region=us-east-1\n                    topology.kubernetes.io/zone=us-east-1a\nAnnotations:        cloud.network.openshift.io/egress-ipconfig:\n                      [{\"interface\":\"eni-057424bf6e62f57ae\",\"ifaddr\":{\"ipv4\":\"10.0.128.0/20\"},\"capacity\":{\"ipv4\":14,\"ipv6\":15}}]\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-01f6483ef30ff66eb\"}\n                    machine.openshift.io/machine: openshift-machine-api/maszulik-xlrqq-worker-us-east-1a-ffzj6\n                    machineconfiguration.openshift.io/controlPlaneTopology: HighlyAvailable\n                    machineconfiguration.openshift.io/currentConfig: rendered-worker-d99e6d32f26590bb7ff58755e43241b7\n                    machineconfiguration.openshift.io/desiredConfig: rendered-worker-d99e6d32f26590bb7ff58755e43241b7\n                    machineconfiguration.openshift.io/desiredDrain: uncordon-rendered-worker-d99e6d32f26590bb7ff58755e43241b7\n                    machineconfiguration.openshift.io/lastAppliedDrain: uncordon-rendered-worker-d99e6d32f26590bb7ff58755e43241b7\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 19 Aug 2022 14:38:51 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-131-169.ec2.internal\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 19 Aug 2022 14:58:41 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 19 Aug 2022 14:58:44 +0000   Fri, 19 Aug 2022 14:38:51 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 19 Aug 2022 14:58:44 +0000   Fri, 19 Aug 2022 14:38:51 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 19 Aug 2022 14:58:44 +0000   Fri, 19 Aug 2022 14:38:51 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 19 Aug 2022 14:58:44 +0000   Fri, 19 Aug 2022 14:39:21 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.131.169\n  Hostname:     ip-10-0-131-169.ec2.internal\n  InternalDNS:  ip-10-0-131-169.ec2.internal\nCapacity:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         4\n  ephemeral-storage:           125293548Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      16129396Ki\n  pods:                        250\nAllocatable:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         3500m\n  ephemeral-storage:           115470533646\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      14978420Ki\n  pods:                        250\nSystem Info:\n  Machine ID:                             ec25f6395d7da7c0a30809ee747d394e\n  System UUID:                            ec25f639-5d7d-a7c0-a308-09ee747d394e\n  Boot ID:                                b10d7761-5816-49f4-bd2f-bf189d63273b\n  Kernel Version:                         4.18.0-372.19.1.el8_6.x86_64\n  OS Image:                               Red Hat Enterprise Linux CoreOS 411.86.202208112011-0 (Ootpa)\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.24.2-4.rhaos4.11.gitd6283df.el8\n  Kubelet Version:                        v1.24.0+4f0dd4d\n  Kube-Proxy Version:                     v1.24.0+4f0dd4d\nProviderID:                               aws:///us-east-1a/i-01f6483ef30ff66eb\nNon-terminated Pods:                      (22 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  kubectl-8934                            agnhost-primary-rbshj                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  openshift-cluster-csi-drivers           aws-ebs-csi-driver-node-zp4zp                              30m (0%)      0 (0%)      150Mi (1%)       0 (0%)         19m\n  openshift-cluster-node-tuning-operator  tuned-7vdfg                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         19m\n  openshift-console                       downloads-858cc8f4cb-zkm66                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         11m\n  openshift-dns                           dns-default-zk2kr                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         19m\n  openshift-dns                           node-resolver-ttx27                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         19m\n  openshift-image-registry                image-registry-79dbb9c69c-2qmtr                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         17m\n  openshift-image-registry                node-ca-qqztn                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         17m\n  openshift-ingress-canary                ingress-canary-lr86r                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         18m\n  openshift-ingress                       router-default-7664744558-bndls                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         18m\n  openshift-machine-config-operator       machine-config-daemon-gpvls                                40m (1%)      0 (0%)      100Mi (0%)       0 (0%)         19m\n  openshift-monitoring                    node-exporter-tv7s8                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         17m\n  openshift-monitoring                    prometheus-adapter-85bd9549d5-h6mlp                        1m (0%)       0 (0%)      40Mi (0%)        0 (0%)         16m\n  openshift-monitoring                    prometheus-operator-admission-webhook-6db58c58f7-2922r     5m (0%)       0 (0%)      30Mi (0%)        0 (0%)         23m\n  openshift-multus                        multus-additional-cni-plugins-2fqhw                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         19m\n  openshift-multus                        multus-pss88                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         19m\n  openshift-multus                        network-metrics-daemon-bkf7f                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         19m\n  openshift-network-diagnostics           network-check-target-zpzkf                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         19m\n  openshift-sdn                           sdn-vjrw2                                                  110m (3%)     0 (0%)      220Mi (1%)       0 (0%)         19m\n  sonobuoy                                sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         82s\n  sonobuoy                                sonobuoy-e2e-job-b3deceaf6b87402a                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         79s\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-57h77    0 (0%)        0 (0%)      0 (0%)           0 (0%)         79s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests      Limits\n  --------                    --------      ------\n  cpu                         550m (15%)    0 (0%)\n  memory                      1570Mi (10%)  0 (0%)\n  ephemeral-storage           0 (0%)        0 (0%)\n  hugepages-1Gi               0 (0%)        0 (0%)\n  hugepages-2Mi               0 (0%)        0 (0%)\n  attachable-volumes-aws-ebs  0             0\nEvents:\n  Type     Reason                                            Age                From                 Message\n  ----     ------                                            ----               ----                 -------\n  Warning  listen tcp4 :31865: bind: address already in use  19m                kube-proxy           can't open port \"nodePort for openshift-ingress/router-default:https\" (:31865/tcp4), skipping it\n  Warning  listen tcp4 :31318: bind: address already in use  19m                kube-proxy           can't open port \"nodePort for openshift-ingress/router-default:http\" (:31318/tcp4), skipping it\n  Normal   NodeHasNoDiskPressure                             19m (x8 over 20m)  kubelet              Node ip-10-0-131-169.ec2.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientMemory                           19m (x8 over 20m)  kubelet              Node ip-10-0-131-169.ec2.internal status is now: NodeHasSufficientMemory\n  Normal   RegisteredNode                                    19m                node-controller      Node ip-10-0-131-169.ec2.internal event: Registered Node ip-10-0-131-169.ec2.internal in Controller\n  Normal   NodeDone                                          19m                machineconfigdaemon  Setting node ip-10-0-131-169.ec2.internal, currentConfig rendered-worker-d99e6d32f26590bb7ff58755e43241b7 to Done\n  Normal   ConfigDriftMonitorStarted                         19m                machineconfigdaemon  Config Drift Monitor started, watching against rendered-worker-d99e6d32f26590bb7ff58755e43241b7\n  Normal   Uncordon                                          19m                machineconfigdaemon  Update completed for config rendered-worker-d99e6d32f26590bb7ff58755e43241b7 and node has been uncordoned\n  Normal   RegisteredNode                                    18m                node-controller      Node ip-10-0-131-169.ec2.internal event: Registered Node ip-10-0-131-169.ec2.internal in Controller\n  Normal   RegisteredNode                                    17m                node-controller      Node ip-10-0-131-169.ec2.internal event: Registered Node ip-10-0-131-169.ec2.internal in Controller\n  Normal   RegisteredNode                                    14m                node-controller      Node ip-10-0-131-169.ec2.internal event: Registered Node ip-10-0-131-169.ec2.internal in Controller\n  Normal   RegisteredNode                                    11m                node-controller      Node ip-10-0-131-169.ec2.internal event: Registered Node ip-10-0-131-169.ec2.internal in Controller\n  Normal   RegisteredNode                                    10m                node-controller      Node ip-10-0-131-169.ec2.internal event: Registered Node ip-10-0-131-169.ec2.internal in Controller\n"
Aug 19 14:58:50.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-8934 describe namespace kubectl-8934'
Aug 19 14:58:50.263: INFO: stderr: ""
Aug 19 14:58:50.263: INFO: stdout: "Name:         kubectl-8934\nLabels:       e2e-framework=kubectl\n              e2e-run=19c9de2a-8e2e-4851-a822-3738a266eea5\n              kubernetes.io/metadata.name=kubectl-8934\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c27,c9\n              openshift.io/sa.scc.supplemental-groups: 1000720000/10000\n              openshift.io/sa.scc.uid-range: 1000720000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 14:58:50.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8934" for this suite.

• [SLOW TEST:5.285 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1110
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":356,"completed":6,"skipped":87,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 14:58:50.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-9195
STEP: creating service affinity-clusterip in namespace services-9195
STEP: creating replication controller affinity-clusterip in namespace services-9195
I0819 14:58:50.340417      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-9195, replica count: 3
I0819 14:58:53.391496      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 19 14:58:53.398: INFO: Creating new exec pod
Aug 19 14:58:58.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-9195 exec execpod-affinityzqbbx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Aug 19 14:58:58.539: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 19 14:58:58.539: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 14:58:58.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-9195 exec execpod-affinityzqbbx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.238.120 80'
Aug 19 14:58:58.645: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.238.120 80\nConnection to 172.30.238.120 80 port [tcp/http] succeeded!\n"
Aug 19 14:58:58.645: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 14:58:58.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-9195 exec execpod-affinityzqbbx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.238.120:80/ ; done'
Aug 19 14:58:58.787: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.238.120:80/\n"
Aug 19 14:58:58.787: INFO: stdout: "\naffinity-clusterip-d4bsm\naffinity-clusterip-d4bsm\naffinity-clusterip-d4bsm\naffinity-clusterip-d4bsm\naffinity-clusterip-d4bsm\naffinity-clusterip-d4bsm\naffinity-clusterip-d4bsm\naffinity-clusterip-d4bsm\naffinity-clusterip-d4bsm\naffinity-clusterip-d4bsm\naffinity-clusterip-d4bsm\naffinity-clusterip-d4bsm\naffinity-clusterip-d4bsm\naffinity-clusterip-d4bsm\naffinity-clusterip-d4bsm\naffinity-clusterip-d4bsm"
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Received response from host: affinity-clusterip-d4bsm
Aug 19 14:58:58.787: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9195, will wait for the garbage collector to delete the pods
Aug 19 14:58:58.864: INFO: Deleting ReplicationController affinity-clusterip took: 5.654583ms
Aug 19 14:58:58.965: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.939286ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 14:59:01.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9195" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:11.221 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":7,"skipped":103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 14:59:01.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Service
STEP: watching for the Service to be added
Aug 19 14:59:01.681: INFO: Found Service test-service-bh2kq in namespace services-3165 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Aug 19 14:59:01.681: INFO: Service test-service-bh2kq created
STEP: Getting /status
Aug 19 14:59:01.703: INFO: Service test-service-bh2kq has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Aug 19 14:59:01.727: INFO: observed Service test-service-bh2kq in namespace services-3165 with annotations: map[] & LoadBalancer: {[]}
Aug 19 14:59:01.727: INFO: Found Service test-service-bh2kq in namespace services-3165 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Aug 19 14:59:01.727: INFO: Service test-service-bh2kq has service status patched
STEP: updating the ServiceStatus
Aug 19 14:59:01.750: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Aug 19 14:59:01.752: INFO: Observed Service test-service-bh2kq in namespace services-3165 with annotations: map[] & Conditions: {[]}
Aug 19 14:59:01.752: INFO: Observed event: &Service{ObjectMeta:{test-service-bh2kq  services-3165  28d97b69-11c7-4b80-9a69-c69bedb477ce 31039 0 2022-08-19 14:59:01 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-08-19 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-08-19 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.30.28.187,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.30.28.187],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Aug 19 14:59:01.752: INFO: Found Service test-service-bh2kq in namespace services-3165 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 19 14:59:01.752: INFO: Service test-service-bh2kq has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Aug 19 14:59:01.770: INFO: observed Service test-service-bh2kq in namespace services-3165 with labels: map[test-service-static:true]
Aug 19 14:59:01.770: INFO: observed Service test-service-bh2kq in namespace services-3165 with labels: map[test-service-static:true]
Aug 19 14:59:01.770: INFO: observed Service test-service-bh2kq in namespace services-3165 with labels: map[test-service-static:true]
Aug 19 14:59:01.770: INFO: Found Service test-service-bh2kq in namespace services-3165 with labels: map[test-service:patched test-service-static:true]
Aug 19 14:59:01.770: INFO: Service test-service-bh2kq patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Aug 19 14:59:01.853: INFO: Observed event: ADDED
Aug 19 14:59:01.853: INFO: Observed event: MODIFIED
Aug 19 14:59:01.853: INFO: Observed event: MODIFIED
Aug 19 14:59:01.853: INFO: Observed event: MODIFIED
Aug 19 14:59:01.853: INFO: Found Service test-service-bh2kq in namespace services-3165 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Aug 19 14:59:01.853: INFO: Service test-service-bh2kq deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 14:59:01.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3165" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":356,"completed":8,"skipped":145,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 14:59:01.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ForbidConcurrent cronjob
W0819 14:59:01.921612      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Aug 19 15:05:01.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4501" for this suite.

• [SLOW TEST:360.085 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":356,"completed":9,"skipped":154,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:05:01.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 19 15:05:02.041: INFO: Waiting up to 5m0s for pod "pod-2910f44e-90ec-4aff-87e9-8e18d7375eb9" in namespace "emptydir-5188" to be "Succeeded or Failed"
Aug 19 15:05:02.053: INFO: Pod "pod-2910f44e-90ec-4aff-87e9-8e18d7375eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.750951ms
Aug 19 15:05:04.059: INFO: Pod "pod-2910f44e-90ec-4aff-87e9-8e18d7375eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018178334s
Aug 19 15:05:06.066: INFO: Pod "pod-2910f44e-90ec-4aff-87e9-8e18d7375eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024856506s
Aug 19 15:05:08.073: INFO: Pod "pod-2910f44e-90ec-4aff-87e9-8e18d7375eb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03158185s
STEP: Saw pod success
Aug 19 15:05:08.073: INFO: Pod "pod-2910f44e-90ec-4aff-87e9-8e18d7375eb9" satisfied condition "Succeeded or Failed"
Aug 19 15:05:08.076: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-2910f44e-90ec-4aff-87e9-8e18d7375eb9 container test-container: <nil>
STEP: delete the pod
Aug 19 15:05:08.098: INFO: Waiting for pod pod-2910f44e-90ec-4aff-87e9-8e18d7375eb9 to disappear
Aug 19 15:05:08.101: INFO: Pod pod-2910f44e-90ec-4aff-87e9-8e18d7375eb9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 19 15:05:08.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5188" for this suite.

• [SLOW TEST:6.157 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":10,"skipped":159,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:05:08.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:05:08.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Aug 19 15:05:14.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-7035 --namespace=crd-publish-openapi-7035 create -f -'
Aug 19 15:05:15.484: INFO: stderr: ""
Aug 19 15:05:15.484: INFO: stdout: "e2e-test-crd-publish-openapi-5088-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 19 15:05:15.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-7035 --namespace=crd-publish-openapi-7035 delete e2e-test-crd-publish-openapi-5088-crds test-cr'
Aug 19 15:05:15.531: INFO: stderr: ""
Aug 19 15:05:15.531: INFO: stdout: "e2e-test-crd-publish-openapi-5088-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 19 15:05:15.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-7035 --namespace=crd-publish-openapi-7035 apply -f -'
Aug 19 15:05:16.398: INFO: stderr: ""
Aug 19 15:05:16.398: INFO: stdout: "e2e-test-crd-publish-openapi-5088-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 19 15:05:16.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-7035 --namespace=crd-publish-openapi-7035 delete e2e-test-crd-publish-openapi-5088-crds test-cr'
Aug 19 15:05:16.445: INFO: stderr: ""
Aug 19 15:05:16.445: INFO: stdout: "e2e-test-crd-publish-openapi-5088-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 19 15:05:16.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-7035 explain e2e-test-crd-publish-openapi-5088-crds'
Aug 19 15:05:16.591: INFO: stderr: ""
Aug 19 15:05:16.591: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5088-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:05:22.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7035" for this suite.

• [SLOW TEST:14.684 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":356,"completed":11,"skipped":166,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:05:22.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Aug 19 15:05:22.903: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:05:24.907: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:05:26.908: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Aug 19 15:05:26.943: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:05:28.948: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 19 15:05:28.951: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9926 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:05:28.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:05:28.952: INFO: ExecWithOptions: Clientset creation
Aug 19 15:05:28.952: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9926/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 19 15:05:29.008: INFO: Exec stderr: ""
Aug 19 15:05:29.008: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9926 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:05:29.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:05:29.009: INFO: ExecWithOptions: Clientset creation
Aug 19 15:05:29.009: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9926/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 19 15:05:29.062: INFO: Exec stderr: ""
Aug 19 15:05:29.062: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9926 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:05:29.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:05:29.063: INFO: ExecWithOptions: Clientset creation
Aug 19 15:05:29.063: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9926/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 19 15:05:29.135: INFO: Exec stderr: ""
Aug 19 15:05:29.135: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9926 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:05:29.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:05:29.135: INFO: ExecWithOptions: Clientset creation
Aug 19 15:05:29.135: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9926/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 19 15:05:29.188: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 19 15:05:29.188: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9926 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:05:29.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:05:29.189: INFO: ExecWithOptions: Clientset creation
Aug 19 15:05:29.189: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9926/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 19 15:05:29.244: INFO: Exec stderr: ""
Aug 19 15:05:29.244: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9926 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:05:29.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:05:29.244: INFO: ExecWithOptions: Clientset creation
Aug 19 15:05:29.244: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9926/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 19 15:05:29.293: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 19 15:05:29.293: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9926 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:05:29.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:05:29.293: INFO: ExecWithOptions: Clientset creation
Aug 19 15:05:29.293: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9926/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 19 15:05:29.373: INFO: Exec stderr: ""
Aug 19 15:05:29.373: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9926 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:05:29.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:05:29.373: INFO: ExecWithOptions: Clientset creation
Aug 19 15:05:29.373: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9926/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 19 15:05:29.433: INFO: Exec stderr: ""
Aug 19 15:05:29.433: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9926 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:05:29.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:05:29.434: INFO: ExecWithOptions: Clientset creation
Aug 19 15:05:29.434: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9926/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 19 15:05:29.486: INFO: Exec stderr: ""
Aug 19 15:05:29.487: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9926 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:05:29.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:05:29.487: INFO: ExecWithOptions: Clientset creation
Aug 19 15:05:29.487: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9926/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 19 15:05:29.543: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:188
Aug 19 15:05:29.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9926" for this suite.

• [SLOW TEST:6.755 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":12,"skipped":177,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:05:29.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0819 15:06:09.639456      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0819 15:06:09.639471      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 19 15:06:09.639: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 19 15:06:09.639: INFO: Deleting pod "simpletest.rc-2424g" in namespace "gc-8511"
Aug 19 15:06:09.653: INFO: Deleting pod "simpletest.rc-456ks" in namespace "gc-8511"
Aug 19 15:06:09.682: INFO: Deleting pod "simpletest.rc-4fmhb" in namespace "gc-8511"
Aug 19 15:06:09.697: INFO: Deleting pod "simpletest.rc-4xdvg" in namespace "gc-8511"
Aug 19 15:06:09.709: INFO: Deleting pod "simpletest.rc-57952" in namespace "gc-8511"
Aug 19 15:06:09.725: INFO: Deleting pod "simpletest.rc-5s5w5" in namespace "gc-8511"
Aug 19 15:06:09.737: INFO: Deleting pod "simpletest.rc-5vjfb" in namespace "gc-8511"
Aug 19 15:06:09.751: INFO: Deleting pod "simpletest.rc-6gx47" in namespace "gc-8511"
Aug 19 15:06:09.764: INFO: Deleting pod "simpletest.rc-6q5xh" in namespace "gc-8511"
Aug 19 15:06:09.780: INFO: Deleting pod "simpletest.rc-6zx74" in namespace "gc-8511"
Aug 19 15:06:09.795: INFO: Deleting pod "simpletest.rc-72mgv" in namespace "gc-8511"
Aug 19 15:06:09.806: INFO: Deleting pod "simpletest.rc-75l4h" in namespace "gc-8511"
Aug 19 15:06:09.819: INFO: Deleting pod "simpletest.rc-76qqd" in namespace "gc-8511"
Aug 19 15:06:09.832: INFO: Deleting pod "simpletest.rc-77wlr" in namespace "gc-8511"
Aug 19 15:06:09.844: INFO: Deleting pod "simpletest.rc-7dglm" in namespace "gc-8511"
Aug 19 15:06:09.858: INFO: Deleting pod "simpletest.rc-7f9tm" in namespace "gc-8511"
Aug 19 15:06:09.871: INFO: Deleting pod "simpletest.rc-7g6t7" in namespace "gc-8511"
Aug 19 15:06:09.885: INFO: Deleting pod "simpletest.rc-7s9hx" in namespace "gc-8511"
Aug 19 15:06:09.900: INFO: Deleting pod "simpletest.rc-7skqf" in namespace "gc-8511"
Aug 19 15:06:09.910: INFO: Deleting pod "simpletest.rc-88d4f" in namespace "gc-8511"
Aug 19 15:06:09.925: INFO: Deleting pod "simpletest.rc-8924b" in namespace "gc-8511"
Aug 19 15:06:09.943: INFO: Deleting pod "simpletest.rc-898ks" in namespace "gc-8511"
Aug 19 15:06:09.957: INFO: Deleting pod "simpletest.rc-8lqgc" in namespace "gc-8511"
Aug 19 15:06:09.978: INFO: Deleting pod "simpletest.rc-9c2fv" in namespace "gc-8511"
Aug 19 15:06:09.989: INFO: Deleting pod "simpletest.rc-9qf48" in namespace "gc-8511"
Aug 19 15:06:10.002: INFO: Deleting pod "simpletest.rc-b8wjh" in namespace "gc-8511"
Aug 19 15:06:10.034: INFO: Deleting pod "simpletest.rc-bcl8g" in namespace "gc-8511"
Aug 19 15:06:10.058: INFO: Deleting pod "simpletest.rc-bfsr8" in namespace "gc-8511"
Aug 19 15:06:10.087: INFO: Deleting pod "simpletest.rc-bjsg5" in namespace "gc-8511"
Aug 19 15:06:10.132: INFO: Deleting pod "simpletest.rc-bt8rh" in namespace "gc-8511"
Aug 19 15:06:10.169: INFO: Deleting pod "simpletest.rc-bzhtp" in namespace "gc-8511"
Aug 19 15:06:10.196: INFO: Deleting pod "simpletest.rc-ch545" in namespace "gc-8511"
Aug 19 15:06:10.263: INFO: Deleting pod "simpletest.rc-d888l" in namespace "gc-8511"
Aug 19 15:06:10.285: INFO: Deleting pod "simpletest.rc-dk459" in namespace "gc-8511"
Aug 19 15:06:10.303: INFO: Deleting pod "simpletest.rc-dl7sz" in namespace "gc-8511"
Aug 19 15:06:10.317: INFO: Deleting pod "simpletest.rc-dnz77" in namespace "gc-8511"
Aug 19 15:06:10.334: INFO: Deleting pod "simpletest.rc-dp77s" in namespace "gc-8511"
Aug 19 15:06:10.347: INFO: Deleting pod "simpletest.rc-dpllc" in namespace "gc-8511"
Aug 19 15:06:10.360: INFO: Deleting pod "simpletest.rc-dtdtc" in namespace "gc-8511"
Aug 19 15:06:10.371: INFO: Deleting pod "simpletest.rc-dwz8c" in namespace "gc-8511"
Aug 19 15:06:10.387: INFO: Deleting pod "simpletest.rc-fk6mv" in namespace "gc-8511"
Aug 19 15:06:10.399: INFO: Deleting pod "simpletest.rc-flfsw" in namespace "gc-8511"
Aug 19 15:06:10.411: INFO: Deleting pod "simpletest.rc-g9vgc" in namespace "gc-8511"
Aug 19 15:06:10.426: INFO: Deleting pod "simpletest.rc-gb794" in namespace "gc-8511"
Aug 19 15:06:10.438: INFO: Deleting pod "simpletest.rc-gjfh6" in namespace "gc-8511"
Aug 19 15:06:10.453: INFO: Deleting pod "simpletest.rc-gpb8g" in namespace "gc-8511"
Aug 19 15:06:10.468: INFO: Deleting pod "simpletest.rc-hfmcx" in namespace "gc-8511"
Aug 19 15:06:10.480: INFO: Deleting pod "simpletest.rc-hg4qg" in namespace "gc-8511"
Aug 19 15:06:10.495: INFO: Deleting pod "simpletest.rc-hnclm" in namespace "gc-8511"
Aug 19 15:06:10.509: INFO: Deleting pod "simpletest.rc-hxt87" in namespace "gc-8511"
Aug 19 15:06:10.531: INFO: Deleting pod "simpletest.rc-hz2sx" in namespace "gc-8511"
Aug 19 15:06:10.548: INFO: Deleting pod "simpletest.rc-j46vd" in namespace "gc-8511"
Aug 19 15:06:10.562: INFO: Deleting pod "simpletest.rc-jhrmk" in namespace "gc-8511"
Aug 19 15:06:10.575: INFO: Deleting pod "simpletest.rc-jjt2w" in namespace "gc-8511"
Aug 19 15:06:10.593: INFO: Deleting pod "simpletest.rc-jq86f" in namespace "gc-8511"
Aug 19 15:06:10.607: INFO: Deleting pod "simpletest.rc-jqcz4" in namespace "gc-8511"
Aug 19 15:06:10.625: INFO: Deleting pod "simpletest.rc-jqnmp" in namespace "gc-8511"
Aug 19 15:06:10.639: INFO: Deleting pod "simpletest.rc-jzvnq" in namespace "gc-8511"
Aug 19 15:06:10.654: INFO: Deleting pod "simpletest.rc-kblc9" in namespace "gc-8511"
Aug 19 15:06:10.668: INFO: Deleting pod "simpletest.rc-kkf97" in namespace "gc-8511"
Aug 19 15:06:10.680: INFO: Deleting pod "simpletest.rc-ksstq" in namespace "gc-8511"
Aug 19 15:06:10.720: INFO: Deleting pod "simpletest.rc-kvjqp" in namespace "gc-8511"
Aug 19 15:06:10.735: INFO: Deleting pod "simpletest.rc-lz6qh" in namespace "gc-8511"
Aug 19 15:06:10.748: INFO: Deleting pod "simpletest.rc-m88k7" in namespace "gc-8511"
Aug 19 15:06:10.761: INFO: Deleting pod "simpletest.rc-m9sbj" in namespace "gc-8511"
Aug 19 15:06:10.773: INFO: Deleting pod "simpletest.rc-mw6d6" in namespace "gc-8511"
Aug 19 15:06:10.791: INFO: Deleting pod "simpletest.rc-n2fx4" in namespace "gc-8511"
Aug 19 15:06:10.806: INFO: Deleting pod "simpletest.rc-n9pfk" in namespace "gc-8511"
Aug 19 15:06:10.818: INFO: Deleting pod "simpletest.rc-ndfzk" in namespace "gc-8511"
Aug 19 15:06:10.828: INFO: Deleting pod "simpletest.rc-nfvmk" in namespace "gc-8511"
Aug 19 15:06:10.840: INFO: Deleting pod "simpletest.rc-pcl6d" in namespace "gc-8511"
Aug 19 15:06:10.853: INFO: Deleting pod "simpletest.rc-pddsp" in namespace "gc-8511"
Aug 19 15:06:10.865: INFO: Deleting pod "simpletest.rc-psjxg" in namespace "gc-8511"
Aug 19 15:06:10.895: INFO: Deleting pod "simpletest.rc-pwlsj" in namespace "gc-8511"
Aug 19 15:06:10.943: INFO: Deleting pod "simpletest.rc-qtxrw" in namespace "gc-8511"
Aug 19 15:06:10.989: INFO: Deleting pod "simpletest.rc-rc7nm" in namespace "gc-8511"
Aug 19 15:06:11.045: INFO: Deleting pod "simpletest.rc-rcdlp" in namespace "gc-8511"
Aug 19 15:06:11.092: INFO: Deleting pod "simpletest.rc-rhl42" in namespace "gc-8511"
Aug 19 15:06:11.144: INFO: Deleting pod "simpletest.rc-rnnxk" in namespace "gc-8511"
Aug 19 15:06:11.199: INFO: Deleting pod "simpletest.rc-rtq2s" in namespace "gc-8511"
Aug 19 15:06:11.267: INFO: Deleting pod "simpletest.rc-rvp64" in namespace "gc-8511"
Aug 19 15:06:11.301: INFO: Deleting pod "simpletest.rc-sbgcx" in namespace "gc-8511"
Aug 19 15:06:11.338: INFO: Deleting pod "simpletest.rc-sg82r" in namespace "gc-8511"
Aug 19 15:06:11.389: INFO: Deleting pod "simpletest.rc-skqkc" in namespace "gc-8511"
Aug 19 15:06:11.442: INFO: Deleting pod "simpletest.rc-t2xtq" in namespace "gc-8511"
Aug 19 15:06:11.491: INFO: Deleting pod "simpletest.rc-t4gkv" in namespace "gc-8511"
Aug 19 15:06:11.540: INFO: Deleting pod "simpletest.rc-v4z5s" in namespace "gc-8511"
Aug 19 15:06:11.592: INFO: Deleting pod "simpletest.rc-v9fh9" in namespace "gc-8511"
Aug 19 15:06:11.641: INFO: Deleting pod "simpletest.rc-vl8p5" in namespace "gc-8511"
Aug 19 15:06:11.690: INFO: Deleting pod "simpletest.rc-vqxqr" in namespace "gc-8511"
Aug 19 15:06:11.738: INFO: Deleting pod "simpletest.rc-wcmht" in namespace "gc-8511"
Aug 19 15:06:11.805: INFO: Deleting pod "simpletest.rc-wgxvp" in namespace "gc-8511"
Aug 19 15:06:11.844: INFO: Deleting pod "simpletest.rc-x8hpn" in namespace "gc-8511"
Aug 19 15:06:11.896: INFO: Deleting pod "simpletest.rc-xbbl5" in namespace "gc-8511"
Aug 19 15:06:11.944: INFO: Deleting pod "simpletest.rc-xtzwk" in namespace "gc-8511"
Aug 19 15:06:11.991: INFO: Deleting pod "simpletest.rc-xw86p" in namespace "gc-8511"
Aug 19 15:06:12.049: INFO: Deleting pod "simpletest.rc-z4mjz" in namespace "gc-8511"
Aug 19 15:06:12.098: INFO: Deleting pod "simpletest.rc-zxrbs" in namespace "gc-8511"
Aug 19 15:06:12.141: INFO: Deleting pod "simpletest.rc-zxwb5" in namespace "gc-8511"
Aug 19 15:06:12.189: INFO: Deleting pod "simpletest.rc-zzwfc" in namespace "gc-8511"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 19 15:06:12.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8511" for this suite.

• [SLOW TEST:42.829 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":356,"completed":13,"skipped":218,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:06:12.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Aug 19 15:06:12.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:06:43.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1920" for this suite.

• [SLOW TEST:31.392 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":356,"completed":14,"skipped":234,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:06:43.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating secret secrets-5785/secret-test-e0931bc7-c423-4990-8424-a7a62305e789
STEP: Creating a pod to test consume secrets
Aug 19 15:06:43.830: INFO: Waiting up to 5m0s for pod "pod-configmaps-86ed11a7-8a37-45f0-b3c2-5ec9722a5d6f" in namespace "secrets-5785" to be "Succeeded or Failed"
Aug 19 15:06:43.833: INFO: Pod "pod-configmaps-86ed11a7-8a37-45f0-b3c2-5ec9722a5d6f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.632086ms
Aug 19 15:06:45.838: INFO: Pod "pod-configmaps-86ed11a7-8a37-45f0-b3c2-5ec9722a5d6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008615423s
Aug 19 15:06:47.842: INFO: Pod "pod-configmaps-86ed11a7-8a37-45f0-b3c2-5ec9722a5d6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012624315s
Aug 19 15:06:49.846: INFO: Pod "pod-configmaps-86ed11a7-8a37-45f0-b3c2-5ec9722a5d6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01623806s
STEP: Saw pod success
Aug 19 15:06:49.846: INFO: Pod "pod-configmaps-86ed11a7-8a37-45f0-b3c2-5ec9722a5d6f" satisfied condition "Succeeded or Failed"
Aug 19 15:06:49.849: INFO: Trying to get logs from node ip-10-0-157-99.ec2.internal pod pod-configmaps-86ed11a7-8a37-45f0-b3c2-5ec9722a5d6f container env-test: <nil>
STEP: delete the pod
Aug 19 15:06:49.882: INFO: Waiting for pod pod-configmaps-86ed11a7-8a37-45f0-b3c2-5ec9722a5d6f to disappear
Aug 19 15:06:49.885: INFO: Pod pod-configmaps-86ed11a7-8a37-45f0-b3c2-5ec9722a5d6f no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Aug 19 15:06:49.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5785" for this suite.

• [SLOW TEST:6.122 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":15,"skipped":263,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:06:49.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 19 15:06:50.200: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Aug 19 15:06:52.211: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 15, 6, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 6, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 15, 6, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 6, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-677b6dd845\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 15:06:55.226: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:06:55.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:06:58.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9585" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:8.565 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":356,"completed":16,"skipped":274,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:06:58.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:06:58.503: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
W0819 15:06:58.521314      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 15:06:58.544: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 19 15:07:03.550: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 19 15:07:05.559: INFO: Creating deployment "test-rolling-update-deployment"
Aug 19 15:07:05.565: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 19 15:07:05.571: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 19 15:07:07.579: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 19 15:07:07.582: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 15, 7, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 7, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 15, 7, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 7, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-5579c56cf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 19 15:07:09.586: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 19 15:07:09.600: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5478  6cd75234-da83-4777-b681-45ca6c31c280 37188 1 2022-08-19 15:07:05 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-08-19 15:07:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 15:07:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00230c4c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-19 15:07:05 +0000 UTC,LastTransitionTime:2022-08-19 15:07:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-5579c56cf8" has successfully progressed.,LastUpdateTime:2022-08-19 15:07:08 +0000 UTC,LastTransitionTime:2022-08-19 15:07:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 19 15:07:09.603: INFO: New ReplicaSet "test-rolling-update-deployment-5579c56cf8" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-5579c56cf8  deployment-5478  bca95906-4d43-4d82-8eb8-5bb27ff5d7d3 37178 1 2022-08-19 15:07:05 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:5579c56cf8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 6cd75234-da83-4777-b681-45ca6c31c280 0xc0014fc8b7 0xc0014fc8b8}] []  [{kube-controller-manager Update apps/v1 2022-08-19 15:07:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cd75234-da83-4777-b681-45ca6c31c280\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 15:07:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 5579c56cf8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:5579c56cf8] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0014fc9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 19 15:07:09.603: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 19 15:07:09.603: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5478  43cb2e87-eb11-4686-af42-df8be72007e0 37187 2 2022-08-19 15:06:58 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 6cd75234-da83-4777-b681-45ca6c31c280 0xc0014fc5b7 0xc0014fc5b8}] []  [{e2e.test Update apps/v1 2022-08-19 15:06:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 15:07:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cd75234-da83-4777-b681-45ca6c31c280\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-08-19 15:07:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0014fc7d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 19 15:07:09.606: INFO: Pod "test-rolling-update-deployment-5579c56cf8-4qxxq" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-5579c56cf8-4qxxq test-rolling-update-deployment-5579c56cf8- deployment-5478  36bff491-5dc7-4c29-b72a-45039488e27d 37177 0 2022-08-19 15:07:05 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:5579c56cf8] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.59"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.59"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-5579c56cf8 bca95906-4d43-4d82-8eb8-5bb27ff5d7d3 0xc0011b26f7 0xc0011b26f8}] []  [{kube-controller-manager Update v1 2022-08-19 15:07:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bca95906-4d43-4d82-8eb8-5bb27ff5d7d3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-19 15:07:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-08-19 15:07:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vkchq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vkchq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-157-99.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c29,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9bphl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:07:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:07:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:07:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:07:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.157.99,PodIP:10.129.2.59,StartTime:2022-08-19 15:07:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 15:07:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403,ContainerID:cri-o://9156d4199532ddbd61b9fd93b8ebfeaf0b822ce0381d8e917fa967e0fcb87395,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 19 15:07:09.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5478" for this suite.

• [SLOW TEST:11.154 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":17,"skipped":308,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:07:09.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Aug 19 15:07:09.664: INFO: Waiting up to 5m0s for pod "downward-api-64fe71f2-e3e6-42b7-94dc-1a29e96b9924" in namespace "downward-api-4338" to be "Succeeded or Failed"
Aug 19 15:07:09.677: INFO: Pod "downward-api-64fe71f2-e3e6-42b7-94dc-1a29e96b9924": Phase="Pending", Reason="", readiness=false. Elapsed: 12.871643ms
Aug 19 15:07:11.682: INFO: Pod "downward-api-64fe71f2-e3e6-42b7-94dc-1a29e96b9924": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01805767s
Aug 19 15:07:13.687: INFO: Pod "downward-api-64fe71f2-e3e6-42b7-94dc-1a29e96b9924": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022771904s
Aug 19 15:07:15.692: INFO: Pod "downward-api-64fe71f2-e3e6-42b7-94dc-1a29e96b9924": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027405948s
STEP: Saw pod success
Aug 19 15:07:15.692: INFO: Pod "downward-api-64fe71f2-e3e6-42b7-94dc-1a29e96b9924" satisfied condition "Succeeded or Failed"
Aug 19 15:07:15.695: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downward-api-64fe71f2-e3e6-42b7-94dc-1a29e96b9924 container dapi-container: <nil>
STEP: delete the pod
Aug 19 15:07:15.717: INFO: Waiting for pod downward-api-64fe71f2-e3e6-42b7-94dc-1a29e96b9924 to disappear
Aug 19 15:07:15.721: INFO: Pod downward-api-64fe71f2-e3e6-42b7-94dc-1a29e96b9924 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Aug 19 15:07:15.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4338" for this suite.

• [SLOW TEST:6.115 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":356,"completed":18,"skipped":332,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:07:15.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0819 15:07:15.778121      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "srv" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "srv" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "srv" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "srv" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 15:07:15.782: INFO: The status of Pod server-envvars-d808cfeb-685c-48d8-ab59-cf67b3d17daa is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:07:17.786: INFO: The status of Pod server-envvars-d808cfeb-685c-48d8-ab59-cf67b3d17daa is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:07:19.786: INFO: The status of Pod server-envvars-d808cfeb-685c-48d8-ab59-cf67b3d17daa is Running (Ready = true)
Aug 19 15:07:19.823: INFO: Waiting up to 5m0s for pod "client-envvars-c2dd60f2-b1a7-4d88-a335-79211a0c75f5" in namespace "pods-30" to be "Succeeded or Failed"
Aug 19 15:07:19.829: INFO: Pod "client-envvars-c2dd60f2-b1a7-4d88-a335-79211a0c75f5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.057648ms
Aug 19 15:07:21.832: INFO: Pod "client-envvars-c2dd60f2-b1a7-4d88-a335-79211a0c75f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008722998s
Aug 19 15:07:23.835: INFO: Pod "client-envvars-c2dd60f2-b1a7-4d88-a335-79211a0c75f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011956182s
Aug 19 15:07:25.839: INFO: Pod "client-envvars-c2dd60f2-b1a7-4d88-a335-79211a0c75f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015913123s
STEP: Saw pod success
Aug 19 15:07:25.839: INFO: Pod "client-envvars-c2dd60f2-b1a7-4d88-a335-79211a0c75f5" satisfied condition "Succeeded or Failed"
Aug 19 15:07:25.843: INFO: Trying to get logs from node ip-10-0-157-99.ec2.internal pod client-envvars-c2dd60f2-b1a7-4d88-a335-79211a0c75f5 container env3cont: <nil>
STEP: delete the pod
Aug 19 15:07:25.866: INFO: Waiting for pod client-envvars-c2dd60f2-b1a7-4d88-a335-79211a0c75f5 to disappear
Aug 19 15:07:25.870: INFO: Pod client-envvars-c2dd60f2-b1a7-4d88-a335-79211a0c75f5 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 19 15:07:25.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-30" for this suite.

• [SLOW TEST:10.147 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":356,"completed":19,"skipped":367,"failed":0}
S
------------------------------
[sig-node] Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:07:25.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Aug 19 15:07:29.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-718" for this suite.
•{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":356,"completed":20,"skipped":368,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:07:29.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:07:30.024: INFO: The status of Pod busybox-readonly-fs07522950-b76d-426d-8d1f-b8d4f4171111 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:07:32.028: INFO: The status of Pod busybox-readonly-fs07522950-b76d-426d-8d1f-b8d4f4171111 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:07:34.027: INFO: The status of Pod busybox-readonly-fs07522950-b76d-426d-8d1f-b8d4f4171111 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Aug 19 15:07:34.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7453" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":21,"skipped":378,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:07:34.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 19 15:07:42.145: INFO: DNS probes using dns-3177/dns-test-76dd5c03-075b-489e-92b8-d4379e4b4124 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 19 15:07:42.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3177" for this suite.

• [SLOW TEST:8.132 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":356,"completed":22,"skipped":387,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:07:42.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
W0819 15:07:42.230213      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "bin-falsef80d51dc-b076-4c8c-8746-cb8fc8fbcec4" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "bin-falsef80d51dc-b076-4c8c-8746-cb8fc8fbcec4" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "bin-falsef80d51dc-b076-4c8c-8746-cb8fc8fbcec4" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "bin-falsef80d51dc-b076-4c8c-8746-cb8fc8fbcec4" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Aug 19 15:07:42.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8286" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":356,"completed":23,"skipped":418,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:07:42.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-58b58f6c-d272-4e5a-82bc-8810c9198030
STEP: Creating a pod to test consume secrets
Aug 19 15:07:42.327: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-da230b52-e01b-4bc1-86cd-8df845053775" in namespace "projected-7297" to be "Succeeded or Failed"
Aug 19 15:07:42.336: INFO: Pod "pod-projected-secrets-da230b52-e01b-4bc1-86cd-8df845053775": Phase="Pending", Reason="", readiness=false. Elapsed: 8.483012ms
Aug 19 15:07:44.340: INFO: Pod "pod-projected-secrets-da230b52-e01b-4bc1-86cd-8df845053775": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012877294s
Aug 19 15:07:46.345: INFO: Pod "pod-projected-secrets-da230b52-e01b-4bc1-86cd-8df845053775": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017175626s
Aug 19 15:07:48.349: INFO: Pod "pod-projected-secrets-da230b52-e01b-4bc1-86cd-8df845053775": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021904504s
STEP: Saw pod success
Aug 19 15:07:48.349: INFO: Pod "pod-projected-secrets-da230b52-e01b-4bc1-86cd-8df845053775" satisfied condition "Succeeded or Failed"
Aug 19 15:07:48.353: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-projected-secrets-da230b52-e01b-4bc1-86cd-8df845053775 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 19 15:07:48.370: INFO: Waiting for pod pod-projected-secrets-da230b52-e01b-4bc1-86cd-8df845053775 to disappear
Aug 19 15:07:48.373: INFO: Pod pod-projected-secrets-da230b52-e01b-4bc1-86cd-8df845053775 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 19 15:07:48.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7297" for this suite.

• [SLOW TEST:6.109 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":24,"skipped":428,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:07:48.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Aug 19 15:07:48.455: INFO: The status of Pod annotationupdatea92ef91b-cc6c-4ee5-aa3c-56b51c1d5640 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:07:50.459: INFO: The status of Pod annotationupdatea92ef91b-cc6c-4ee5-aa3c-56b51c1d5640 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:07:52.459: INFO: The status of Pod annotationupdatea92ef91b-cc6c-4ee5-aa3c-56b51c1d5640 is Running (Ready = true)
Aug 19 15:07:52.986: INFO: Successfully updated pod "annotationupdatea92ef91b-cc6c-4ee5-aa3c-56b51c1d5640"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 19 15:07:55.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5333" for this suite.

• [SLOW TEST:6.627 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":25,"skipped":433,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:07:55.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service nodeport-test with type=NodePort in namespace services-7780
STEP: creating replication controller nodeport-test in namespace services-7780
I0819 15:07:55.086745      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7780, replica count: 2
I0819 15:07:58.138606      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 19 15:08:01.139: INFO: Creating new exec pod
I0819 15:08:01.139561      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 19 15:08:06.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-7780 exec execpodzj2lf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Aug 19 15:08:06.290: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 19 15:08:06.290: INFO: stdout: "nodeport-test-vm9vj"
Aug 19 15:08:06.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-7780 exec execpodzj2lf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.230.32 80'
Aug 19 15:08:06.380: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.230.32 80\nConnection to 172.30.230.32 80 port [tcp/http] succeeded!\n"
Aug 19 15:08:06.380: INFO: stdout: "nodeport-test-vm9vj"
Aug 19 15:08:06.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-7780 exec execpodzj2lf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.157.99 30779'
Aug 19 15:08:06.478: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.157.99 30779\nConnection to 10.0.157.99 30779 port [tcp/*] succeeded!\n"
Aug 19 15:08:06.478: INFO: stdout: ""
Aug 19 15:08:07.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-7780 exec execpodzj2lf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.157.99 30779'
Aug 19 15:08:07.594: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.157.99 30779\nConnection to 10.0.157.99 30779 port [tcp/*] succeeded!\n"
Aug 19 15:08:07.594: INFO: stdout: "nodeport-test-rfdmw"
Aug 19 15:08:07.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-7780 exec execpodzj2lf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.164.144 30779'
Aug 19 15:08:07.702: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.164.144 30779\nConnection to 10.0.164.144 30779 port [tcp/*] succeeded!\n"
Aug 19 15:08:07.702: INFO: stdout: "nodeport-test-rfdmw"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 15:08:07.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7780" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:12.701 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":356,"completed":26,"skipped":438,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:08:07.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0819 15:08:18.318073      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0819 15:08:18.318088      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 19 15:08:18.318: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 19 15:08:18.318: INFO: Deleting pod "simpletest-rc-to-be-deleted-2dg6r" in namespace "gc-9390"
Aug 19 15:08:18.338: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gnqs" in namespace "gc-9390"
Aug 19 15:08:18.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mf4b" in namespace "gc-9390"
Aug 19 15:08:18.421: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qpll" in namespace "gc-9390"
Aug 19 15:08:18.434: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tz57" in namespace "gc-9390"
Aug 19 15:08:18.448: INFO: Deleting pod "simpletest-rc-to-be-deleted-44lg8" in namespace "gc-9390"
Aug 19 15:08:18.476: INFO: Deleting pod "simpletest-rc-to-be-deleted-45k5b" in namespace "gc-9390"
Aug 19 15:08:18.487: INFO: Deleting pod "simpletest-rc-to-be-deleted-466dv" in namespace "gc-9390"
Aug 19 15:08:18.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ft4n" in namespace "gc-9390"
Aug 19 15:08:18.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hs25" in namespace "gc-9390"
Aug 19 15:08:18.530: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jc2m" in namespace "gc-9390"
Aug 19 15:08:18.543: INFO: Deleting pod "simpletest-rc-to-be-deleted-4plcg" in namespace "gc-9390"
Aug 19 15:08:18.556: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mtcb" in namespace "gc-9390"
Aug 19 15:08:18.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pml2" in namespace "gc-9390"
Aug 19 15:08:18.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-62w8c" in namespace "gc-9390"
Aug 19 15:08:18.597: INFO: Deleting pod "simpletest-rc-to-be-deleted-68dn2" in namespace "gc-9390"
Aug 19 15:08:18.612: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zjm6" in namespace "gc-9390"
Aug 19 15:08:18.632: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kc49" in namespace "gc-9390"
Aug 19 15:08:18.650: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qb6w" in namespace "gc-9390"
Aug 19 15:08:18.663: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rwdv" in namespace "gc-9390"
Aug 19 15:08:18.679: INFO: Deleting pod "simpletest-rc-to-be-deleted-96rgn" in namespace "gc-9390"
Aug 19 15:08:18.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dmll" in namespace "gc-9390"
Aug 19 15:08:18.704: INFO: Deleting pod "simpletest-rc-to-be-deleted-9g6d7" in namespace "gc-9390"
Aug 19 15:08:18.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gbm6" in namespace "gc-9390"
Aug 19 15:08:18.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wkz9" in namespace "gc-9390"
Aug 19 15:08:18.744: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9f8s" in namespace "gc-9390"
Aug 19 15:08:18.755: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcf6n" in namespace "gc-9390"
Aug 19 15:08:18.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdnjq" in namespace "gc-9390"
Aug 19 15:08:18.781: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvssv" in namespace "gc-9390"
Aug 19 15:08:18.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccdbb" in namespace "gc-9390"
Aug 19 15:08:18.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-clmvp" in namespace "gc-9390"
Aug 19 15:08:18.820: INFO: Deleting pod "simpletest-rc-to-be-deleted-cq4r4" in namespace "gc-9390"
Aug 19 15:08:18.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6lv5" in namespace "gc-9390"
Aug 19 15:08:18.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-dc9vg" in namespace "gc-9390"
Aug 19 15:08:18.873: INFO: Deleting pod "simpletest-rc-to-be-deleted-dd2lz" in namespace "gc-9390"
Aug 19 15:08:18.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-dg95z" in namespace "gc-9390"
Aug 19 15:08:18.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-dn99m" in namespace "gc-9390"
Aug 19 15:08:18.910: INFO: Deleting pod "simpletest-rc-to-be-deleted-drdqr" in namespace "gc-9390"
Aug 19 15:08:18.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhm5m" in namespace "gc-9390"
Aug 19 15:08:18.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhqjl" in namespace "gc-9390"
Aug 19 15:08:18.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs6cg" in namespace "gc-9390"
Aug 19 15:08:18.977: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwq6n" in namespace "gc-9390"
Aug 19 15:08:18.993: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwrbd" in namespace "gc-9390"
Aug 19 15:08:19.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx4q7" in namespace "gc-9390"
Aug 19 15:08:19.031: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7jkw" in namespace "gc-9390"
Aug 19 15:08:19.043: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfsgl" in namespace "gc-9390"
Aug 19 15:08:19.068: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkkm7" in namespace "gc-9390"
Aug 19 15:08:19.160: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmx4n" in namespace "gc-9390"
Aug 19 15:08:19.172: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxcm7" in namespace "gc-9390"
Aug 19 15:08:19.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-h67qs" in namespace "gc-9390"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 19 15:08:19.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9390" for this suite.

• [SLOW TEST:11.510 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":356,"completed":27,"skipped":442,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:08:19.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Aug 19 15:08:19.359: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7826  1c3ae8af-7756-4acc-9935-9eb267e2b0e6 39793 0 2022-08-19 15:08:19 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2022-08-19 15:08:19 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lzvwm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lzvwm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c31,c5,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 15:08:19.363: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:08:21.375: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:08:23.368: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Aug 19 15:08:23.368: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7826 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:08:23.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:08:23.369: INFO: ExecWithOptions: Clientset creation
Aug 19 15:08:23.369: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-7826/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod...
Aug 19 15:08:23.442: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7826 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:08:23.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:08:23.442: INFO: ExecWithOptions: Clientset creation
Aug 19 15:08:23.442: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-7826/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 19 15:08:23.513: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 19 15:08:23.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7826" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":356,"completed":28,"skipped":444,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:08:23.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:08:23.611: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 19 15:08:23.623: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:23.623: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Aug 19 15:08:23.659: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:23.659: INFO: Node ip-10-0-164-144.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:08:24.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:24.664: INFO: Node ip-10-0-164-144.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:08:25.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:25.664: INFO: Node ip-10-0-164-144.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:08:26.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:26.664: INFO: Node ip-10-0-164-144.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:08:27.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 19 15:08:27.664: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 19 15:08:27.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 19 15:08:27.683: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Aug 19 15:08:28.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:28.687: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 19 15:08:28.698: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:28.698: INFO: Node ip-10-0-164-144.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:08:29.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:29.702: INFO: Node ip-10-0-164-144.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:08:30.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:30.702: INFO: Node ip-10-0-164-144.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:08:31.703: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:31.703: INFO: Node ip-10-0-164-144.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:08:32.703: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:32.703: INFO: Node ip-10-0-164-144.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:08:33.703: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 19 15:08:33.703: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1593, will wait for the garbage collector to delete the pods
Aug 19 15:08:33.771: INFO: Deleting DaemonSet.extensions daemon-set took: 7.154441ms
Aug 19 15:08:33.872: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.805507ms
Aug 19 15:08:35.976: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:35.976: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 19 15:08:35.980: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40390"},"items":null}

Aug 19 15:08:35.986: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40390"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 19 15:08:36.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1593" for this suite.

• [SLOW TEST:12.506 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":356,"completed":29,"skipped":471,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:08:36.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 19 15:08:36.181: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:36.181: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:36.181: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:36.188: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:36.188: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:08:37.193: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:37.193: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:37.193: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:37.197: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:37.197: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:08:38.194: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:38.195: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:38.195: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:38.202: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:38.202: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:08:39.196: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:39.196: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:39.196: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:39.199: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 19 15:08:39.199: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:08:40.196: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:40.196: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:40.196: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:40.199: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 19 15:08:40.199: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:08:41.194: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:41.194: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:41.194: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:41.198: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 19 15:08:41.198: INFO: Node ip-10-0-157-99.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:08:42.193: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:42.193: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:42.193: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:42.197: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 19 15:08:42.197: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 19 15:08:42.215: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:42.215: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:42.215: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:08:42.221: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 19 15:08:42.221: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4426, will wait for the garbage collector to delete the pods
Aug 19 15:08:42.287: INFO: Deleting DaemonSet.extensions daemon-set took: 5.815863ms
Aug 19 15:08:42.388: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.405168ms
Aug 19 15:08:45.392: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:08:45.392: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 19 15:08:45.395: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40606"},"items":null}

Aug 19 15:08:45.398: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40606"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 19 15:08:45.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4426" for this suite.

• [SLOW TEST:9.377 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":356,"completed":30,"skipped":472,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:08:45.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-ea4ccbdf-5ae6-46c6-950e-3287f9654e0a
STEP: Creating a pod to test consume configMaps
Aug 19 15:08:45.486: INFO: Waiting up to 5m0s for pod "pod-configmaps-01d287b1-9815-4482-b6e3-6f95a158d73a" in namespace "configmap-3070" to be "Succeeded or Failed"
Aug 19 15:08:45.494: INFO: Pod "pod-configmaps-01d287b1-9815-4482-b6e3-6f95a158d73a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.769956ms
Aug 19 15:08:47.499: INFO: Pod "pod-configmaps-01d287b1-9815-4482-b6e3-6f95a158d73a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012643619s
Aug 19 15:08:49.502: INFO: Pod "pod-configmaps-01d287b1-9815-4482-b6e3-6f95a158d73a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015942172s
Aug 19 15:08:51.507: INFO: Pod "pod-configmaps-01d287b1-9815-4482-b6e3-6f95a158d73a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02052827s
STEP: Saw pod success
Aug 19 15:08:51.507: INFO: Pod "pod-configmaps-01d287b1-9815-4482-b6e3-6f95a158d73a" satisfied condition "Succeeded or Failed"
Aug 19 15:08:51.510: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-configmaps-01d287b1-9815-4482-b6e3-6f95a158d73a container agnhost-container: <nil>
STEP: delete the pod
Aug 19 15:08:51.526: INFO: Waiting for pod pod-configmaps-01d287b1-9815-4482-b6e3-6f95a158d73a to disappear
Aug 19 15:08:51.529: INFO: Pod pod-configmaps-01d287b1-9815-4482-b6e3-6f95a158d73a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 19 15:08:51.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3070" for this suite.

• [SLOW TEST:6.116 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":31,"skipped":493,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:08:51.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 19 15:08:51.585: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76097067-42a0-4219-86c6-be5732a15fc4" in namespace "projected-3987" to be "Succeeded or Failed"
Aug 19 15:08:51.594: INFO: Pod "downwardapi-volume-76097067-42a0-4219-86c6-be5732a15fc4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.48228ms
Aug 19 15:08:53.598: INFO: Pod "downwardapi-volume-76097067-42a0-4219-86c6-be5732a15fc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013619855s
Aug 19 15:08:55.603: INFO: Pod "downwardapi-volume-76097067-42a0-4219-86c6-be5732a15fc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018633559s
STEP: Saw pod success
Aug 19 15:08:55.603: INFO: Pod "downwardapi-volume-76097067-42a0-4219-86c6-be5732a15fc4" satisfied condition "Succeeded or Failed"
Aug 19 15:08:55.607: INFO: Trying to get logs from node ip-10-0-157-99.ec2.internal pod downwardapi-volume-76097067-42a0-4219-86c6-be5732a15fc4 container client-container: <nil>
STEP: delete the pod
Aug 19 15:08:55.625: INFO: Waiting for pod downwardapi-volume-76097067-42a0-4219-86c6-be5732a15fc4 to disappear
Aug 19 15:08:55.629: INFO: Pod downwardapi-volume-76097067-42a0-4219-86c6-be5732a15fc4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 19 15:08:55.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3987" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":32,"skipped":502,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:08:55.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 15:08:55.952: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 15:08:58.974: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Aug 19 15:09:03.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=webhook-6364 attach --namespace=webhook-6364 to-be-attached-pod -i -c=container1'
Aug 19 15:09:03.069: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:09:03.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6364" for this suite.
STEP: Destroying namespace "webhook-6364-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.506 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":356,"completed":33,"skipped":525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:09:03.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 19 15:09:03.245: INFO: The status of Pod pod-update-52a4d921-1615-49d2-bcfc-e2d357a0a699 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:09:05.250: INFO: The status of Pod pod-update-52a4d921-1615-49d2-bcfc-e2d357a0a699 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:09:07.250: INFO: The status of Pod pod-update-52a4d921-1615-49d2-bcfc-e2d357a0a699 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 19 15:09:07.774: INFO: Successfully updated pod "pod-update-52a4d921-1615-49d2-bcfc-e2d357a0a699"
STEP: verifying the updated pod is in kubernetes
Aug 19 15:09:07.781: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 19 15:09:07.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6388" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":356,"completed":34,"skipped":584,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:09:07.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4261
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-4261
Aug 19 15:09:07.849: INFO: Found 0 stateful pods, waiting for 1
Aug 19 15:09:17.854: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Aug 19 15:09:17.872: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Aug 19 15:09:17.881: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Aug 19 15:09:17.882: INFO: Observed &StatefulSet event: ADDED
Aug 19 15:09:17.882: INFO: Found Statefulset ss in namespace statefulset-4261 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 19 15:09:17.882: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Aug 19 15:09:17.882: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 19 15:09:17.888: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Aug 19 15:09:17.890: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 19 15:09:17.890: INFO: Deleting all statefulset in ns statefulset-4261
Aug 19 15:09:17.893: INFO: Scaling statefulset ss to 0
Aug 19 15:09:27.908: INFO: Waiting for statefulset status.replicas updated to 0
Aug 19 15:09:27.911: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 19 15:09:27.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4261" for this suite.

• [SLOW TEST:20.144 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":356,"completed":35,"skipped":587,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:09:27.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-2f3ed6f1-e4da-4af7-acd9-bbf0777ad96c
STEP: Creating a pod to test consume secrets
Aug 19 15:09:28.070: INFO: Waiting up to 5m0s for pod "pod-secrets-ef4e0cd0-d4b4-4a4a-b0bd-f89f4c95640a" in namespace "secrets-9062" to be "Succeeded or Failed"
Aug 19 15:09:28.077: INFO: Pod "pod-secrets-ef4e0cd0-d4b4-4a4a-b0bd-f89f4c95640a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.833365ms
Aug 19 15:09:30.082: INFO: Pod "pod-secrets-ef4e0cd0-d4b4-4a4a-b0bd-f89f4c95640a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011080017s
Aug 19 15:09:32.087: INFO: Pod "pod-secrets-ef4e0cd0-d4b4-4a4a-b0bd-f89f4c95640a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016329627s
Aug 19 15:09:34.091: INFO: Pod "pod-secrets-ef4e0cd0-d4b4-4a4a-b0bd-f89f4c95640a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021004436s
STEP: Saw pod success
Aug 19 15:09:34.091: INFO: Pod "pod-secrets-ef4e0cd0-d4b4-4a4a-b0bd-f89f4c95640a" satisfied condition "Succeeded or Failed"
Aug 19 15:09:34.096: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-secrets-ef4e0cd0-d4b4-4a4a-b0bd-f89f4c95640a container secret-volume-test: <nil>
STEP: delete the pod
Aug 19 15:09:34.113: INFO: Waiting for pod pod-secrets-ef4e0cd0-d4b4-4a4a-b0bd-f89f4c95640a to disappear
Aug 19 15:09:34.116: INFO: Pod pod-secrets-ef4e0cd0-d4b4-4a4a-b0bd-f89f4c95640a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 19 15:09:34.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9062" for this suite.
STEP: Destroying namespace "secret-namespace-2076" for this suite.

• [SLOW TEST:6.200 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":356,"completed":36,"skipped":605,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:09:34.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 19 15:09:34.236: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:09:34.236: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:09:34.236: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:09:34.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:09:34.239: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:09:35.245: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:09:35.245: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:09:35.245: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:09:35.248: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:09:35.248: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:09:36.245: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:09:36.245: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:09:36.245: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:09:36.249: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 19 15:09:36.249: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:09:37.247: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:09:37.247: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:09:37.247: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:09:37.250: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 19 15:09:37.250: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:09:38.246: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:09:38.246: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:09:38.246: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:09:38.249: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 19 15:09:38.249: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Aug 19 15:09:38.278: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41598"},"items":null}

Aug 19 15:09:38.283: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41599"},"items":[{"metadata":{"name":"daemon-set-667gk","generateName":"daemon-set-","namespace":"daemonsets-6305","uid":"d7c53193-8ab6-467e-931b-49255ef24a12","resourceVersion":"41573","creationTimestamp":"2022-08-19T15:09:34Z","labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.0.104\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.0.104\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"77e55f77-6ed4-4275-8058-542e6ecf50a5","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-08-19T15:09:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77e55f77-6ed4-4275-8058-542e6ecf50a5\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-08-19T15:09:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-08-19T15:09:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gh4qr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gh4qr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-164-144.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c32,c29"}},"imagePullSecrets":[{"name":"default-dockercfg-bbrcv"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-164-144.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-19T15:09:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-19T15:09:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-19T15:09:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-19T15:09:34Z"}],"hostIP":"10.0.164.144","podIP":"10.131.0.104","podIPs":[{"ip":"10.131.0.104"}],"startTime":"2022-08-19T15:09:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-08-19T15:09:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://7a206ed63b832fd390ae5d472e2bd96bf0724ee5405235351038379118da2d3c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-bq4ld","generateName":"daemon-set-","namespace":"daemonsets-6305","uid":"f9d7b26b-974c-4a7e-b82d-fee4502fa295","resourceVersion":"41593","creationTimestamp":"2022-08-19T15:09:34Z","labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.129.2.103\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.129.2.103\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"77e55f77-6ed4-4275-8058-542e6ecf50a5","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-08-19T15:09:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77e55f77-6ed4-4275-8058-542e6ecf50a5\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-08-19T15:09:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-08-19T15:09:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8h58h","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8h58h","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-157-99.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c32,c29"}},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-157-99.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-19T15:09:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-19T15:09:37Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-19T15:09:37Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-19T15:09:34Z"}],"hostIP":"10.0.157.99","podIP":"10.129.2.103","podIPs":[{"ip":"10.129.2.103"}],"startTime":"2022-08-19T15:09:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-08-19T15:09:36Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://b90b54968831b9cc7af9fe722ca53bed32946037c0965d9be140fdb62147d15d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-v8llr","generateName":"daemon-set-","namespace":"daemonsets-6305","uid":"68bc39e7-63a1-4698-bba0-dd375cbdf58b","resourceVersion":"41599","creationTimestamp":"2022-08-19T15:09:34Z","deletionTimestamp":"2022-08-19T15:10:08Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.2.87\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.2.87\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"77e55f77-6ed4-4275-8058-542e6ecf50a5","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-08-19T15:09:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77e55f77-6ed4-4275-8058-542e6ecf50a5\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-08-19T15:09:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-08-19T15:09:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-mtw4f","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-mtw4f","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-131-169.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c32,c29"}},"imagePullSecrets":[{"name":"default-dockercfg-bbrcv"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-131-169.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-19T15:09:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-19T15:09:37Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-19T15:09:37Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-19T15:09:34Z"}],"hostIP":"10.0.131.169","podIP":"10.128.2.87","podIPs":[{"ip":"10.128.2.87"}],"startTime":"2022-08-19T15:09:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-08-19T15:09:37Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://656e2a5af818427c9745da741cea5f2a0a46c008b1ead7cbec5762f7ffa3a542","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 19 15:09:38.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6305" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":356,"completed":37,"skipped":641,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:09:38.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
W0819 15:09:38.361542      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 19 15:09:43.388: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Aug 19 15:09:43.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6558" for this suite.

• [SLOW TEST:5.105 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":38,"skipped":651,"failed":0}
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:09:43.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Aug 19 15:09:43.437: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 19 15:10:43.553: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:10:43.559: INFO: Starting informer...
STEP: Starting pod...
Aug 19 15:10:43.779: INFO: Pod is running on ip-10-0-164-144.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Aug 19 15:10:43.797: INFO: Pod wasn't evicted. Proceeding
Aug 19 15:10:43.797: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Aug 19 15:11:58.841: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:188
Aug 19 15:11:58.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4460" for this suite.

• [SLOW TEST:135.436 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":356,"completed":39,"skipped":651,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:11:58.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Aug 19 15:11:58.933: INFO: pods: 0 < 3
Aug 19 15:12:00.937: INFO: running pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Aug 19 15:12:05.074: INFO: running pods: 2 < 3
Aug 19 15:12:07.079: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Aug 19 15:12:09.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8296" for this suite.

• [SLOW TEST:10.277 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":356,"completed":40,"skipped":662,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:12:09.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 15:12:09.490: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 19 15:12:11.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 15, 12, 9, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 12, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 15, 12, 9, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 12, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 15:12:14.521: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:12:24.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8970" for this suite.
STEP: Destroying namespace "webhook-8970-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:15.622 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":356,"completed":41,"skipped":687,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:12:24.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 19 15:12:24.846: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb215687-f2c0-4fc0-ab36-52da40bc9a03" in namespace "projected-9333" to be "Succeeded or Failed"
Aug 19 15:12:24.861: INFO: Pod "downwardapi-volume-eb215687-f2c0-4fc0-ab36-52da40bc9a03": Phase="Pending", Reason="", readiness=false. Elapsed: 14.236156ms
Aug 19 15:12:26.865: INFO: Pod "downwardapi-volume-eb215687-f2c0-4fc0-ab36-52da40bc9a03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018269914s
Aug 19 15:12:28.869: INFO: Pod "downwardapi-volume-eb215687-f2c0-4fc0-ab36-52da40bc9a03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022581304s
Aug 19 15:12:30.874: INFO: Pod "downwardapi-volume-eb215687-f2c0-4fc0-ab36-52da40bc9a03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027604133s
STEP: Saw pod success
Aug 19 15:12:30.874: INFO: Pod "downwardapi-volume-eb215687-f2c0-4fc0-ab36-52da40bc9a03" satisfied condition "Succeeded or Failed"
Aug 19 15:12:30.877: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-eb215687-f2c0-4fc0-ab36-52da40bc9a03 container client-container: <nil>
STEP: delete the pod
Aug 19 15:12:30.902: INFO: Waiting for pod downwardapi-volume-eb215687-f2c0-4fc0-ab36-52da40bc9a03 to disappear
Aug 19 15:12:30.905: INFO: Pod downwardapi-volume-eb215687-f2c0-4fc0-ab36-52da40bc9a03 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 19 15:12:30.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9333" for this suite.

• [SLOW TEST:6.164 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":42,"skipped":689,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:12:30.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Aug 19 15:12:30.984: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 19 15:12:35.990: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Aug 19 15:12:35.993: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Aug 19 15:12:36.001: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Aug 19 15:12:36.003: INFO: Observed &ReplicaSet event: ADDED
Aug 19 15:12:36.003: INFO: Observed &ReplicaSet event: MODIFIED
Aug 19 15:12:36.003: INFO: Observed &ReplicaSet event: MODIFIED
Aug 19 15:12:36.003: INFO: Observed &ReplicaSet event: MODIFIED
Aug 19 15:12:36.003: INFO: Found replicaset test-rs in namespace replicaset-9680 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 19 15:12:36.003: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Aug 19 15:12:36.003: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 19 15:12:36.009: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Aug 19 15:12:36.010: INFO: Observed &ReplicaSet event: ADDED
Aug 19 15:12:36.010: INFO: Observed &ReplicaSet event: MODIFIED
Aug 19 15:12:36.010: INFO: Observed &ReplicaSet event: MODIFIED
Aug 19 15:12:36.010: INFO: Observed &ReplicaSet event: MODIFIED
Aug 19 15:12:36.010: INFO: Observed replicaset test-rs in namespace replicaset-9680 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 19 15:12:36.011: INFO: Observed &ReplicaSet event: MODIFIED
Aug 19 15:12:36.011: INFO: Found replicaset test-rs in namespace replicaset-9680 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Aug 19 15:12:36.011: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 19 15:12:36.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9680" for this suite.

• [SLOW TEST:5.104 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":356,"completed":43,"skipped":762,"failed":0}
[sig-storage] CSIStorageCapacity 
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:12:36.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename csistoragecapacity
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/storage.k8s.io
STEP: getting /apis/storage.k8s.io/v1
STEP: creating
STEP: watching
Aug 19 15:12:36.073: INFO: starting watch
STEP: getting
STEP: listing in namespace
STEP: listing across namespaces
STEP: patching
STEP: updating
Aug 19 15:12:36.107: INFO: waiting for watch events with expected annotations in namespace
Aug 19 15:12:36.107: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:188
Aug 19 15:12:36.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-4953" for this suite.
•{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","total":356,"completed":44,"skipped":762,"failed":0}

------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:12:36.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-acfe9898-83e6-4d08-b943-979a8e295811 in namespace container-probe-115
Aug 19 15:12:41.240: INFO: Started pod liveness-acfe9898-83e6-4d08-b943-979a8e295811 in namespace container-probe-115
STEP: checking the pod's current state and verifying that restartCount is present
Aug 19 15:12:41.243: INFO: Initial restart count of pod liveness-acfe9898-83e6-4d08-b943-979a8e295811 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 19 15:16:41.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-115" for this suite.

• [SLOW TEST:245.677 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":356,"completed":45,"skipped":762,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:16:41.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3932.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3932.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3932.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3932.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 19 15:16:45.988: INFO: DNS probes using dns-test-ff7fe558-ea4e-4472-b8ef-448047a607f2 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3932.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3932.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3932.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3932.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 19 15:16:50.035: INFO: DNS probes using dns-test-e7784c8b-5b5e-477a-ac7e-daf783be6176 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3932.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3932.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3932.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3932.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 19 15:16:54.101: INFO: DNS probes using dns-test-dc520d48-6f8f-46ef-a7f2-72077e4f3d1c succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 19 15:16:54.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3932" for this suite.

• [SLOW TEST:12.281 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":356,"completed":46,"skipped":765,"failed":0}
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:16:54.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:16:54.201: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3f74899d-8882-4c86-bf05-0f7254b9a262" in namespace "security-context-test-8474" to be "Succeeded or Failed"
Aug 19 15:16:54.206: INFO: Pod "busybox-privileged-false-3f74899d-8882-4c86-bf05-0f7254b9a262": Phase="Pending", Reason="", readiness=false. Elapsed: 5.355275ms
Aug 19 15:16:56.211: INFO: Pod "busybox-privileged-false-3f74899d-8882-4c86-bf05-0f7254b9a262": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010783834s
Aug 19 15:16:58.216: INFO: Pod "busybox-privileged-false-3f74899d-8882-4c86-bf05-0f7254b9a262": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015817332s
Aug 19 15:17:00.220: INFO: Pod "busybox-privileged-false-3f74899d-8882-4c86-bf05-0f7254b9a262": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019553528s
Aug 19 15:17:00.220: INFO: Pod "busybox-privileged-false-3f74899d-8882-4c86-bf05-0f7254b9a262" satisfied condition "Succeeded or Failed"
Aug 19 15:17:00.230: INFO: Got logs for pod "busybox-privileged-false-3f74899d-8882-4c86-bf05-0f7254b9a262": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 19 15:17:00.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8474" for this suite.

• [SLOW TEST:6.104 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:234
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":47,"skipped":765,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:17:00.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:17:00.270: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9a3cdcee-474c-41bd-843b-a6933544699d
STEP: Creating the pod
Aug 19 15:17:00.309: INFO: The status of Pod pod-projected-configmaps-d22c3adb-ec1f-4ff2-9dfc-8a9845152409 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:17:02.314: INFO: The status of Pod pod-projected-configmaps-d22c3adb-ec1f-4ff2-9dfc-8a9845152409 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:17:04.314: INFO: The status of Pod pod-projected-configmaps-d22c3adb-ec1f-4ff2-9dfc-8a9845152409 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-9a3cdcee-474c-41bd-843b-a6933544699d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 19 15:18:20.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3757" for this suite.

• [SLOW TEST:80.394 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":48,"skipped":806,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:18:20.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption is created
Aug 19 15:18:20.711: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:18:22.717: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:18:24.715: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Aug 19 15:18:25.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6568" for this suite.

• [SLOW TEST:5.109 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":356,"completed":49,"skipped":853,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:18:25.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Aug 19 15:18:41.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5607" for this suite.
STEP: Destroying namespace "nsdeletetest-7952" for this suite.
Aug 19 15:18:41.935: INFO: Namespace nsdeletetest-7952 was already deleted
STEP: Destroying namespace "nsdeletetest-8892" for this suite.

• [SLOW TEST:16.201 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":356,"completed":50,"skipped":887,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:18:41.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pods
Aug 19 15:18:43.006: INFO: created test-pod-1
Aug 19 15:18:43.020: INFO: created test-pod-2
Aug 19 15:18:43.037: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running
Aug 19 15:18:43.037: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6721' to be running and ready
Aug 19 15:18:43.049: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 19 15:18:43.049: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 19 15:18:43.049: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 19 15:18:43.049: INFO: 0 / 3 pods in namespace 'pods-6721' are running and ready (0 seconds elapsed)
Aug 19 15:18:43.049: INFO: expected 0 pod replicas in namespace 'pods-6721', 0 are Running and Ready.
Aug 19 15:18:43.049: INFO: POD         NODE                          PHASE    GRACE  CONDITIONS
Aug 19 15:18:43.049: INFO: test-pod-1  ip-10-0-164-144.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC  }]
Aug 19 15:18:43.049: INFO: test-pod-2  ip-10-0-164-144.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC  }]
Aug 19 15:18:43.049: INFO: test-pod-3  ip-10-0-164-144.ec2.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC  }]
Aug 19 15:18:43.049: INFO: 
Aug 19 15:18:45.060: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 19 15:18:45.060: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 19 15:18:45.060: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 19 15:18:45.060: INFO: 0 / 3 pods in namespace 'pods-6721' are running and ready (2 seconds elapsed)
Aug 19 15:18:45.060: INFO: expected 0 pod replicas in namespace 'pods-6721', 0 are Running and Ready.
Aug 19 15:18:45.060: INFO: POD         NODE                          PHASE    GRACE  CONDITIONS
Aug 19 15:18:45.060: INFO: test-pod-1  ip-10-0-164-144.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC  }]
Aug 19 15:18:45.060: INFO: test-pod-2  ip-10-0-164-144.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC  }]
Aug 19 15:18:45.060: INFO: test-pod-3  ip-10-0-164-144.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:18:43 +0000 UTC  }]
Aug 19 15:18:45.060: INFO: 
Aug 19 15:18:47.059: INFO: 3 / 3 pods in namespace 'pods-6721' are running and ready (4 seconds elapsed)
Aug 19 15:18:47.059: INFO: expected 0 pod replicas in namespace 'pods-6721', 0 are Running and Ready.
STEP: waiting for all pods to be deleted
Aug 19 15:18:47.089: INFO: Pod quantity 3 is different from expected quantity 0
Aug 19 15:18:48.093: INFO: Pod quantity 3 is different from expected quantity 0
Aug 19 15:18:49.094: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 19 15:18:50.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6721" for this suite.

• [SLOW TEST:8.155 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":356,"completed":51,"skipped":905,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:18:50.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 15:18:50.367: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 15:18:53.389: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:18:53.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9021" for this suite.
STEP: Destroying namespace "webhook-9021-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":356,"completed":52,"skipped":918,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:18:53.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-c23af408-297d-4d79-aed6-a98322cffe13
STEP: Creating a pod to test consume secrets
Aug 19 15:18:53.570: INFO: Waiting up to 5m0s for pod "pod-secrets-0940224d-2863-409b-98e8-6737a648bbff" in namespace "secrets-267" to be "Succeeded or Failed"
Aug 19 15:18:53.574: INFO: Pod "pod-secrets-0940224d-2863-409b-98e8-6737a648bbff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.746567ms
Aug 19 15:18:55.578: INFO: Pod "pod-secrets-0940224d-2863-409b-98e8-6737a648bbff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008211644s
Aug 19 15:18:57.583: INFO: Pod "pod-secrets-0940224d-2863-409b-98e8-6737a648bbff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013792186s
Aug 19 15:18:59.588: INFO: Pod "pod-secrets-0940224d-2863-409b-98e8-6737a648bbff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017951125s
STEP: Saw pod success
Aug 19 15:18:59.588: INFO: Pod "pod-secrets-0940224d-2863-409b-98e8-6737a648bbff" satisfied condition "Succeeded or Failed"
Aug 19 15:18:59.591: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-secrets-0940224d-2863-409b-98e8-6737a648bbff container secret-volume-test: <nil>
STEP: delete the pod
Aug 19 15:18:59.608: INFO: Waiting for pod pod-secrets-0940224d-2863-409b-98e8-6737a648bbff to disappear
Aug 19 15:18:59.610: INFO: Pod pod-secrets-0940224d-2863-409b-98e8-6737a648bbff no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 19 15:18:59.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-267" for this suite.

• [SLOW TEST:6.132 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":53,"skipped":920,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:18:59.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
Aug 19 15:18:59.659: INFO: Creating simple deployment test-deployment-x2p6n
W0819 15:18:59.667583      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 15:18:59.675: INFO: deployment "test-deployment-x2p6n" doesn't have the required revision set
Aug 19 15:19:01.691: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 15, 18, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 18, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 15, 18, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 18, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-x2p6n-688c4d6789\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Aug 19 15:19:03.703: INFO: Deployment test-deployment-x2p6n has Conditions: [{Available True 2022-08-19 15:19:02 +0000 UTC 2022-08-19 15:19:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-08-19 15:19:02 +0000 UTC 2022-08-19 15:18:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x2p6n-688c4d6789" has successfully progressed.}]
STEP: updating Deployment Status
Aug 19 15:19:03.714: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 15, 19, 2, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 19, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 15, 19, 2, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 18, 59, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-x2p6n-688c4d6789\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Aug 19 15:19:03.716: INFO: Observed &Deployment event: ADDED
Aug 19 15:19:03.716: INFO: Observed Deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-19 15:18:59 +0000 UTC 2022-08-19 15:18:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x2p6n-688c4d6789"}
Aug 19 15:19:03.716: INFO: Observed &Deployment event: MODIFIED
Aug 19 15:19:03.716: INFO: Observed Deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-19 15:18:59 +0000 UTC 2022-08-19 15:18:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x2p6n-688c4d6789"}
Aug 19 15:19:03.716: INFO: Observed Deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-19 15:18:59 +0000 UTC 2022-08-19 15:18:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 19 15:19:03.716: INFO: Observed &Deployment event: MODIFIED
Aug 19 15:19:03.716: INFO: Observed Deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-19 15:18:59 +0000 UTC 2022-08-19 15:18:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 19 15:19:03.716: INFO: Observed Deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-19 15:18:59 +0000 UTC 2022-08-19 15:18:59 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-x2p6n-688c4d6789" is progressing.}
Aug 19 15:19:03.716: INFO: Observed &Deployment event: MODIFIED
Aug 19 15:19:03.716: INFO: Observed Deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-19 15:19:02 +0000 UTC 2022-08-19 15:19:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 19 15:19:03.716: INFO: Observed Deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-19 15:19:02 +0000 UTC 2022-08-19 15:18:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x2p6n-688c4d6789" has successfully progressed.}
Aug 19 15:19:03.716: INFO: Observed &Deployment event: MODIFIED
Aug 19 15:19:03.716: INFO: Observed Deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-19 15:19:02 +0000 UTC 2022-08-19 15:19:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 19 15:19:03.716: INFO: Observed Deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-19 15:19:02 +0000 UTC 2022-08-19 15:18:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x2p6n-688c4d6789" has successfully progressed.}
Aug 19 15:19:03.716: INFO: Found Deployment test-deployment-x2p6n in namespace deployment-2521 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 19 15:19:03.716: INFO: Deployment test-deployment-x2p6n has an updated status
STEP: patching the Statefulset Status
Aug 19 15:19:03.716: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 19 15:19:03.722: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Aug 19 15:19:03.724: INFO: Observed &Deployment event: ADDED
Aug 19 15:19:03.724: INFO: Observed deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-19 15:18:59 +0000 UTC 2022-08-19 15:18:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x2p6n-688c4d6789"}
Aug 19 15:19:03.724: INFO: Observed &Deployment event: MODIFIED
Aug 19 15:19:03.724: INFO: Observed deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-19 15:18:59 +0000 UTC 2022-08-19 15:18:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x2p6n-688c4d6789"}
Aug 19 15:19:03.724: INFO: Observed deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-19 15:18:59 +0000 UTC 2022-08-19 15:18:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 19 15:19:03.724: INFO: Observed &Deployment event: MODIFIED
Aug 19 15:19:03.724: INFO: Observed deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-19 15:18:59 +0000 UTC 2022-08-19 15:18:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 19 15:19:03.724: INFO: Observed deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-19 15:18:59 +0000 UTC 2022-08-19 15:18:59 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-x2p6n-688c4d6789" is progressing.}
Aug 19 15:19:03.724: INFO: Observed &Deployment event: MODIFIED
Aug 19 15:19:03.724: INFO: Observed deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-19 15:19:02 +0000 UTC 2022-08-19 15:19:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 19 15:19:03.724: INFO: Observed deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-19 15:19:02 +0000 UTC 2022-08-19 15:18:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x2p6n-688c4d6789" has successfully progressed.}
Aug 19 15:19:03.724: INFO: Observed &Deployment event: MODIFIED
Aug 19 15:19:03.724: INFO: Observed deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-19 15:19:02 +0000 UTC 2022-08-19 15:19:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 19 15:19:03.724: INFO: Observed deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-19 15:19:02 +0000 UTC 2022-08-19 15:18:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x2p6n-688c4d6789" has successfully progressed.}
Aug 19 15:19:03.724: INFO: Observed deployment test-deployment-x2p6n in namespace deployment-2521 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 19 15:19:03.724: INFO: Observed &Deployment event: MODIFIED
Aug 19 15:19:03.724: INFO: Found deployment test-deployment-x2p6n in namespace deployment-2521 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Aug 19 15:19:03.724: INFO: Deployment test-deployment-x2p6n has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 19 15:19:03.727: INFO: Deployment "test-deployment-x2p6n":
&Deployment{ObjectMeta:{test-deployment-x2p6n  deployment-2521  8417fe06-6422-4746-a031-9bd473989637 47122 1 2022-08-19 15:18:59 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-08-19 15:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 15:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-08-19 15:19:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0014fcdb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 19 15:19:03.731: INFO: New ReplicaSet "test-deployment-x2p6n-688c4d6789" of Deployment "test-deployment-x2p6n":
&ReplicaSet{ObjectMeta:{test-deployment-x2p6n-688c4d6789  deployment-2521  0ca10457-5e7c-494c-a4f0-f8246e1126bc 47114 1 2022-08-19 15:18:59 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-x2p6n 8417fe06-6422-4746-a031-9bd473989637 0xc00230cb70 0xc00230cb71}] []  [{kube-controller-manager Update apps/v1 2022-08-19 15:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8417fe06-6422-4746-a031-9bd473989637\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 15:19:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 688c4d6789,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00230ccf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 19 15:19:03.734: INFO: Pod "test-deployment-x2p6n-688c4d6789-t9km7" is available:
&Pod{ObjectMeta:{test-deployment-x2p6n-688c4d6789-t9km7 test-deployment-x2p6n-688c4d6789- deployment-2521  73ee6252-2fcd-4086-99b6-6868fa08918d 47113 0 2022-08-19 15:18:59 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.131"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.131"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-x2p6n-688c4d6789 0ca10457-5e7c-494c-a4f0-f8246e1126bc 0xc00230d087 0xc00230d088}] []  [{kube-controller-manager Update v1 2022-08-19 15:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ca10457-5e7c-494c-a4f0-f8246e1126bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-08-19 15:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-19 15:19:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lw9wg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lw9wg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-164-144.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c36,c5,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.164.144,PodIP:10.131.0.131,StartTime:2022-08-19 15:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 15:19:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://6caa2bc94128afc327d4b7947f22e0cdb188a16e927b6c5600d22550165d7cb2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 19 15:19:03.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2521" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":356,"completed":54,"skipped":988,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:19:03.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 19 15:19:03.805: INFO: Waiting up to 5m0s for pod "pod-f08119d8-de7d-41ca-9d19-2098dd3cae15" in namespace "emptydir-8985" to be "Succeeded or Failed"
Aug 19 15:19:03.808: INFO: Pod "pod-f08119d8-de7d-41ca-9d19-2098dd3cae15": Phase="Pending", Reason="", readiness=false. Elapsed: 3.035688ms
Aug 19 15:19:05.813: INFO: Pod "pod-f08119d8-de7d-41ca-9d19-2098dd3cae15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007811908s
Aug 19 15:19:07.818: INFO: Pod "pod-f08119d8-de7d-41ca-9d19-2098dd3cae15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013039878s
Aug 19 15:19:09.822: INFO: Pod "pod-f08119d8-de7d-41ca-9d19-2098dd3cae15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017238777s
STEP: Saw pod success
Aug 19 15:19:09.822: INFO: Pod "pod-f08119d8-de7d-41ca-9d19-2098dd3cae15" satisfied condition "Succeeded or Failed"
Aug 19 15:19:09.825: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-f08119d8-de7d-41ca-9d19-2098dd3cae15 container test-container: <nil>
STEP: delete the pod
Aug 19 15:19:09.843: INFO: Waiting for pod pod-f08119d8-de7d-41ca-9d19-2098dd3cae15 to disappear
Aug 19 15:19:09.846: INFO: Pod pod-f08119d8-de7d-41ca-9d19-2098dd3cae15 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 19 15:19:09.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8985" for this suite.

• [SLOW TEST:6.111 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":55,"skipped":1027,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:19:09.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
W0819 15:19:09.893938      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 1 pods
STEP: Gathering metrics
Aug 19 15:19:10.453: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0819 15:19:10.453456      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0819 15:19:10.453468      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 19 15:19:10.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1387" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":356,"completed":56,"skipped":1036,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:19:10.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 19 15:19:10.859: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Aug 19 15:19:12.870: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 15, 19, 10, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 19, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 15, 19, 10, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 19, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-677b6dd845\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 15:19:15.892: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:19:15.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:19:18.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6782" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:8.628 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":356,"completed":57,"skipped":1050,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:19:19.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
W0819 15:19:19.141229      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Aug 19 15:21:01.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9578" for this suite.

• [SLOW TEST:102.078 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":356,"completed":58,"skipped":1084,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:21:01.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 19 15:21:01.740: INFO: starting watch
STEP: patching
STEP: updating
Aug 19 15:21:01.756: INFO: waiting for watch events with expected annotations
Aug 19 15:21:01.756: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:21:01.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-2619" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":356,"completed":59,"skipped":1129,"failed":0}
SSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:21:01.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override all
Aug 19 15:21:01.880: INFO: Waiting up to 5m0s for pod "client-containers-9ef537d5-2fca-4470-8a3b-11cb283e7bd2" in namespace "containers-4103" to be "Succeeded or Failed"
Aug 19 15:21:01.882: INFO: Pod "client-containers-9ef537d5-2fca-4470-8a3b-11cb283e7bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.76673ms
Aug 19 15:21:03.886: INFO: Pod "client-containers-9ef537d5-2fca-4470-8a3b-11cb283e7bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006127451s
Aug 19 15:21:05.891: INFO: Pod "client-containers-9ef537d5-2fca-4470-8a3b-11cb283e7bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011313781s
Aug 19 15:21:07.895: INFO: Pod "client-containers-9ef537d5-2fca-4470-8a3b-11cb283e7bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01482391s
STEP: Saw pod success
Aug 19 15:21:07.895: INFO: Pod "client-containers-9ef537d5-2fca-4470-8a3b-11cb283e7bd2" satisfied condition "Succeeded or Failed"
Aug 19 15:21:07.898: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod client-containers-9ef537d5-2fca-4470-8a3b-11cb283e7bd2 container agnhost-container: <nil>
STEP: delete the pod
Aug 19 15:21:07.923: INFO: Waiting for pod client-containers-9ef537d5-2fca-4470-8a3b-11cb283e7bd2 to disappear
Aug 19 15:21:07.926: INFO: Pod client-containers-9ef537d5-2fca-4470-8a3b-11cb283e7bd2 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Aug 19 15:21:07.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4103" for this suite.

• [SLOW TEST:6.111 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":356,"completed":60,"skipped":1136,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:21:07.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9382.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9382.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9382.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9382.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9382.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9382.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9382.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9382.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9382.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9382.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9382.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9382.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9382.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9382.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9382.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9382.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 19 15:21:12.039: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9382.svc.cluster.local from pod dns-9382/dns-test-49154fef-6006-4665-9251-1725f6fc1b68: the server could not find the requested resource (get pods dns-test-49154fef-6006-4665-9251-1725f6fc1b68)
Aug 19 15:21:12.042: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9382.svc.cluster.local from pod dns-9382/dns-test-49154fef-6006-4665-9251-1725f6fc1b68: the server could not find the requested resource (get pods dns-test-49154fef-6006-4665-9251-1725f6fc1b68)
Aug 19 15:21:12.048: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9382.svc.cluster.local from pod dns-9382/dns-test-49154fef-6006-4665-9251-1725f6fc1b68: the server could not find the requested resource (get pods dns-test-49154fef-6006-4665-9251-1725f6fc1b68)
Aug 19 15:21:12.051: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9382.svc.cluster.local from pod dns-9382/dns-test-49154fef-6006-4665-9251-1725f6fc1b68: the server could not find the requested resource (get pods dns-test-49154fef-6006-4665-9251-1725f6fc1b68)
Aug 19 15:21:12.058: INFO: Lookups using dns-9382/dns-test-49154fef-6006-4665-9251-1725f6fc1b68 failed for: [wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9382.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9382.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9382.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9382.svc.cluster.local]

Aug 19 15:21:17.088: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9382.svc.cluster.local from pod dns-9382/dns-test-49154fef-6006-4665-9251-1725f6fc1b68: Get "https://172.30.0.1:443/api/v1/namespaces/dns-9382/pods/dns-test-49154fef-6006-4665-9251-1725f6fc1b68/proxy/results/jessie_tcp@dns-test-service-2.dns-9382.svc.cluster.local": stream error: stream ID 3049; INTERNAL_ERROR; received from peer
Aug 19 15:21:17.088: INFO: Lookups using dns-9382/dns-test-49154fef-6006-4665-9251-1725f6fc1b68 failed for: [jessie_tcp@dns-test-service-2.dns-9382.svc.cluster.local]

Aug 19 15:21:22.088: INFO: DNS probes using dns-9382/dns-test-49154fef-6006-4665-9251-1725f6fc1b68 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 19 15:21:22.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9382" for this suite.

• [SLOW TEST:14.197 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":356,"completed":61,"skipped":1168,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:21:22.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a pod template
W0819 15:21:22.167619      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Replace a pod template
W0819 15:21:22.180536      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 15:21:22.180: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Aug 19 15:21:22.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-320" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","total":356,"completed":62,"skipped":1203,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:21:22.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-7bcc0779-d73d-46be-a5f4-523724ee7a52
STEP: Creating a pod to test consume secrets
Aug 19 15:21:22.254: INFO: Waiting up to 5m0s for pod "pod-secrets-423abd1b-7a60-4c92-8360-752b7a9b353c" in namespace "secrets-8551" to be "Succeeded or Failed"
Aug 19 15:21:22.260: INFO: Pod "pod-secrets-423abd1b-7a60-4c92-8360-752b7a9b353c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.620522ms
Aug 19 15:21:24.264: INFO: Pod "pod-secrets-423abd1b-7a60-4c92-8360-752b7a9b353c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010455502s
Aug 19 15:21:26.269: INFO: Pod "pod-secrets-423abd1b-7a60-4c92-8360-752b7a9b353c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015599694s
Aug 19 15:21:28.274: INFO: Pod "pod-secrets-423abd1b-7a60-4c92-8360-752b7a9b353c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020458879s
STEP: Saw pod success
Aug 19 15:21:28.274: INFO: Pod "pod-secrets-423abd1b-7a60-4c92-8360-752b7a9b353c" satisfied condition "Succeeded or Failed"
Aug 19 15:21:28.277: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-secrets-423abd1b-7a60-4c92-8360-752b7a9b353c container secret-volume-test: <nil>
STEP: delete the pod
Aug 19 15:21:28.302: INFO: Waiting for pod pod-secrets-423abd1b-7a60-4c92-8360-752b7a9b353c to disappear
Aug 19 15:21:28.305: INFO: Pod pod-secrets-423abd1b-7a60-4c92-8360-752b7a9b353c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 19 15:21:28.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8551" for this suite.

• [SLOW TEST:6.123 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":63,"skipped":1237,"failed":0}
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:21:28.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Aug 19 15:21:32.389: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8238 PodName:pod-sharedvolume-4c74b88b-3cb2-4b7b-bc4e-703f7791f69f ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:21:32.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:21:32.390: INFO: ExecWithOptions: Clientset creation
Aug 19 15:21:32.390: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/emptydir-8238/pods/pod-sharedvolume-4c74b88b-3cb2-4b7b-bc4e-703f7791f69f/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Aug 19 15:21:32.446: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 19 15:21:32.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8238" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":356,"completed":64,"skipped":1237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:21:32.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in volume subpath
Aug 19 15:21:33.564: INFO: Waiting up to 5m0s for pod "var-expansion-5ca5c913-b942-4cac-a65b-481f5ae70e48" in namespace "var-expansion-7912" to be "Succeeded or Failed"
Aug 19 15:21:33.568: INFO: Pod "var-expansion-5ca5c913-b942-4cac-a65b-481f5ae70e48": Phase="Pending", Reason="", readiness=false. Elapsed: 3.277125ms
Aug 19 15:21:35.571: INFO: Pod "var-expansion-5ca5c913-b942-4cac-a65b-481f5ae70e48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007200279s
Aug 19 15:21:37.575: INFO: Pod "var-expansion-5ca5c913-b942-4cac-a65b-481f5ae70e48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010630765s
Aug 19 15:21:39.579: INFO: Pod "var-expansion-5ca5c913-b942-4cac-a65b-481f5ae70e48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014806074s
STEP: Saw pod success
Aug 19 15:21:39.579: INFO: Pod "var-expansion-5ca5c913-b942-4cac-a65b-481f5ae70e48" satisfied condition "Succeeded or Failed"
Aug 19 15:21:39.582: INFO: Trying to get logs from node ip-10-0-157-99.ec2.internal pod var-expansion-5ca5c913-b942-4cac-a65b-481f5ae70e48 container dapi-container: <nil>
STEP: delete the pod
Aug 19 15:21:39.605: INFO: Waiting for pod var-expansion-5ca5c913-b942-4cac-a65b-481f5ae70e48 to disappear
Aug 19 15:21:39.608: INFO: Pod var-expansion-5ca5c913-b942-4cac-a65b-481f5ae70e48 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 19 15:21:39.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7912" for this suite.

• [SLOW TEST:7.159 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":356,"completed":65,"skipped":1310,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:21:39.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-downwardapi-xsfp
STEP: Creating a pod to test atomic-volume-subpath
Aug 19 15:21:39.714: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-xsfp" in namespace "subpath-9814" to be "Succeeded or Failed"
Aug 19 15:21:39.727: INFO: Pod "pod-subpath-test-downwardapi-xsfp": Phase="Pending", Reason="", readiness=false. Elapsed: 12.661005ms
Aug 19 15:21:41.731: INFO: Pod "pod-subpath-test-downwardapi-xsfp": Phase="Running", Reason="", readiness=true. Elapsed: 2.01641765s
Aug 19 15:21:43.735: INFO: Pod "pod-subpath-test-downwardapi-xsfp": Phase="Running", Reason="", readiness=true. Elapsed: 4.020783307s
Aug 19 15:21:45.740: INFO: Pod "pod-subpath-test-downwardapi-xsfp": Phase="Running", Reason="", readiness=true. Elapsed: 6.025309272s
Aug 19 15:21:47.745: INFO: Pod "pod-subpath-test-downwardapi-xsfp": Phase="Running", Reason="", readiness=true. Elapsed: 8.0307571s
Aug 19 15:21:49.750: INFO: Pod "pod-subpath-test-downwardapi-xsfp": Phase="Running", Reason="", readiness=true. Elapsed: 10.03595186s
Aug 19 15:21:51.755: INFO: Pod "pod-subpath-test-downwardapi-xsfp": Phase="Running", Reason="", readiness=true. Elapsed: 12.041021301s
Aug 19 15:21:53.760: INFO: Pod "pod-subpath-test-downwardapi-xsfp": Phase="Running", Reason="", readiness=true. Elapsed: 14.045949146s
Aug 19 15:21:55.766: INFO: Pod "pod-subpath-test-downwardapi-xsfp": Phase="Running", Reason="", readiness=true. Elapsed: 16.051979747s
Aug 19 15:21:57.771: INFO: Pod "pod-subpath-test-downwardapi-xsfp": Phase="Running", Reason="", readiness=true. Elapsed: 18.05649317s
Aug 19 15:21:59.776: INFO: Pod "pod-subpath-test-downwardapi-xsfp": Phase="Running", Reason="", readiness=true. Elapsed: 20.061269808s
Aug 19 15:22:01.781: INFO: Pod "pod-subpath-test-downwardapi-xsfp": Phase="Running", Reason="", readiness=false. Elapsed: 22.066680544s
Aug 19 15:22:03.786: INFO: Pod "pod-subpath-test-downwardapi-xsfp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.071607348s
STEP: Saw pod success
Aug 19 15:22:03.786: INFO: Pod "pod-subpath-test-downwardapi-xsfp" satisfied condition "Succeeded or Failed"
Aug 19 15:22:03.797: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-subpath-test-downwardapi-xsfp container test-container-subpath-downwardapi-xsfp: <nil>
STEP: delete the pod
Aug 19 15:22:03.814: INFO: Waiting for pod pod-subpath-test-downwardapi-xsfp to disappear
Aug 19 15:22:03.817: INFO: Pod pod-subpath-test-downwardapi-xsfp no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-xsfp
Aug 19 15:22:03.817: INFO: Deleting pod "pod-subpath-test-downwardapi-xsfp" in namespace "subpath-9814"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Aug 19 15:22:03.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9814" for this suite.

• [SLOW TEST:24.212 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","total":356,"completed":66,"skipped":1355,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:22:03.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 15:22:04.211: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 19 15:22:06.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 15, 22, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 22, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 15, 22, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 22, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 15:22:09.237: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:22:09.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1201" for this suite.
STEP: Destroying namespace "webhook-1201-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.539 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":356,"completed":67,"skipped":1366,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:22:09.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
W0819 15:22:09.423734      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 15:22:13.439: INFO: Deleting pod "var-expansion-4413910b-2387-4701-8421-1a2f02eede36" in namespace "var-expansion-209"
Aug 19 15:22:13.458: INFO: Wait up to 5m0s for pod "var-expansion-4413910b-2387-4701-8421-1a2f02eede36" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 19 15:22:15.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-209" for this suite.

• [SLOW TEST:6.106 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":356,"completed":68,"skipped":1389,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:22:15.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0819 15:22:15.531168      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 15:22:15.531: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b591394a-a911-46bf-a44f-f682113fb184" in namespace "downward-api-6340" to be "Succeeded or Failed"
Aug 19 15:22:15.547: INFO: Pod "downwardapi-volume-b591394a-a911-46bf-a44f-f682113fb184": Phase="Pending", Reason="", readiness=false. Elapsed: 16.488039ms
Aug 19 15:22:17.552: INFO: Pod "downwardapi-volume-b591394a-a911-46bf-a44f-f682113fb184": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021245059s
Aug 19 15:22:19.556: INFO: Pod "downwardapi-volume-b591394a-a911-46bf-a44f-f682113fb184": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025062821s
Aug 19 15:22:21.560: INFO: Pod "downwardapi-volume-b591394a-a911-46bf-a44f-f682113fb184": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029625457s
STEP: Saw pod success
Aug 19 15:22:21.560: INFO: Pod "downwardapi-volume-b591394a-a911-46bf-a44f-f682113fb184" satisfied condition "Succeeded or Failed"
Aug 19 15:22:21.563: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-b591394a-a911-46bf-a44f-f682113fb184 container client-container: <nil>
STEP: delete the pod
Aug 19 15:22:21.581: INFO: Waiting for pod downwardapi-volume-b591394a-a911-46bf-a44f-f682113fb184 to disappear
Aug 19 15:22:21.585: INFO: Pod downwardapi-volume-b591394a-a911-46bf-a44f-f682113fb184 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 19 15:22:21.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6340" for this suite.

• [SLOW TEST:6.118 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":69,"skipped":1391,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:22:21.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1334
STEP: creating the pod
Aug 19 15:22:21.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-7974 create -f -'
Aug 19 15:22:23.105: INFO: stderr: ""
Aug 19 15:22:23.105: INFO: stdout: "pod/pause created\n"
Aug 19 15:22:23.105: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 19 15:22:23.105: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7974" to be "running and ready"
Aug 19 15:22:23.108: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.712974ms
Aug 19 15:22:25.112: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007627291s
Aug 19 15:22:27.117: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.012176906s
Aug 19 15:22:27.117: INFO: Pod "pause" satisfied condition "running and ready"
Aug 19 15:22:27.117: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 19 15:22:27.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-7974 label pods pause testing-label=testing-label-value'
Aug 19 15:22:27.171: INFO: stderr: ""
Aug 19 15:22:27.171: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 19 15:22:27.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-7974 get pod pause -L testing-label'
Aug 19 15:22:27.211: INFO: stderr: ""
Aug 19 15:22:27.211: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 19 15:22:27.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-7974 label pods pause testing-label-'
Aug 19 15:22:27.272: INFO: stderr: ""
Aug 19 15:22:27.272: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 19 15:22:27.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-7974 get pod pause -L testing-label'
Aug 19 15:22:27.312: INFO: stderr: ""
Aug 19 15:22:27.312: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Aug 19 15:22:27.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-7974 delete --grace-period=0 --force -f -'
Aug 19 15:22:27.377: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 19 15:22:27.377: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 19 15:22:27.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-7974 get rc,svc -l name=pause --no-headers'
Aug 19 15:22:27.424: INFO: stderr: "No resources found in kubectl-7974 namespace.\n"
Aug 19 15:22:27.424: INFO: stdout: ""
Aug 19 15:22:27.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-7974 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 19 15:22:27.465: INFO: stderr: ""
Aug 19 15:22:27.465: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 15:22:27.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7974" for this suite.

• [SLOW TEST:5.881 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1332
    should update the label on a resource  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":356,"completed":70,"skipped":1394,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:22:27.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:22:27.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Aug 19 15:22:34.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-9507 --namespace=crd-publish-openapi-9507 create -f -'
Aug 19 15:22:35.333: INFO: stderr: ""
Aug 19 15:22:35.333: INFO: stdout: "e2e-test-crd-publish-openapi-9673-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 19 15:22:35.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-9507 --namespace=crd-publish-openapi-9507 delete e2e-test-crd-publish-openapi-9673-crds test-cr'
Aug 19 15:22:35.383: INFO: stderr: ""
Aug 19 15:22:35.383: INFO: stdout: "e2e-test-crd-publish-openapi-9673-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 19 15:22:35.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-9507 --namespace=crd-publish-openapi-9507 apply -f -'
Aug 19 15:22:36.216: INFO: stderr: ""
Aug 19 15:22:36.216: INFO: stdout: "e2e-test-crd-publish-openapi-9673-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 19 15:22:36.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-9507 --namespace=crd-publish-openapi-9507 delete e2e-test-crd-publish-openapi-9673-crds test-cr'
Aug 19 15:22:36.263: INFO: stderr: ""
Aug 19 15:22:36.263: INFO: stdout: "e2e-test-crd-publish-openapi-9673-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 19 15:22:36.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-9507 explain e2e-test-crd-publish-openapi-9673-crds'
Aug 19 15:22:36.421: INFO: stderr: ""
Aug 19 15:22:36.421: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9673-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:22:42.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9507" for this suite.

• [SLOW TEST:15.077 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":356,"completed":71,"skipped":1406,"failed":0}
SSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:22:42.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Aug 19 15:22:44.622: INFO: running pods: 0 < 1
Aug 19 15:22:46.627: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Aug 19 15:22:48.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6088" for this suite.

• [SLOW TEST:6.117 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":356,"completed":72,"skipped":1412,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:22:48.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 19 15:22:48.724: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c91a24d1-4e3a-4a66-94ce-9ba37588e164" in namespace "downward-api-1595" to be "Succeeded or Failed"
Aug 19 15:22:48.731: INFO: Pod "downwardapi-volume-c91a24d1-4e3a-4a66-94ce-9ba37588e164": Phase="Pending", Reason="", readiness=false. Elapsed: 7.101504ms
Aug 19 15:22:50.735: INFO: Pod "downwardapi-volume-c91a24d1-4e3a-4a66-94ce-9ba37588e164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011128257s
Aug 19 15:22:52.740: INFO: Pod "downwardapi-volume-c91a24d1-4e3a-4a66-94ce-9ba37588e164": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015640189s
Aug 19 15:22:54.745: INFO: Pod "downwardapi-volume-c91a24d1-4e3a-4a66-94ce-9ba37588e164": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021172772s
STEP: Saw pod success
Aug 19 15:22:54.745: INFO: Pod "downwardapi-volume-c91a24d1-4e3a-4a66-94ce-9ba37588e164" satisfied condition "Succeeded or Failed"
Aug 19 15:22:54.749: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-c91a24d1-4e3a-4a66-94ce-9ba37588e164 container client-container: <nil>
STEP: delete the pod
Aug 19 15:22:54.776: INFO: Waiting for pod downwardapi-volume-c91a24d1-4e3a-4a66-94ce-9ba37588e164 to disappear
Aug 19 15:22:54.778: INFO: Pod downwardapi-volume-c91a24d1-4e3a-4a66-94ce-9ba37588e164 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 19 15:22:54.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1595" for this suite.

• [SLOW TEST:6.117 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":73,"skipped":1419,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:22:54.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:22:54.819: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Aug 19 15:22:56.854: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Aug 19 15:22:57.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6216" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":356,"completed":74,"skipped":1428,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:22:57.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 19 15:22:57.902: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 19 15:22:57.916: INFO: Waiting for terminating namespaces to be deleted...
Aug 19 15:22:57.922: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-131-169.ec2.internal before test
Aug 19 15:22:57.947: INFO: aws-ebs-csi-driver-node-zp4zp from openshift-cluster-csi-drivers started at 2022-08-19 14:38:55 +0000 UTC (3 container statuses recorded)
Aug 19 15:22:57.947: INFO: 	Container csi-driver ready: true, restart count 0
Aug 19 15:22:57.947: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Aug 19 15:22:57.947: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 19 15:22:57.947: INFO: tuned-7vdfg from openshift-cluster-node-tuning-operator started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.947: INFO: 	Container tuned ready: true, restart count 0
Aug 19 15:22:57.947: INFO: downloads-858cc8f4cb-zkm66 from openshift-console started at 2022-08-19 14:47:16 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.947: INFO: 	Container download-server ready: true, restart count 0
Aug 19 15:22:57.948: INFO: dns-default-zk2kr from openshift-dns started at 2022-08-19 14:39:21 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container dns ready: true, restart count 0
Aug 19 15:22:57.948: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.948: INFO: node-resolver-ttx27 from openshift-dns started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 19 15:22:57.948: INFO: image-registry-79dbb9c69c-2qmtr from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container registry ready: true, restart count 0
Aug 19 15:22:57.948: INFO: node-ca-qqztn from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container node-ca ready: true, restart count 0
Aug 19 15:22:57.948: INFO: ingress-canary-lr86r from openshift-ingress-canary started at 2022-08-19 14:40:04 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 19 15:22:57.948: INFO: router-default-7664744558-bndls from openshift-ingress started at 2022-08-19 14:40:03 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container router ready: true, restart count 0
Aug 19 15:22:57.948: INFO: machine-config-daemon-gpvls from openshift-machine-config-operator started at 2022-08-19 14:38:55 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 19 15:22:57.948: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 15:22:57.948: INFO: node-exporter-tv7s8 from openshift-monitoring started at 2022-08-19 14:41:12 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.948: INFO: 	Container node-exporter ready: true, restart count 0
Aug 19 15:22:57.948: INFO: prometheus-adapter-85bd9549d5-h6mlp from openshift-monitoring started at 2022-08-19 14:42:01 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 19 15:22:57.948: INFO: prometheus-operator-admission-webhook-6db58c58f7-2922r from openshift-monitoring started at 2022-08-19 14:39:23 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 19 15:22:57.948: INFO: thanos-querier-8559769b94-jjnm2 from openshift-monitoring started at 2022-08-19 15:10:44 +0000 UTC (6 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.948: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 19 15:22:57.948: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 19 15:22:57.948: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 15:22:57.948: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 15:22:57.948: INFO: 	Container thanos-query ready: true, restart count 0
Aug 19 15:22:57.948: INFO: multus-additional-cni-plugins-2fqhw from openshift-multus started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 19 15:22:57.948: INFO: multus-pss88 from openshift-multus started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container kube-multus ready: true, restart count 0
Aug 19 15:22:57.948: INFO: network-metrics-daemon-bkf7f from openshift-multus started at 2022-08-19 14:38:55 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.948: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 19 15:22:57.948: INFO: network-check-target-zpzkf from openshift-network-diagnostics started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 19 15:22:57.948: INFO: collect-profiles-27682005-54twx from openshift-operator-lifecycle-manager started at 2022-08-19 14:45:00 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 19 15:22:57.948: INFO: collect-profiles-27682020-wctjw from openshift-operator-lifecycle-manager started at 2022-08-19 15:00:00 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 19 15:22:57.948: INFO: collect-profiles-27682035-68mc8 from openshift-operator-lifecycle-manager started at 2022-08-19 15:15:00 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 19 15:22:57.948: INFO: sdn-vjrw2 from openshift-sdn started at 2022-08-19 14:38:55 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.948: INFO: 	Container sdn ready: true, restart count 0
Aug 19 15:22:57.948: INFO: sonobuoy from sonobuoy started at 2022-08-19 14:57:28 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 19 15:22:57.948: INFO: sonobuoy-e2e-job-b3deceaf6b87402a from sonobuoy started at 2022-08-19 14:57:31 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container e2e ready: true, restart count 0
Aug 19 15:22:57.948: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 15:22:57.948: INFO: sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-57h77 from sonobuoy started at 2022-08-19 14:57:31 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.948: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 15:22:57.948: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 19 15:22:57.948: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-157-99.ec2.internal before test
Aug 19 15:22:57.975: INFO: aws-ebs-csi-driver-node-sknln from openshift-cluster-csi-drivers started at 2022-08-19 14:39:05 +0000 UTC (3 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container csi-driver ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 19 15:22:57.975: INFO: tuned-ttgq4 from openshift-cluster-node-tuning-operator started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container tuned ready: true, restart count 0
Aug 19 15:22:57.975: INFO: dns-default-m65sc from openshift-dns started at 2022-08-19 14:40:04 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container dns ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.975: INFO: node-resolver-dzbsb from openshift-dns started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 19 15:22:57.975: INFO: image-registry-79dbb9c69c-bbrd6 from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container registry ready: true, restart count 0
Aug 19 15:22:57.975: INFO: node-ca-2jbtx from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container node-ca ready: true, restart count 0
Aug 19 15:22:57.975: INFO: ingress-canary-cmqqt from openshift-ingress-canary started at 2022-08-19 14:40:04 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 19 15:22:57.975: INFO: router-default-7664744558-px7pp from openshift-ingress started at 2022-08-19 15:10:43 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container router ready: true, restart count 0
Aug 19 15:22:57.975: INFO: machine-config-daemon-zvdcx from openshift-machine-config-operator started at 2022-08-19 14:39:05 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 15:22:57.975: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-08-19 14:48:20 +0000 UTC (6 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container alertmanager ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 15:22:57.975: INFO: kube-state-metrics-9f5df78c9-s6s4x from openshift-monitoring started at 2022-08-19 14:41:11 +0000 UTC (3 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 19 15:22:57.975: INFO: node-exporter-zw8vw from openshift-monitoring started at 2022-08-19 14:41:11 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container node-exporter ready: true, restart count 0
Aug 19 15:22:57.975: INFO: openshift-state-metrics-6c88b54494-79p9n from openshift-monitoring started at 2022-08-19 14:41:11 +0000 UTC (3 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 19 15:22:57.975: INFO: prometheus-adapter-85bd9549d5-nnh58 from openshift-monitoring started at 2022-08-19 14:42:01 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 19 15:22:57.975: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-08-19 14:47:30 +0000 UTC (6 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container prometheus ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 19 15:22:57.975: INFO: prometheus-operator-admission-webhook-6db58c58f7-khv8v from openshift-monitoring started at 2022-08-19 15:10:44 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 19 15:22:57.975: INFO: telemeter-client-86d58945d5-4tqj6 from openshift-monitoring started at 2022-08-19 14:42:06 +0000 UTC (3 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container reload ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 19 15:22:57.975: INFO: thanos-querier-8559769b94-2pf4w from openshift-monitoring started at 2022-08-19 14:41:18 +0000 UTC (6 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container thanos-query ready: true, restart count 0
Aug 19 15:22:57.975: INFO: multus-additional-cni-plugins-n26hm from openshift-multus started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 19 15:22:57.975: INFO: multus-w6shh from openshift-multus started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container kube-multus ready: true, restart count 0
Aug 19 15:22:57.975: INFO: network-metrics-daemon-jhrx8 from openshift-multus started at 2022-08-19 14:39:05 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 19 15:22:57.975: INFO: network-check-target-xmjhc from openshift-network-diagnostics started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 19 15:22:57.975: INFO: sdn-m6b96 from openshift-sdn started at 2022-08-19 14:39:05 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container sdn ready: true, restart count 0
Aug 19 15:22:57.975: INFO: condition-test-5cq62 from replication-controller-6216 started at 2022-08-19 15:22:55 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container httpd ready: false, restart count 0
Aug 19 15:22:57.975: INFO: sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-fh9k8 from sonobuoy started at 2022-08-19 14:57:32 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.975: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 19 15:22:57.975: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-164-144.ec2.internal before test
Aug 19 15:22:57.999: INFO: aws-ebs-csi-driver-node-l9xm2 from openshift-cluster-csi-drivers started at 2022-08-19 14:38:49 +0000 UTC (3 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container csi-driver ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 19 15:22:57.999: INFO: tuned-f4rpl from openshift-cluster-node-tuning-operator started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container tuned ready: true, restart count 0
Aug 19 15:22:57.999: INFO: downloads-858cc8f4cb-lsxdb from openshift-console started at 2022-08-19 15:10:44 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container download-server ready: true, restart count 0
Aug 19 15:22:57.999: INFO: dns-default-ff8h7 from openshift-dns started at 2022-08-19 15:11:04 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container dns ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.999: INFO: node-resolver-pmmzs from openshift-dns started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 19 15:22:57.999: INFO: node-ca-xt2pz from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container node-ca ready: true, restart count 0
Aug 19 15:22:57.999: INFO: ingress-canary-h9x9x from openshift-ingress-canary started at 2022-08-19 15:10:45 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 19 15:22:57.999: INFO: machine-config-daemon-xz9st from openshift-machine-config-operator started at 2022-08-19 14:38:49 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 15:22:57.999: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-08-19 15:10:46 +0000 UTC (6 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container alertmanager ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 15:22:57.999: INFO: node-exporter-64n9p from openshift-monitoring started at 2022-08-19 14:41:12 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container node-exporter ready: true, restart count 0
Aug 19 15:22:57.999: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-08-19 15:10:46 +0000 UTC (6 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container prometheus ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 19 15:22:57.999: INFO: multus-85nv9 from openshift-multus started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container kube-multus ready: true, restart count 0
Aug 19 15:22:57.999: INFO: multus-additional-cni-plugins-9kc7x from openshift-multus started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 19 15:22:57.999: INFO: network-metrics-daemon-skfmf from openshift-multus started at 2022-08-19 14:38:49 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 19 15:22:57.999: INFO: network-check-source-5c64cf6958-ftsg5 from openshift-network-diagnostics started at 2022-08-19 15:10:44 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 19 15:22:57.999: INFO: network-check-target-vklmp from openshift-network-diagnostics started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 19 15:22:57.999: INFO: sdn-zfgs6 from openshift-sdn started at 2022-08-19 14:38:49 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container sdn ready: true, restart count 0
Aug 19 15:22:57.999: INFO: condition-test-qjtv2 from replication-controller-6216 started at 2022-08-19 15:22:55 +0000 UTC (1 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container httpd ready: false, restart count 0
Aug 19 15:22:57.999: INFO: sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-x4rlh from sonobuoy started at 2022-08-19 14:57:32 +0000 UTC (2 container statuses recorded)
Aug 19 15:22:57.999: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 15:22:57.999: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3db15cee-6820-4035-8f30-4026b09e2001 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-3db15cee-6820-4035-8f30-4026b09e2001 off the node ip-10-0-164-144.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3db15cee-6820-4035-8f30-4026b09e2001
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Aug 19 15:23:04.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5559" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:6.262 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":356,"completed":75,"skipped":1459,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:23:04.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:23:04.170: INFO: Creating ReplicaSet my-hostname-basic-44d20459-c851-4b7e-b6aa-0a4cb0d00ad0
W0819 15:23:04.180186      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-44d20459-c851-4b7e-b6aa-0a4cb0d00ad0" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-44d20459-c851-4b7e-b6aa-0a4cb0d00ad0" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-44d20459-c851-4b7e-b6aa-0a4cb0d00ad0" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-44d20459-c851-4b7e-b6aa-0a4cb0d00ad0" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 15:23:04.184: INFO: Pod name my-hostname-basic-44d20459-c851-4b7e-b6aa-0a4cb0d00ad0: Found 0 pods out of 1
Aug 19 15:23:09.189: INFO: Pod name my-hostname-basic-44d20459-c851-4b7e-b6aa-0a4cb0d00ad0: Found 1 pods out of 1
Aug 19 15:23:09.189: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-44d20459-c851-4b7e-b6aa-0a4cb0d00ad0" is running
Aug 19 15:23:09.192: INFO: Pod "my-hostname-basic-44d20459-c851-4b7e-b6aa-0a4cb0d00ad0-rrwdl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-19 15:23:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-19 15:23:06 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-19 15:23:06 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-19 15:23:04 +0000 UTC Reason: Message:}])
Aug 19 15:23:09.192: INFO: Trying to dial the pod
Aug 19 15:23:14.205: INFO: Controller my-hostname-basic-44d20459-c851-4b7e-b6aa-0a4cb0d00ad0: Got expected result from replica 1 [my-hostname-basic-44d20459-c851-4b7e-b6aa-0a4cb0d00ad0-rrwdl]: "my-hostname-basic-44d20459-c851-4b7e-b6aa-0a4cb0d00ad0-rrwdl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 19 15:23:14.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9749" for this suite.

• [SLOW TEST:10.082 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":76,"skipped":1478,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:23:14.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Aug 19 15:23:17.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5015" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":356,"completed":77,"skipped":1482,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:23:17.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  test/e2e/framework/framework.go:652
STEP: Request ServerVersion
STEP: Confirm major version
Aug 19 15:23:17.160: INFO: Major version: 1
STEP: Confirm minor version
Aug 19 15:23:17.160: INFO: cleanMinorVersion: 24
Aug 19 15:23:17.160: INFO: Minor version: 24
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:188
Aug 19 15:23:17.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-7790" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":356,"completed":78,"skipped":1510,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:23:17.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-bca75c27-1f8e-449d-b25b-58d375a7f04a
STEP: Creating a pod to test consume configMaps
Aug 19 15:23:17.260: INFO: Waiting up to 5m0s for pod "pod-configmaps-7eb548c2-8bed-431f-8760-0d7ae03c9cb1" in namespace "configmap-7005" to be "Succeeded or Failed"
Aug 19 15:23:17.265: INFO: Pod "pod-configmaps-7eb548c2-8bed-431f-8760-0d7ae03c9cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.364664ms
Aug 19 15:23:19.270: INFO: Pod "pod-configmaps-7eb548c2-8bed-431f-8760-0d7ae03c9cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009192712s
Aug 19 15:23:21.274: INFO: Pod "pod-configmaps-7eb548c2-8bed-431f-8760-0d7ae03c9cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014048837s
Aug 19 15:23:23.279: INFO: Pod "pod-configmaps-7eb548c2-8bed-431f-8760-0d7ae03c9cb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018368617s
STEP: Saw pod success
Aug 19 15:23:23.279: INFO: Pod "pod-configmaps-7eb548c2-8bed-431f-8760-0d7ae03c9cb1" satisfied condition "Succeeded or Failed"
Aug 19 15:23:23.282: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-configmaps-7eb548c2-8bed-431f-8760-0d7ae03c9cb1 container agnhost-container: <nil>
STEP: delete the pod
Aug 19 15:23:23.302: INFO: Waiting for pod pod-configmaps-7eb548c2-8bed-431f-8760-0d7ae03c9cb1 to disappear
Aug 19 15:23:23.306: INFO: Pod pod-configmaps-7eb548c2-8bed-431f-8760-0d7ae03c9cb1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 19 15:23:23.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7005" for this suite.

• [SLOW TEST:6.142 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":79,"skipped":1532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:23:23.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 19 15:23:23.393: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 19 15:23:23.399: INFO: starting watch
STEP: patching
STEP: updating
Aug 19 15:23:23.414: INFO: waiting for watch events with expected annotations
Aug 19 15:23:23.414: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:188
Aug 19 15:23:23.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-4629" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":356,"completed":80,"skipped":1608,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:23:23.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Aug 19 15:23:23.531: INFO: Waiting up to 5m0s for pod "downward-api-7d848f44-8630-46ef-be3e-ab49dc0509d6" in namespace "downward-api-7901" to be "Succeeded or Failed"
Aug 19 15:23:23.537: INFO: Pod "downward-api-7d848f44-8630-46ef-be3e-ab49dc0509d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.407057ms
Aug 19 15:23:25.542: INFO: Pod "downward-api-7d848f44-8630-46ef-be3e-ab49dc0509d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010585529s
Aug 19 15:23:27.547: INFO: Pod "downward-api-7d848f44-8630-46ef-be3e-ab49dc0509d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015886708s
Aug 19 15:23:29.551: INFO: Pod "downward-api-7d848f44-8630-46ef-be3e-ab49dc0509d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020282859s
STEP: Saw pod success
Aug 19 15:23:29.551: INFO: Pod "downward-api-7d848f44-8630-46ef-be3e-ab49dc0509d6" satisfied condition "Succeeded or Failed"
Aug 19 15:23:29.554: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downward-api-7d848f44-8630-46ef-be3e-ab49dc0509d6 container dapi-container: <nil>
STEP: delete the pod
Aug 19 15:23:29.576: INFO: Waiting for pod downward-api-7d848f44-8630-46ef-be3e-ab49dc0509d6 to disappear
Aug 19 15:23:29.578: INFO: Pod downward-api-7d848f44-8630-46ef-be3e-ab49dc0509d6 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Aug 19 15:23:29.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7901" for this suite.

• [SLOW TEST:6.119 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":356,"completed":81,"skipped":1623,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:23:29.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 19 15:23:29.668: INFO: Waiting up to 5m0s for pod "pod-6219e560-6630-486e-91ee-7a2d2429a7c0" in namespace "emptydir-3301" to be "Succeeded or Failed"
Aug 19 15:23:29.677: INFO: Pod "pod-6219e560-6630-486e-91ee-7a2d2429a7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.881442ms
Aug 19 15:23:31.681: INFO: Pod "pod-6219e560-6630-486e-91ee-7a2d2429a7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012882819s
Aug 19 15:23:33.685: INFO: Pod "pod-6219e560-6630-486e-91ee-7a2d2429a7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017132968s
Aug 19 15:23:35.692: INFO: Pod "pod-6219e560-6630-486e-91ee-7a2d2429a7c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024190209s
STEP: Saw pod success
Aug 19 15:23:35.692: INFO: Pod "pod-6219e560-6630-486e-91ee-7a2d2429a7c0" satisfied condition "Succeeded or Failed"
Aug 19 15:23:35.696: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-6219e560-6630-486e-91ee-7a2d2429a7c0 container test-container: <nil>
STEP: delete the pod
Aug 19 15:23:35.713: INFO: Waiting for pod pod-6219e560-6630-486e-91ee-7a2d2429a7c0 to disappear
Aug 19 15:23:35.717: INFO: Pod pod-6219e560-6630-486e-91ee-7a2d2429a7c0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 19 15:23:35.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3301" for this suite.

• [SLOW TEST:6.140 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":82,"skipped":1650,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:23:35.729: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Aug 19 15:23:35.808: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:23:37.811: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:23:39.813: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 19 15:23:40.831: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 19 15:23:41.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-576" for this suite.

• [SLOW TEST:6.134 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":356,"completed":83,"skipped":1660,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:23:41.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pod templates
Aug 19 15:23:41.906: INFO: created test-podtemplate-1
Aug 19 15:23:41.910: INFO: created test-podtemplate-2
Aug 19 15:23:41.916: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Aug 19 15:23:41.922: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Aug 19 15:23:41.968: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Aug 19 15:23:41.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3850" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":356,"completed":84,"skipped":1690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:23:41.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-faaf6af3-9689-4cc2-8db0-4588f75f62fc
STEP: Creating a pod to test consume secrets
Aug 19 15:23:42.052: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0f727951-870a-4e53-8dd6-5803453e2eb9" in namespace "projected-2310" to be "Succeeded or Failed"
Aug 19 15:23:42.058: INFO: Pod "pod-projected-secrets-0f727951-870a-4e53-8dd6-5803453e2eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.669634ms
Aug 19 15:23:44.061: INFO: Pod "pod-projected-secrets-0f727951-870a-4e53-8dd6-5803453e2eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009340061s
Aug 19 15:23:46.066: INFO: Pod "pod-projected-secrets-0f727951-870a-4e53-8dd6-5803453e2eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014469723s
Aug 19 15:23:48.071: INFO: Pod "pod-projected-secrets-0f727951-870a-4e53-8dd6-5803453e2eb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018836235s
STEP: Saw pod success
Aug 19 15:23:48.071: INFO: Pod "pod-projected-secrets-0f727951-870a-4e53-8dd6-5803453e2eb9" satisfied condition "Succeeded or Failed"
Aug 19 15:23:48.074: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-projected-secrets-0f727951-870a-4e53-8dd6-5803453e2eb9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 19 15:23:48.090: INFO: Waiting for pod pod-projected-secrets-0f727951-870a-4e53-8dd6-5803453e2eb9 to disappear
Aug 19 15:23:48.093: INFO: Pod pod-projected-secrets-0f727951-870a-4e53-8dd6-5803453e2eb9 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 19 15:23:48.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2310" for this suite.

• [SLOW TEST:6.129 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":85,"skipped":1728,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:23:48.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 19 15:23:48.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-46" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":356,"completed":86,"skipped":1731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:23:48.209: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Aug 19 15:23:54.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8776" for this suite.
STEP: Destroying namespace "nsdeletetest-1821" for this suite.
Aug 19 15:23:54.394: INFO: Namespace nsdeletetest-1821 was already deleted
STEP: Destroying namespace "nsdeletetest-5774" for this suite.

• [SLOW TEST:6.198 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":356,"completed":87,"skipped":1753,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:23:54.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 19 15:23:54.458: INFO: Waiting up to 5m0s for pod "pod-dddc0c3b-1ce9-4ec1-b138-ae262801358f" in namespace "emptydir-4749" to be "Succeeded or Failed"
Aug 19 15:23:54.461: INFO: Pod "pod-dddc0c3b-1ce9-4ec1-b138-ae262801358f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.900663ms
Aug 19 15:23:56.465: INFO: Pod "pod-dddc0c3b-1ce9-4ec1-b138-ae262801358f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006909182s
Aug 19 15:23:58.470: INFO: Pod "pod-dddc0c3b-1ce9-4ec1-b138-ae262801358f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011286419s
Aug 19 15:24:00.474: INFO: Pod "pod-dddc0c3b-1ce9-4ec1-b138-ae262801358f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015515766s
STEP: Saw pod success
Aug 19 15:24:00.474: INFO: Pod "pod-dddc0c3b-1ce9-4ec1-b138-ae262801358f" satisfied condition "Succeeded or Failed"
Aug 19 15:24:00.477: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-dddc0c3b-1ce9-4ec1-b138-ae262801358f container test-container: <nil>
STEP: delete the pod
Aug 19 15:24:00.493: INFO: Waiting for pod pod-dddc0c3b-1ce9-4ec1-b138-ae262801358f to disappear
Aug 19 15:24:00.497: INFO: Pod pod-dddc0c3b-1ce9-4ec1-b138-ae262801358f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 19 15:24:00.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4749" for this suite.

• [SLOW TEST:6.100 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":88,"skipped":1762,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:24:00.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:24:00.547: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-1a8b9bb3-0078-40ed-a578-a6089fb1819c
STEP: Creating configMap with name cm-test-opt-upd-132219d0-16ef-42d4-8459-d0a952167ce1
STEP: Creating the pod
Aug 19 15:24:00.590: INFO: The status of Pod pod-configmaps-9c27ef8d-e9ce-4306-9e07-6a604fe1c98b is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:24:02.596: INFO: The status of Pod pod-configmaps-9c27ef8d-e9ce-4306-9e07-6a604fe1c98b is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:24:04.594: INFO: The status of Pod pod-configmaps-9c27ef8d-e9ce-4306-9e07-6a604fe1c98b is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-1a8b9bb3-0078-40ed-a578-a6089fb1819c
STEP: Updating configmap cm-test-opt-upd-132219d0-16ef-42d4-8459-d0a952167ce1
STEP: Creating configMap with name cm-test-opt-create-5f2bd706-44ac-460d-a09e-bc907365ce0b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 19 15:24:06.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4015" for this suite.

• [SLOW TEST:6.161 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":89,"skipped":1800,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:24:06.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 15:24:06.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9680" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":356,"completed":90,"skipped":1804,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:24:06.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 19 15:24:06.884: INFO: Waiting up to 5m0s for pod "pod-10f8caf3-14e5-4c9b-b5b1-3bd8379f4c36" in namespace "emptydir-3302" to be "Succeeded or Failed"
Aug 19 15:24:06.890: INFO: Pod "pod-10f8caf3-14e5-4c9b-b5b1-3bd8379f4c36": Phase="Pending", Reason="", readiness=false. Elapsed: 6.148882ms
Aug 19 15:24:08.894: INFO: Pod "pod-10f8caf3-14e5-4c9b-b5b1-3bd8379f4c36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009763126s
Aug 19 15:24:10.899: INFO: Pod "pod-10f8caf3-14e5-4c9b-b5b1-3bd8379f4c36": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014784185s
Aug 19 15:24:12.902: INFO: Pod "pod-10f8caf3-14e5-4c9b-b5b1-3bd8379f4c36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018033088s
STEP: Saw pod success
Aug 19 15:24:12.902: INFO: Pod "pod-10f8caf3-14e5-4c9b-b5b1-3bd8379f4c36" satisfied condition "Succeeded or Failed"
Aug 19 15:24:12.905: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-10f8caf3-14e5-4c9b-b5b1-3bd8379f4c36 container test-container: <nil>
STEP: delete the pod
Aug 19 15:24:12.929: INFO: Waiting for pod pod-10f8caf3-14e5-4c9b-b5b1-3bd8379f4c36 to disappear
Aug 19 15:24:12.932: INFO: Pod pod-10f8caf3-14e5-4c9b-b5b1-3bd8379f4c36 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 19 15:24:12.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3302" for this suite.

• [SLOW TEST:6.166 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":91,"skipped":1832,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:24:12.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service multi-endpoint-test in namespace services-3012
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3012 to expose endpoints map[]
Aug 19 15:24:12.995: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Aug 19 15:24:14.003: INFO: successfully validated that service multi-endpoint-test in namespace services-3012 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3012
Aug 19 15:24:14.021: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:24:16.025: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:24:18.024: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3012 to expose endpoints map[pod1:[100]]
Aug 19 15:24:18.040: INFO: successfully validated that service multi-endpoint-test in namespace services-3012 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3012
Aug 19 15:24:18.056: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:24:20.061: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:24:22.061: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3012 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 19 15:24:22.077: INFO: successfully validated that service multi-endpoint-test in namespace services-3012 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Aug 19 15:24:22.077: INFO: Creating new exec pod
Aug 19 15:24:27.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3012 exec execpod6kxgn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Aug 19 15:24:27.210: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Aug 19 15:24:27.210: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 15:24:27.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3012 exec execpod6kxgn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.122.190 80'
Aug 19 15:24:27.304: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.122.190 80\nConnection to 172.30.122.190 80 port [tcp/http] succeeded!\n"
Aug 19 15:24:27.304: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 15:24:27.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3012 exec execpod6kxgn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Aug 19 15:24:27.405: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Aug 19 15:24:27.405: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 15:24:27.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3012 exec execpod6kxgn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.122.190 81'
Aug 19 15:24:27.497: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.122.190 81\nConnection to 172.30.122.190 81 port [tcp/*] succeeded!\n"
Aug 19 15:24:27.497: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3012
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3012 to expose endpoints map[pod2:[101]]
Aug 19 15:24:27.519: INFO: successfully validated that service multi-endpoint-test in namespace services-3012 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3012
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3012 to expose endpoints map[]
Aug 19 15:24:28.550: INFO: successfully validated that service multi-endpoint-test in namespace services-3012 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 15:24:28.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3012" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:15.648 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":356,"completed":92,"skipped":1839,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:24:28.592: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Aug 19 15:24:28.659: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 19 15:24:33.666: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 19 15:24:33.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7502" for this suite.

• [SLOW TEST:5.108 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":356,"completed":93,"skipped":1866,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:24:33.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-57e6d288-bc6a-41d9-bc70-eef14ce3ae49
STEP: Creating a pod to test consume secrets
Aug 19 15:24:33.771: INFO: Waiting up to 5m0s for pod "pod-secrets-51a4bb8a-a743-44d6-af80-7119e38a3807" in namespace "secrets-6027" to be "Succeeded or Failed"
Aug 19 15:24:33.776: INFO: Pod "pod-secrets-51a4bb8a-a743-44d6-af80-7119e38a3807": Phase="Pending", Reason="", readiness=false. Elapsed: 5.616341ms
Aug 19 15:24:35.780: INFO: Pod "pod-secrets-51a4bb8a-a743-44d6-af80-7119e38a3807": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00924411s
Aug 19 15:24:37.785: INFO: Pod "pod-secrets-51a4bb8a-a743-44d6-af80-7119e38a3807": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014197371s
Aug 19 15:24:39.789: INFO: Pod "pod-secrets-51a4bb8a-a743-44d6-af80-7119e38a3807": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018350087s
STEP: Saw pod success
Aug 19 15:24:39.789: INFO: Pod "pod-secrets-51a4bb8a-a743-44d6-af80-7119e38a3807" satisfied condition "Succeeded or Failed"
Aug 19 15:24:39.792: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-secrets-51a4bb8a-a743-44d6-af80-7119e38a3807 container secret-env-test: <nil>
STEP: delete the pod
Aug 19 15:24:39.812: INFO: Waiting for pod pod-secrets-51a4bb8a-a743-44d6-af80-7119e38a3807 to disappear
Aug 19 15:24:39.816: INFO: Pod pod-secrets-51a4bb8a-a743-44d6-af80-7119e38a3807 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Aug 19 15:24:39.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6027" for this suite.

• [SLOW TEST:6.126 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":356,"completed":94,"skipped":1887,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:24:39.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
W0819 15:24:39.865755      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 15:24:39.873: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 19 15:24:44.877: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 19 15:24:44.877: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 19 15:24:44.900: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1384  627a1195-7882-47d3-a6b3-6d8b86c7d315 52385 1 2022-08-19 15:24:44 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-08-19 15:24:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0090f4478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 19 15:24:44.904: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Aug 19 15:24:44.904: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 19 15:24:44.904: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1384  dd8e1960-d759-4b8f-973e-a47318e06a1f 52389 1 2022-08-19 15:24:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 627a1195-7882-47d3-a6b3-6d8b86c7d315 0xc0090f47b7 0xc0090f47b8}] []  [{e2e.test Update apps/v1 2022-08-19 15:24:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 15:24:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-08-19 15:24:44 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"627a1195-7882-47d3-a6b3-6d8b86c7d315\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0090f4878 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 19 15:24:44.908: INFO: Pod "test-cleanup-controller-dtz2k" is available:
&Pod{ObjectMeta:{test-cleanup-controller-dtz2k test-cleanup-controller- deployment-1384  5fe041e5-3d23-470b-ae63-9543521becfd 52347 0 2022-08-19 15:24:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.164"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.164"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller dd8e1960-d759-4b8f-973e-a47318e06a1f 0xc008fd0467 0xc008fd0468}] []  [{kube-controller-manager Update v1 2022-08-19 15:24:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dd8e1960-d759-4b8f-973e-a47318e06a1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-08-19 15:24:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-19 15:24:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxbq2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxbq2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-164-144.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c40,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:24:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:24:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:24:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:24:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.164.144,PodIP:10.131.0.164,StartTime:2022-08-19 15:24:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 15:24:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8e8d44b3de7b2d58e5bcd8873a81ad0716ecdbce675da0400c51b1a73d62f4f8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 19 15:24:44.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1384" for this suite.

• [SLOW TEST:5.092 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":356,"completed":95,"skipped":1920,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:24:44.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-19435c68-04a2-4be3-b5c9-83f17b06604b
STEP: Creating a pod to test consume secrets
Aug 19 15:24:45.001: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d307a6eb-1994-429a-8668-6cbee4a73c64" in namespace "projected-2528" to be "Succeeded or Failed"
Aug 19 15:24:45.008: INFO: Pod "pod-projected-secrets-d307a6eb-1994-429a-8668-6cbee4a73c64": Phase="Pending", Reason="", readiness=false. Elapsed: 6.497967ms
Aug 19 15:24:47.012: INFO: Pod "pod-projected-secrets-d307a6eb-1994-429a-8668-6cbee4a73c64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010755126s
Aug 19 15:24:49.016: INFO: Pod "pod-projected-secrets-d307a6eb-1994-429a-8668-6cbee4a73c64": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014738754s
Aug 19 15:24:51.021: INFO: Pod "pod-projected-secrets-d307a6eb-1994-429a-8668-6cbee4a73c64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019283946s
STEP: Saw pod success
Aug 19 15:24:51.021: INFO: Pod "pod-projected-secrets-d307a6eb-1994-429a-8668-6cbee4a73c64" satisfied condition "Succeeded or Failed"
Aug 19 15:24:51.024: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-projected-secrets-d307a6eb-1994-429a-8668-6cbee4a73c64 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 19 15:24:51.046: INFO: Waiting for pod pod-projected-secrets-d307a6eb-1994-429a-8668-6cbee4a73c64 to disappear
Aug 19 15:24:51.048: INFO: Pod pod-projected-secrets-d307a6eb-1994-429a-8668-6cbee4a73c64 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 19 15:24:51.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2528" for this suite.

• [SLOW TEST:6.140 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":96,"skipped":1934,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:24:51.059: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:188
Aug 19 15:24:51.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-775" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":356,"completed":97,"skipped":1951,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:24:51.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-744eca81-0970-4c0d-ab21-c4a9b8eb1711 in namespace container-probe-5075
Aug 19 15:24:53.239: INFO: Started pod busybox-744eca81-0970-4c0d-ab21-c4a9b8eb1711 in namespace container-probe-5075
STEP: checking the pod's current state and verifying that restartCount is present
Aug 19 15:24:53.242: INFO: Initial restart count of pod busybox-744eca81-0970-4c0d-ab21-c4a9b8eb1711 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 19 15:28:53.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5075" for this suite.

• [SLOW TEST:242.682 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":98,"skipped":1971,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:28:53.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Aug 19 15:28:53.897: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 19 15:28:59.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9384" for this suite.

• [SLOW TEST:5.871 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":356,"completed":99,"skipped":1982,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:28:59.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 19 15:28:59.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-23" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":356,"completed":100,"skipped":2037,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:28:59.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-xz2fp in namespace proxy-4566
I0819 15:29:00.076171      22 runners.go:193] Created replication controller with name: proxy-service-xz2fp, namespace: proxy-4566, replica count: 1
I0819 15:29:01.126951      22 runners.go:193] proxy-service-xz2fp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0819 15:29:02.127489      22 runners.go:193] proxy-service-xz2fp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0819 15:29:03.128549      22 runners.go:193] proxy-service-xz2fp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 19 15:29:03.131: INFO: setup took 3.136680335s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 19 15:29:03.138: INFO: (0) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 6.597007ms)
Aug 19 15:29:03.138: INFO: (0) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 6.898344ms)
Aug 19 15:29:03.138: INFO: (0) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 7.093109ms)
Aug 19 15:29:03.139: INFO: (0) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 7.146085ms)
Aug 19 15:29:03.139: INFO: (0) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 7.22528ms)
Aug 19 15:29:03.140: INFO: (0) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 8.212422ms)
Aug 19 15:29:03.140: INFO: (0) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 8.394379ms)
Aug 19 15:29:03.140: INFO: (0) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 8.790456ms)
Aug 19 15:29:03.142: INFO: (0) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 10.745949ms)
Aug 19 15:29:03.142: INFO: (0) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 10.690366ms)
Aug 19 15:29:03.143: INFO: (0) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 11.224505ms)
Aug 19 15:29:03.143: INFO: (0) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 11.227041ms)
Aug 19 15:29:03.144: INFO: (0) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 12.331952ms)
Aug 19 15:29:03.144: INFO: (0) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 12.266064ms)
Aug 19 15:29:03.146: INFO: (0) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 15.079679ms)
Aug 19 15:29:03.147: INFO: (0) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 15.233251ms)
Aug 19 15:29:03.152: INFO: (1) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 5.099875ms)
Aug 19 15:29:03.152: INFO: (1) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 5.172042ms)
Aug 19 15:29:03.152: INFO: (1) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 5.453915ms)
Aug 19 15:29:03.152: INFO: (1) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 5.824571ms)
Aug 19 15:29:03.154: INFO: (1) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 7.069768ms)
Aug 19 15:29:03.154: INFO: (1) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 7.270479ms)
Aug 19 15:29:03.155: INFO: (1) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 8.648278ms)
Aug 19 15:29:03.156: INFO: (1) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 9.912118ms)
Aug 19 15:29:03.157: INFO: (1) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 10.02215ms)
Aug 19 15:29:03.157: INFO: (1) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 10.046809ms)
Aug 19 15:29:03.157: INFO: (1) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 10.153596ms)
Aug 19 15:29:03.157: INFO: (1) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 10.290735ms)
Aug 19 15:29:03.157: INFO: (1) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 10.231606ms)
Aug 19 15:29:03.157: INFO: (1) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 10.495781ms)
Aug 19 15:29:03.158: INFO: (1) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 11.064205ms)
Aug 19 15:29:03.158: INFO: (1) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 11.347463ms)
Aug 19 15:29:03.162: INFO: (2) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 3.689093ms)
Aug 19 15:29:03.162: INFO: (2) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 4.173304ms)
Aug 19 15:29:03.162: INFO: (2) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 4.139216ms)
Aug 19 15:29:03.163: INFO: (2) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 4.694044ms)
Aug 19 15:29:03.164: INFO: (2) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 5.524427ms)
Aug 19 15:29:03.164: INFO: (2) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 5.587039ms)
Aug 19 15:29:03.164: INFO: (2) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 5.600581ms)
Aug 19 15:29:03.164: INFO: (2) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 6.038041ms)
Aug 19 15:29:03.164: INFO: (2) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 6.05091ms)
Aug 19 15:29:03.164: INFO: (2) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 6.072526ms)
Aug 19 15:29:03.164: INFO: (2) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 6.124768ms)
Aug 19 15:29:03.164: INFO: (2) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 6.262688ms)
Aug 19 15:29:03.165: INFO: (2) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 6.524156ms)
Aug 19 15:29:03.165: INFO: (2) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 6.707696ms)
Aug 19 15:29:03.165: INFO: (2) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 6.943273ms)
Aug 19 15:29:03.165: INFO: (2) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 7.345044ms)
Aug 19 15:29:03.170: INFO: (3) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 3.995063ms)
Aug 19 15:29:03.170: INFO: (3) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 4.40923ms)
Aug 19 15:29:03.171: INFO: (3) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 5.193478ms)
Aug 19 15:29:03.171: INFO: (3) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 5.772955ms)
Aug 19 15:29:03.171: INFO: (3) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 5.84796ms)
Aug 19 15:29:03.172: INFO: (3) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 6.371879ms)
Aug 19 15:29:03.172: INFO: (3) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 6.485719ms)
Aug 19 15:29:03.172: INFO: (3) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 6.790565ms)
Aug 19 15:29:03.172: INFO: (3) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 6.942971ms)
Aug 19 15:29:03.172: INFO: (3) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 6.81741ms)
Aug 19 15:29:03.172: INFO: (3) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 6.859824ms)
Aug 19 15:29:03.173: INFO: (3) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 7.479426ms)
Aug 19 15:29:03.173: INFO: (3) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 7.482867ms)
Aug 19 15:29:03.173: INFO: (3) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 7.580392ms)
Aug 19 15:29:03.173: INFO: (3) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 7.940968ms)
Aug 19 15:29:03.174: INFO: (3) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 8.141864ms)
Aug 19 15:29:03.178: INFO: (4) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 3.858708ms)
Aug 19 15:29:03.178: INFO: (4) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 4.251722ms)
Aug 19 15:29:03.178: INFO: (4) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 4.421825ms)
Aug 19 15:29:03.179: INFO: (4) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 5.314924ms)
Aug 19 15:29:03.179: INFO: (4) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 5.386375ms)
Aug 19 15:29:03.179: INFO: (4) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 5.370443ms)
Aug 19 15:29:03.180: INFO: (4) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 5.873206ms)
Aug 19 15:29:03.180: INFO: (4) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 5.842744ms)
Aug 19 15:29:03.180: INFO: (4) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 6.224087ms)
Aug 19 15:29:03.180: INFO: (4) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 6.33624ms)
Aug 19 15:29:03.180: INFO: (4) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 6.284725ms)
Aug 19 15:29:03.180: INFO: (4) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 6.475194ms)
Aug 19 15:29:03.181: INFO: (4) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 6.868995ms)
Aug 19 15:29:03.181: INFO: (4) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 6.8209ms)
Aug 19 15:29:03.181: INFO: (4) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 7.490796ms)
Aug 19 15:29:03.182: INFO: (4) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 8.196854ms)
Aug 19 15:29:03.186: INFO: (5) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 4.05785ms)
Aug 19 15:29:03.187: INFO: (5) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 4.71988ms)
Aug 19 15:29:03.188: INFO: (5) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 5.511971ms)
Aug 19 15:29:03.188: INFO: (5) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 5.584501ms)
Aug 19 15:29:03.188: INFO: (5) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 5.629663ms)
Aug 19 15:29:03.188: INFO: (5) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 5.988235ms)
Aug 19 15:29:03.188: INFO: (5) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 6.227618ms)
Aug 19 15:29:03.189: INFO: (5) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 6.381835ms)
Aug 19 15:29:03.189: INFO: (5) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 6.879645ms)
Aug 19 15:29:03.190: INFO: (5) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 7.5632ms)
Aug 19 15:29:03.190: INFO: (5) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 7.517715ms)
Aug 19 15:29:03.190: INFO: (5) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 8.043478ms)
Aug 19 15:29:03.190: INFO: (5) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 8.170889ms)
Aug 19 15:29:03.191: INFO: (5) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 8.606503ms)
Aug 19 15:29:03.191: INFO: (5) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 8.929614ms)
Aug 19 15:29:03.191: INFO: (5) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 8.993358ms)
Aug 19 15:29:03.195: INFO: (6) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 3.957869ms)
Aug 19 15:29:03.195: INFO: (6) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 3.918878ms)
Aug 19 15:29:03.195: INFO: (6) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 4.07844ms)
Aug 19 15:29:03.196: INFO: (6) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 4.790236ms)
Aug 19 15:29:03.196: INFO: (6) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 4.881866ms)
Aug 19 15:29:03.197: INFO: (6) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 5.204604ms)
Aug 19 15:29:03.198: INFO: (6) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 6.370029ms)
Aug 19 15:29:03.198: INFO: (6) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 6.298543ms)
Aug 19 15:29:03.198: INFO: (6) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 6.57239ms)
Aug 19 15:29:03.198: INFO: (6) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 6.717558ms)
Aug 19 15:29:03.198: INFO: (6) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 7.035672ms)
Aug 19 15:29:03.199: INFO: (6) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 7.079587ms)
Aug 19 15:29:03.199: INFO: (6) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 7.519039ms)
Aug 19 15:29:03.199: INFO: (6) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 7.577747ms)
Aug 19 15:29:03.199: INFO: (6) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 8.008402ms)
Aug 19 15:29:03.200: INFO: (6) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 8.620251ms)
Aug 19 15:29:03.204: INFO: (7) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 3.847475ms)
Aug 19 15:29:03.204: INFO: (7) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 4.212916ms)
Aug 19 15:29:03.205: INFO: (7) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 4.721841ms)
Aug 19 15:29:03.205: INFO: (7) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 4.942456ms)
Aug 19 15:29:03.205: INFO: (7) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 5.111661ms)
Aug 19 15:29:03.205: INFO: (7) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 5.099693ms)
Aug 19 15:29:03.206: INFO: (7) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 6.31076ms)
Aug 19 15:29:03.206: INFO: (7) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 6.123025ms)
Aug 19 15:29:03.206: INFO: (7) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 6.279878ms)
Aug 19 15:29:03.206: INFO: (7) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 6.175629ms)
Aug 19 15:29:03.206: INFO: (7) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 6.227437ms)
Aug 19 15:29:03.207: INFO: (7) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 6.316723ms)
Aug 19 15:29:03.208: INFO: (7) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 7.463398ms)
Aug 19 15:29:03.208: INFO: (7) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 7.679529ms)
Aug 19 15:29:03.208: INFO: (7) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 7.740208ms)
Aug 19 15:29:03.209: INFO: (7) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 8.881119ms)
Aug 19 15:29:03.212: INFO: (8) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 3.522046ms)
Aug 19 15:29:03.213: INFO: (8) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 4.154557ms)
Aug 19 15:29:03.213: INFO: (8) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 4.166314ms)
Aug 19 15:29:03.215: INFO: (8) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 5.559236ms)
Aug 19 15:29:03.215: INFO: (8) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 5.639462ms)
Aug 19 15:29:03.215: INFO: (8) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 6.303397ms)
Aug 19 15:29:03.215: INFO: (8) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 6.215303ms)
Aug 19 15:29:03.215: INFO: (8) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 6.283214ms)
Aug 19 15:29:03.216: INFO: (8) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 6.548535ms)
Aug 19 15:29:03.216: INFO: (8) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 6.521819ms)
Aug 19 15:29:03.216: INFO: (8) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 6.692618ms)
Aug 19 15:29:03.216: INFO: (8) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 6.955955ms)
Aug 19 15:29:03.216: INFO: (8) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 6.947263ms)
Aug 19 15:29:03.216: INFO: (8) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 6.998849ms)
Aug 19 15:29:03.216: INFO: (8) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 7.057647ms)
Aug 19 15:29:03.217: INFO: (8) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 8.037738ms)
Aug 19 15:29:03.221: INFO: (9) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 4.027746ms)
Aug 19 15:29:03.221: INFO: (9) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 4.140893ms)
Aug 19 15:29:03.222: INFO: (9) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 4.285809ms)
Aug 19 15:29:03.222: INFO: (9) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 4.749267ms)
Aug 19 15:29:03.223: INFO: (9) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 5.390869ms)
Aug 19 15:29:03.223: INFO: (9) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 5.509719ms)
Aug 19 15:29:03.223: INFO: (9) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 5.827029ms)
Aug 19 15:29:03.223: INFO: (9) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 5.689046ms)
Aug 19 15:29:03.223: INFO: (9) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 6.197015ms)
Aug 19 15:29:03.223: INFO: (9) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 6.127521ms)
Aug 19 15:29:03.223: INFO: (9) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 6.194347ms)
Aug 19 15:29:03.224: INFO: (9) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 6.411908ms)
Aug 19 15:29:03.224: INFO: (9) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 7.224773ms)
Aug 19 15:29:03.225: INFO: (9) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 7.359356ms)
Aug 19 15:29:03.225: INFO: (9) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 7.87878ms)
Aug 19 15:29:03.225: INFO: (9) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 7.905339ms)
Aug 19 15:29:03.229: INFO: (10) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 4.024497ms)
Aug 19 15:29:03.229: INFO: (10) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 4.155443ms)
Aug 19 15:29:03.230: INFO: (10) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 4.456053ms)
Aug 19 15:29:03.230: INFO: (10) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 4.679455ms)
Aug 19 15:29:03.230: INFO: (10) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 5.176818ms)
Aug 19 15:29:03.231: INFO: (10) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 5.633801ms)
Aug 19 15:29:03.231: INFO: (10) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 5.976596ms)
Aug 19 15:29:03.231: INFO: (10) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 6.052478ms)
Aug 19 15:29:03.231: INFO: (10) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 6.168227ms)
Aug 19 15:29:03.231: INFO: (10) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 6.230563ms)
Aug 19 15:29:03.231: INFO: (10) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 6.168476ms)
Aug 19 15:29:03.232: INFO: (10) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 6.45885ms)
Aug 19 15:29:03.233: INFO: (10) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 7.295003ms)
Aug 19 15:29:03.233: INFO: (10) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 7.85034ms)
Aug 19 15:29:03.235: INFO: (10) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 10.193794ms)
Aug 19 15:29:03.236: INFO: (10) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 10.700496ms)
Aug 19 15:29:03.240: INFO: (11) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 3.830501ms)
Aug 19 15:29:03.241: INFO: (11) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 4.942006ms)
Aug 19 15:29:03.242: INFO: (11) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 5.633213ms)
Aug 19 15:29:03.242: INFO: (11) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 5.699503ms)
Aug 19 15:29:03.242: INFO: (11) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 5.888484ms)
Aug 19 15:29:03.242: INFO: (11) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 5.904889ms)
Aug 19 15:29:03.242: INFO: (11) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 6.079789ms)
Aug 19 15:29:03.242: INFO: (11) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 6.251774ms)
Aug 19 15:29:03.242: INFO: (11) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 6.332126ms)
Aug 19 15:29:03.243: INFO: (11) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 6.236573ms)
Aug 19 15:29:03.243: INFO: (11) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 6.582536ms)
Aug 19 15:29:03.243: INFO: (11) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 6.577747ms)
Aug 19 15:29:03.243: INFO: (11) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 6.554854ms)
Aug 19 15:29:03.243: INFO: (11) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 6.973222ms)
Aug 19 15:29:03.244: INFO: (11) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 8.033287ms)
Aug 19 15:29:03.244: INFO: (11) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 7.985014ms)
Aug 19 15:29:03.248: INFO: (12) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 3.960161ms)
Aug 19 15:29:03.248: INFO: (12) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 4.132625ms)
Aug 19 15:29:03.249: INFO: (12) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 4.313254ms)
Aug 19 15:29:03.249: INFO: (12) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 4.614999ms)
Aug 19 15:29:03.250: INFO: (12) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 5.393085ms)
Aug 19 15:29:03.250: INFO: (12) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 5.331512ms)
Aug 19 15:29:03.250: INFO: (12) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 5.427912ms)
Aug 19 15:29:03.250: INFO: (12) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 5.680228ms)
Aug 19 15:29:03.250: INFO: (12) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 5.905067ms)
Aug 19 15:29:03.250: INFO: (12) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 6.074329ms)
Aug 19 15:29:03.251: INFO: (12) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 6.510175ms)
Aug 19 15:29:03.251: INFO: (12) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 7.243437ms)
Aug 19 15:29:03.252: INFO: (12) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 7.168587ms)
Aug 19 15:29:03.252: INFO: (12) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 7.76881ms)
Aug 19 15:29:03.252: INFO: (12) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 8.236857ms)
Aug 19 15:29:03.252: INFO: (12) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 8.25465ms)
Aug 19 15:29:03.256: INFO: (13) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 3.499304ms)
Aug 19 15:29:03.256: INFO: (13) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 3.79397ms)
Aug 19 15:29:03.257: INFO: (13) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 4.530602ms)
Aug 19 15:29:03.258: INFO: (13) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 4.779366ms)
Aug 19 15:29:03.258: INFO: (13) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 5.13541ms)
Aug 19 15:29:03.258: INFO: (13) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 5.501568ms)
Aug 19 15:29:03.258: INFO: (13) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 5.610114ms)
Aug 19 15:29:03.258: INFO: (13) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 5.810089ms)
Aug 19 15:29:03.259: INFO: (13) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 5.96531ms)
Aug 19 15:29:03.259: INFO: (13) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 6.012151ms)
Aug 19 15:29:03.259: INFO: (13) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 6.044437ms)
Aug 19 15:29:03.259: INFO: (13) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 6.470122ms)
Aug 19 15:29:03.259: INFO: (13) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 6.521155ms)
Aug 19 15:29:03.260: INFO: (13) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 7.426231ms)
Aug 19 15:29:03.260: INFO: (13) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 7.621167ms)
Aug 19 15:29:03.260: INFO: (13) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 7.461199ms)
Aug 19 15:29:03.264: INFO: (14) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 4.007507ms)
Aug 19 15:29:03.264: INFO: (14) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 3.875747ms)
Aug 19 15:29:03.265: INFO: (14) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 4.326472ms)
Aug 19 15:29:03.265: INFO: (14) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 4.67473ms)
Aug 19 15:29:03.266: INFO: (14) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 5.398367ms)
Aug 19 15:29:03.266: INFO: (14) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 5.737459ms)
Aug 19 15:29:03.266: INFO: (14) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 5.806053ms)
Aug 19 15:29:03.266: INFO: (14) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 5.728588ms)
Aug 19 15:29:03.266: INFO: (14) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 5.77817ms)
Aug 19 15:29:03.266: INFO: (14) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 5.992064ms)
Aug 19 15:29:03.266: INFO: (14) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 6.142639ms)
Aug 19 15:29:03.266: INFO: (14) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 6.032874ms)
Aug 19 15:29:03.267: INFO: (14) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 6.386004ms)
Aug 19 15:29:03.267: INFO: (14) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 7.110454ms)
Aug 19 15:29:03.268: INFO: (14) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 7.40407ms)
Aug 19 15:29:03.268: INFO: (14) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 7.365717ms)
Aug 19 15:29:03.272: INFO: (15) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 4.118337ms)
Aug 19 15:29:03.272: INFO: (15) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 4.647057ms)
Aug 19 15:29:03.272: INFO: (15) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 4.662388ms)
Aug 19 15:29:03.273: INFO: (15) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 4.959212ms)
Aug 19 15:29:03.273: INFO: (15) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 5.219876ms)
Aug 19 15:29:03.273: INFO: (15) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 5.17405ms)
Aug 19 15:29:03.275: INFO: (15) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 6.747808ms)
Aug 19 15:29:03.275: INFO: (15) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 7.067272ms)
Aug 19 15:29:03.275: INFO: (15) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 6.921505ms)
Aug 19 15:29:03.275: INFO: (15) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 6.973935ms)
Aug 19 15:29:03.275: INFO: (15) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 7.292116ms)
Aug 19 15:29:03.276: INFO: (15) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 8.503409ms)
Aug 19 15:29:03.276: INFO: (15) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 8.585988ms)
Aug 19 15:29:03.277: INFO: (15) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 9.254445ms)
Aug 19 15:29:03.277: INFO: (15) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 9.515399ms)
Aug 19 15:29:03.278: INFO: (15) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 9.725488ms)
Aug 19 15:29:03.281: INFO: (16) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 3.846069ms)
Aug 19 15:29:03.282: INFO: (16) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 3.965515ms)
Aug 19 15:29:03.282: INFO: (16) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 4.582797ms)
Aug 19 15:29:03.283: INFO: (16) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 5.381891ms)
Aug 19 15:29:03.283: INFO: (16) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 5.409353ms)
Aug 19 15:29:03.283: INFO: (16) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 5.665366ms)
Aug 19 15:29:03.284: INFO: (16) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 6.006547ms)
Aug 19 15:29:03.284: INFO: (16) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 6.310401ms)
Aug 19 15:29:03.284: INFO: (16) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 6.376642ms)
Aug 19 15:29:03.284: INFO: (16) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 6.390951ms)
Aug 19 15:29:03.284: INFO: (16) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 6.690051ms)
Aug 19 15:29:03.284: INFO: (16) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 6.67092ms)
Aug 19 15:29:03.285: INFO: (16) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 6.790162ms)
Aug 19 15:29:03.285: INFO: (16) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 7.024806ms)
Aug 19 15:29:03.285: INFO: (16) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 7.462856ms)
Aug 19 15:29:03.285: INFO: (16) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 7.758893ms)
Aug 19 15:29:03.289: INFO: (17) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 3.421699ms)
Aug 19 15:29:03.290: INFO: (17) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 4.352027ms)
Aug 19 15:29:03.291: INFO: (17) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 5.263166ms)
Aug 19 15:29:03.291: INFO: (17) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 5.487894ms)
Aug 19 15:29:03.291: INFO: (17) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 5.465295ms)
Aug 19 15:29:03.292: INFO: (17) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 6.014134ms)
Aug 19 15:29:03.292: INFO: (17) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 6.041747ms)
Aug 19 15:29:03.292: INFO: (17) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 6.224426ms)
Aug 19 15:29:03.292: INFO: (17) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 6.412873ms)
Aug 19 15:29:03.292: INFO: (17) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 6.424728ms)
Aug 19 15:29:03.292: INFO: (17) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 6.461608ms)
Aug 19 15:29:03.292: INFO: (17) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 6.612504ms)
Aug 19 15:29:03.292: INFO: (17) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 6.627427ms)
Aug 19 15:29:03.293: INFO: (17) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 6.917848ms)
Aug 19 15:29:03.293: INFO: (17) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 7.476488ms)
Aug 19 15:29:03.294: INFO: (17) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 7.899494ms)
Aug 19 15:29:03.298: INFO: (18) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 4.160163ms)
Aug 19 15:29:03.298: INFO: (18) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 4.30744ms)
Aug 19 15:29:03.298: INFO: (18) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 4.709486ms)
Aug 19 15:29:03.299: INFO: (18) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 5.31223ms)
Aug 19 15:29:03.299: INFO: (18) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 5.464605ms)
Aug 19 15:29:03.299: INFO: (18) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 5.740979ms)
Aug 19 15:29:03.300: INFO: (18) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 6.822034ms)
Aug 19 15:29:03.300: INFO: (18) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 6.900473ms)
Aug 19 15:29:03.301: INFO: (18) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 6.932781ms)
Aug 19 15:29:03.301: INFO: (18) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 7.0443ms)
Aug 19 15:29:03.301: INFO: (18) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 7.095551ms)
Aug 19 15:29:03.301: INFO: (18) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 7.115222ms)
Aug 19 15:29:03.301: INFO: (18) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 7.195061ms)
Aug 19 15:29:03.302: INFO: (18) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 7.984105ms)
Aug 19 15:29:03.302: INFO: (18) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 8.105302ms)
Aug 19 15:29:03.308: INFO: (18) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 13.946764ms)
Aug 19 15:29:03.312: INFO: (19) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:443/proxy/tlsrewritem... (200; 4.710111ms)
Aug 19 15:29:03.312: INFO: (19) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp/proxy/rewriteme">test</a> (200; 4.787062ms)
Aug 19 15:29:03.313: INFO: (19) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 5.139778ms)
Aug 19 15:29:03.313: INFO: (19) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:162/proxy/: bar (200; 5.324722ms)
Aug 19 15:29:03.314: INFO: (19) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">test<... (200; 5.8323ms)
Aug 19 15:29:03.314: INFO: (19) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:462/proxy/: tls qux (200; 5.902385ms)
Aug 19 15:29:03.314: INFO: (19) /api/v1/namespaces/proxy-4566/pods/https:proxy-service-xz2fp-slmgp:460/proxy/: tls baz (200; 5.886114ms)
Aug 19 15:29:03.314: INFO: (19) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname1/proxy/: foo (200; 6.251045ms)
Aug 19 15:29:03.314: INFO: (19) /api/v1/namespaces/proxy-4566/services/proxy-service-xz2fp:portname2/proxy/: bar (200; 6.240614ms)
Aug 19 15:29:03.314: INFO: (19) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 6.466251ms)
Aug 19 15:29:03.315: INFO: (19) /api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/: <a href="/api/v1/namespaces/proxy-4566/pods/http:proxy-service-xz2fp-slmgp:1080/proxy/rewriteme">... (200; 6.741923ms)
Aug 19 15:29:03.315: INFO: (19) /api/v1/namespaces/proxy-4566/pods/proxy-service-xz2fp-slmgp:160/proxy/: foo (200; 6.797106ms)
Aug 19 15:29:03.315: INFO: (19) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname1/proxy/: foo (200; 7.200358ms)
Aug 19 15:29:03.316: INFO: (19) /api/v1/namespaces/proxy-4566/services/http:proxy-service-xz2fp:portname2/proxy/: bar (200; 7.966587ms)
Aug 19 15:29:03.316: INFO: (19) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname2/proxy/: tls qux (200; 8.129254ms)
Aug 19 15:29:03.316: INFO: (19) /api/v1/namespaces/proxy-4566/services/https:proxy-service-xz2fp:tlsportname1/proxy/: tls baz (200; 8.473439ms)
STEP: deleting ReplicationController proxy-service-xz2fp in namespace proxy-4566, will wait for the garbage collector to delete the pods
Aug 19 15:29:03.377: INFO: Deleting ReplicationController proxy-service-xz2fp took: 7.859405ms
Aug 19 15:29:03.478: INFO: Terminating ReplicationController proxy-service-xz2fp pods took: 100.196212ms
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Aug 19 15:29:05.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4566" for this suite.

• [SLOW TEST:5.869 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":356,"completed":101,"skipped":2085,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:29:05.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9719
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Aug 19 15:29:05.869: INFO: Found 0 stateful pods, waiting for 3
Aug 19 15:29:15.874: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 19 15:29:15.874: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 19 15:29:15.874: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Aug 19 15:29:15.905: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 19 15:29:25.949: INFO: Updating stateful set ss2
Aug 19 15:29:25.955: INFO: Waiting for Pod statefulset-9719/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Aug 19 15:29:36.002: INFO: Found 1 stateful pods, waiting for 3
Aug 19 15:29:46.009: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 19 15:29:46.009: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 19 15:29:46.009: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 19 15:29:46.034: INFO: Updating stateful set ss2
Aug 19 15:29:46.040: INFO: Waiting for Pod statefulset-9719/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Aug 19 15:29:56.069: INFO: Updating stateful set ss2
Aug 19 15:29:56.077: INFO: Waiting for StatefulSet statefulset-9719/ss2 to complete update
Aug 19 15:29:56.077: INFO: Waiting for Pod statefulset-9719/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 19 15:30:06.104: INFO: Deleting all statefulset in ns statefulset-9719
Aug 19 15:30:06.108: INFO: Scaling statefulset ss2 to 0
Aug 19 15:30:16.130: INFO: Waiting for statefulset status.replicas updated to 0
Aug 19 15:30:16.134: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 19 15:30:16.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9719" for this suite.

• [SLOW TEST:70.371 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":356,"completed":102,"skipped":2100,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:30:16.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:30:16.242: INFO: The status of Pod test-webserver-7eb97e7a-eef2-420a-97df-65a897356636 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:30:18.247: INFO: The status of Pod test-webserver-7eb97e7a-eef2-420a-97df-65a897356636 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:30:20.246: INFO: The status of Pod test-webserver-7eb97e7a-eef2-420a-97df-65a897356636 is Running (Ready = false)
Aug 19 15:30:22.247: INFO: The status of Pod test-webserver-7eb97e7a-eef2-420a-97df-65a897356636 is Running (Ready = false)
Aug 19 15:30:24.247: INFO: The status of Pod test-webserver-7eb97e7a-eef2-420a-97df-65a897356636 is Running (Ready = false)
Aug 19 15:30:26.247: INFO: The status of Pod test-webserver-7eb97e7a-eef2-420a-97df-65a897356636 is Running (Ready = false)
Aug 19 15:30:28.248: INFO: The status of Pod test-webserver-7eb97e7a-eef2-420a-97df-65a897356636 is Running (Ready = false)
Aug 19 15:30:30.246: INFO: The status of Pod test-webserver-7eb97e7a-eef2-420a-97df-65a897356636 is Running (Ready = false)
Aug 19 15:30:32.246: INFO: The status of Pod test-webserver-7eb97e7a-eef2-420a-97df-65a897356636 is Running (Ready = false)
Aug 19 15:30:34.247: INFO: The status of Pod test-webserver-7eb97e7a-eef2-420a-97df-65a897356636 is Running (Ready = false)
Aug 19 15:30:36.248: INFO: The status of Pod test-webserver-7eb97e7a-eef2-420a-97df-65a897356636 is Running (Ready = false)
Aug 19 15:30:38.248: INFO: The status of Pod test-webserver-7eb97e7a-eef2-420a-97df-65a897356636 is Running (Ready = true)
Aug 19 15:30:38.251: INFO: Container started at 2022-08-19 15:30:17 +0000 UTC, pod became ready at 2022-08-19 15:30:36 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 19 15:30:38.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5213" for this suite.

• [SLOW TEST:22.100 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":356,"completed":103,"skipped":2155,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:30:38.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Aug 19 15:30:38.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9157" for this suite.
STEP: Destroying namespace "nspatchtest-a40089a8-9f5f-4a77-943e-cc9e9fd5f829-815" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":356,"completed":104,"skipped":2201,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests 
  should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:30:38.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename conformance-tests
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
STEP: Getting node addresses
Aug 19 15:30:38.444: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:188
Aug 19 15:30:38.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-333" for this suite.
•{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","total":356,"completed":105,"skipped":2220,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:30:38.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 19 15:30:38.561: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d16f6c10-0acb-44cc-8362-d3bd72d43103" in namespace "downward-api-6758" to be "Succeeded or Failed"
Aug 19 15:30:38.578: INFO: Pod "downwardapi-volume-d16f6c10-0acb-44cc-8362-d3bd72d43103": Phase="Pending", Reason="", readiness=false. Elapsed: 16.707245ms
Aug 19 15:30:40.581: INFO: Pod "downwardapi-volume-d16f6c10-0acb-44cc-8362-d3bd72d43103": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020648411s
Aug 19 15:30:42.586: INFO: Pod "downwardapi-volume-d16f6c10-0acb-44cc-8362-d3bd72d43103": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025449169s
Aug 19 15:30:44.591: INFO: Pod "downwardapi-volume-d16f6c10-0acb-44cc-8362-d3bd72d43103": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030164631s
STEP: Saw pod success
Aug 19 15:30:44.591: INFO: Pod "downwardapi-volume-d16f6c10-0acb-44cc-8362-d3bd72d43103" satisfied condition "Succeeded or Failed"
Aug 19 15:30:44.594: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-d16f6c10-0acb-44cc-8362-d3bd72d43103 container client-container: <nil>
STEP: delete the pod
Aug 19 15:30:44.620: INFO: Waiting for pod downwardapi-volume-d16f6c10-0acb-44cc-8362-d3bd72d43103 to disappear
Aug 19 15:30:44.623: INFO: Pod downwardapi-volume-d16f6c10-0acb-44cc-8362-d3bd72d43103 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 19 15:30:44.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6758" for this suite.

• [SLOW TEST:6.158 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":356,"completed":106,"skipped":2222,"failed":0}
SS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:30:44.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:30:44.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-8404
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:188
Aug 19 15:30:48.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-3945" for this suite.
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Aug 19 15:30:48.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8404" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":356,"completed":107,"skipped":2224,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:30:48.816: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 15:30:49.057: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 19 15:30:51.068: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 15, 30, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 30, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 15, 30, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 30, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 15:30:54.084: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:30:54.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2245" for this suite.
STEP: Destroying namespace "webhook-2245-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.400 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":356,"completed":108,"skipped":2224,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:30:54.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating server pod server in namespace prestop-3321
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3321
STEP: Deleting pre-stop pod
Aug 19 15:31:07.379: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:188
Aug 19 15:31:07.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3321" for this suite.

• [SLOW TEST:13.189 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":356,"completed":109,"skipped":2244,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:31:07.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-e88265e0-da70-4a4e-909f-04bcb2eef07b
STEP: Creating a pod to test consume secrets
Aug 19 15:31:07.481: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6d60f007-9236-4edc-9ae2-41f081670690" in namespace "projected-4425" to be "Succeeded or Failed"
Aug 19 15:31:07.488: INFO: Pod "pod-projected-secrets-6d60f007-9236-4edc-9ae2-41f081670690": Phase="Pending", Reason="", readiness=false. Elapsed: 7.125027ms
Aug 19 15:31:09.493: INFO: Pod "pod-projected-secrets-6d60f007-9236-4edc-9ae2-41f081670690": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011486633s
Aug 19 15:31:11.499: INFO: Pod "pod-projected-secrets-6d60f007-9236-4edc-9ae2-41f081670690": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017444205s
Aug 19 15:31:13.503: INFO: Pod "pod-projected-secrets-6d60f007-9236-4edc-9ae2-41f081670690": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021428452s
STEP: Saw pod success
Aug 19 15:31:13.503: INFO: Pod "pod-projected-secrets-6d60f007-9236-4edc-9ae2-41f081670690" satisfied condition "Succeeded or Failed"
Aug 19 15:31:13.506: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-projected-secrets-6d60f007-9236-4edc-9ae2-41f081670690 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 19 15:31:13.526: INFO: Waiting for pod pod-projected-secrets-6d60f007-9236-4edc-9ae2-41f081670690 to disappear
Aug 19 15:31:13.530: INFO: Pod pod-projected-secrets-6d60f007-9236-4edc-9ae2-41f081670690 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 19 15:31:13.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4425" for this suite.

• [SLOW TEST:6.137 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":110,"skipped":2246,"failed":0}
SSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:31:13.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Aug 19 15:31:15.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8247" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","total":356,"completed":111,"skipped":2253,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:31:15.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 19 15:31:15.723: INFO: Waiting up to 5m0s for pod "pod-b5fcfb7f-14f3-4a1e-91d6-b6227089a032" in namespace "emptydir-2707" to be "Succeeded or Failed"
Aug 19 15:31:15.729: INFO: Pod "pod-b5fcfb7f-14f3-4a1e-91d6-b6227089a032": Phase="Pending", Reason="", readiness=false. Elapsed: 5.506194ms
Aug 19 15:31:17.733: INFO: Pod "pod-b5fcfb7f-14f3-4a1e-91d6-b6227089a032": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009931317s
Aug 19 15:31:19.737: INFO: Pod "pod-b5fcfb7f-14f3-4a1e-91d6-b6227089a032": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014136605s
Aug 19 15:31:21.742: INFO: Pod "pod-b5fcfb7f-14f3-4a1e-91d6-b6227089a032": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01867486s
STEP: Saw pod success
Aug 19 15:31:21.742: INFO: Pod "pod-b5fcfb7f-14f3-4a1e-91d6-b6227089a032" satisfied condition "Succeeded or Failed"
Aug 19 15:31:21.745: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-b5fcfb7f-14f3-4a1e-91d6-b6227089a032 container test-container: <nil>
STEP: delete the pod
Aug 19 15:31:21.766: INFO: Waiting for pod pod-b5fcfb7f-14f3-4a1e-91d6-b6227089a032 to disappear
Aug 19 15:31:21.769: INFO: Pod pod-b5fcfb7f-14f3-4a1e-91d6-b6227089a032 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 19 15:31:21.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2707" for this suite.

• [SLOW TEST:6.131 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":112,"skipped":2286,"failed":0}
SSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:31:21.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ReplaceConcurrent cronjob
W0819 15:31:21.823328      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Aug 19 15:33:01.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8022" for this suite.

• [SLOW TEST:100.079 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":356,"completed":113,"skipped":2290,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:33:01.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 19 15:33:01.921: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05dc1bc4-bdf1-4b85-8dda-be151c6a950e" in namespace "projected-7879" to be "Succeeded or Failed"
Aug 19 15:33:01.923: INFO: Pod "downwardapi-volume-05dc1bc4-bdf1-4b85-8dda-be151c6a950e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.820264ms
Aug 19 15:33:03.928: INFO: Pod "downwardapi-volume-05dc1bc4-bdf1-4b85-8dda-be151c6a950e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007423466s
Aug 19 15:33:05.932: INFO: Pod "downwardapi-volume-05dc1bc4-bdf1-4b85-8dda-be151c6a950e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011171974s
Aug 19 15:33:07.936: INFO: Pod "downwardapi-volume-05dc1bc4-bdf1-4b85-8dda-be151c6a950e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015431011s
STEP: Saw pod success
Aug 19 15:33:07.936: INFO: Pod "downwardapi-volume-05dc1bc4-bdf1-4b85-8dda-be151c6a950e" satisfied condition "Succeeded or Failed"
Aug 19 15:33:07.939: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-05dc1bc4-bdf1-4b85-8dda-be151c6a950e container client-container: <nil>
STEP: delete the pod
Aug 19 15:33:07.964: INFO: Waiting for pod downwardapi-volume-05dc1bc4-bdf1-4b85-8dda-be151c6a950e to disappear
Aug 19 15:33:07.967: INFO: Pod downwardapi-volume-05dc1bc4-bdf1-4b85-8dda-be151c6a950e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 19 15:33:07.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7879" for this suite.

• [SLOW TEST:6.119 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":114,"skipped":2295,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:33:07.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5860
STEP: creating service affinity-nodeport-transition in namespace services-5860
STEP: creating replication controller affinity-nodeport-transition in namespace services-5860
I0819 15:33:08.054648      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5860, replica count: 3
I0819 15:33:11.106492      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0819 15:33:14.108328      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 19 15:33:14.118: INFO: Creating new exec pod
Aug 19 15:33:19.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-5860 exec execpod-affinityrbwt9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Aug 19 15:33:19.271: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 19 15:33:19.271: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 15:33:19.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-5860 exec execpod-affinityrbwt9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.152.251 80'
Aug 19 15:33:19.378: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.152.251 80\nConnection to 172.30.152.251 80 port [tcp/http] succeeded!\n"
Aug 19 15:33:19.378: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 15:33:19.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-5860 exec execpod-affinityrbwt9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.131.169 31989'
Aug 19 15:33:19.485: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.131.169 31989\nConnection to 10.0.131.169 31989 port [tcp/*] succeeded!\n"
Aug 19 15:33:19.485: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 15:33:19.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-5860 exec execpod-affinityrbwt9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.164.144 31989'
Aug 19 15:33:19.579: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.164.144 31989\nConnection to 10.0.164.144 31989 port [tcp/*] succeeded!\n"
Aug 19 15:33:19.579: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 15:33:19.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-5860 exec execpod-affinityrbwt9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.131.169:31989/ ; done'
Aug 19 15:33:19.753: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n"
Aug 19 15:33:19.753: INFO: stdout: "\naffinity-nodeport-transition-pwngs\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-th6jm\naffinity-nodeport-transition-pwngs\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-th6jm\naffinity-nodeport-transition-th6jm\naffinity-nodeport-transition-pwngs\naffinity-nodeport-transition-th6jm\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-th6jm\naffinity-nodeport-transition-pwngs\naffinity-nodeport-transition-th6jm"
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-pwngs
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-th6jm
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-pwngs
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-th6jm
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-th6jm
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-pwngs
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-th6jm
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-th6jm
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-pwngs
Aug 19 15:33:19.753: INFO: Received response from host: affinity-nodeport-transition-th6jm
Aug 19 15:33:19.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-5860 exec execpod-affinityrbwt9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.131.169:31989/ ; done'
Aug 19 15:33:19.928: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:31989/\n"
Aug 19 15:33:19.928: INFO: stdout: "\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf\naffinity-nodeport-transition-xmqnf"
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Received response from host: affinity-nodeport-transition-xmqnf
Aug 19 15:33:19.928: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5860, will wait for the garbage collector to delete the pods
Aug 19 15:33:20.003: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.566791ms
Aug 19 15:33:20.104: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.626213ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 15:33:22.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5860" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:14.361 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":115,"skipped":2296,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:33:22.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 19 15:33:22.387: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1692  273520c7-1f94-4845-9c0d-7fd3f78857ea 57439 0 2022-08-19 15:33:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-19 15:33:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 19 15:33:22.387: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1692  273520c7-1f94-4845-9c0d-7fd3f78857ea 57439 0 2022-08-19 15:33:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-19 15:33:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 19 15:33:22.410: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1692  273520c7-1f94-4845-9c0d-7fd3f78857ea 57440 0 2022-08-19 15:33:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-19 15:33:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 19 15:33:22.410: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1692  273520c7-1f94-4845-9c0d-7fd3f78857ea 57440 0 2022-08-19 15:33:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-19 15:33:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 19 15:33:22.421: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1692  273520c7-1f94-4845-9c0d-7fd3f78857ea 57443 0 2022-08-19 15:33:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-19 15:33:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 19 15:33:22.421: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1692  273520c7-1f94-4845-9c0d-7fd3f78857ea 57443 0 2022-08-19 15:33:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-19 15:33:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 19 15:33:22.428: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1692  273520c7-1f94-4845-9c0d-7fd3f78857ea 57445 0 2022-08-19 15:33:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-19 15:33:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 19 15:33:22.428: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1692  273520c7-1f94-4845-9c0d-7fd3f78857ea 57445 0 2022-08-19 15:33:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-19 15:33:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 19 15:33:22.437: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1692  b922f5fd-4e71-443a-a9a2-f00245f224ab 57448 0 2022-08-19 15:33:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-19 15:33:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 19 15:33:22.437: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1692  b922f5fd-4e71-443a-a9a2-f00245f224ab 57448 0 2022-08-19 15:33:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-19 15:33:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 19 15:33:32.446: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1692  b922f5fd-4e71-443a-a9a2-f00245f224ab 57597 0 2022-08-19 15:33:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-19 15:33:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 19 15:33:32.446: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1692  b922f5fd-4e71-443a-a9a2-f00245f224ab 57597 0 2022-08-19 15:33:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-19 15:33:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Aug 19 15:33:42.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1692" for this suite.

• [SLOW TEST:20.123 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":356,"completed":116,"skipped":2307,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:33:42.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service endpoint-test2 in namespace services-3204
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3204 to expose endpoints map[]
Aug 19 15:33:42.532: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Aug 19 15:33:43.540: INFO: successfully validated that service endpoint-test2 in namespace services-3204 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3204
Aug 19 15:33:43.563: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:33:45.566: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:33:47.569: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3204 to expose endpoints map[pod1:[80]]
Aug 19 15:33:47.581: INFO: successfully validated that service endpoint-test2 in namespace services-3204 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Aug 19 15:33:47.581: INFO: Creating new exec pod
Aug 19 15:33:52.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3204 exec execpodm48cj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 19 15:33:52.711: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 19 15:33:52.711: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 15:33:52.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3204 exec execpodm48cj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.65.91 80'
Aug 19 15:33:52.805: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.65.91 80\nConnection to 172.30.65.91 80 port [tcp/http] succeeded!\n"
Aug 19 15:33:52.805: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-3204
Aug 19 15:33:52.821: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:33:54.826: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:33:56.827: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3204 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 19 15:33:56.842: INFO: successfully validated that service endpoint-test2 in namespace services-3204 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Aug 19 15:33:57.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3204 exec execpodm48cj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 19 15:33:57.964: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 19 15:33:57.964: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 15:33:57.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3204 exec execpodm48cj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.65.91 80'
Aug 19 15:33:58.070: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.65.91 80\nConnection to 172.30.65.91 80 port [tcp/http] succeeded!\n"
Aug 19 15:33:58.070: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3204
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3204 to expose endpoints map[pod2:[80]]
Aug 19 15:33:58.094: INFO: successfully validated that service endpoint-test2 in namespace services-3204 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Aug 19 15:33:59.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3204 exec execpodm48cj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 19 15:33:59.193: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 19 15:33:59.193: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 15:33:59.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3204 exec execpodm48cj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.65.91 80'
Aug 19 15:33:59.305: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.65.91 80\nConnection to 172.30.65.91 80 port [tcp/http] succeeded!\n"
Aug 19 15:33:59.305: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-3204
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3204 to expose endpoints map[]
Aug 19 15:33:59.331: INFO: successfully validated that service endpoint-test2 in namespace services-3204 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 15:33:59.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3204" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:16.910 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":356,"completed":117,"skipped":2312,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:33:59.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Aug 19 15:33:59.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 create -f -'
Aug 19 15:34:00.355: INFO: stderr: ""
Aug 19 15:34:00.355: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 19 15:34:00.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 19 15:34:00.402: INFO: stderr: ""
Aug 19 15:34:00.402: INFO: stdout: "update-demo-nautilus-qdgkn update-demo-nautilus-xzdwv "
Aug 19 15:34:00.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods update-demo-nautilus-qdgkn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 19 15:34:00.441: INFO: stderr: ""
Aug 19 15:34:00.441: INFO: stdout: ""
Aug 19 15:34:00.441: INFO: update-demo-nautilus-qdgkn is created but not running
Aug 19 15:34:05.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 19 15:34:05.484: INFO: stderr: ""
Aug 19 15:34:05.484: INFO: stdout: "update-demo-nautilus-qdgkn update-demo-nautilus-xzdwv "
Aug 19 15:34:05.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods update-demo-nautilus-qdgkn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 19 15:34:05.524: INFO: stderr: ""
Aug 19 15:34:05.524: INFO: stdout: ""
Aug 19 15:34:05.524: INFO: update-demo-nautilus-qdgkn is created but not running
Aug 19 15:34:10.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 19 15:34:10.568: INFO: stderr: ""
Aug 19 15:34:10.568: INFO: stdout: "update-demo-nautilus-qdgkn update-demo-nautilus-xzdwv "
Aug 19 15:34:10.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods update-demo-nautilus-qdgkn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 19 15:34:10.618: INFO: stderr: ""
Aug 19 15:34:10.618: INFO: stdout: "true"
Aug 19 15:34:10.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods update-demo-nautilus-qdgkn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 19 15:34:10.658: INFO: stderr: ""
Aug 19 15:34:10.658: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 19 15:34:10.658: INFO: validating pod update-demo-nautilus-qdgkn
Aug 19 15:34:10.663: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 19 15:34:10.663: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 19 15:34:10.663: INFO: update-demo-nautilus-qdgkn is verified up and running
Aug 19 15:34:10.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods update-demo-nautilus-xzdwv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 19 15:34:10.704: INFO: stderr: ""
Aug 19 15:34:10.704: INFO: stdout: "true"
Aug 19 15:34:10.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods update-demo-nautilus-xzdwv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 19 15:34:10.749: INFO: stderr: ""
Aug 19 15:34:10.749: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 19 15:34:10.749: INFO: validating pod update-demo-nautilus-xzdwv
Aug 19 15:34:10.755: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 19 15:34:10.755: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 19 15:34:10.755: INFO: update-demo-nautilus-xzdwv is verified up and running
STEP: scaling down the replication controller
Aug 19 15:34:10.757: INFO: scanned /root for discovery docs: <nil>
Aug 19 15:34:10.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 19 15:34:11.821: INFO: stderr: ""
Aug 19 15:34:11.821: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 19 15:34:11.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 19 15:34:11.869: INFO: stderr: ""
Aug 19 15:34:11.869: INFO: stdout: "update-demo-nautilus-qdgkn update-demo-nautilus-xzdwv "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 19 15:34:16.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 19 15:34:16.918: INFO: stderr: ""
Aug 19 15:34:16.918: INFO: stdout: "update-demo-nautilus-qdgkn "
Aug 19 15:34:16.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods update-demo-nautilus-qdgkn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 19 15:34:16.959: INFO: stderr: ""
Aug 19 15:34:16.959: INFO: stdout: "true"
Aug 19 15:34:16.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods update-demo-nautilus-qdgkn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 19 15:34:16.997: INFO: stderr: ""
Aug 19 15:34:16.997: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 19 15:34:16.998: INFO: validating pod update-demo-nautilus-qdgkn
Aug 19 15:34:17.001: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 19 15:34:17.001: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 19 15:34:17.001: INFO: update-demo-nautilus-qdgkn is verified up and running
STEP: scaling up the replication controller
Aug 19 15:34:17.002: INFO: scanned /root for discovery docs: <nil>
Aug 19 15:34:17.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 19 15:34:18.064: INFO: stderr: ""
Aug 19 15:34:18.064: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 19 15:34:18.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 19 15:34:18.108: INFO: stderr: ""
Aug 19 15:34:18.108: INFO: stdout: "update-demo-nautilus-5v66m update-demo-nautilus-qdgkn "
Aug 19 15:34:18.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods update-demo-nautilus-5v66m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 19 15:34:18.148: INFO: stderr: ""
Aug 19 15:34:18.148: INFO: stdout: ""
Aug 19 15:34:18.148: INFO: update-demo-nautilus-5v66m is created but not running
Aug 19 15:34:23.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 19 15:34:23.194: INFO: stderr: ""
Aug 19 15:34:23.194: INFO: stdout: "update-demo-nautilus-5v66m update-demo-nautilus-qdgkn "
Aug 19 15:34:23.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods update-demo-nautilus-5v66m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 19 15:34:23.235: INFO: stderr: ""
Aug 19 15:34:23.235: INFO: stdout: "true"
Aug 19 15:34:23.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods update-demo-nautilus-5v66m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 19 15:34:23.276: INFO: stderr: ""
Aug 19 15:34:23.276: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 19 15:34:23.276: INFO: validating pod update-demo-nautilus-5v66m
Aug 19 15:34:23.284: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 19 15:34:23.284: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 19 15:34:23.284: INFO: update-demo-nautilus-5v66m is verified up and running
Aug 19 15:34:23.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods update-demo-nautilus-qdgkn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 19 15:34:23.325: INFO: stderr: ""
Aug 19 15:34:23.325: INFO: stdout: "true"
Aug 19 15:34:23.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods update-demo-nautilus-qdgkn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 19 15:34:23.365: INFO: stderr: ""
Aug 19 15:34:23.365: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 19 15:34:23.365: INFO: validating pod update-demo-nautilus-qdgkn
Aug 19 15:34:23.368: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 19 15:34:23.368: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 19 15:34:23.368: INFO: update-demo-nautilus-qdgkn is verified up and running
STEP: using delete to clean up resources
Aug 19 15:34:23.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 delete --grace-period=0 --force -f -'
Aug 19 15:34:23.411: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 19 15:34:23.411: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 19 15:34:23.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get rc,svc -l name=update-demo --no-headers'
Aug 19 15:34:23.457: INFO: stderr: "No resources found in kubectl-2788 namespace.\n"
Aug 19 15:34:23.457: INFO: stdout: ""
Aug 19 15:34:23.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2788 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 19 15:34:23.500: INFO: stderr: ""
Aug 19 15:34:23.500: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 15:34:23.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2788" for this suite.

• [SLOW TEST:24.139 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should scale a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":356,"completed":118,"skipped":2317,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:34:23.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3065
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3065
STEP: Waiting until pod test-pod will start running in namespace statefulset-3065
STEP: Creating statefulset with conflicting port in namespace statefulset-3065
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3065
Aug 19 15:34:27.636: INFO: Observed stateful pod in namespace: statefulset-3065, name: ss-0, uid: bb038b62-f8e2-4643-9978-b3c2539e342c, status phase: Pending. Waiting for statefulset controller to delete.
Aug 19 15:34:27.648: INFO: Observed stateful pod in namespace: statefulset-3065, name: ss-0, uid: bb038b62-f8e2-4643-9978-b3c2539e342c, status phase: Failed. Waiting for statefulset controller to delete.
Aug 19 15:34:27.674: INFO: Observed stateful pod in namespace: statefulset-3065, name: ss-0, uid: bb038b62-f8e2-4643-9978-b3c2539e342c, status phase: Failed. Waiting for statefulset controller to delete.
Aug 19 15:34:27.678: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3065
STEP: Removing pod with conflicting port in namespace statefulset-3065
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3065 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 19 15:34:31.708: INFO: Deleting all statefulset in ns statefulset-3065
Aug 19 15:34:31.711: INFO: Scaling statefulset ss to 0
Aug 19 15:34:41.731: INFO: Waiting for statefulset status.replicas updated to 0
Aug 19 15:34:41.733: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 19 15:34:41.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3065" for this suite.

• [SLOW TEST:18.247 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":356,"completed":119,"skipped":2330,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:34:41.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Aug 19 15:34:41.854: INFO: The status of Pod labelsupdatef09773db-e46a-4613-b930-73a2c821bd11 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:34:43.858: INFO: The status of Pod labelsupdatef09773db-e46a-4613-b930-73a2c821bd11 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:34:45.859: INFO: The status of Pod labelsupdatef09773db-e46a-4613-b930-73a2c821bd11 is Running (Ready = true)
Aug 19 15:34:46.391: INFO: Successfully updated pod "labelsupdatef09773db-e46a-4613-b930-73a2c821bd11"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 19 15:34:48.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2082" for this suite.

• [SLOW TEST:6.657 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":120,"skipped":2349,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:34:48.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:34:48.446: INFO: Got root ca configmap in namespace "svcaccounts-681"
Aug 19 15:34:48.469: INFO: Deleted root ca configmap in namespace "svcaccounts-681"
STEP: waiting for a new root ca configmap created
Aug 19 15:34:48.972: INFO: Recreated root ca configmap in namespace "svcaccounts-681"
Aug 19 15:34:48.977: INFO: Updated root ca configmap in namespace "svcaccounts-681"
STEP: waiting for the root ca configmap reconciled
Aug 19 15:34:49.482: INFO: Reconciled root ca configmap in namespace "svcaccounts-681"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 19 15:34:49.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-681" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":356,"completed":121,"skipped":2379,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:34:49.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Deleting RuntimeClass runtimeclass-4371-delete-me
STEP: Waiting for the RuntimeClass to disappear
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Aug 19 15:34:49.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4371" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":122,"skipped":2451,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:34:49.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Aug 19 15:34:49.603: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 19 15:34:54.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4586" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":356,"completed":123,"skipped":2473,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:34:54.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod test-webserver-764a3079-5fd4-4246-b9db-7becd45ed04b in namespace container-probe-1726
Aug 19 15:34:58.661: INFO: Started pod test-webserver-764a3079-5fd4-4246-b9db-7becd45ed04b in namespace container-probe-1726
STEP: checking the pod's current state and verifying that restartCount is present
Aug 19 15:34:58.665: INFO: Initial restart count of pod test-webserver-764a3079-5fd4-4246-b9db-7becd45ed04b is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 19 15:38:59.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1726" for this suite.

• [SLOW TEST:244.749 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":124,"skipped":2521,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:38:59.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-secret-lk65
STEP: Creating a pod to test atomic-volume-subpath
Aug 19 15:38:59.378: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lk65" in namespace "subpath-5274" to be "Succeeded or Failed"
Aug 19 15:38:59.386: INFO: Pod "pod-subpath-test-secret-lk65": Phase="Pending", Reason="", readiness=false. Elapsed: 8.158521ms
Aug 19 15:39:01.391: INFO: Pod "pod-subpath-test-secret-lk65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013052389s
Aug 19 15:39:03.397: INFO: Pod "pod-subpath-test-secret-lk65": Phase="Running", Reason="", readiness=true. Elapsed: 4.01914466s
Aug 19 15:39:05.402: INFO: Pod "pod-subpath-test-secret-lk65": Phase="Running", Reason="", readiness=true. Elapsed: 6.024206804s
Aug 19 15:39:07.405: INFO: Pod "pod-subpath-test-secret-lk65": Phase="Running", Reason="", readiness=true. Elapsed: 8.027681381s
Aug 19 15:39:09.409: INFO: Pod "pod-subpath-test-secret-lk65": Phase="Running", Reason="", readiness=true. Elapsed: 10.031324286s
Aug 19 15:39:11.414: INFO: Pod "pod-subpath-test-secret-lk65": Phase="Running", Reason="", readiness=true. Elapsed: 12.036744187s
Aug 19 15:39:13.420: INFO: Pod "pod-subpath-test-secret-lk65": Phase="Running", Reason="", readiness=true. Elapsed: 14.042935661s
Aug 19 15:39:15.426: INFO: Pod "pod-subpath-test-secret-lk65": Phase="Running", Reason="", readiness=true. Elapsed: 16.048141439s
Aug 19 15:39:17.430: INFO: Pod "pod-subpath-test-secret-lk65": Phase="Running", Reason="", readiness=true. Elapsed: 18.052243505s
Aug 19 15:39:19.434: INFO: Pod "pod-subpath-test-secret-lk65": Phase="Running", Reason="", readiness=true. Elapsed: 20.056449094s
Aug 19 15:39:21.439: INFO: Pod "pod-subpath-test-secret-lk65": Phase="Running", Reason="", readiness=true. Elapsed: 22.061765276s
Aug 19 15:39:23.444: INFO: Pod "pod-subpath-test-secret-lk65": Phase="Running", Reason="", readiness=false. Elapsed: 24.066045185s
Aug 19 15:39:25.449: INFO: Pod "pod-subpath-test-secret-lk65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.071083625s
STEP: Saw pod success
Aug 19 15:39:25.449: INFO: Pod "pod-subpath-test-secret-lk65" satisfied condition "Succeeded or Failed"
Aug 19 15:39:25.451: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-subpath-test-secret-lk65 container test-container-subpath-secret-lk65: <nil>
STEP: delete the pod
Aug 19 15:39:25.477: INFO: Waiting for pod pod-subpath-test-secret-lk65 to disappear
Aug 19 15:39:25.479: INFO: Pod pod-subpath-test-secret-lk65 no longer exists
STEP: Deleting pod pod-subpath-test-secret-lk65
Aug 19 15:39:25.479: INFO: Deleting pod "pod-subpath-test-secret-lk65" in namespace "subpath-5274"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Aug 19 15:39:25.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5274" for this suite.

• [SLOW TEST:26.203 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","total":356,"completed":125,"skipped":2523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:39:25.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:188
Aug 19 15:39:25.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9728" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":356,"completed":126,"skipped":2585,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:39:25.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test service account token: 
Aug 19 15:39:25.676: INFO: Waiting up to 5m0s for pod "test-pod-de17834a-c5b2-444b-b51b-cddd7eb98308" in namespace "svcaccounts-4696" to be "Succeeded or Failed"
Aug 19 15:39:25.680: INFO: Pod "test-pod-de17834a-c5b2-444b-b51b-cddd7eb98308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.816391ms
Aug 19 15:39:27.683: INFO: Pod "test-pod-de17834a-c5b2-444b-b51b-cddd7eb98308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007093873s
Aug 19 15:39:29.688: INFO: Pod "test-pod-de17834a-c5b2-444b-b51b-cddd7eb98308": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011898225s
Aug 19 15:39:31.692: INFO: Pod "test-pod-de17834a-c5b2-444b-b51b-cddd7eb98308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016217571s
STEP: Saw pod success
Aug 19 15:39:31.692: INFO: Pod "test-pod-de17834a-c5b2-444b-b51b-cddd7eb98308" satisfied condition "Succeeded or Failed"
Aug 19 15:39:31.695: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod test-pod-de17834a-c5b2-444b-b51b-cddd7eb98308 container agnhost-container: <nil>
STEP: delete the pod
Aug 19 15:39:31.717: INFO: Waiting for pod test-pod-de17834a-c5b2-444b-b51b-cddd7eb98308 to disappear
Aug 19 15:39:31.722: INFO: Pod test-pod-de17834a-c5b2-444b-b51b-cddd7eb98308 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 19 15:39:31.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4696" for this suite.

• [SLOW TEST:6.159 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":356,"completed":127,"skipped":2635,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:39:31.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9720 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9720;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9720 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9720;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9720.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9720.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9720.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9720.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9720.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9720.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9720.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9720.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9720.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9720.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9720.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9720.svc;check="$$(dig +notcp +noall +answer +search 47.190.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.190.47_udp@PTR;check="$$(dig +tcp +noall +answer +search 47.190.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.190.47_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9720 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9720;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9720 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9720;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9720.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9720.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9720.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9720.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9720.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9720.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9720.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9720.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9720.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9720.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9720.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9720.svc;check="$$(dig +notcp +noall +answer +search 47.190.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.190.47_udp@PTR;check="$$(dig +tcp +noall +answer +search 47.190.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.190.47_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 19 15:39:35.862: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3: the server could not find the requested resource (get pods dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3)
Aug 19 15:39:35.866: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3: the server could not find the requested resource (get pods dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3)
Aug 19 15:39:35.870: INFO: Unable to read wheezy_udp@dns-test-service.dns-9720 from pod dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3: the server could not find the requested resource (get pods dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3)
Aug 19 15:39:35.873: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9720 from pod dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3: the server could not find the requested resource (get pods dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3)
Aug 19 15:39:35.877: INFO: Unable to read wheezy_udp@dns-test-service.dns-9720.svc from pod dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3: the server could not find the requested resource (get pods dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3)
Aug 19 15:39:35.881: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9720.svc from pod dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3: the server could not find the requested resource (get pods dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3)
Aug 19 15:39:35.884: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9720.svc from pod dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3: the server could not find the requested resource (get pods dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3)
Aug 19 15:39:35.888: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9720.svc from pod dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3: the server could not find the requested resource (get pods dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3)
Aug 19 15:39:35.906: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3: the server could not find the requested resource (get pods dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3)
Aug 19 15:39:35.917: INFO: Unable to read jessie_tcp@dns-test-service.dns-9720 from pod dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3: the server could not find the requested resource (get pods dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3)
Aug 19 15:39:35.920: INFO: Unable to read jessie_udp@dns-test-service.dns-9720.svc from pod dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3: the server could not find the requested resource (get pods dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3)
Aug 19 15:39:35.924: INFO: Unable to read jessie_tcp@dns-test-service.dns-9720.svc from pod dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3: the server could not find the requested resource (get pods dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3)
Aug 19 15:39:35.927: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9720.svc from pod dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3: the server could not find the requested resource (get pods dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3)
Aug 19 15:39:35.931: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9720.svc from pod dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3: the server could not find the requested resource (get pods dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3)
Aug 19 15:39:35.945: INFO: Lookups using dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9720 wheezy_tcp@dns-test-service.dns-9720 wheezy_udp@dns-test-service.dns-9720.svc wheezy_tcp@dns-test-service.dns-9720.svc wheezy_udp@_http._tcp.dns-test-service.dns-9720.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9720.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service.dns-9720 jessie_udp@dns-test-service.dns-9720.svc jessie_tcp@dns-test-service.dns-9720.svc jessie_udp@_http._tcp.dns-test-service.dns-9720.svc jessie_tcp@_http._tcp.dns-test-service.dns-9720.svc]

Aug 19 15:39:41.033: INFO: DNS probes using dns-9720/dns-test-ad72a0d6-30df-4fd0-9e58-1692181213f3 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 19 15:39:41.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9720" for this suite.

• [SLOW TEST:9.361 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":356,"completed":128,"skipped":2641,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:39:41.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-9r4z
STEP: Creating a pod to test atomic-volume-subpath
Aug 19 15:39:41.183: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9r4z" in namespace "subpath-3740" to be "Succeeded or Failed"
Aug 19 15:39:41.195: INFO: Pod "pod-subpath-test-configmap-9r4z": Phase="Pending", Reason="", readiness=false. Elapsed: 12.401895ms
Aug 19 15:39:43.199: INFO: Pod "pod-subpath-test-configmap-9r4z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016205457s
Aug 19 15:39:45.204: INFO: Pod "pod-subpath-test-configmap-9r4z": Phase="Running", Reason="", readiness=true. Elapsed: 4.020664673s
Aug 19 15:39:47.220: INFO: Pod "pod-subpath-test-configmap-9r4z": Phase="Running", Reason="", readiness=true. Elapsed: 6.036522163s
Aug 19 15:39:49.224: INFO: Pod "pod-subpath-test-configmap-9r4z": Phase="Running", Reason="", readiness=true. Elapsed: 8.040784718s
Aug 19 15:39:51.228: INFO: Pod "pod-subpath-test-configmap-9r4z": Phase="Running", Reason="", readiness=true. Elapsed: 10.044691679s
Aug 19 15:39:53.233: INFO: Pod "pod-subpath-test-configmap-9r4z": Phase="Running", Reason="", readiness=true. Elapsed: 12.049535632s
Aug 19 15:39:55.236: INFO: Pod "pod-subpath-test-configmap-9r4z": Phase="Running", Reason="", readiness=true. Elapsed: 14.052986719s
Aug 19 15:39:57.241: INFO: Pod "pod-subpath-test-configmap-9r4z": Phase="Running", Reason="", readiness=true. Elapsed: 16.057975167s
Aug 19 15:39:59.245: INFO: Pod "pod-subpath-test-configmap-9r4z": Phase="Running", Reason="", readiness=true. Elapsed: 18.062466762s
Aug 19 15:40:01.250: INFO: Pod "pod-subpath-test-configmap-9r4z": Phase="Running", Reason="", readiness=true. Elapsed: 20.067034274s
Aug 19 15:40:03.254: INFO: Pod "pod-subpath-test-configmap-9r4z": Phase="Running", Reason="", readiness=true. Elapsed: 22.070493726s
Aug 19 15:40:05.257: INFO: Pod "pod-subpath-test-configmap-9r4z": Phase="Running", Reason="", readiness=false. Elapsed: 24.074318169s
Aug 19 15:40:07.262: INFO: Pod "pod-subpath-test-configmap-9r4z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.079023976s
STEP: Saw pod success
Aug 19 15:40:07.262: INFO: Pod "pod-subpath-test-configmap-9r4z" satisfied condition "Succeeded or Failed"
Aug 19 15:40:07.265: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-subpath-test-configmap-9r4z container test-container-subpath-configmap-9r4z: <nil>
STEP: delete the pod
Aug 19 15:40:07.286: INFO: Waiting for pod pod-subpath-test-configmap-9r4z to disappear
Aug 19 15:40:07.288: INFO: Pod pod-subpath-test-configmap-9r4z no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9r4z
Aug 19 15:40:07.288: INFO: Deleting pod "pod-subpath-test-configmap-9r4z" in namespace "subpath-3740"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Aug 19 15:40:07.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3740" for this suite.

• [SLOW TEST:26.207 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","total":356,"completed":129,"skipped":2661,"failed":0}
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:40:07.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Aug 19 15:40:21.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9592" for this suite.

• [SLOW TEST:14.067 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":356,"completed":130,"skipped":2661,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:40:21.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod with failed condition
STEP: updating the pod
Aug 19 15:42:21.969: INFO: Successfully updated pod "var-expansion-533a8e58-8100-494d-a87f-49919b41b61c"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Aug 19 15:42:23.977: INFO: Deleting pod "var-expansion-533a8e58-8100-494d-a87f-49919b41b61c" in namespace "var-expansion-5196"
Aug 19 15:42:23.985: INFO: Wait up to 5m0s for pod "var-expansion-533a8e58-8100-494d-a87f-49919b41b61c" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 19 15:42:55.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5196" for this suite.

• [SLOW TEST:154.638 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":356,"completed":131,"skipped":2667,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:42:56.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-4140
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 19 15:42:56.033: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 19 15:42:56.163: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:42:58.166: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 15:43:00.167: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 15:43:02.167: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 15:43:04.166: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 15:43:06.166: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 15:43:08.167: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 15:43:10.168: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 15:43:12.166: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 15:43:14.167: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 15:43:16.167: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 15:43:18.168: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 19 15:43:18.174: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 19 15:43:18.179: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 19 15:43:22.206: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 19 15:43:22.206: INFO: Breadth first check of 10.128.2.98 on host 10.0.131.169...
Aug 19 15:43:22.209: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.201:9080/dial?request=hostname&protocol=udp&host=10.128.2.98&port=8081&tries=1'] Namespace:pod-network-test-4140 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:43:22.209: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:43:22.210: INFO: ExecWithOptions: Clientset creation
Aug 19 15:43:22.210: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-4140/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.0.201%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.2.98%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 19 15:43:22.272: INFO: Waiting for responses: map[]
Aug 19 15:43:22.272: INFO: reached 10.128.2.98 after 0/1 tries
Aug 19 15:43:22.272: INFO: Breadth first check of 10.129.2.115 on host 10.0.157.99...
Aug 19 15:43:22.276: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.201:9080/dial?request=hostname&protocol=udp&host=10.129.2.115&port=8081&tries=1'] Namespace:pod-network-test-4140 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:43:22.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:43:22.276: INFO: ExecWithOptions: Clientset creation
Aug 19 15:43:22.276: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-4140/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.0.201%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.129.2.115%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 19 15:43:22.337: INFO: Waiting for responses: map[]
Aug 19 15:43:22.337: INFO: reached 10.129.2.115 after 0/1 tries
Aug 19 15:43:22.337: INFO: Breadth first check of 10.131.0.200 on host 10.0.164.144...
Aug 19 15:43:22.340: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.201:9080/dial?request=hostname&protocol=udp&host=10.131.0.200&port=8081&tries=1'] Namespace:pod-network-test-4140 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:43:22.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:43:22.341: INFO: ExecWithOptions: Clientset creation
Aug 19 15:43:22.341: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-4140/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.0.201%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.131.0.200%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 19 15:43:22.397: INFO: Waiting for responses: map[]
Aug 19 15:43:22.397: INFO: reached 10.131.0.200 after 0/1 tries
Aug 19 15:43:22.397: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Aug 19 15:43:22.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4140" for this suite.

• [SLOW TEST:26.403 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":356,"completed":132,"skipped":2680,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:43:22.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:43:22.478: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 19 15:43:27.481: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Aug 19 15:43:27.493: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Aug 19 15:43:27.501: INFO: observed ReplicaSet test-rs in namespace replicaset-5674 with ReadyReplicas 1, AvailableReplicas 1
Aug 19 15:43:27.520: INFO: observed ReplicaSet test-rs in namespace replicaset-5674 with ReadyReplicas 1, AvailableReplicas 1
Aug 19 15:43:27.540: INFO: observed ReplicaSet test-rs in namespace replicaset-5674 with ReadyReplicas 1, AvailableReplicas 1
Aug 19 15:43:27.546: INFO: observed ReplicaSet test-rs in namespace replicaset-5674 with ReadyReplicas 1, AvailableReplicas 1
Aug 19 15:43:29.915: INFO: observed ReplicaSet test-rs in namespace replicaset-5674 with ReadyReplicas 2, AvailableReplicas 2
Aug 19 15:43:30.221: INFO: observed Replicaset test-rs in namespace replicaset-5674 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 19 15:43:30.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5674" for this suite.

• [SLOW TEST:7.824 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":356,"completed":133,"skipped":2699,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:43:30.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:43:30.262: INFO: Creating deployment "webserver-deployment"
W0819 15:43:30.269973      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 15:43:30.270: INFO: Waiting for observed generation 1
Aug 19 15:43:32.284: INFO: Waiting for all required pods to come up
Aug 19 15:43:32.288: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 19 15:43:34.297: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 19 15:43:34.303: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 19 15:43:34.326: INFO: Updating deployment webserver-deployment
Aug 19 15:43:34.326: INFO: Waiting for observed generation 2
Aug 19 15:43:36.333: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 19 15:43:36.338: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 19 15:43:36.341: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 19 15:43:36.349: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 19 15:43:36.349: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 19 15:43:36.352: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 19 15:43:36.357: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 19 15:43:36.357: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 19 15:43:36.367: INFO: Updating deployment webserver-deployment
Aug 19 15:43:36.367: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 19 15:43:36.373: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 19 15:43:36.375: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 19 15:43:36.390: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3500  a4205e92-7d96-4119-aeda-6754be1af961 62868 3 2022-08-19 15:43:30 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-08-19 15:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 15:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bd8a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-57ccb67bb8" is progressing.,LastUpdateTime:2022-08-19 15:43:34 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-08-19 15:43:36 +0000 UTC,LastTransitionTime:2022-08-19 15:43:36 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 19 15:43:36.397: INFO: New ReplicaSet "webserver-deployment-57ccb67bb8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-57ccb67bb8  deployment-3500  9ea4cfe8-ba6d-4994-a7c4-95abf3424a4a 62866 3 2022-08-19 15:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment a4205e92-7d96-4119-aeda-6754be1af961 0xc0090f53e7 0xc0090f53e8}] []  [{kube-controller-manager Update apps/v1 2022-08-19 15:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4205e92-7d96-4119-aeda-6754be1af961\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 15:43:34 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 57ccb67bb8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0090f54f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 19 15:43:36.397: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 19 15:43:36.397: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-55df494869  deployment-3500  7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b 62864 3 2022-08-19 15:43:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment a4205e92-7d96-4119-aeda-6754be1af961 0xc0090f52e7 0xc0090f52e8}] []  [{kube-controller-manager Update apps/v1 2022-08-19 15:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4205e92-7d96-4119-aeda-6754be1af961\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 15:43:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0090f5378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 19 15:43:36.401: INFO: Pod "webserver-deployment-55df494869-8857x" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-8857x webserver-deployment-55df494869- deployment-3500  99657d43-03a9-4ea5-b2d6-add570726bae 62698 0 2022-08-19 15:43:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.117"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.117"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b 0xc0090f59e7 0xc0090f59e8}] []  [{kube-controller-manager Update v1 2022-08-19 15:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-19 15:43:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-08-19 15:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmrzk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmrzk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-157-99.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rpnjt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.157.99,PodIP:10.129.2.117,StartTime:2022-08-19 15:43:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 15:43:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://400ac7293a5bb1141a2fb70f2ec870c7445e4cb94d2cec7a17c6da6872a86aed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 15:43:36.401: INFO: Pod "webserver-deployment-55df494869-8q6n4" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-8q6n4 webserver-deployment-55df494869- deployment-3500  cb5f976b-2aef-49f8-9693-bba587fd94b8 62708 0 2022-08-19 15:43:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.101"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.101"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b 0xc0090f5c27 0xc0090f5c28}] []  [{kube-controller-manager Update v1 2022-08-19 15:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-08-19 15:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-19 15:43:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-th2rc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-th2rc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-169.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rpnjt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.169,PodIP:10.128.2.101,StartTime:2022-08-19 15:43:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 15:43:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://ea59394298068d948a33667a90ef2ad0d7302d9399fd7f645f76ef5b2efa3b9b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 15:43:36.401: INFO: Pod "webserver-deployment-55df494869-b69hx" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-b69hx webserver-deployment-55df494869- deployment-3500  cec706dc-808d-4109-980e-e44f0e86ee16 62733 0 2022-08-19 15:43:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.119"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.119"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b 0xc0090f5e67 0xc0090f5e68}] []  [{kube-controller-manager Update v1 2022-08-19 15:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-19 15:43:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-08-19 15:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rnfkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rnfkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-157-99.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rpnjt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.157.99,PodIP:10.129.2.119,StartTime:2022-08-19 15:43:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 15:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8a814b8112545148c0f179de90d1a4ed3724a6350216933b2e66cfc743cb21fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 15:43:36.402: INFO: Pod "webserver-deployment-55df494869-c57vc" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-c57vc webserver-deployment-55df494869- deployment-3500  63927ad2-3d6f-4c35-b98b-a0742129c462 62719 0 2022-08-19 15:43:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.206"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.206"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b 0xc0040360b7 0xc0040360b8}] []  [{kube-controller-manager Update v1 2022-08-19 15:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-08-19 15:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-19 15:43:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9z8x6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9z8x6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-164-144.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rpnjt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.164.144,PodIP:10.131.0.206,StartTime:2022-08-19 15:43:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 15:43:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d573022872378fcd80fe09c5d537c25fa18c19c41861c43f198ebc8b668f6c50,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 15:43:36.402: INFO: Pod "webserver-deployment-55df494869-ddb8z" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-ddb8z webserver-deployment-55df494869- deployment-3500  67721934-673d-4bfb-b9a5-3e8f2b136305 62714 0 2022-08-19 15:43:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.100"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.100"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b 0xc004036317 0xc004036318}] []  [{kube-controller-manager Update v1 2022-08-19 15:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-08-19 15:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-19 15:43:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jvr7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jvr7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-169.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rpnjt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.169,PodIP:10.128.2.100,StartTime:2022-08-19 15:43:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 15:43:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://145e95bf75fbf0c88a69dc6a79a6d5ae19d600ea8c69d2676073f98bd82c492c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 15:43:36.402: INFO: Pod "webserver-deployment-55df494869-dkc2j" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-dkc2j webserver-deployment-55df494869- deployment-3500  fce9e2a2-579b-464e-8b8b-43c30e1969b7 62676 0 2022-08-19 15:43:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.204"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.204"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b 0xc004036557 0xc004036558}] []  [{kube-controller-manager Update v1 2022-08-19 15:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-19 15:43:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-08-19 15:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ps652,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ps652,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-164-144.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rpnjt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.164.144,PodIP:10.131.0.204,StartTime:2022-08-19 15:43:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 15:43:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://905500f460e87a3d6249eaf08fb8a4d2188ed7e4e2b26553fd80033b9d7bcfa5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 15:43:36.402: INFO: Pod "webserver-deployment-55df494869-n76gq" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-n76gq webserver-deployment-55df494869- deployment-3500  c15fc6dc-560f-4aaf-b8cb-9af398ce5c19 62873 0 2022-08-19 15:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b 0xc004036797 0xc004036798}] []  [{kube-controller-manager Update v1 2022-08-19 15:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sgtk5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sgtk5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-164-144.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rpnjt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 15:43:36.402: INFO: Pod "webserver-deployment-55df494869-sjl5n" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-sjl5n webserver-deployment-55df494869- deployment-3500  53771d85-609e-407e-b8f4-25864af9e62c 62701 0 2022-08-19 15:43:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.118"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.118"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b 0xc004036937 0xc004036938}] []  [{kube-controller-manager Update v1 2022-08-19 15:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-19 15:43:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-08-19 15:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfw4m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfw4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-157-99.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rpnjt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.157.99,PodIP:10.129.2.118,StartTime:2022-08-19 15:43:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 15:43:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8d66c37c65f239443a97f92ac29c4afeccde17f34a985d39999a92180eb06137,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 15:43:36.402: INFO: Pod "webserver-deployment-55df494869-xqmm5" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-xqmm5 webserver-deployment-55df494869- deployment-3500  42e86e2f-7822-4929-a4c0-87ebf1845cf7 62710 0 2022-08-19 15:43:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.102"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.102"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b 0xc004036b77 0xc004036b78}] []  [{kube-controller-manager Update v1 2022-08-19 15:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7c0ffb77-4a2a-4755-b72d-7f68d1d0b20b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-08-19 15:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-19 15:43:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gz5hd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gz5hd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-169.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rpnjt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.169,PodIP:10.128.2.102,StartTime:2022-08-19 15:43:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 15:43:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://0f3d570019be6724f06ca08727a2c5e27c3f7e839cd7f794b7b833fc7057d2ed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 15:43:36.402: INFO: Pod "webserver-deployment-57ccb67bb8-hbmbn" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-hbmbn webserver-deployment-57ccb67bb8- deployment-3500  df8e58be-a71f-42bd-958a-bb0800cd673f 62859 0 2022-08-19 15:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.208"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.208"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 9ea4cfe8-ba6d-4994-a7c4-95abf3424a4a 0xc004036db7 0xc004036db8}] []  [{kube-controller-manager Update v1 2022-08-19 15:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ea4cfe8-ba6d-4994-a7c4-95abf3424a4a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-19 15:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2022-08-19 15:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6jkpd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6jkpd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-164-144.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rpnjt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.164.144,PodIP:,StartTime:2022-08-19 15:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 15:43:36.403: INFO: Pod "webserver-deployment-57ccb67bb8-lld4t" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-lld4t webserver-deployment-57ccb67bb8- deployment-3500  788fdf4a-4d52-467e-a506-e3adfb92bc04 62768 0 2022-08-19 15:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 9ea4cfe8-ba6d-4994-a7c4-95abf3424a4a 0xc004037017 0xc004037018}] []  [{kube-controller-manager Update v1 2022-08-19 15:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ea4cfe8-ba6d-4994-a7c4-95abf3424a4a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-19 15:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ktx7s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ktx7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-157-99.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rpnjt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.157.99,PodIP:,StartTime:2022-08-19 15:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 15:43:36.403: INFO: Pod "webserver-deployment-57ccb67bb8-lm654" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-lm654 webserver-deployment-57ccb67bb8- deployment-3500  e7d93554-5734-49d7-a221-ac837dc6b381 62874 0 2022-08-19 15:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 9ea4cfe8-ba6d-4994-a7c4-95abf3424a4a 0xc004037297 0xc004037298}] []  [{kube-controller-manager Update v1 2022-08-19 15:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ea4cfe8-ba6d-4994-a7c4-95abf3424a4a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lzqls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lzqls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rpnjt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 15:43:36.403: INFO: Pod "webserver-deployment-57ccb67bb8-m5g8w" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-m5g8w webserver-deployment-57ccb67bb8- deployment-3500  7b772486-29ca-4f8e-bc2d-705234f0a982 62784 0 2022-08-19 15:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 9ea4cfe8-ba6d-4994-a7c4-95abf3424a4a 0xc004037447 0xc004037448}] []  [{kube-controller-manager Update v1 2022-08-19 15:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ea4cfe8-ba6d-4994-a7c4-95abf3424a4a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-19 15:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r8q4l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r8q4l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-157-99.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rpnjt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.157.99,PodIP:,StartTime:2022-08-19 15:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 15:43:36.403: INFO: Pod "webserver-deployment-57ccb67bb8-sj9pf" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-sj9pf webserver-deployment-57ccb67bb8- deployment-3500  1338883d-1c0f-473c-b502-d80310e09845 62857 0 2022-08-19 15:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.207"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.207"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 9ea4cfe8-ba6d-4994-a7c4-95abf3424a4a 0xc0040376c7 0xc0040376c8}] []  [{kube-controller-manager Update v1 2022-08-19 15:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ea4cfe8-ba6d-4994-a7c4-95abf3424a4a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-19 15:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2022-08-19 15:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xq57p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xq57p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-164-144.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rpnjt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.164.144,PodIP:,StartTime:2022-08-19 15:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 15:43:36.403: INFO: Pod "webserver-deployment-57ccb67bb8-xzbcs" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-xzbcs webserver-deployment-57ccb67bb8- deployment-3500  c8f1267c-12c3-4b61-b740-00ec3f0d1e39 62854 0 2022-08-19 15:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.103"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.103"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 9ea4cfe8-ba6d-4994-a7c4-95abf3424a4a 0xc004037907 0xc004037908}] []  [{kube-controller-manager Update v1 2022-08-19 15:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ea4cfe8-ba6d-4994-a7c4-95abf3424a4a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-08-19 15:43:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-19 15:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7t42h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7t42h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-169.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rpnjt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 15:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.169,PodIP:10.128.2.103,StartTime:2022-08-19 15:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 19 15:43:36.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3500" for this suite.

• [SLOW TEST:6.197 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":356,"completed":134,"skipped":2707,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:43:36.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:43:36.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 19 15:43:36.563: INFO: The status of Pod pod-logs-websocket-12913ed3-11b9-4530-8ebe-aed331f7c9cd is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:43:38.569: INFO: The status of Pod pod-logs-websocket-12913ed3-11b9-4530-8ebe-aed331f7c9cd is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:43:40.569: INFO: The status of Pod pod-logs-websocket-12913ed3-11b9-4530-8ebe-aed331f7c9cd is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 19 15:43:40.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-777" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":356,"completed":135,"skipped":2710,"failed":0}
SSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:43:40.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override command
Aug 19 15:43:40.648: INFO: Waiting up to 5m0s for pod "client-containers-d22d4b3b-3373-44e5-94f1-61313c956db6" in namespace "containers-4782" to be "Succeeded or Failed"
Aug 19 15:43:40.652: INFO: Pod "client-containers-d22d4b3b-3373-44e5-94f1-61313c956db6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.447755ms
Aug 19 15:43:42.656: INFO: Pod "client-containers-d22d4b3b-3373-44e5-94f1-61313c956db6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007539397s
Aug 19 15:43:44.660: INFO: Pod "client-containers-d22d4b3b-3373-44e5-94f1-61313c956db6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012120448s
Aug 19 15:43:46.664: INFO: Pod "client-containers-d22d4b3b-3373-44e5-94f1-61313c956db6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015570411s
STEP: Saw pod success
Aug 19 15:43:46.664: INFO: Pod "client-containers-d22d4b3b-3373-44e5-94f1-61313c956db6" satisfied condition "Succeeded or Failed"
Aug 19 15:43:46.667: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod client-containers-d22d4b3b-3373-44e5-94f1-61313c956db6 container agnhost-container: <nil>
STEP: delete the pod
Aug 19 15:43:46.691: INFO: Waiting for pod client-containers-d22d4b3b-3373-44e5-94f1-61313c956db6 to disappear
Aug 19 15:43:46.694: INFO: Pod client-containers-d22d4b3b-3373-44e5-94f1-61313c956db6 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Aug 19 15:43:46.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4782" for this suite.

• [SLOW TEST:6.110 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","total":356,"completed":136,"skipped":2716,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:43:46.706: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name projected-secret-test-58b2aa7f-7685-4ec2-9fb0-b2ce40602b0f
STEP: Creating a pod to test consume secrets
Aug 19 15:43:46.778: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-26cdaba6-9bbb-4b70-b778-cc67156b6d77" in namespace "projected-9475" to be "Succeeded or Failed"
Aug 19 15:43:46.785: INFO: Pod "pod-projected-secrets-26cdaba6-9bbb-4b70-b778-cc67156b6d77": Phase="Pending", Reason="", readiness=false. Elapsed: 6.940048ms
Aug 19 15:43:48.789: INFO: Pod "pod-projected-secrets-26cdaba6-9bbb-4b70-b778-cc67156b6d77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010912108s
Aug 19 15:43:50.793: INFO: Pod "pod-projected-secrets-26cdaba6-9bbb-4b70-b778-cc67156b6d77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014961815s
Aug 19 15:43:52.798: INFO: Pod "pod-projected-secrets-26cdaba6-9bbb-4b70-b778-cc67156b6d77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019958399s
STEP: Saw pod success
Aug 19 15:43:52.798: INFO: Pod "pod-projected-secrets-26cdaba6-9bbb-4b70-b778-cc67156b6d77" satisfied condition "Succeeded or Failed"
Aug 19 15:43:52.801: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-projected-secrets-26cdaba6-9bbb-4b70-b778-cc67156b6d77 container secret-volume-test: <nil>
STEP: delete the pod
Aug 19 15:43:52.822: INFO: Waiting for pod pod-projected-secrets-26cdaba6-9bbb-4b70-b778-cc67156b6d77 to disappear
Aug 19 15:43:52.826: INFO: Pod pod-projected-secrets-26cdaba6-9bbb-4b70-b778-cc67156b6d77 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 19 15:43:52.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9475" for this suite.

• [SLOW TEST:6.134 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":137,"skipped":2732,"failed":0}
SSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:43:52.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:43:52.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6146
W0819 15:43:52.889828      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0819 15:43:52.889980      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6146, replica count: 1
I0819 15:43:53.940928      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0819 15:43:54.941212      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0819 15:43:55.941931      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 19 15:43:56.056: INFO: Created: latency-svc-9zlb6
Aug 19 15:43:56.065: INFO: Got endpoints: latency-svc-9zlb6 [22.945859ms]
Aug 19 15:43:56.084: INFO: Created: latency-svc-j285d
Aug 19 15:43:56.102: INFO: Got endpoints: latency-svc-j285d [36.051611ms]
Aug 19 15:43:56.118: INFO: Created: latency-svc-7zw72
Aug 19 15:43:56.127: INFO: Got endpoints: latency-svc-7zw72 [60.824783ms]
Aug 19 15:43:56.231: INFO: Created: latency-svc-msgv6
Aug 19 15:43:56.233: INFO: Created: latency-svc-89sns
Aug 19 15:43:56.238: INFO: Created: latency-svc-wvvbf
Aug 19 15:43:56.238: INFO: Created: latency-svc-twwtw
Aug 19 15:43:56.239: INFO: Created: latency-svc-gzppd
Aug 19 15:43:56.239: INFO: Created: latency-svc-8vxvw
Aug 19 15:43:56.239: INFO: Created: latency-svc-mmn2s
Aug 19 15:43:56.239: INFO: Created: latency-svc-brtwc
Aug 19 15:43:56.239: INFO: Created: latency-svc-4mft5
Aug 19 15:43:56.239: INFO: Created: latency-svc-c922p
Aug 19 15:43:56.239: INFO: Created: latency-svc-t9lqj
Aug 19 15:43:56.239: INFO: Created: latency-svc-2h6f8
Aug 19 15:43:56.239: INFO: Created: latency-svc-b64rq
Aug 19 15:43:56.241: INFO: Created: latency-svc-rhcsf
Aug 19 15:43:56.247: INFO: Got endpoints: latency-svc-msgv6 [181.166872ms]
Aug 19 15:43:56.247: INFO: Created: latency-svc-dxk5v
Aug 19 15:43:56.249: INFO: Got endpoints: latency-svc-89sns [182.975554ms]
Aug 19 15:43:56.250: INFO: Got endpoints: latency-svc-brtwc [184.361916ms]
Aug 19 15:43:56.252: INFO: Got endpoints: latency-svc-8vxvw [125.897473ms]
Aug 19 15:43:56.253: INFO: Got endpoints: latency-svc-gzppd [186.714042ms]
Aug 19 15:43:56.253: INFO: Got endpoints: latency-svc-c922p [187.883225ms]
Aug 19 15:43:56.258: INFO: Got endpoints: latency-svc-mmn2s [191.977784ms]
Aug 19 15:43:56.260: INFO: Got endpoints: latency-svc-twwtw [194.09779ms]
Aug 19 15:43:56.262: INFO: Got endpoints: latency-svc-4mft5 [195.595886ms]
Aug 19 15:43:56.264: INFO: Got endpoints: latency-svc-b64rq [198.202944ms]
Aug 19 15:43:56.264: INFO: Got endpoints: latency-svc-2h6f8 [162.506548ms]
Aug 19 15:43:56.272: INFO: Got endpoints: latency-svc-wvvbf [206.018579ms]
Aug 19 15:43:56.272: INFO: Got endpoints: latency-svc-rhcsf [206.211093ms]
Aug 19 15:43:56.272: INFO: Got endpoints: latency-svc-t9lqj [206.211954ms]
Aug 19 15:43:56.276: INFO: Got endpoints: latency-svc-dxk5v [209.66513ms]
Aug 19 15:43:56.278: INFO: Created: latency-svc-pd2sf
Aug 19 15:43:56.284: INFO: Got endpoints: latency-svc-pd2sf [37.413384ms]
Aug 19 15:43:56.289: INFO: Created: latency-svc-nj6pt
Aug 19 15:43:56.299: INFO: Created: latency-svc-n2lhw
Aug 19 15:43:56.301: INFO: Got endpoints: latency-svc-nj6pt [52.639307ms]
Aug 19 15:43:56.305: INFO: Got endpoints: latency-svc-n2lhw [54.785367ms]
Aug 19 15:43:56.305: INFO: Created: latency-svc-6pzr5
Aug 19 15:43:56.314: INFO: Got endpoints: latency-svc-6pzr5 [61.582647ms]
Aug 19 15:43:56.319: INFO: Created: latency-svc-kz5pf
Aug 19 15:43:56.324: INFO: Created: latency-svc-bmtmq
Aug 19 15:43:56.329: INFO: Got endpoints: latency-svc-kz5pf [75.747656ms]
Aug 19 15:43:56.330: INFO: Got endpoints: latency-svc-bmtmq [77.249761ms]
Aug 19 15:43:56.335: INFO: Created: latency-svc-ztmq6
Aug 19 15:43:56.339: INFO: Created: latency-svc-9bbqk
Aug 19 15:43:56.344: INFO: Got endpoints: latency-svc-ztmq6 [86.019164ms]
Aug 19 15:43:56.348: INFO: Got endpoints: latency-svc-9bbqk [88.220394ms]
Aug 19 15:43:56.351: INFO: Created: latency-svc-7j9kp
Aug 19 15:43:56.359: INFO: Got endpoints: latency-svc-7j9kp [97.331563ms]
Aug 19 15:43:56.361: INFO: Created: latency-svc-6vf26
Aug 19 15:43:56.370: INFO: Got endpoints: latency-svc-6vf26 [105.802492ms]
Aug 19 15:43:56.372: INFO: Created: latency-svc-t8hhl
Aug 19 15:43:56.377: INFO: Got endpoints: latency-svc-t8hhl [112.856411ms]
Aug 19 15:43:56.379: INFO: Created: latency-svc-5tsdr
Aug 19 15:43:56.387: INFO: Got endpoints: latency-svc-5tsdr [115.115573ms]
Aug 19 15:43:56.390: INFO: Created: latency-svc-hrbk8
Aug 19 15:43:56.399: INFO: Created: latency-svc-d8lj6
Aug 19 15:43:56.400: INFO: Got endpoints: latency-svc-hrbk8 [127.552217ms]
Aug 19 15:43:56.403: INFO: Got endpoints: latency-svc-d8lj6 [131.21717ms]
Aug 19 15:43:56.407: INFO: Created: latency-svc-8mxdt
Aug 19 15:43:56.414: INFO: Got endpoints: latency-svc-8mxdt [138.563935ms]
Aug 19 15:43:56.416: INFO: Created: latency-svc-cgcqs
Aug 19 15:43:56.424: INFO: Got endpoints: latency-svc-cgcqs [140.037396ms]
Aug 19 15:43:56.428: INFO: Created: latency-svc-qz5tb
Aug 19 15:43:56.436: INFO: Created: latency-svc-x4kmj
Aug 19 15:43:56.436: INFO: Got endpoints: latency-svc-qz5tb [134.617366ms]
Aug 19 15:43:56.444: INFO: Got endpoints: latency-svc-x4kmj [139.140014ms]
Aug 19 15:43:56.452: INFO: Created: latency-svc-wtm9g
Aug 19 15:43:56.454: INFO: Got endpoints: latency-svc-wtm9g [140.285417ms]
Aug 19 15:43:56.458: INFO: Created: latency-svc-nndw9
Aug 19 15:43:56.472: INFO: Got endpoints: latency-svc-nndw9 [142.671472ms]
Aug 19 15:43:56.477: INFO: Created: latency-svc-smh5g
Aug 19 15:43:56.487: INFO: Got endpoints: latency-svc-smh5g [157.618904ms]
Aug 19 15:43:56.495: INFO: Created: latency-svc-kbtcr
Aug 19 15:43:56.506: INFO: Got endpoints: latency-svc-kbtcr [162.334744ms]
Aug 19 15:43:56.508: INFO: Created: latency-svc-v4f7f
Aug 19 15:43:56.517: INFO: Got endpoints: latency-svc-v4f7f [168.578219ms]
Aug 19 15:43:56.528: INFO: Created: latency-svc-nnc52
Aug 19 15:43:56.535: INFO: Got endpoints: latency-svc-nnc52 [176.163611ms]
Aug 19 15:43:56.544: INFO: Created: latency-svc-qfdt8
Aug 19 15:43:56.546: INFO: Created: latency-svc-74zrx
Aug 19 15:43:56.552: INFO: Got endpoints: latency-svc-qfdt8 [181.736492ms]
Aug 19 15:43:56.559: INFO: Got endpoints: latency-svc-74zrx [181.937709ms]
Aug 19 15:43:56.560: INFO: Created: latency-svc-tmfqf
Aug 19 15:43:56.568: INFO: Got endpoints: latency-svc-tmfqf [180.990464ms]
Aug 19 15:43:56.572: INFO: Created: latency-svc-mqcf4
Aug 19 15:43:56.579: INFO: Got endpoints: latency-svc-mqcf4 [179.545494ms]
Aug 19 15:43:56.581: INFO: Created: latency-svc-n5975
Aug 19 15:43:56.591: INFO: Created: latency-svc-d7cnm
Aug 19 15:43:56.605: INFO: Created: latency-svc-5wfft
Aug 19 15:43:56.609: INFO: Got endpoints: latency-svc-n5975 [205.910937ms]
Aug 19 15:43:56.610: INFO: Got endpoints: latency-svc-d7cnm [195.288218ms]
Aug 19 15:43:56.613: INFO: Got endpoints: latency-svc-5wfft [188.504832ms]
Aug 19 15:43:56.616: INFO: Created: latency-svc-grwtv
Aug 19 15:43:56.623: INFO: Got endpoints: latency-svc-grwtv [187.373997ms]
Aug 19 15:43:56.627: INFO: Created: latency-svc-2pwt8
Aug 19 15:43:56.634: INFO: Got endpoints: latency-svc-2pwt8 [189.660437ms]
Aug 19 15:43:56.636: INFO: Created: latency-svc-r7w5z
Aug 19 15:43:56.645: INFO: Created: latency-svc-224n9
Aug 19 15:43:56.648: INFO: Got endpoints: latency-svc-r7w5z [193.409318ms]
Aug 19 15:43:56.654: INFO: Got endpoints: latency-svc-224n9 [181.650218ms]
Aug 19 15:43:56.659: INFO: Created: latency-svc-6m2r9
Aug 19 15:43:56.665: INFO: Got endpoints: latency-svc-6m2r9 [177.32154ms]
Aug 19 15:43:56.763: INFO: Created: latency-svc-6dmkg
Aug 19 15:43:56.765: INFO: Created: latency-svc-x8qjn
Aug 19 15:43:56.766: INFO: Created: latency-svc-pdngb
Aug 19 15:43:56.766: INFO: Created: latency-svc-s66xg
Aug 19 15:43:56.767: INFO: Created: latency-svc-lks8z
Aug 19 15:43:56.768: INFO: Created: latency-svc-nlmsv
Aug 19 15:43:56.770: INFO: Created: latency-svc-xxsfs
Aug 19 15:43:56.770: INFO: Created: latency-svc-kk4gc
Aug 19 15:43:56.773: INFO: Created: latency-svc-5czp4
Aug 19 15:43:56.773: INFO: Created: latency-svc-mhzn7
Aug 19 15:43:56.773: INFO: Created: latency-svc-spth7
Aug 19 15:43:56.773: INFO: Created: latency-svc-stv9h
Aug 19 15:43:56.773: INFO: Created: latency-svc-rb8qd
Aug 19 15:43:56.774: INFO: Created: latency-svc-rsn8x
Aug 19 15:43:56.774: INFO: Created: latency-svc-q76bt
Aug 19 15:43:56.779: INFO: Got endpoints: latency-svc-pdngb [227.559618ms]
Aug 19 15:43:56.781: INFO: Got endpoints: latency-svc-lks8z [171.74697ms]
Aug 19 15:43:56.782: INFO: Got endpoints: latency-svc-s66xg [117.107735ms]
Aug 19 15:43:56.782: INFO: Got endpoints: latency-svc-6dmkg [246.815498ms]
Aug 19 15:43:56.784: INFO: Got endpoints: latency-svc-x8qjn [277.505954ms]
Aug 19 15:43:56.785: INFO: Got endpoints: latency-svc-nlmsv [175.203891ms]
Aug 19 15:43:56.789: INFO: Got endpoints: latency-svc-rb8qd [135.024386ms]
Aug 19 15:43:56.792: INFO: Got endpoints: latency-svc-rsn8x [212.795948ms]
Aug 19 15:43:56.793: INFO: Got endpoints: latency-svc-q76bt [145.673206ms]
Aug 19 15:43:56.794: INFO: Got endpoints: latency-svc-kk4gc [160.356215ms]
Aug 19 15:43:56.794: INFO: Got endpoints: latency-svc-xxsfs [277.460252ms]
Aug 19 15:43:56.798: INFO: Got endpoints: latency-svc-mhzn7 [185.193431ms]
Aug 19 15:43:56.802: INFO: Created: latency-svc-qk99z
Aug 19 15:43:56.804: INFO: Got endpoints: latency-svc-stv9h [180.382439ms]
Aug 19 15:43:56.806: INFO: Got endpoints: latency-svc-spth7 [246.58049ms]
Aug 19 15:43:56.806: INFO: Got endpoints: latency-svc-5czp4 [237.426434ms]
Aug 19 15:43:56.808: INFO: Got endpoints: latency-svc-qk99z [28.813171ms]
Aug 19 15:43:56.816: INFO: Created: latency-svc-x4j2z
Aug 19 15:43:56.823: INFO: Created: latency-svc-dmw54
Aug 19 15:43:56.823: INFO: Got endpoints: latency-svc-x4j2z [41.738691ms]
Aug 19 15:43:56.829: INFO: Got endpoints: latency-svc-dmw54 [46.737326ms]
Aug 19 15:43:56.830: INFO: Created: latency-svc-7nx8z
Aug 19 15:43:56.836: INFO: Got endpoints: latency-svc-7nx8z [53.804837ms]
Aug 19 15:43:56.838: INFO: Created: latency-svc-vkcq2
Aug 19 15:43:56.846: INFO: Got endpoints: latency-svc-vkcq2 [62.181302ms]
Aug 19 15:43:56.850: INFO: Created: latency-svc-8mnsd
Aug 19 15:43:56.856: INFO: Got endpoints: latency-svc-8mnsd [71.524612ms]
Aug 19 15:43:56.860: INFO: Created: latency-svc-j4gd7
Aug 19 15:43:56.864: INFO: Created: latency-svc-m5q2c
Aug 19 15:43:56.867: INFO: Got endpoints: latency-svc-j4gd7 [78.068154ms]
Aug 19 15:43:56.871: INFO: Got endpoints: latency-svc-m5q2c [78.673104ms]
Aug 19 15:43:56.876: INFO: Created: latency-svc-vxmhx
Aug 19 15:43:56.882: INFO: Got endpoints: latency-svc-vxmhx [88.463701ms]
Aug 19 15:43:56.887: INFO: Created: latency-svc-nhcb2
Aug 19 15:43:56.891: INFO: Created: latency-svc-t48h5
Aug 19 15:43:56.899: INFO: Got endpoints: latency-svc-nhcb2 [104.515707ms]
Aug 19 15:43:56.899: INFO: Got endpoints: latency-svc-t48h5 [105.220406ms]
Aug 19 15:43:56.901: INFO: Created: latency-svc-mwn4x
Aug 19 15:43:56.906: INFO: Got endpoints: latency-svc-mwn4x [108.244995ms]
Aug 19 15:43:56.909: INFO: Created: latency-svc-5cj5b
Aug 19 15:43:56.915: INFO: Got endpoints: latency-svc-5cj5b [110.956477ms]
Aug 19 15:43:56.918: INFO: Created: latency-svc-895tz
Aug 19 15:43:56.923: INFO: Created: latency-svc-8572k
Aug 19 15:43:56.924: INFO: Got endpoints: latency-svc-895tz [118.91387ms]
Aug 19 15:43:56.931: INFO: Got endpoints: latency-svc-8572k [125.446792ms]
Aug 19 15:43:56.934: INFO: Created: latency-svc-tq4qq
Aug 19 15:43:56.942: INFO: Got endpoints: latency-svc-tq4qq [133.477653ms]
Aug 19 15:43:56.944: INFO: Created: latency-svc-pt2wb
Aug 19 15:43:56.952: INFO: Got endpoints: latency-svc-pt2wb [128.672492ms]
Aug 19 15:43:56.954: INFO: Created: latency-svc-6gvft
Aug 19 15:43:56.961: INFO: Got endpoints: latency-svc-6gvft [132.579616ms]
Aug 19 15:43:56.965: INFO: Created: latency-svc-vzkpz
Aug 19 15:43:56.971: INFO: Got endpoints: latency-svc-vzkpz [135.659721ms]
Aug 19 15:43:56.974: INFO: Created: latency-svc-cz49q
Aug 19 15:43:56.981: INFO: Got endpoints: latency-svc-cz49q [134.837744ms]
Aug 19 15:43:56.982: INFO: Created: latency-svc-r4xq7
Aug 19 15:43:56.987: INFO: Got endpoints: latency-svc-r4xq7 [130.910528ms]
Aug 19 15:43:56.992: INFO: Created: latency-svc-vsxgx
Aug 19 15:43:56.996: INFO: Got endpoints: latency-svc-vsxgx [129.485535ms]
Aug 19 15:43:56.999: INFO: Created: latency-svc-fxxl2
Aug 19 15:43:57.006: INFO: Created: latency-svc-klh5p
Aug 19 15:43:57.006: INFO: Got endpoints: latency-svc-fxxl2 [134.79162ms]
Aug 19 15:43:57.011: INFO: Got endpoints: latency-svc-klh5p [129.285222ms]
Aug 19 15:43:57.015: INFO: Created: latency-svc-5kwns
Aug 19 15:43:57.021: INFO: Got endpoints: latency-svc-5kwns [122.036765ms]
Aug 19 15:43:57.026: INFO: Created: latency-svc-vh2df
Aug 19 15:43:57.030: INFO: Created: latency-svc-9mkzn
Aug 19 15:43:57.034: INFO: Got endpoints: latency-svc-vh2df [134.037621ms]
Aug 19 15:43:57.039: INFO: Created: latency-svc-qn2vq
Aug 19 15:43:57.041: INFO: Got endpoints: latency-svc-9mkzn [134.809162ms]
Aug 19 15:43:57.045: INFO: Got endpoints: latency-svc-qn2vq [130.662233ms]
Aug 19 15:43:57.050: INFO: Created: latency-svc-p8qbm
Aug 19 15:43:57.056: INFO: Got endpoints: latency-svc-p8qbm [131.076067ms]
Aug 19 15:43:57.059: INFO: Created: latency-svc-hqqq7
Aug 19 15:43:57.066: INFO: Got endpoints: latency-svc-hqqq7 [134.625822ms]
Aug 19 15:43:57.068: INFO: Created: latency-svc-gx2kv
Aug 19 15:43:57.080: INFO: Got endpoints: latency-svc-gx2kv [138.791285ms]
Aug 19 15:43:57.082: INFO: Created: latency-svc-x54ln
Aug 19 15:43:57.088: INFO: Got endpoints: latency-svc-x54ln [136.407373ms]
Aug 19 15:43:57.096: INFO: Created: latency-svc-6cbnx
Aug 19 15:43:57.107: INFO: Got endpoints: latency-svc-6cbnx [145.445482ms]
Aug 19 15:43:57.219: INFO: Created: latency-svc-stwrd
Aug 19 15:43:57.220: INFO: Created: latency-svc-78nxz
Aug 19 15:43:57.220: INFO: Created: latency-svc-kz49z
Aug 19 15:43:57.220: INFO: Created: latency-svc-bsjpp
Aug 19 15:43:57.224: INFO: Created: latency-svc-f7hpc
Aug 19 15:43:57.224: INFO: Created: latency-svc-7krgc
Aug 19 15:43:57.224: INFO: Created: latency-svc-kxclb
Aug 19 15:43:57.224: INFO: Created: latency-svc-lq5hq
Aug 19 15:43:57.224: INFO: Created: latency-svc-gzvwh
Aug 19 15:43:57.224: INFO: Created: latency-svc-czd7r
Aug 19 15:43:57.225: INFO: Created: latency-svc-c98r7
Aug 19 15:43:57.225: INFO: Created: latency-svc-85lrv
Aug 19 15:43:57.225: INFO: Created: latency-svc-fcjj6
Aug 19 15:43:57.225: INFO: Created: latency-svc-n4bw8
Aug 19 15:43:57.225: INFO: Created: latency-svc-hchfk
Aug 19 15:43:57.230: INFO: Got endpoints: latency-svc-hchfk [258.091061ms]
Aug 19 15:43:57.230: INFO: Got endpoints: latency-svc-czd7r [248.971438ms]
Aug 19 15:43:57.232: INFO: Got endpoints: latency-svc-kz49z [151.793394ms]
Aug 19 15:43:57.236: INFO: Got endpoints: latency-svc-stwrd [195.270972ms]
Aug 19 15:43:57.242: INFO: Got endpoints: latency-svc-78nxz [154.301472ms]
Aug 19 15:43:57.243: INFO: Got endpoints: latency-svc-lq5hq [231.797239ms]
Aug 19 15:43:57.243: INFO: Got endpoints: latency-svc-bsjpp [187.538538ms]
Aug 19 15:43:57.245: INFO: Got endpoints: latency-svc-kxclb [179.484004ms]
Aug 19 15:43:57.245: INFO: Got endpoints: latency-svc-7krgc [199.646502ms]
Aug 19 15:43:57.247: INFO: Got endpoints: latency-svc-fcjj6 [251.023548ms]
Aug 19 15:43:57.260: INFO: Got endpoints: latency-svc-c98r7 [153.174967ms]
Aug 19 15:43:57.260: INFO: Got endpoints: latency-svc-n4bw8 [254.312888ms]
Aug 19 15:43:57.260: INFO: Got endpoints: latency-svc-f7hpc [226.438944ms]
Aug 19 15:43:57.261: INFO: Got endpoints: latency-svc-85lrv [239.71522ms]
Aug 19 15:43:57.261: INFO: Got endpoints: latency-svc-gzvwh [273.49302ms]
Aug 19 15:43:57.265: INFO: Created: latency-svc-jzcvm
Aug 19 15:43:57.270: INFO: Got endpoints: latency-svc-jzcvm [40.272745ms]
Aug 19 15:43:57.274: INFO: Created: latency-svc-h6brx
Aug 19 15:43:57.282: INFO: Got endpoints: latency-svc-h6brx [52.125099ms]
Aug 19 15:43:57.282: INFO: Created: latency-svc-6nwgw
Aug 19 15:43:57.287: INFO: Created: latency-svc-22gqf
Aug 19 15:43:57.288: INFO: Got endpoints: latency-svc-6nwgw [56.242653ms]
Aug 19 15:43:57.293: INFO: Got endpoints: latency-svc-22gqf [56.747293ms]
Aug 19 15:43:57.296: INFO: Created: latency-svc-sxgg8
Aug 19 15:43:57.304: INFO: Got endpoints: latency-svc-sxgg8 [61.199113ms]
Aug 19 15:43:57.308: INFO: Created: latency-svc-bw4g6
Aug 19 15:43:57.315: INFO: Got endpoints: latency-svc-bw4g6 [71.917964ms]
Aug 19 15:43:57.317: INFO: Created: latency-svc-p6ptn
Aug 19 15:43:57.323: INFO: Got endpoints: latency-svc-p6ptn [80.244184ms]
Aug 19 15:43:57.328: INFO: Created: latency-svc-8hhpm
Aug 19 15:43:57.334: INFO: Created: latency-svc-vpss9
Aug 19 15:43:57.336: INFO: Got endpoints: latency-svc-8hhpm [90.959213ms]
Aug 19 15:43:57.341: INFO: Got endpoints: latency-svc-vpss9 [95.355704ms]
Aug 19 15:43:57.348: INFO: Created: latency-svc-wqz8h
Aug 19 15:43:57.350: INFO: Created: latency-svc-57kzx
Aug 19 15:43:57.354: INFO: Got endpoints: latency-svc-wqz8h [106.960444ms]
Aug 19 15:43:57.358: INFO: Got endpoints: latency-svc-57kzx [98.046302ms]
Aug 19 15:43:57.359: INFO: Created: latency-svc-b4ss6
Aug 19 15:43:57.363: INFO: Got endpoints: latency-svc-b4ss6 [103.30477ms]
Aug 19 15:43:57.369: INFO: Created: latency-svc-tpnm5
Aug 19 15:43:57.374: INFO: Got endpoints: latency-svc-tpnm5 [113.712001ms]
Aug 19 15:43:57.379: INFO: Created: latency-svc-fv2zk
Aug 19 15:43:57.384: INFO: Got endpoints: latency-svc-fv2zk [123.184519ms]
Aug 19 15:43:57.387: INFO: Created: latency-svc-nxswt
Aug 19 15:43:57.390: INFO: Created: latency-svc-4m4ln
Aug 19 15:43:57.394: INFO: Got endpoints: latency-svc-nxswt [134.153218ms]
Aug 19 15:43:57.396: INFO: Got endpoints: latency-svc-4m4ln [126.422285ms]
Aug 19 15:43:57.402: INFO: Created: latency-svc-drcp7
Aug 19 15:43:57.411: INFO: Got endpoints: latency-svc-drcp7 [129.214298ms]
Aug 19 15:43:57.414: INFO: Created: latency-svc-zj22q
Aug 19 15:43:57.420: INFO: Created: latency-svc-z8rdl
Aug 19 15:43:57.420: INFO: Got endpoints: latency-svc-zj22q [131.288015ms]
Aug 19 15:43:57.426: INFO: Created: latency-svc-2wgx2
Aug 19 15:43:57.427: INFO: Got endpoints: latency-svc-z8rdl [133.790574ms]
Aug 19 15:43:57.432: INFO: Got endpoints: latency-svc-2wgx2 [128.493583ms]
Aug 19 15:43:57.436: INFO: Created: latency-svc-txlgm
Aug 19 15:43:57.441: INFO: Got endpoints: latency-svc-txlgm [125.725835ms]
Aug 19 15:43:57.445: INFO: Created: latency-svc-k5zfc
Aug 19 15:43:57.451: INFO: Got endpoints: latency-svc-k5zfc [127.821721ms]
Aug 19 15:43:57.453: INFO: Created: latency-svc-vl7xv
Aug 19 15:43:57.459: INFO: Created: latency-svc-kcjpd
Aug 19 15:43:57.461: INFO: Got endpoints: latency-svc-vl7xv [124.580888ms]
Aug 19 15:43:57.466: INFO: Got endpoints: latency-svc-kcjpd [125.146008ms]
Aug 19 15:43:57.469: INFO: Created: latency-svc-mnwbn
Aug 19 15:43:57.475: INFO: Got endpoints: latency-svc-mnwbn [120.946091ms]
Aug 19 15:43:57.481: INFO: Created: latency-svc-vgwvd
Aug 19 15:43:57.486: INFO: Created: latency-svc-h8dsk
Aug 19 15:43:57.488: INFO: Got endpoints: latency-svc-vgwvd [130.067956ms]
Aug 19 15:43:57.492: INFO: Got endpoints: latency-svc-h8dsk [128.746286ms]
Aug 19 15:43:57.495: INFO: Created: latency-svc-5rvpr
Aug 19 15:43:57.501: INFO: Got endpoints: latency-svc-5rvpr [126.817284ms]
Aug 19 15:43:57.506: INFO: Created: latency-svc-9zfpz
Aug 19 15:43:57.512: INFO: Got endpoints: latency-svc-9zfpz [128.199371ms]
Aug 19 15:43:57.518: INFO: Created: latency-svc-52f5l
Aug 19 15:43:57.523: INFO: Got endpoints: latency-svc-52f5l [128.656148ms]
Aug 19 15:43:57.526: INFO: Created: latency-svc-sffcs
Aug 19 15:43:57.533: INFO: Got endpoints: latency-svc-sffcs [136.885105ms]
Aug 19 15:43:57.537: INFO: Created: latency-svc-5p2qd
Aug 19 15:43:57.542: INFO: Created: latency-svc-lcmfk
Aug 19 15:43:57.543: INFO: Got endpoints: latency-svc-5p2qd [132.425906ms]
Aug 19 15:43:57.548: INFO: Got endpoints: latency-svc-lcmfk [128.559365ms]
Aug 19 15:43:57.553: INFO: Created: latency-svc-g4q5s
Aug 19 15:43:57.557: INFO: Created: latency-svc-9tsps
Aug 19 15:43:57.557: INFO: Got endpoints: latency-svc-g4q5s [130.234444ms]
Aug 19 15:43:57.565: INFO: Got endpoints: latency-svc-9tsps [132.356595ms]
Aug 19 15:43:57.566: INFO: Created: latency-svc-2m4xd
Aug 19 15:43:57.571: INFO: Got endpoints: latency-svc-2m4xd [130.183219ms]
Aug 19 15:43:57.573: INFO: Created: latency-svc-mzb94
Aug 19 15:43:57.579: INFO: Got endpoints: latency-svc-mzb94 [127.613348ms]
Aug 19 15:43:57.583: INFO: Created: latency-svc-lw7zj
Aug 19 15:43:57.590: INFO: Got endpoints: latency-svc-lw7zj [129.342401ms]
Aug 19 15:43:57.593: INFO: Created: latency-svc-l5fxp
Aug 19 15:43:57.599: INFO: Got endpoints: latency-svc-l5fxp [133.576242ms]
Aug 19 15:43:57.600: INFO: Created: latency-svc-mg6c9
Aug 19 15:43:57.607: INFO: Got endpoints: latency-svc-mg6c9 [131.818156ms]
Aug 19 15:43:57.618: INFO: Created: latency-svc-6q5vl
Aug 19 15:43:57.624: INFO: Created: latency-svc-lbxzr
Aug 19 15:43:57.627: INFO: Got endpoints: latency-svc-6q5vl [139.185795ms]
Aug 19 15:43:57.632: INFO: Got endpoints: latency-svc-lbxzr [139.383603ms]
Aug 19 15:43:57.632: INFO: Created: latency-svc-5ljhx
Aug 19 15:43:57.639: INFO: Created: latency-svc-56z2q
Aug 19 15:43:57.644: INFO: Got endpoints: latency-svc-5ljhx [142.966994ms]
Aug 19 15:43:57.646: INFO: Created: latency-svc-bgqng
Aug 19 15:43:57.649: INFO: Got endpoints: latency-svc-56z2q [136.483135ms]
Aug 19 15:43:57.653: INFO: Got endpoints: latency-svc-bgqng [130.552336ms]
Aug 19 15:43:57.670: INFO: Created: latency-svc-z5sxz
Aug 19 15:43:57.676: INFO: Got endpoints: latency-svc-z5sxz [142.387407ms]
Aug 19 15:43:57.676: INFO: Created: latency-svc-bmv62
Aug 19 15:43:57.682: INFO: Got endpoints: latency-svc-bmv62 [138.599349ms]
Aug 19 15:43:57.686: INFO: Created: latency-svc-fkxt5
Aug 19 15:43:57.692: INFO: Got endpoints: latency-svc-fkxt5 [143.753557ms]
Aug 19 15:43:57.695: INFO: Created: latency-svc-6rm7h
Aug 19 15:43:57.702: INFO: Got endpoints: latency-svc-6rm7h [145.097471ms]
Aug 19 15:43:57.707: INFO: Created: latency-svc-7pzjf
Aug 19 15:43:57.711: INFO: Created: latency-svc-zzcth
Aug 19 15:43:57.716: INFO: Got endpoints: latency-svc-7pzjf [151.301431ms]
Aug 19 15:43:57.719: INFO: Got endpoints: latency-svc-zzcth [148.449959ms]
Aug 19 15:43:57.720: INFO: Created: latency-svc-zvmkz
Aug 19 15:43:57.726: INFO: Got endpoints: latency-svc-zvmkz [147.294723ms]
Aug 19 15:43:57.734: INFO: Created: latency-svc-4vrk9
Aug 19 15:43:57.740: INFO: Got endpoints: latency-svc-4vrk9 [150.077214ms]
Aug 19 15:43:57.745: INFO: Created: latency-svc-rfq48
Aug 19 15:43:57.749: INFO: Got endpoints: latency-svc-rfq48 [150.167597ms]
Aug 19 15:43:57.754: INFO: Created: latency-svc-b2g2z
Aug 19 15:43:57.761: INFO: Got endpoints: latency-svc-b2g2z [153.615184ms]
Aug 19 15:43:57.763: INFO: Created: latency-svc-sjvsk
Aug 19 15:43:57.770: INFO: Got endpoints: latency-svc-sjvsk [142.770363ms]
Aug 19 15:43:57.773: INFO: Created: latency-svc-7rjld
Aug 19 15:43:57.780: INFO: Got endpoints: latency-svc-7rjld [148.729439ms]
Aug 19 15:43:57.786: INFO: Created: latency-svc-m8px9
Aug 19 15:43:57.791: INFO: Created: latency-svc-5b4b7
Aug 19 15:43:57.792: INFO: Got endpoints: latency-svc-m8px9 [147.667194ms]
Aug 19 15:43:57.795: INFO: Got endpoints: latency-svc-5b4b7 [146.827446ms]
Aug 19 15:43:57.803: INFO: Created: latency-svc-5bhrl
Aug 19 15:43:57.808: INFO: Got endpoints: latency-svc-5bhrl [154.844437ms]
Aug 19 15:43:57.810: INFO: Created: latency-svc-jg2xz
Aug 19 15:43:57.818: INFO: Got endpoints: latency-svc-jg2xz [142.68629ms]
Aug 19 15:43:57.821: INFO: Created: latency-svc-qrgd2
Aug 19 15:43:57.827: INFO: Got endpoints: latency-svc-qrgd2 [144.514755ms]
Aug 19 15:43:57.830: INFO: Created: latency-svc-mb79r
Aug 19 15:43:57.836: INFO: Got endpoints: latency-svc-mb79r [144.308933ms]
Aug 19 15:43:57.838: INFO: Created: latency-svc-g5hwf
Aug 19 15:43:57.843: INFO: Got endpoints: latency-svc-g5hwf [140.997339ms]
Aug 19 15:43:57.846: INFO: Created: latency-svc-kbpsq
Aug 19 15:43:57.852: INFO: Got endpoints: latency-svc-kbpsq [136.515748ms]
Aug 19 15:43:57.864: INFO: Created: latency-svc-kbj7h
Aug 19 15:43:57.865: INFO: Created: latency-svc-4fxwd
Aug 19 15:43:57.867: INFO: Got endpoints: latency-svc-kbj7h [147.339909ms]
Aug 19 15:43:57.871: INFO: Got endpoints: latency-svc-4fxwd [144.471704ms]
Aug 19 15:43:57.875: INFO: Created: latency-svc-h9hvr
Aug 19 15:43:57.880: INFO: Got endpoints: latency-svc-h9hvr [140.213453ms]
Aug 19 15:43:57.885: INFO: Created: latency-svc-tjxxf
Aug 19 15:43:57.893: INFO: Created: latency-svc-wxf5x
Aug 19 15:43:57.894: INFO: Got endpoints: latency-svc-tjxxf [144.662746ms]
Aug 19 15:43:57.898: INFO: Got endpoints: latency-svc-wxf5x [137.647511ms]
Aug 19 15:43:57.898: INFO: Created: latency-svc-n67pm
Aug 19 15:43:57.905: INFO: Got endpoints: latency-svc-n67pm [134.731962ms]
Aug 19 15:43:57.913: INFO: Created: latency-svc-9pvzq
Aug 19 15:43:57.918: INFO: Got endpoints: latency-svc-9pvzq [137.701692ms]
Aug 19 15:43:57.922: INFO: Created: latency-svc-l25l5
Aug 19 15:43:57.929: INFO: Got endpoints: latency-svc-l25l5 [137.583191ms]
Aug 19 15:43:57.934: INFO: Created: latency-svc-k7kj7
Aug 19 15:43:57.941: INFO: Got endpoints: latency-svc-k7kj7 [145.527767ms]
Aug 19 15:43:57.944: INFO: Created: latency-svc-6rhrj
Aug 19 15:43:57.952: INFO: Got endpoints: latency-svc-6rhrj [144.096236ms]
Aug 19 15:43:57.954: INFO: Created: latency-svc-jgpxp
Aug 19 15:43:57.958: INFO: Got endpoints: latency-svc-jgpxp [140.160243ms]
Aug 19 15:43:57.963: INFO: Created: latency-svc-wqv5d
Aug 19 15:43:57.969: INFO: Got endpoints: latency-svc-wqv5d [142.441842ms]
Aug 19 15:43:57.977: INFO: Created: latency-svc-cbzmg
Aug 19 15:43:57.985: INFO: Got endpoints: latency-svc-cbzmg [148.452097ms]
Aug 19 15:43:57.986: INFO: Created: latency-svc-zqqjq
Aug 19 15:43:57.997: INFO: Created: latency-svc-f6j5w
Aug 19 15:43:57.999: INFO: Got endpoints: latency-svc-zqqjq [155.683163ms]
Aug 19 15:43:58.002: INFO: Got endpoints: latency-svc-f6j5w [149.517964ms]
Aug 19 15:43:58.003: INFO: Created: latency-svc-mx4rc
Aug 19 15:43:58.010: INFO: Got endpoints: latency-svc-mx4rc [142.865128ms]
Aug 19 15:43:58.014: INFO: Created: latency-svc-gpxgf
Aug 19 15:43:58.021: INFO: Created: latency-svc-cp9s5
Aug 19 15:43:58.022: INFO: Got endpoints: latency-svc-gpxgf [151.494533ms]
Aug 19 15:43:58.030: INFO: Got endpoints: latency-svc-cp9s5 [149.629254ms]
Aug 19 15:43:58.032: INFO: Created: latency-svc-7mgxb
Aug 19 15:43:58.037: INFO: Got endpoints: latency-svc-7mgxb [143.176881ms]
Aug 19 15:43:58.037: INFO: Latencies: [28.813171ms 36.051611ms 37.413384ms 40.272745ms 41.738691ms 46.737326ms 52.125099ms 52.639307ms 53.804837ms 54.785367ms 56.242653ms 56.747293ms 60.824783ms 61.199113ms 61.582647ms 62.181302ms 71.524612ms 71.917964ms 75.747656ms 77.249761ms 78.068154ms 78.673104ms 80.244184ms 86.019164ms 88.220394ms 88.463701ms 90.959213ms 95.355704ms 97.331563ms 98.046302ms 103.30477ms 104.515707ms 105.220406ms 105.802492ms 106.960444ms 108.244995ms 110.956477ms 112.856411ms 113.712001ms 115.115573ms 117.107735ms 118.91387ms 120.946091ms 122.036765ms 123.184519ms 124.580888ms 125.146008ms 125.446792ms 125.725835ms 125.897473ms 126.422285ms 126.817284ms 127.552217ms 127.613348ms 127.821721ms 128.199371ms 128.493583ms 128.559365ms 128.656148ms 128.672492ms 128.746286ms 129.214298ms 129.285222ms 129.342401ms 129.485535ms 130.067956ms 130.183219ms 130.234444ms 130.552336ms 130.662233ms 130.910528ms 131.076067ms 131.21717ms 131.288015ms 131.818156ms 132.356595ms 132.425906ms 132.579616ms 133.477653ms 133.576242ms 133.790574ms 134.037621ms 134.153218ms 134.617366ms 134.625822ms 134.731962ms 134.79162ms 134.809162ms 134.837744ms 135.024386ms 135.659721ms 136.407373ms 136.483135ms 136.515748ms 136.885105ms 137.583191ms 137.647511ms 137.701692ms 138.563935ms 138.599349ms 138.791285ms 139.140014ms 139.185795ms 139.383603ms 140.037396ms 140.160243ms 140.213453ms 140.285417ms 140.997339ms 142.387407ms 142.441842ms 142.671472ms 142.68629ms 142.770363ms 142.865128ms 142.966994ms 143.176881ms 143.753557ms 144.096236ms 144.308933ms 144.471704ms 144.514755ms 144.662746ms 145.097471ms 145.445482ms 145.527767ms 145.673206ms 146.827446ms 147.294723ms 147.339909ms 147.667194ms 148.449959ms 148.452097ms 148.729439ms 149.517964ms 149.629254ms 150.077214ms 150.167597ms 151.301431ms 151.494533ms 151.793394ms 153.174967ms 153.615184ms 154.301472ms 154.844437ms 155.683163ms 157.618904ms 160.356215ms 162.334744ms 162.506548ms 168.578219ms 171.74697ms 175.203891ms 176.163611ms 177.32154ms 179.484004ms 179.545494ms 180.382439ms 180.990464ms 181.166872ms 181.650218ms 181.736492ms 181.937709ms 182.975554ms 184.361916ms 185.193431ms 186.714042ms 187.373997ms 187.538538ms 187.883225ms 188.504832ms 189.660437ms 191.977784ms 193.409318ms 194.09779ms 195.270972ms 195.288218ms 195.595886ms 198.202944ms 199.646502ms 205.910937ms 206.018579ms 206.211093ms 206.211954ms 209.66513ms 212.795948ms 226.438944ms 227.559618ms 231.797239ms 237.426434ms 239.71522ms 246.58049ms 246.815498ms 248.971438ms 251.023548ms 254.312888ms 258.091061ms 273.49302ms 277.460252ms 277.505954ms]
Aug 19 15:43:58.037: INFO: 50 %ile: 138.791285ms
Aug 19 15:43:58.037: INFO: 90 %ile: 205.910937ms
Aug 19 15:43:58.037: INFO: 99 %ile: 277.460252ms
Aug 19 15:43:58.037: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:188
Aug 19 15:43:58.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6146" for this suite.

• [SLOW TEST:5.211 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":356,"completed":138,"skipped":2735,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:43:58.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:43:58.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8933" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":356,"completed":139,"skipped":2739,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:43:58.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Aug 19 15:43:58.211: INFO: observed Pod pod-test in namespace pods-3983 in phase Pending with labels: map[test-pod-static:true] & conditions []
Aug 19 15:43:58.219: INFO: observed Pod pod-test in namespace pods-3983 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:43:58 +0000 UTC  }]
Aug 19 15:43:58.239: INFO: observed Pod pod-test in namespace pods-3983 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:43:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:43:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:43:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:43:58 +0000 UTC  }]
Aug 19 15:43:59.998: INFO: observed Pod pod-test in namespace pods-3983 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:43:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:43:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:43:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:43:58 +0000 UTC  }]
Aug 19 15:44:00.622: INFO: Found Pod pod-test in namespace pods-3983 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:43:58 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:44:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:44:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 15:43:58 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Aug 19 15:44:00.664: INFO: observed event type MODIFIED
Aug 19 15:44:01.808: INFO: observed event type MODIFIED
Aug 19 15:44:03.635: INFO: observed event type MODIFIED
Aug 19 15:44:03.647: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 19 15:44:03.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3983" for this suite.

• [SLOW TEST:5.563 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":356,"completed":140,"skipped":2743,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:44:03.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 15:44:04.281: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 19 15:44:06.294: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 15, 44, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 44, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 15, 44, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 44, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 15:44:09.317: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:44:09.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1259-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:44:12.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4154" for this suite.
STEP: Destroying namespace "webhook-4154-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.911 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":356,"completed":141,"skipped":2743,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:44:12.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 15:44:12.913: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 19 15:44:14.926: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 15, 44, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 44, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 15, 44, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 44, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 15:44:17.943: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:44:17.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7324" for this suite.
STEP: Destroying namespace "webhook-7324-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.435 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":356,"completed":142,"skipped":2793,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:44:18.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Aug 19 15:44:18.074: INFO: namespace kubectl-659
Aug 19 15:44:18.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-659 create -f -'
Aug 19 15:44:19.706: INFO: stderr: ""
Aug 19 15:44:19.706: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 19 15:44:20.711: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 19 15:44:20.711: INFO: Found 0 / 1
Aug 19 15:44:21.709: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 19 15:44:21.709: INFO: Found 0 / 1
Aug 19 15:44:22.710: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 19 15:44:22.710: INFO: Found 1 / 1
Aug 19 15:44:22.710: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 19 15:44:22.713: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 19 15:44:22.713: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 19 15:44:22.713: INFO: wait on agnhost-primary startup in kubectl-659 
Aug 19 15:44:22.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-659 logs agnhost-primary-wnfms agnhost-primary'
Aug 19 15:44:22.764: INFO: stderr: ""
Aug 19 15:44:22.764: INFO: stdout: "Paused\n"
STEP: exposing RC
Aug 19 15:44:22.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-659 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 19 15:44:22.818: INFO: stderr: ""
Aug 19 15:44:22.818: INFO: stdout: "service/rm2 exposed\n"
Aug 19 15:44:22.824: INFO: Service rm2 in namespace kubectl-659 found.
STEP: exposing service
Aug 19 15:44:24.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-659 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 19 15:44:24.883: INFO: stderr: ""
Aug 19 15:44:24.883: INFO: stdout: "service/rm3 exposed\n"
Aug 19 15:44:24.894: INFO: Service rm3 in namespace kubectl-659 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 15:44:26.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-659" for this suite.

• [SLOW TEST:8.891 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1249
    should create services for rc  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":356,"completed":143,"skipped":2814,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:44:26.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 19 15:44:26.974: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96c00e12-43e9-400d-8a75-c8b5bb96713a" in namespace "projected-1507" to be "Succeeded or Failed"
Aug 19 15:44:26.983: INFO: Pod "downwardapi-volume-96c00e12-43e9-400d-8a75-c8b5bb96713a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.479737ms
Aug 19 15:44:28.993: INFO: Pod "downwardapi-volume-96c00e12-43e9-400d-8a75-c8b5bb96713a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018938926s
Aug 19 15:44:30.997: INFO: Pod "downwardapi-volume-96c00e12-43e9-400d-8a75-c8b5bb96713a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0226254s
Aug 19 15:44:33.001: INFO: Pod "downwardapi-volume-96c00e12-43e9-400d-8a75-c8b5bb96713a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027489487s
STEP: Saw pod success
Aug 19 15:44:33.001: INFO: Pod "downwardapi-volume-96c00e12-43e9-400d-8a75-c8b5bb96713a" satisfied condition "Succeeded or Failed"
Aug 19 15:44:33.005: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-96c00e12-43e9-400d-8a75-c8b5bb96713a container client-container: <nil>
STEP: delete the pod
Aug 19 15:44:33.029: INFO: Waiting for pod downwardapi-volume-96c00e12-43e9-400d-8a75-c8b5bb96713a to disappear
Aug 19 15:44:33.032: INFO: Pod downwardapi-volume-96c00e12-43e9-400d-8a75-c8b5bb96713a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 19 15:44:33.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1507" for this suite.

• [SLOW TEST:6.132 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":144,"skipped":2869,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:44:33.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 19 15:44:33.093: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 19 15:45:33.213: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:45:33.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:45:33.302: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Aug 19 15:45:33.308: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:188
Aug 19 15:45:33.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1469" for this suite.
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Aug 19 15:45:33.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3488" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:60.390 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":356,"completed":145,"skipped":2870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:45:33.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
STEP: mirroring a new custom Endpoint
Aug 19 15:45:33.525: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Aug 19 15:45:35.539: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Aug 19 15:45:37.552: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:188
Aug 19 15:45:39.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-9907" for this suite.

• [SLOW TEST:6.133 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":356,"completed":146,"skipped":2906,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:45:39.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2241.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2241.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2241.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2241.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 19 15:45:43.672: INFO: DNS probes using dns-2241/dns-test-fe1176b0-263e-40dc-b7da-4eb76adbe44c succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 19 15:45:43.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2241" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","total":356,"completed":147,"skipped":2911,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:45:43.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating pod
Aug 19 15:45:43.778: INFO: The status of Pod pod-hostip-a9fefffe-d3a8-4a7f-8f38-e58de0bac8c2 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:45:45.781: INFO: The status of Pod pod-hostip-a9fefffe-d3a8-4a7f-8f38-e58de0bac8c2 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:45:47.783: INFO: The status of Pod pod-hostip-a9fefffe-d3a8-4a7f-8f38-e58de0bac8c2 is Running (Ready = true)
Aug 19 15:45:47.790: INFO: Pod pod-hostip-a9fefffe-d3a8-4a7f-8f38-e58de0bac8c2 has hostIP: 10.0.164.144
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 19 15:45:47.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6594" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":356,"completed":148,"skipped":2917,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:45:47.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 19 15:45:47.855: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9031  db463864-360d-4e28-9d8b-27c7c171b5b4 66613 0 2022-08-19 15:45:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-19 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 19 15:45:47.855: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9031  db463864-360d-4e28-9d8b-27c7c171b5b4 66615 0 2022-08-19 15:45:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-19 15:45:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 19 15:45:47.886: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9031  db463864-360d-4e28-9d8b-27c7c171b5b4 66620 0 2022-08-19 15:45:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-19 15:45:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 19 15:45:47.886: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9031  db463864-360d-4e28-9d8b-27c7c171b5b4 66625 0 2022-08-19 15:45:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-19 15:45:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Aug 19 15:45:47.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9031" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":356,"completed":149,"skipped":2920,"failed":0}
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:45:47.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Aug 19 15:45:47.922: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 19 15:46:48.049: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:46:48.054: INFO: Starting informer...
STEP: Starting pods...
Aug 19 15:46:48.279: INFO: Pod1 is running on ip-10-0-164-144.ec2.internal. Tainting Node
Aug 19 15:46:52.505: INFO: Pod2 is running on ip-10-0-164-144.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Aug 19 15:46:58.343: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 19 15:47:18.074: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:188
Aug 19 15:47:18.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9239" for this suite.

• [SLOW TEST:90.216 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":356,"completed":150,"skipped":2925,"failed":0}
SSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:47:18.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:188
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 19 15:47:18.205: INFO: starting watch
STEP: patching
STEP: updating
Aug 19 15:47:18.228: INFO: waiting for watch events with expected annotations
Aug 19 15:47:18.228: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:188
Aug 19 15:47:18.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-6081" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":356,"completed":151,"skipped":2929,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:47:18.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 15:47:18.579: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 19 15:47:20.589: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 15, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 47, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 15, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 47, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 15:47:23.606: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:47:23.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-360" for this suite.
STEP: Destroying namespace "webhook-360-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.474 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":356,"completed":152,"skipped":2951,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:47:23.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:47:24.095: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 19 15:47:24.096: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 19 15:47:24.096: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Aug 19 15:47:24.096: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 19 15:47:24.096: INFO: Checking APIGroup: apps
Aug 19 15:47:24.097: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 19 15:47:24.097: INFO: Versions found [{apps/v1 v1}]
Aug 19 15:47:24.097: INFO: apps/v1 matches apps/v1
Aug 19 15:47:24.097: INFO: Checking APIGroup: events.k8s.io
Aug 19 15:47:24.098: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 19 15:47:24.098: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Aug 19 15:47:24.098: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 19 15:47:24.098: INFO: Checking APIGroup: authentication.k8s.io
Aug 19 15:47:24.098: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 19 15:47:24.098: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Aug 19 15:47:24.098: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 19 15:47:24.098: INFO: Checking APIGroup: authorization.k8s.io
Aug 19 15:47:24.099: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 19 15:47:24.099: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Aug 19 15:47:24.099: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 19 15:47:24.099: INFO: Checking APIGroup: autoscaling
Aug 19 15:47:24.103: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Aug 19 15:47:24.103: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Aug 19 15:47:24.103: INFO: autoscaling/v2 matches autoscaling/v2
Aug 19 15:47:24.103: INFO: Checking APIGroup: batch
Aug 19 15:47:24.113: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 19 15:47:24.113: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Aug 19 15:47:24.113: INFO: batch/v1 matches batch/v1
Aug 19 15:47:24.113: INFO: Checking APIGroup: certificates.k8s.io
Aug 19 15:47:24.120: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 19 15:47:24.120: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Aug 19 15:47:24.120: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 19 15:47:24.120: INFO: Checking APIGroup: networking.k8s.io
Aug 19 15:47:24.121: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 19 15:47:24.121: INFO: Versions found [{networking.k8s.io/v1 v1}]
Aug 19 15:47:24.121: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 19 15:47:24.121: INFO: Checking APIGroup: policy
Aug 19 15:47:24.122: INFO: PreferredVersion.GroupVersion: policy/v1
Aug 19 15:47:24.122: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Aug 19 15:47:24.122: INFO: policy/v1 matches policy/v1
Aug 19 15:47:24.122: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 19 15:47:24.123: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 19 15:47:24.123: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Aug 19 15:47:24.123: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 19 15:47:24.123: INFO: Checking APIGroup: storage.k8s.io
Aug 19 15:47:24.124: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 19 15:47:24.124: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 19 15:47:24.124: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 19 15:47:24.124: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 19 15:47:24.124: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 19 15:47:24.124: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Aug 19 15:47:24.124: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 19 15:47:24.124: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 19 15:47:24.125: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 19 15:47:24.125: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Aug 19 15:47:24.125: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 19 15:47:24.125: INFO: Checking APIGroup: scheduling.k8s.io
Aug 19 15:47:24.126: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 19 15:47:24.126: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Aug 19 15:47:24.126: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 19 15:47:24.126: INFO: Checking APIGroup: coordination.k8s.io
Aug 19 15:47:24.127: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 19 15:47:24.127: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Aug 19 15:47:24.127: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 19 15:47:24.127: INFO: Checking APIGroup: node.k8s.io
Aug 19 15:47:24.128: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 19 15:47:24.128: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Aug 19 15:47:24.128: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 19 15:47:24.128: INFO: Checking APIGroup: discovery.k8s.io
Aug 19 15:47:24.128: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Aug 19 15:47:24.128: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Aug 19 15:47:24.128: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Aug 19 15:47:24.128: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 19 15:47:24.129: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Aug 19 15:47:24.129: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Aug 19 15:47:24.129: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Aug 19 15:47:24.129: INFO: Checking APIGroup: apps.openshift.io
Aug 19 15:47:24.130: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Aug 19 15:47:24.130: INFO: Versions found [{apps.openshift.io/v1 v1}]
Aug 19 15:47:24.130: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Aug 19 15:47:24.130: INFO: Checking APIGroup: authorization.openshift.io
Aug 19 15:47:24.131: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Aug 19 15:47:24.131: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Aug 19 15:47:24.131: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Aug 19 15:47:24.131: INFO: Checking APIGroup: build.openshift.io
Aug 19 15:47:24.131: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Aug 19 15:47:24.131: INFO: Versions found [{build.openshift.io/v1 v1}]
Aug 19 15:47:24.131: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Aug 19 15:47:24.131: INFO: Checking APIGroup: image.openshift.io
Aug 19 15:47:24.132: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Aug 19 15:47:24.132: INFO: Versions found [{image.openshift.io/v1 v1}]
Aug 19 15:47:24.132: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Aug 19 15:47:24.132: INFO: Checking APIGroup: oauth.openshift.io
Aug 19 15:47:24.133: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Aug 19 15:47:24.133: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Aug 19 15:47:24.133: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Aug 19 15:47:24.133: INFO: Checking APIGroup: project.openshift.io
Aug 19 15:47:24.134: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Aug 19 15:47:24.134: INFO: Versions found [{project.openshift.io/v1 v1}]
Aug 19 15:47:24.134: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Aug 19 15:47:24.134: INFO: Checking APIGroup: quota.openshift.io
Aug 19 15:47:24.135: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Aug 19 15:47:24.135: INFO: Versions found [{quota.openshift.io/v1 v1}]
Aug 19 15:47:24.135: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Aug 19 15:47:24.135: INFO: Checking APIGroup: route.openshift.io
Aug 19 15:47:24.135: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Aug 19 15:47:24.135: INFO: Versions found [{route.openshift.io/v1 v1}]
Aug 19 15:47:24.135: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Aug 19 15:47:24.135: INFO: Checking APIGroup: security.openshift.io
Aug 19 15:47:24.136: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Aug 19 15:47:24.136: INFO: Versions found [{security.openshift.io/v1 v1}]
Aug 19 15:47:24.136: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Aug 19 15:47:24.136: INFO: Checking APIGroup: template.openshift.io
Aug 19 15:47:24.137: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Aug 19 15:47:24.137: INFO: Versions found [{template.openshift.io/v1 v1}]
Aug 19 15:47:24.137: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Aug 19 15:47:24.137: INFO: Checking APIGroup: user.openshift.io
Aug 19 15:47:24.138: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Aug 19 15:47:24.138: INFO: Versions found [{user.openshift.io/v1 v1}]
Aug 19 15:47:24.138: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Aug 19 15:47:24.138: INFO: Checking APIGroup: packages.operators.coreos.com
Aug 19 15:47:24.138: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Aug 19 15:47:24.138: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Aug 19 15:47:24.138: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Aug 19 15:47:24.138: INFO: Checking APIGroup: config.openshift.io
Aug 19 15:47:24.139: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Aug 19 15:47:24.139: INFO: Versions found [{config.openshift.io/v1 v1}]
Aug 19 15:47:24.139: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Aug 19 15:47:24.139: INFO: Checking APIGroup: operator.openshift.io
Aug 19 15:47:24.140: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Aug 19 15:47:24.140: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Aug 19 15:47:24.140: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Aug 19 15:47:24.140: INFO: Checking APIGroup: apiserver.openshift.io
Aug 19 15:47:24.141: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
Aug 19 15:47:24.141: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
Aug 19 15:47:24.141: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
Aug 19 15:47:24.141: INFO: Checking APIGroup: autoscaling.openshift.io
Aug 19 15:47:24.141: INFO: PreferredVersion.GroupVersion: autoscaling.openshift.io/v1
Aug 19 15:47:24.141: INFO: Versions found [{autoscaling.openshift.io/v1 v1} {autoscaling.openshift.io/v1beta1 v1beta1}]
Aug 19 15:47:24.141: INFO: autoscaling.openshift.io/v1 matches autoscaling.openshift.io/v1
Aug 19 15:47:24.141: INFO: Checking APIGroup: cloud.network.openshift.io
Aug 19 15:47:24.142: INFO: PreferredVersion.GroupVersion: cloud.network.openshift.io/v1
Aug 19 15:47:24.142: INFO: Versions found [{cloud.network.openshift.io/v1 v1}]
Aug 19 15:47:24.142: INFO: cloud.network.openshift.io/v1 matches cloud.network.openshift.io/v1
Aug 19 15:47:24.142: INFO: Checking APIGroup: cloudcredential.openshift.io
Aug 19 15:47:24.143: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Aug 19 15:47:24.143: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Aug 19 15:47:24.143: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Aug 19 15:47:24.143: INFO: Checking APIGroup: console.openshift.io
Aug 19 15:47:24.144: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Aug 19 15:47:24.144: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Aug 19 15:47:24.144: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Aug 19 15:47:24.144: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Aug 19 15:47:24.144: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Aug 19 15:47:24.144: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Aug 19 15:47:24.144: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Aug 19 15:47:24.144: INFO: Checking APIGroup: ingress.operator.openshift.io
Aug 19 15:47:24.145: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Aug 19 15:47:24.145: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Aug 19 15:47:24.145: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Aug 19 15:47:24.145: INFO: Checking APIGroup: k8s.cni.cncf.io
Aug 19 15:47:24.146: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Aug 19 15:47:24.146: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Aug 19 15:47:24.146: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Aug 19 15:47:24.146: INFO: Checking APIGroup: machineconfiguration.openshift.io
Aug 19 15:47:24.147: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Aug 19 15:47:24.147: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Aug 19 15:47:24.147: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Aug 19 15:47:24.147: INFO: Checking APIGroup: monitoring.coreos.com
Aug 19 15:47:24.147: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Aug 19 15:47:24.147: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Aug 19 15:47:24.147: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Aug 19 15:47:24.147: INFO: Checking APIGroup: network.openshift.io
Aug 19 15:47:24.148: INFO: PreferredVersion.GroupVersion: network.openshift.io/v1
Aug 19 15:47:24.148: INFO: Versions found [{network.openshift.io/v1 v1}]
Aug 19 15:47:24.148: INFO: network.openshift.io/v1 matches network.openshift.io/v1
Aug 19 15:47:24.148: INFO: Checking APIGroup: network.operator.openshift.io
Aug 19 15:47:24.149: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Aug 19 15:47:24.149: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Aug 19 15:47:24.149: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Aug 19 15:47:24.149: INFO: Checking APIGroup: operators.coreos.com
Aug 19 15:47:24.150: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Aug 19 15:47:24.150: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Aug 19 15:47:24.150: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Aug 19 15:47:24.150: INFO: Checking APIGroup: performance.openshift.io
Aug 19 15:47:24.151: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
Aug 19 15:47:24.151: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
Aug 19 15:47:24.151: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
Aug 19 15:47:24.151: INFO: Checking APIGroup: samples.operator.openshift.io
Aug 19 15:47:24.151: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Aug 19 15:47:24.151: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Aug 19 15:47:24.151: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Aug 19 15:47:24.151: INFO: Checking APIGroup: security.internal.openshift.io
Aug 19 15:47:24.152: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Aug 19 15:47:24.152: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Aug 19 15:47:24.152: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Aug 19 15:47:24.152: INFO: Checking APIGroup: snapshot.storage.k8s.io
Aug 19 15:47:24.153: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Aug 19 15:47:24.153: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Aug 19 15:47:24.153: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Aug 19 15:47:24.153: INFO: Checking APIGroup: tuned.openshift.io
Aug 19 15:47:24.194: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Aug 19 15:47:24.194: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Aug 19 15:47:24.194: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Aug 19 15:47:24.194: INFO: Checking APIGroup: controlplane.operator.openshift.io
Aug 19 15:47:24.245: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Aug 19 15:47:24.245: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Aug 19 15:47:24.245: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Aug 19 15:47:24.245: INFO: Checking APIGroup: metal3.io
Aug 19 15:47:24.295: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
Aug 19 15:47:24.295: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
Aug 19 15:47:24.295: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
Aug 19 15:47:24.295: INFO: Checking APIGroup: migration.k8s.io
Aug 19 15:47:24.344: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Aug 19 15:47:24.344: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Aug 19 15:47:24.344: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Aug 19 15:47:24.344: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Aug 19 15:47:24.395: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Aug 19 15:47:24.395: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Aug 19 15:47:24.395: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Aug 19 15:47:24.395: INFO: Checking APIGroup: helm.openshift.io
Aug 19 15:47:24.444: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Aug 19 15:47:24.444: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Aug 19 15:47:24.444: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Aug 19 15:47:24.444: INFO: Checking APIGroup: machine.openshift.io
Aug 19 15:47:24.495: INFO: PreferredVersion.GroupVersion: machine.openshift.io/v1beta1
Aug 19 15:47:24.495: INFO: Versions found [{machine.openshift.io/v1beta1 v1beta1}]
Aug 19 15:47:24.495: INFO: machine.openshift.io/v1beta1 matches machine.openshift.io/v1beta1
Aug 19 15:47:24.495: INFO: Checking APIGroup: metrics.k8s.io
Aug 19 15:47:24.545: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Aug 19 15:47:24.545: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Aug 19 15:47:24.545: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:188
Aug 19 15:47:24.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-995" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":356,"completed":153,"skipped":2957,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:47:24.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's args
Aug 19 15:47:24.714: INFO: Waiting up to 5m0s for pod "var-expansion-1b812a23-2054-47c8-ba4b-7810b33cf55e" in namespace "var-expansion-7393" to be "Succeeded or Failed"
Aug 19 15:47:24.719: INFO: Pod "var-expansion-1b812a23-2054-47c8-ba4b-7810b33cf55e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.85056ms
Aug 19 15:47:26.723: INFO: Pod "var-expansion-1b812a23-2054-47c8-ba4b-7810b33cf55e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009529133s
Aug 19 15:47:28.728: INFO: Pod "var-expansion-1b812a23-2054-47c8-ba4b-7810b33cf55e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013976187s
Aug 19 15:47:30.732: INFO: Pod "var-expansion-1b812a23-2054-47c8-ba4b-7810b33cf55e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018469403s
STEP: Saw pod success
Aug 19 15:47:30.732: INFO: Pod "var-expansion-1b812a23-2054-47c8-ba4b-7810b33cf55e" satisfied condition "Succeeded or Failed"
Aug 19 15:47:30.735: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod var-expansion-1b812a23-2054-47c8-ba4b-7810b33cf55e container dapi-container: <nil>
STEP: delete the pod
Aug 19 15:47:30.757: INFO: Waiting for pod var-expansion-1b812a23-2054-47c8-ba4b-7810b33cf55e to disappear
Aug 19 15:47:30.761: INFO: Pod var-expansion-1b812a23-2054-47c8-ba4b-7810b33cf55e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 19 15:47:30.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7393" for this suite.

• [SLOW TEST:6.122 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":356,"completed":154,"skipped":2978,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:47:30.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4681
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4681
STEP: creating replication controller externalsvc in namespace services-4681
I0819 15:47:30.864596      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4681, replica count: 2
I0819 15:47:33.915481      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0819 15:47:36.916887      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Aug 19 15:47:36.937: INFO: Creating new exec pod
Aug 19 15:47:40.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-4681 exec execpoddlvn4 -- /bin/sh -x -c nslookup nodeport-service.services-4681.svc.cluster.local'
Aug 19 15:47:41.106: INFO: stderr: "+ nslookup nodeport-service.services-4681.svc.cluster.local\n"
Aug 19 15:47:41.106: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nnodeport-service.services-4681.svc.cluster.local\tcanonical name = externalsvc.services-4681.svc.cluster.local.\nName:\texternalsvc.services-4681.svc.cluster.local\nAddress: 172.30.246.181\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4681, will wait for the garbage collector to delete the pods
Aug 19 15:47:41.168: INFO: Deleting ReplicationController externalsvc took: 6.561729ms
Aug 19 15:47:41.269: INFO: Terminating ReplicationController externalsvc pods took: 100.356188ms
Aug 19 15:47:43.593: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 15:47:43.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4681" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:12.854 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":356,"completed":155,"skipped":2981,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:47:43.626: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 19 15:47:43.707: INFO: The status of Pod pod-update-activedeadlineseconds-6b6a2c32-7733-4152-ab71-a48e1f78d7da is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:47:45.713: INFO: The status of Pod pod-update-activedeadlineseconds-6b6a2c32-7733-4152-ab71-a48e1f78d7da is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:47:47.712: INFO: The status of Pod pod-update-activedeadlineseconds-6b6a2c32-7733-4152-ab71-a48e1f78d7da is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 19 15:47:48.236: INFO: Successfully updated pod "pod-update-activedeadlineseconds-6b6a2c32-7733-4152-ab71-a48e1f78d7da"
Aug 19 15:47:48.236: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6b6a2c32-7733-4152-ab71-a48e1f78d7da" in namespace "pods-3546" to be "terminated due to deadline exceeded"
Aug 19 15:47:48.238: INFO: Pod "pod-update-activedeadlineseconds-6b6a2c32-7733-4152-ab71-a48e1f78d7da": Phase="Running", Reason="", readiness=true. Elapsed: 2.751259ms
Aug 19 15:47:50.243: INFO: Pod "pod-update-activedeadlineseconds-6b6a2c32-7733-4152-ab71-a48e1f78d7da": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.007262116s
Aug 19 15:47:50.243: INFO: Pod "pod-update-activedeadlineseconds-6b6a2c32-7733-4152-ab71-a48e1f78d7da" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 19 15:47:50.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3546" for this suite.

• [SLOW TEST:6.628 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":356,"completed":156,"skipped":2988,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:47:50.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a ReplicaSet
W0819 15:47:50.291856      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Verify that the required pods have come up
Aug 19 15:47:50.296: INFO: Pod name sample-pod: Found 0 pods out of 3
Aug 19 15:47:55.302: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Aug 19 15:47:55.316: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 19 15:47:55.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5668" for this suite.

• [SLOW TEST:5.189 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":356,"completed":157,"skipped":3006,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:47:55.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 15:47:56.045: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 19 15:47:58.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 15, 47, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 47, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 15, 47, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 47, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 15:48:01.070: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:48:01.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8590" for this suite.
STEP: Destroying namespace "webhook-8590-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.894 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":356,"completed":158,"skipped":3046,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:48:01.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-cd3375bf-99e4-46fd-9a19-365d56494fcc
STEP: Creating a pod to test consume secrets
Aug 19 15:48:01.420: INFO: Waiting up to 5m0s for pod "pod-secrets-b21442e1-449a-418d-83e2-e83c99ee42a9" in namespace "secrets-9768" to be "Succeeded or Failed"
Aug 19 15:48:01.427: INFO: Pod "pod-secrets-b21442e1-449a-418d-83e2-e83c99ee42a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.774576ms
Aug 19 15:48:03.455: INFO: Pod "pod-secrets-b21442e1-449a-418d-83e2-e83c99ee42a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03468189s
Aug 19 15:48:05.460: INFO: Pod "pod-secrets-b21442e1-449a-418d-83e2-e83c99ee42a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039046423s
Aug 19 15:48:07.463: INFO: Pod "pod-secrets-b21442e1-449a-418d-83e2-e83c99ee42a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042965584s
STEP: Saw pod success
Aug 19 15:48:07.463: INFO: Pod "pod-secrets-b21442e1-449a-418d-83e2-e83c99ee42a9" satisfied condition "Succeeded or Failed"
Aug 19 15:48:07.467: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-secrets-b21442e1-449a-418d-83e2-e83c99ee42a9 container secret-volume-test: <nil>
STEP: delete the pod
Aug 19 15:48:07.483: INFO: Waiting for pod pod-secrets-b21442e1-449a-418d-83e2-e83c99ee42a9 to disappear
Aug 19 15:48:07.489: INFO: Pod pod-secrets-b21442e1-449a-418d-83e2-e83c99ee42a9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 19 15:48:07.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9768" for this suite.

• [SLOW TEST:6.175 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":159,"skipped":3049,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:48:07.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Aug 19 15:48:11.655: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7366 PodName:var-expansion-6963b01c-363c-4aa8-91ce-b26ca04baf86 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:48:11.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:48:11.655: INFO: ExecWithOptions: Clientset creation
Aug 19 15:48:11.655: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-7366/pods/var-expansion-6963b01c-363c-4aa8-91ce-b26ca04baf86/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path
Aug 19 15:48:11.734: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7366 PodName:var-expansion-6963b01c-363c-4aa8-91ce-b26ca04baf86 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 15:48:11.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 15:48:11.734: INFO: ExecWithOptions: Clientset creation
Aug 19 15:48:11.734: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-7366/pods/var-expansion-6963b01c-363c-4aa8-91ce-b26ca04baf86/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value
Aug 19 15:48:12.327: INFO: Successfully updated pod "var-expansion-6963b01c-363c-4aa8-91ce-b26ca04baf86"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Aug 19 15:48:12.331: INFO: Deleting pod "var-expansion-6963b01c-363c-4aa8-91ce-b26ca04baf86" in namespace "var-expansion-7366"
Aug 19 15:48:12.338: INFO: Wait up to 5m0s for pod "var-expansion-6963b01c-363c-4aa8-91ce-b26ca04baf86" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 19 15:48:44.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7366" for this suite.

• [SLOW TEST:36.848 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":356,"completed":160,"skipped":3059,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:48:44.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:48:44.447: INFO: created pod pod-service-account-defaultsa
Aug 19 15:48:44.447: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 19 15:48:44.478: INFO: created pod pod-service-account-mountsa
Aug 19 15:48:44.478: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 19 15:48:44.498: INFO: created pod pod-service-account-nomountsa
Aug 19 15:48:44.498: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 19 15:48:44.516: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 19 15:48:44.516: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 19 15:48:44.536: INFO: created pod pod-service-account-mountsa-mountspec
Aug 19 15:48:44.536: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 19 15:48:44.551: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 19 15:48:44.551: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 19 15:48:44.574: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 19 15:48:44.574: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 19 15:48:44.600: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 19 15:48:44.600: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 19 15:48:44.616: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 19 15:48:44.616: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 19 15:48:44.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6328" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":356,"completed":161,"skipped":3068,"failed":0}
SSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:48:44.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Aug 19 15:48:44.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4815" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":356,"completed":162,"skipped":3074,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:48:44.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3685
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-3685
I0819 15:48:44.921474      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3685, replica count: 2
Aug 19 15:48:47.973: INFO: Creating new exec pod
I0819 15:48:47.973410      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 19 15:48:53.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3685 exec execpodfd827 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 19 15:48:53.116: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 19 15:48:53.116: INFO: stdout: ""
Aug 19 15:48:54.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3685 exec execpodfd827 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 19 15:48:54.232: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 19 15:48:54.232: INFO: stdout: ""
Aug 19 15:48:55.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3685 exec execpodfd827 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 19 15:48:55.253: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 19 15:48:55.253: INFO: stdout: "externalname-service-ns5j7"
Aug 19 15:48:55.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3685 exec execpodfd827 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.126.235 80'
Aug 19 15:48:55.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.126.235 80\nConnection to 172.30.126.235 80 port [tcp/http] succeeded!\n"
Aug 19 15:48:55.366: INFO: stdout: ""
Aug 19 15:48:56.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3685 exec execpodfd827 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.126.235 80'
Aug 19 15:48:56.484: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.126.235 80\nConnection to 172.30.126.235 80 port [tcp/http] succeeded!\n"
Aug 19 15:48:56.484: INFO: stdout: ""
Aug 19 15:48:57.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3685 exec execpodfd827 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.126.235 80'
Aug 19 15:48:57.482: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.126.235 80\nConnection to 172.30.126.235 80 port [tcp/http] succeeded!\n"
Aug 19 15:48:57.482: INFO: stdout: "externalname-service-wgz88"
Aug 19 15:48:57.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3685 exec execpodfd827 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.131.169 31928'
Aug 19 15:48:57.591: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.131.169 31928\nConnection to 10.0.131.169 31928 port [tcp/*] succeeded!\n"
Aug 19 15:48:57.592: INFO: stdout: "externalname-service-wgz88"
Aug 19 15:48:57.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3685 exec execpodfd827 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.164.144 31928'
Aug 19 15:48:57.693: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.164.144 31928\nConnection to 10.0.164.144 31928 port [tcp/*] succeeded!\n"
Aug 19 15:48:57.693: INFO: stdout: ""
Aug 19 15:48:58.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-3685 exec execpodfd827 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.164.144 31928'
Aug 19 15:48:58.809: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.164.144 31928\nConnection to 10.0.164.144 31928 port [tcp/*] succeeded!\n"
Aug 19 15:48:58.809: INFO: stdout: "externalname-service-ns5j7"
Aug 19 15:48:58.809: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 15:48:58.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3685" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:14.010 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":356,"completed":163,"skipped":3074,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:48:58.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting the proxy server
Aug 19 15:48:58.886: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-7529 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 15:48:58.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7529" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":356,"completed":164,"skipped":3075,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:48:58.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 19 15:48:59.039: INFO: Waiting up to 5m0s for pod "pod-26b0bd5d-c34d-4b96-83d4-665bfdcde0ae" in namespace "emptydir-9271" to be "Succeeded or Failed"
Aug 19 15:48:59.046: INFO: Pod "pod-26b0bd5d-c34d-4b96-83d4-665bfdcde0ae": Phase="Pending", Reason="", readiness=false. Elapsed: 7.177368ms
Aug 19 15:49:01.051: INFO: Pod "pod-26b0bd5d-c34d-4b96-83d4-665bfdcde0ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011886064s
Aug 19 15:49:03.055: INFO: Pod "pod-26b0bd5d-c34d-4b96-83d4-665bfdcde0ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015870377s
Aug 19 15:49:05.058: INFO: Pod "pod-26b0bd5d-c34d-4b96-83d4-665bfdcde0ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019347573s
STEP: Saw pod success
Aug 19 15:49:05.058: INFO: Pod "pod-26b0bd5d-c34d-4b96-83d4-665bfdcde0ae" satisfied condition "Succeeded or Failed"
Aug 19 15:49:05.062: INFO: Trying to get logs from node ip-10-0-157-99.ec2.internal pod pod-26b0bd5d-c34d-4b96-83d4-665bfdcde0ae container test-container: <nil>
STEP: delete the pod
Aug 19 15:49:05.087: INFO: Waiting for pod pod-26b0bd5d-c34d-4b96-83d4-665bfdcde0ae to disappear
Aug 19 15:49:05.091: INFO: Pod pod-26b0bd5d-c34d-4b96-83d4-665bfdcde0ae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 19 15:49:05.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9271" for this suite.

• [SLOW TEST:6.158 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":165,"skipped":3088,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:49:05.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-8949
Aug 19 15:49:05.179: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Aug 19 15:49:07.184: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Aug 19 15:49:07.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-8949 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 19 15:49:07.315: INFO: rc: 7
Aug 19 15:49:07.334: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 19 15:49:07.338: INFO: Pod kube-proxy-mode-detector no longer exists
Aug 19 15:49:07.338: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-8949 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-8949
STEP: creating replication controller affinity-nodeport-timeout in namespace services-8949
I0819 15:49:07.366376      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-8949, replica count: 3
I0819 15:49:10.417840      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0819 15:49:13.420427      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 19 15:49:13.432: INFO: Creating new exec pod
Aug 19 15:49:18.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-8949 exec execpod-affinitybgtqr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Aug 19 15:49:18.584: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Aug 19 15:49:18.584: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 15:49:18.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-8949 exec execpod-affinitybgtqr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.210.131 80'
Aug 19 15:49:18.692: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.210.131 80\nConnection to 172.30.210.131 80 port [tcp/http] succeeded!\n"
Aug 19 15:49:18.692: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 15:49:18.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-8949 exec execpod-affinitybgtqr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.157.99 30030'
Aug 19 15:49:18.791: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.157.99 30030\nConnection to 10.0.157.99 30030 port [tcp/*] succeeded!\n"
Aug 19 15:49:18.791: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 15:49:18.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-8949 exec execpod-affinitybgtqr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.131.169 30030'
Aug 19 15:49:18.904: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.131.169 30030\nConnection to 10.0.131.169 30030 port [tcp/*] succeeded!\n"
Aug 19 15:49:18.904: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 15:49:18.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-8949 exec execpod-affinitybgtqr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.131.169:30030/ ; done'
Aug 19 15:49:19.058: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n"
Aug 19 15:49:19.058: INFO: stdout: "\naffinity-nodeport-timeout-dg4mg\naffinity-nodeport-timeout-dg4mg\naffinity-nodeport-timeout-dg4mg\naffinity-nodeport-timeout-dg4mg\naffinity-nodeport-timeout-dg4mg\naffinity-nodeport-timeout-dg4mg\naffinity-nodeport-timeout-dg4mg\naffinity-nodeport-timeout-dg4mg\naffinity-nodeport-timeout-dg4mg\naffinity-nodeport-timeout-dg4mg\naffinity-nodeport-timeout-dg4mg\naffinity-nodeport-timeout-dg4mg\naffinity-nodeport-timeout-dg4mg\naffinity-nodeport-timeout-dg4mg\naffinity-nodeport-timeout-dg4mg\naffinity-nodeport-timeout-dg4mg"
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Received response from host: affinity-nodeport-timeout-dg4mg
Aug 19 15:49:19.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-8949 exec execpod-affinitybgtqr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.131.169:30030/'
Aug 19 15:49:19.155: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n"
Aug 19 15:49:19.155: INFO: stdout: "affinity-nodeport-timeout-dg4mg"
Aug 19 15:49:39.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-8949 exec execpod-affinitybgtqr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.131.169:30030/'
Aug 19 15:49:39.275: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.131.169:30030/\n"
Aug 19 15:49:39.275: INFO: stdout: "affinity-nodeport-timeout-qxg55"
Aug 19 15:49:39.275: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-8949, will wait for the garbage collector to delete the pods
Aug 19 15:49:39.350: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.990031ms
Aug 19 15:49:39.451: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.683554ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 15:49:41.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8949" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:36.391 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":166,"skipped":3102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:49:41.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 19 15:49:41.571: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3087  cada4f91-bfdf-4263-983a-9a7a28ce1e99 70192 0 2022-08-19 15:49:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-08-19 15:49:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 19 15:49:41.571: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3087  cada4f91-bfdf-4263-983a-9a7a28ce1e99 70195 0 2022-08-19 15:49:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-08-19 15:49:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Aug 19 15:49:41.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3087" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":356,"completed":167,"skipped":3125,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:49:41.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 19 15:49:41.658: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ce4f48c8-a7d5-41e3-b431-101774d40553" in namespace "projected-4681" to be "Succeeded or Failed"
Aug 19 15:49:41.663: INFO: Pod "downwardapi-volume-ce4f48c8-a7d5-41e3-b431-101774d40553": Phase="Pending", Reason="", readiness=false. Elapsed: 4.368456ms
Aug 19 15:49:43.666: INFO: Pod "downwardapi-volume-ce4f48c8-a7d5-41e3-b431-101774d40553": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008135149s
Aug 19 15:49:45.670: INFO: Pod "downwardapi-volume-ce4f48c8-a7d5-41e3-b431-101774d40553": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011964878s
Aug 19 15:49:47.674: INFO: Pod "downwardapi-volume-ce4f48c8-a7d5-41e3-b431-101774d40553": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015784264s
STEP: Saw pod success
Aug 19 15:49:47.674: INFO: Pod "downwardapi-volume-ce4f48c8-a7d5-41e3-b431-101774d40553" satisfied condition "Succeeded or Failed"
Aug 19 15:49:47.678: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-ce4f48c8-a7d5-41e3-b431-101774d40553 container client-container: <nil>
STEP: delete the pod
Aug 19 15:49:47.702: INFO: Waiting for pod downwardapi-volume-ce4f48c8-a7d5-41e3-b431-101774d40553 to disappear
Aug 19 15:49:47.705: INFO: Pod downwardapi-volume-ce4f48c8-a7d5-41e3-b431-101774d40553 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 19 15:49:47.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4681" for this suite.

• [SLOW TEST:6.109 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":168,"skipped":3128,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:49:47.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 19 15:50:03.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5450" for this suite.

• [SLOW TEST:16.167 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":356,"completed":169,"skipped":3148,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:50:03.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 19 15:50:14.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-153" for this suite.

• [SLOW TEST:11.098 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":356,"completed":170,"skipped":3156,"failed":0}
SSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:50:14.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a suspended cronjob
W0819 15:50:15.044705      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Aug 19 15:55:15.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-333" for this suite.

• [SLOW TEST:300.093 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":356,"completed":171,"skipped":3162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:55:15.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3496
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Aug 19 15:55:15.133: INFO: Found 0 stateful pods, waiting for 3
Aug 19 15:55:25.137: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 19 15:55:25.137: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 19 15:55:25.137: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 19 15:55:25.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-3496 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 19 15:55:25.271: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 19 15:55:25.271: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 19 15:55:25.271: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Aug 19 15:55:35.307: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 19 15:55:45.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-3496 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 19 15:55:45.441: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 19 15:55:45.441: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 19 15:55:45.441: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 19 15:55:55.462: INFO: Waiting for StatefulSet statefulset-3496/ss2 to complete update
STEP: Rolling back to a previous revision
Aug 19 15:56:05.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-3496 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 19 15:56:05.568: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 19 15:56:05.568: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 19 15:56:05.568: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 19 15:56:15.603: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 19 15:56:25.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-3496 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 19 15:56:25.748: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 19 15:56:25.748: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 19 15:56:25.748: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 19 15:56:35.768: INFO: Waiting for StatefulSet statefulset-3496/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 19 15:56:45.775: INFO: Deleting all statefulset in ns statefulset-3496
Aug 19 15:56:45.777: INFO: Scaling statefulset ss2 to 0
Aug 19 15:56:55.793: INFO: Waiting for statefulset status.replicas updated to 0
Aug 19 15:56:55.798: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 19 15:56:55.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3496" for this suite.

• [SLOW TEST:100.756 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":356,"completed":172,"skipped":3197,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:56:55.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 15:56:56.208: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 19 15:56:58.217: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 15, 56, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 56, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 15, 56, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 15, 56, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 15:57:01.232: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 15:57:01.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8644" for this suite.
STEP: Destroying namespace "webhook-8644-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.552 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":356,"completed":173,"skipped":3205,"failed":0}
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:57:01.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6607
[It] should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-6607
Aug 19 15:57:01.449: INFO: Found 0 stateful pods, waiting for 1
Aug 19 15:57:11.454: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 19 15:57:11.479: INFO: Deleting all statefulset in ns statefulset-6607
Aug 19 15:57:11.483: INFO: Scaling statefulset ss to 0
Aug 19 15:57:21.503: INFO: Waiting for statefulset status.replicas updated to 0
Aug 19 15:57:21.506: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 19 15:57:21.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6607" for this suite.

• [SLOW TEST:20.156 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":356,"completed":174,"skipped":3205,"failed":0}
[sig-apps] Job 
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:57:21.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Indexed job
W0819 15:57:21.580863      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions
STEP: Ensuring pods with index for job exist
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Aug 19 15:57:33.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5655" for this suite.

• [SLOW TEST:12.064 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","total":356,"completed":175,"skipped":3205,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:57:33.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 15:57:33.682: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 19 15:57:33.702: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:33.702: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:33.702: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:33.709: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:57:33.709: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:57:34.715: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:34.715: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:34.715: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:34.718: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:57:34.718: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:57:35.713: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:35.713: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:35.713: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:35.717: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:57:35.717: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:57:36.714: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:36.714: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:36.714: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:36.717: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 19 15:57:36.717: INFO: Node ip-10-0-157-99.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:57:37.713: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:37.713: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:37.713: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:37.717: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 19 15:57:37.717: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 19 15:57:37.744: INFO: Wrong image for pod: daemon-set-brs8d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 19 15:57:37.744: INFO: Wrong image for pod: daemon-set-dxrzb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 19 15:57:37.744: INFO: Wrong image for pod: daemon-set-rnxds. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 19 15:57:37.754: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:37.754: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:37.754: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:38.758: INFO: Wrong image for pod: daemon-set-brs8d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 19 15:57:38.758: INFO: Wrong image for pod: daemon-set-dxrzb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 19 15:57:38.763: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:38.763: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:38.763: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:39.758: INFO: Wrong image for pod: daemon-set-brs8d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 19 15:57:39.758: INFO: Wrong image for pod: daemon-set-dxrzb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 19 15:57:39.762: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:39.762: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:39.762: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:40.758: INFO: Wrong image for pod: daemon-set-brs8d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 19 15:57:40.758: INFO: Wrong image for pod: daemon-set-dxrzb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 19 15:57:40.758: INFO: Pod daemon-set-l9gpq is not available
Aug 19 15:57:40.763: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:40.763: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:40.763: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:41.758: INFO: Wrong image for pod: daemon-set-brs8d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 19 15:57:41.758: INFO: Wrong image for pod: daemon-set-dxrzb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 19 15:57:41.758: INFO: Pod daemon-set-l9gpq is not available
Aug 19 15:57:41.762: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:41.762: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:41.762: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:42.759: INFO: Wrong image for pod: daemon-set-dxrzb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 19 15:57:42.763: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:42.763: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:42.763: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:43.758: INFO: Wrong image for pod: daemon-set-dxrzb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 19 15:57:43.758: INFO: Pod daemon-set-vjkxf is not available
Aug 19 15:57:43.762: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:43.762: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:43.762: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:44.758: INFO: Wrong image for pod: daemon-set-dxrzb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 19 15:57:44.758: INFO: Pod daemon-set-vjkxf is not available
Aug 19 15:57:44.763: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:44.763: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:44.763: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:45.758: INFO: Wrong image for pod: daemon-set-dxrzb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 19 15:57:45.758: INFO: Pod daemon-set-vjkxf is not available
Aug 19 15:57:45.763: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:45.763: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:45.763: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:46.761: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:46.761: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:46.761: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:47.758: INFO: Pod daemon-set-l9hkp is not available
Aug 19 15:57:47.763: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:47.763: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:47.763: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 19 15:57:47.768: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:47.768: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:47.768: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:47.771: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 19 15:57:47.771: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:57:48.777: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:48.777: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:48.777: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:48.780: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 19 15:57:48.780: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:57:49.777: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:49.777: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:49.777: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:49.780: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 19 15:57:49.780: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 15:57:50.776: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:50.776: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:50.776: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 15:57:50.780: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 19 15:57:50.780: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8246, will wait for the garbage collector to delete the pods
Aug 19 15:57:50.854: INFO: Deleting DaemonSet.extensions daemon-set took: 6.418523ms
Aug 19 15:57:50.955: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.73139ms
Aug 19 15:57:53.161: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 15:57:53.161: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 19 15:57:53.164: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74389"},"items":null}

Aug 19 15:57:53.168: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74389"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 19 15:57:53.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8246" for this suite.

• [SLOW TEST:19.589 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":356,"completed":176,"skipped":3207,"failed":0}
S
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 15:57:53.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-d866c219-2446-482d-a3b4-d2d3d6bf5606 in namespace container-probe-5730
Aug 19 15:57:57.273: INFO: Started pod liveness-d866c219-2446-482d-a3b4-d2d3d6bf5606 in namespace container-probe-5730
STEP: checking the pod's current state and verifying that restartCount is present
Aug 19 15:57:57.276: INFO: Initial restart count of pod liveness-d866c219-2446-482d-a3b4-d2d3d6bf5606 is 0
Aug 19 15:58:15.326: INFO: Restart count of pod container-probe-5730/liveness-d866c219-2446-482d-a3b4-d2d3d6bf5606 is now 1 (18.049729435s elapsed)
Aug 19 15:58:35.376: INFO: Restart count of pod container-probe-5730/liveness-d866c219-2446-482d-a3b4-d2d3d6bf5606 is now 2 (38.099940692s elapsed)
Aug 19 15:58:55.424: INFO: Restart count of pod container-probe-5730/liveness-d866c219-2446-482d-a3b4-d2d3d6bf5606 is now 3 (58.147988038s elapsed)
Aug 19 15:59:15.467: INFO: Restart count of pod container-probe-5730/liveness-d866c219-2446-482d-a3b4-d2d3d6bf5606 is now 4 (1m18.190896927s elapsed)
Aug 19 16:00:27.649: INFO: Restart count of pod container-probe-5730/liveness-d866c219-2446-482d-a3b4-d2d3d6bf5606 is now 5 (2m30.373432942s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 19 16:00:27.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5730" for this suite.

• [SLOW TEST:154.483 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":356,"completed":177,"skipped":3208,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:00:27.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
STEP: create deployment with httpd image
Aug 19 16:00:27.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-7818 create -f -'
Aug 19 16:00:29.110: INFO: stderr: ""
Aug 19 16:00:29.110: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Aug 19 16:00:29.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-7818 diff -f -'
Aug 19 16:00:30.552: INFO: rc: 1
Aug 19 16:00:30.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-7818 delete -f -'
Aug 19 16:00:30.593: INFO: stderr: ""
Aug 19 16:00:30.593: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 16:00:30.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7818" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":356,"completed":178,"skipped":3217,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:00:30.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Aug 19 16:00:30.726: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:00:32.731: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:00:34.730: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Aug 19 16:00:34.753: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:00:36.758: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:00:38.757: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Aug 19 16:00:38.770: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 19 16:00:38.773: INFO: Pod pod-with-prestop-http-hook still exists
Aug 19 16:00:40.774: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 19 16:00:40.778: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Aug 19 16:00:40.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4247" for this suite.

• [SLOW TEST:10.192 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":356,"completed":179,"skipped":3251,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:00:40.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Aug 19 16:00:40.899: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Aug 19 16:00:40.927: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Aug 19 16:00:40.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7054" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":356,"completed":180,"skipped":3262,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:00:40.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 19 16:00:41.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1043" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":356,"completed":181,"skipped":3301,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:00:41.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 19 16:00:41.135: INFO: Waiting up to 5m0s for pod "pod-0f3d37f0-590c-452e-a6a2-b131a4fb4f86" in namespace "emptydir-981" to be "Succeeded or Failed"
Aug 19 16:00:41.138: INFO: Pod "pod-0f3d37f0-590c-452e-a6a2-b131a4fb4f86": Phase="Pending", Reason="", readiness=false. Elapsed: 3.77856ms
Aug 19 16:00:43.145: INFO: Pod "pod-0f3d37f0-590c-452e-a6a2-b131a4fb4f86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010528671s
Aug 19 16:00:45.150: INFO: Pod "pod-0f3d37f0-590c-452e-a6a2-b131a4fb4f86": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015089492s
Aug 19 16:00:47.154: INFO: Pod "pod-0f3d37f0-590c-452e-a6a2-b131a4fb4f86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019188384s
STEP: Saw pod success
Aug 19 16:00:47.154: INFO: Pod "pod-0f3d37f0-590c-452e-a6a2-b131a4fb4f86" satisfied condition "Succeeded or Failed"
Aug 19 16:00:47.157: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-0f3d37f0-590c-452e-a6a2-b131a4fb4f86 container test-container: <nil>
STEP: delete the pod
Aug 19 16:00:47.177: INFO: Waiting for pod pod-0f3d37f0-590c-452e-a6a2-b131a4fb4f86 to disappear
Aug 19 16:00:47.179: INFO: Pod pod-0f3d37f0-590c-452e-a6a2-b131a4fb4f86 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 19 16:00:47.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-981" for this suite.

• [SLOW TEST:6.112 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":182,"skipped":3304,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:00:47.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 19 16:00:47.247: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f42c2807-135c-4159-be2c-a074a30adfd0" in namespace "downward-api-4979" to be "Succeeded or Failed"
Aug 19 16:00:47.251: INFO: Pod "downwardapi-volume-f42c2807-135c-4159-be2c-a074a30adfd0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.775968ms
Aug 19 16:00:49.256: INFO: Pod "downwardapi-volume-f42c2807-135c-4159-be2c-a074a30adfd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009363079s
Aug 19 16:00:51.260: INFO: Pod "downwardapi-volume-f42c2807-135c-4159-be2c-a074a30adfd0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013113552s
Aug 19 16:00:53.264: INFO: Pod "downwardapi-volume-f42c2807-135c-4159-be2c-a074a30adfd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017520563s
STEP: Saw pod success
Aug 19 16:00:53.264: INFO: Pod "downwardapi-volume-f42c2807-135c-4159-be2c-a074a30adfd0" satisfied condition "Succeeded or Failed"
Aug 19 16:00:53.267: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-f42c2807-135c-4159-be2c-a074a30adfd0 container client-container: <nil>
STEP: delete the pod
Aug 19 16:00:53.284: INFO: Waiting for pod downwardapi-volume-f42c2807-135c-4159-be2c-a074a30adfd0 to disappear
Aug 19 16:00:53.286: INFO: Pod downwardapi-volume-f42c2807-135c-4159-be2c-a074a30adfd0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 19 16:00:53.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4979" for this suite.

• [SLOW TEST:6.107 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":183,"skipped":3344,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:00:53.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 16:00:53.728: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 19 16:00:55.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 16, 0, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 0, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 0, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 0, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 16:00:58.792: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Aug 19 16:00:58.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:00:58.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6857" for this suite.
STEP: Destroying namespace "webhook-6857-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.609 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":356,"completed":184,"skipped":3346,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:00:58.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
W0819 16:00:58.988741      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0819 16:01:00.035821      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0819 16:01:00.035834      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 19 16:01:00.035: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 19 16:01:00.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3309" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":356,"completed":185,"skipped":3351,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:01:00.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Aug 19 16:01:00.121: INFO: Waiting up to 5m0s for pod "downward-api-0919546b-2067-47e0-b39a-47eb1572ce10" in namespace "downward-api-1256" to be "Succeeded or Failed"
Aug 19 16:01:00.130: INFO: Pod "downward-api-0919546b-2067-47e0-b39a-47eb1572ce10": Phase="Pending", Reason="", readiness=false. Elapsed: 9.131525ms
Aug 19 16:01:02.134: INFO: Pod "downward-api-0919546b-2067-47e0-b39a-47eb1572ce10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013331795s
Aug 19 16:01:04.139: INFO: Pod "downward-api-0919546b-2067-47e0-b39a-47eb1572ce10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01831133s
Aug 19 16:01:06.145: INFO: Pod "downward-api-0919546b-2067-47e0-b39a-47eb1572ce10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024354793s
STEP: Saw pod success
Aug 19 16:01:06.145: INFO: Pod "downward-api-0919546b-2067-47e0-b39a-47eb1572ce10" satisfied condition "Succeeded or Failed"
Aug 19 16:01:06.148: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downward-api-0919546b-2067-47e0-b39a-47eb1572ce10 container dapi-container: <nil>
STEP: delete the pod
Aug 19 16:01:06.171: INFO: Waiting for pod downward-api-0919546b-2067-47e0-b39a-47eb1572ce10 to disappear
Aug 19 16:01:06.174: INFO: Pod downward-api-0919546b-2067-47e0-b39a-47eb1572ce10 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Aug 19 16:01:06.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1256" for this suite.

• [SLOW TEST:6.139 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":356,"completed":186,"skipped":3355,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:01:06.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating cluster-info
Aug 19 16:01:06.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2249 cluster-info'
Aug 19 16:01:06.260: INFO: stderr: ""
Aug 19 16:01:06.260: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 16:01:06.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2249" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":356,"completed":187,"skipped":3374,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:01:06.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 19 16:01:06.335: INFO: Waiting up to 5m0s for pod "pod-3c02105d-d0b1-41cb-a986-6240ba84caaf" in namespace "emptydir-9465" to be "Succeeded or Failed"
Aug 19 16:01:06.347: INFO: Pod "pod-3c02105d-d0b1-41cb-a986-6240ba84caaf": Phase="Pending", Reason="", readiness=false. Elapsed: 12.485283ms
Aug 19 16:01:08.352: INFO: Pod "pod-3c02105d-d0b1-41cb-a986-6240ba84caaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017323215s
Aug 19 16:01:10.356: INFO: Pod "pod-3c02105d-d0b1-41cb-a986-6240ba84caaf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020945079s
Aug 19 16:01:12.360: INFO: Pod "pod-3c02105d-d0b1-41cb-a986-6240ba84caaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025608476s
STEP: Saw pod success
Aug 19 16:01:12.361: INFO: Pod "pod-3c02105d-d0b1-41cb-a986-6240ba84caaf" satisfied condition "Succeeded or Failed"
Aug 19 16:01:12.364: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-3c02105d-d0b1-41cb-a986-6240ba84caaf container test-container: <nil>
STEP: delete the pod
Aug 19 16:01:12.382: INFO: Waiting for pod pod-3c02105d-d0b1-41cb-a986-6240ba84caaf to disappear
Aug 19 16:01:12.384: INFO: Pod pod-3c02105d-d0b1-41cb-a986-6240ba84caaf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 19 16:01:12.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9465" for this suite.

• [SLOW TEST:6.117 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":188,"skipped":3401,"failed":0}
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:01:12.394: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2904
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2904
STEP: creating replication controller externalsvc in namespace services-2904
I0819 16:01:12.488040      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2904, replica count: 2
I0819 16:01:15.547386      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Aug 19 16:01:15.564: INFO: Creating new exec pod
Aug 19 16:01:19.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-2904 exec execpod8d7hm -- /bin/sh -x -c nslookup clusterip-service.services-2904.svc.cluster.local'
Aug 19 16:01:19.745: INFO: stderr: "+ nslookup clusterip-service.services-2904.svc.cluster.local\n"
Aug 19 16:01:19.745: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nclusterip-service.services-2904.svc.cluster.local\tcanonical name = externalsvc.services-2904.svc.cluster.local.\nName:\texternalsvc.services-2904.svc.cluster.local\nAddress: 172.30.97.182\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2904, will wait for the garbage collector to delete the pods
Aug 19 16:01:19.807: INFO: Deleting ReplicationController externalsvc took: 7.631352ms
Aug 19 16:01:19.907: INFO: Terminating ReplicationController externalsvc pods took: 100.519979ms
Aug 19 16:01:21.729: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 16:01:21.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2904" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:9.360 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":356,"completed":189,"skipped":3401,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:01:21.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:01:21.793: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-f8b1a4c7-b919-4d06-abe6-1c484f2267ca
STEP: Creating the pod
Aug 19 16:01:21.854: INFO: The status of Pod pod-configmaps-53208605-916b-4df0-b91b-effdf19bb8b2 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:01:23.859: INFO: The status of Pod pod-configmaps-53208605-916b-4df0-b91b-effdf19bb8b2 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:01:25.858: INFO: The status of Pod pod-configmaps-53208605-916b-4df0-b91b-effdf19bb8b2 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-f8b1a4c7-b919-4d06-abe6-1c484f2267ca
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 19 16:02:54.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9420" for this suite.

• [SLOW TEST:92.496 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":190,"skipped":3432,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:02:54.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-1915b80d-033d-4c0f-b86b-0e649373a42e
STEP: Creating a pod to test consume configMaps
Aug 19 16:02:54.368: INFO: Waiting up to 5m0s for pod "pod-configmaps-ed5ce620-7140-4fe2-aeb4-e390363e0091" in namespace "configmap-7520" to be "Succeeded or Failed"
Aug 19 16:02:54.375: INFO: Pod "pod-configmaps-ed5ce620-7140-4fe2-aeb4-e390363e0091": Phase="Pending", Reason="", readiness=false. Elapsed: 6.832182ms
Aug 19 16:02:56.380: INFO: Pod "pod-configmaps-ed5ce620-7140-4fe2-aeb4-e390363e0091": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011388623s
Aug 19 16:02:58.384: INFO: Pod "pod-configmaps-ed5ce620-7140-4fe2-aeb4-e390363e0091": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015879905s
Aug 19 16:03:00.389: INFO: Pod "pod-configmaps-ed5ce620-7140-4fe2-aeb4-e390363e0091": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020870573s
STEP: Saw pod success
Aug 19 16:03:00.389: INFO: Pod "pod-configmaps-ed5ce620-7140-4fe2-aeb4-e390363e0091" satisfied condition "Succeeded or Failed"
Aug 19 16:03:00.392: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-configmaps-ed5ce620-7140-4fe2-aeb4-e390363e0091 container agnhost-container: <nil>
STEP: delete the pod
Aug 19 16:03:00.409: INFO: Waiting for pod pod-configmaps-ed5ce620-7140-4fe2-aeb4-e390363e0091 to disappear
Aug 19 16:03:00.411: INFO: Pod pod-configmaps-ed5ce620-7140-4fe2-aeb4-e390363e0091 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 19 16:03:00.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7520" for this suite.

• [SLOW TEST:6.172 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":191,"skipped":3468,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:03:00.423: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Aug 19 16:03:00.482: INFO: Waiting up to 5m0s for pod "security-context-06139e87-5de0-4abc-b449-c6b552ff8c89" in namespace "security-context-1432" to be "Succeeded or Failed"
Aug 19 16:03:00.487: INFO: Pod "security-context-06139e87-5de0-4abc-b449-c6b552ff8c89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.978122ms
Aug 19 16:03:02.492: INFO: Pod "security-context-06139e87-5de0-4abc-b449-c6b552ff8c89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009496089s
Aug 19 16:03:04.495: INFO: Pod "security-context-06139e87-5de0-4abc-b449-c6b552ff8c89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012723845s
Aug 19 16:03:06.500: INFO: Pod "security-context-06139e87-5de0-4abc-b449-c6b552ff8c89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017718171s
STEP: Saw pod success
Aug 19 16:03:06.500: INFO: Pod "security-context-06139e87-5de0-4abc-b449-c6b552ff8c89" satisfied condition "Succeeded or Failed"
Aug 19 16:03:06.503: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod security-context-06139e87-5de0-4abc-b449-c6b552ff8c89 container test-container: <nil>
STEP: delete the pod
Aug 19 16:03:06.521: INFO: Waiting for pod security-context-06139e87-5de0-4abc-b449-c6b552ff8c89 to disappear
Aug 19 16:03:06.524: INFO: Pod security-context-06139e87-5de0-4abc-b449-c6b552ff8c89 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 19 16:03:06.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1432" for this suite.

• [SLOW TEST:6.114 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":192,"skipped":3475,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:03:06.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:03:06.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-4032 version'
Aug 19 16:03:06.599: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Aug 19 16:03:06.599: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.0\", GitCommit:\"4ce5a8954017644c5420bae81d72b09b735c21f0\", GitTreeState:\"clean\", BuildDate:\"2022-05-03T13:46:05Z\", GoVersion:\"go1.18.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.4\nServer Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.0+4f0dd4d\", GitCommit:\"0a57f1f59bda75ea2cf13d9f3b4ac5d202134f2d\", GitTreeState:\"clean\", BuildDate:\"2022-08-10T15:36:44Z\", GoVersion:\"go1.18.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 16:03:06.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4032" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":356,"completed":193,"skipped":3504,"failed":0}

------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:03:06.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 19 16:03:12.729: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Aug 19 16:03:12.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3309" for this suite.

• [SLOW TEST:6.145 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":194,"skipped":3504,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:03:12.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-projected-n4k5
STEP: Creating a pod to test atomic-volume-subpath
Aug 19 16:03:12.847: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-n4k5" in namespace "subpath-5931" to be "Succeeded or Failed"
Aug 19 16:03:12.855: INFO: Pod "pod-subpath-test-projected-n4k5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.897716ms
Aug 19 16:03:14.861: INFO: Pod "pod-subpath-test-projected-n4k5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013660389s
Aug 19 16:03:16.865: INFO: Pod "pod-subpath-test-projected-n4k5": Phase="Running", Reason="", readiness=true. Elapsed: 4.017585806s
Aug 19 16:03:18.871: INFO: Pod "pod-subpath-test-projected-n4k5": Phase="Running", Reason="", readiness=true. Elapsed: 6.023426384s
Aug 19 16:03:20.876: INFO: Pod "pod-subpath-test-projected-n4k5": Phase="Running", Reason="", readiness=true. Elapsed: 8.028347915s
Aug 19 16:03:22.880: INFO: Pod "pod-subpath-test-projected-n4k5": Phase="Running", Reason="", readiness=true. Elapsed: 10.032577059s
Aug 19 16:03:24.884: INFO: Pod "pod-subpath-test-projected-n4k5": Phase="Running", Reason="", readiness=true. Elapsed: 12.036446641s
Aug 19 16:03:26.889: INFO: Pod "pod-subpath-test-projected-n4k5": Phase="Running", Reason="", readiness=true. Elapsed: 14.041716648s
Aug 19 16:03:28.894: INFO: Pod "pod-subpath-test-projected-n4k5": Phase="Running", Reason="", readiness=true. Elapsed: 16.04682801s
Aug 19 16:03:30.898: INFO: Pod "pod-subpath-test-projected-n4k5": Phase="Running", Reason="", readiness=true. Elapsed: 18.050231873s
Aug 19 16:03:32.903: INFO: Pod "pod-subpath-test-projected-n4k5": Phase="Running", Reason="", readiness=true. Elapsed: 20.055848844s
Aug 19 16:03:34.908: INFO: Pod "pod-subpath-test-projected-n4k5": Phase="Running", Reason="", readiness=true. Elapsed: 22.060043381s
Aug 19 16:03:36.912: INFO: Pod "pod-subpath-test-projected-n4k5": Phase="Running", Reason="", readiness=false. Elapsed: 24.064484325s
Aug 19 16:03:38.917: INFO: Pod "pod-subpath-test-projected-n4k5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.069729291s
STEP: Saw pod success
Aug 19 16:03:38.917: INFO: Pod "pod-subpath-test-projected-n4k5" satisfied condition "Succeeded or Failed"
Aug 19 16:03:38.921: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-subpath-test-projected-n4k5 container test-container-subpath-projected-n4k5: <nil>
STEP: delete the pod
Aug 19 16:03:38.941: INFO: Waiting for pod pod-subpath-test-projected-n4k5 to disappear
Aug 19 16:03:38.944: INFO: Pod pod-subpath-test-projected-n4k5 no longer exists
STEP: Deleting pod pod-subpath-test-projected-n4k5
Aug 19 16:03:38.944: INFO: Deleting pod "pod-subpath-test-projected-n4k5" in namespace "subpath-5931"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Aug 19 16:03:38.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5931" for this suite.

• [SLOW TEST:26.192 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","total":356,"completed":195,"skipped":3518,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:03:38.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensure pods equal to paralellism count is attached to the job
STEP: patching /status
STEP: updating /status
STEP: get /status
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Aug 19 16:03:43.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5506" for this suite.
•{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","total":356,"completed":196,"skipped":3531,"failed":0}
SSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:03:43.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:03:43.074: INFO: Creating pod...
Aug 19 16:03:47.114: INFO: Creating service...
Aug 19 16:03:47.126: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8389/pods/agnhost/proxy?method=DELETE
Aug 19 16:03:47.131: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 19 16:03:47.131: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8389/pods/agnhost/proxy?method=OPTIONS
Aug 19 16:03:47.137: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 19 16:03:47.137: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8389/pods/agnhost/proxy?method=PATCH
Aug 19 16:03:47.141: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 19 16:03:47.141: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8389/pods/agnhost/proxy?method=POST
Aug 19 16:03:47.145: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 19 16:03:47.145: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8389/pods/agnhost/proxy?method=PUT
Aug 19 16:03:47.149: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 19 16:03:47.149: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8389/services/e2e-proxy-test-service/proxy?method=DELETE
Aug 19 16:03:47.156: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 19 16:03:47.156: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8389/services/e2e-proxy-test-service/proxy?method=OPTIONS
Aug 19 16:03:47.165: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 19 16:03:47.165: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8389/services/e2e-proxy-test-service/proxy?method=PATCH
Aug 19 16:03:47.177: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 19 16:03:47.177: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8389/services/e2e-proxy-test-service/proxy?method=POST
Aug 19 16:03:47.183: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 19 16:03:47.183: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8389/services/e2e-proxy-test-service/proxy?method=PUT
Aug 19 16:03:47.188: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 19 16:03:47.188: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8389/pods/agnhost/proxy?method=GET
Aug 19 16:03:47.191: INFO: http.Client request:GET StatusCode:301
Aug 19 16:03:47.191: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8389/services/e2e-proxy-test-service/proxy?method=GET
Aug 19 16:03:47.198: INFO: http.Client request:GET StatusCode:301
Aug 19 16:03:47.198: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8389/pods/agnhost/proxy?method=HEAD
Aug 19 16:03:47.201: INFO: http.Client request:HEAD StatusCode:301
Aug 19 16:03:47.201: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8389/services/e2e-proxy-test-service/proxy?method=HEAD
Aug 19 16:03:47.205: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Aug 19 16:03:47.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8389" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","total":356,"completed":197,"skipped":3536,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:03:47.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 19 16:03:52.343: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Aug 19 16:03:52.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7214" for this suite.

• [SLOW TEST:5.144 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":356,"completed":198,"skipped":3544,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:03:52.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:03:52.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties
Aug 19 16:03:58.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-5461 --namespace=crd-publish-openapi-5461 create -f -'
Aug 19 16:03:59.704: INFO: stderr: ""
Aug 19 16:03:59.704: INFO: stdout: "e2e-test-crd-publish-openapi-6894-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 19 16:03:59.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-5461 --namespace=crd-publish-openapi-5461 delete e2e-test-crd-publish-openapi-6894-crds test-foo'
Aug 19 16:03:59.775: INFO: stderr: ""
Aug 19 16:03:59.775: INFO: stdout: "e2e-test-crd-publish-openapi-6894-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 19 16:03:59.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-5461 --namespace=crd-publish-openapi-5461 apply -f -'
Aug 19 16:03:59.941: INFO: stderr: ""
Aug 19 16:03:59.941: INFO: stdout: "e2e-test-crd-publish-openapi-6894-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 19 16:03:59.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-5461 --namespace=crd-publish-openapi-5461 delete e2e-test-crd-publish-openapi-6894-crds test-foo'
Aug 19 16:03:59.989: INFO: stderr: ""
Aug 19 16:03:59.989: INFO: stdout: "e2e-test-crd-publish-openapi-6894-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values
Aug 19 16:03:59.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-5461 --namespace=crd-publish-openapi-5461 create -f -'
Aug 19 16:04:00.884: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Aug 19 16:04:00.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-5461 --namespace=crd-publish-openapi-5461 create -f -'
Aug 19 16:04:01.029: INFO: rc: 1
Aug 19 16:04:01.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-5461 --namespace=crd-publish-openapi-5461 apply -f -'
Aug 19 16:04:01.183: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties
Aug 19 16:04:01.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-5461 --namespace=crd-publish-openapi-5461 create -f -'
Aug 19 16:04:01.328: INFO: rc: 1
Aug 19 16:04:01.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-5461 --namespace=crd-publish-openapi-5461 apply -f -'
Aug 19 16:04:02.179: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Aug 19 16:04:02.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-5461 explain e2e-test-crd-publish-openapi-6894-crds'
Aug 19 16:04:02.336: INFO: stderr: ""
Aug 19 16:04:02.336: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6894-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Aug 19 16:04:02.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-5461 explain e2e-test-crd-publish-openapi-6894-crds.metadata'
Aug 19 16:04:02.511: INFO: stderr: ""
Aug 19 16:04:02.511: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6894-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     Deprecated: ClusterName is a legacy field that was always cleared by the\n     system and never used; it will be removed completely in 1.25.\n\n     The name in the go struct is changed to help clients detect accidental use.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 19 16:04:02.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-5461 explain e2e-test-crd-publish-openapi-6894-crds.spec'
Aug 19 16:04:02.677: INFO: stderr: ""
Aug 19 16:04:02.677: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6894-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 19 16:04:02.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-5461 explain e2e-test-crd-publish-openapi-6894-crds.spec.bars'
Aug 19 16:04:02.838: INFO: stderr: ""
Aug 19 16:04:02.838: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6894-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Aug 19 16:04:02.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-5461 explain e2e-test-crd-publish-openapi-6894-crds.spec.bars2'
Aug 19 16:04:02.994: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:04:09.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5461" for this suite.

• [SLOW TEST:16.828 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":356,"completed":199,"skipped":3544,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:04:09.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1574
[It] should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 19 16:04:09.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-9573 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 19 16:04:09.299: INFO: stderr: ""
Aug 19 16:04:09.299: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Aug 19 16:04:14.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-9573 get pod e2e-test-httpd-pod -o json'
Aug 19 16:04:14.394: INFO: stderr: ""
Aug 19 16:04:14.394: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"openshift-sdn\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.131.1.14\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"openshift-sdn\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.131.1.14\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2022-08-19T16:04:09Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9573\",\n        \"resourceVersion\": \"78672\",\n        \"uid\": \"d7e8f3b2-fe2c-4b69-b832-f5c5d773f0f9\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-p5crb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-164-144.ec2.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c54,c4\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-p5crb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-19T16:04:09Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-19T16:04:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-19T16:04:11Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-19T16:04:09Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://f0d5e2b948c0b046bd777b46a939d0216275e7df5959d910b503c842ba73d1cf\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-08-19T16:04:11Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.164.144\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.131.1.14\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.131.1.14\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-08-19T16:04:09Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 19 16:04:14.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-9573 replace -f -'
Aug 19 16:04:15.436: INFO: stderr: ""
Aug 19 16:04:15.436: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1578
Aug 19 16:04:15.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-9573 delete pods e2e-test-httpd-pod'
Aug 19 16:04:16.969: INFO: stderr: ""
Aug 19 16:04:16.969: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 16:04:16.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9573" for this suite.

• [SLOW TEST:7.777 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1571
    should update a single-container pod's image  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":356,"completed":200,"skipped":3547,"failed":0}
SSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:04:16.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Aug 19 16:04:19.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5323" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":356,"completed":201,"skipped":3555,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:04:19.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6567, will wait for the garbage collector to delete the pods
Aug 19 16:04:23.314: INFO: Deleting Job.batch foo took: 6.562055ms
Aug 19 16:04:23.415: INFO: Terminating Job.batch foo pods took: 100.971414ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Aug 19 16:04:55.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6567" for this suite.

• [SLOW TEST:35.968 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":356,"completed":202,"skipped":3576,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:04:55.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:04:55.156: INFO: Creating simple deployment test-new-deployment
W0819 16:04:55.163047      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 16:04:55.177: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 19 16:04:57.215: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9157  8c0fe7cf-8165-4350-b98c-fc03a7972dec 79147 3 2022-08-19 16:04:55 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-08-19 16:04:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 16:04:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c31de18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-19 16:04:57 +0000 UTC,LastTransitionTime:2022-08-19 16:04:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-55df494869" has successfully progressed.,LastUpdateTime:2022-08-19 16:04:57 +0000 UTC,LastTransitionTime:2022-08-19 16:04:55 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 19 16:04:57.220: INFO: New ReplicaSet "test-new-deployment-55df494869" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-55df494869  deployment-9157  ccc5f3c0-411c-4d2a-a845-159d753b5a48 79150 3 2022-08-19 16:04:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 8c0fe7cf-8165-4350-b98c-fc03a7972dec 0xc00a634497 0xc00a634498}] []  [{kube-controller-manager Update apps/v1 2022-08-19 16:04:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c0fe7cf-8165-4350-b98c-fc03a7972dec\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 16:04:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a634578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 19 16:04:57.225: INFO: Pod "test-new-deployment-55df494869-2c4gt" is available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-2c4gt test-new-deployment-55df494869- deployment-9157  1d2396ce-79de-4b05-9e91-fe59433207dd 79140 0 2022-08-19 16:04:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.1.17"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.1.17"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-55df494869 ccc5f3c0-411c-4d2a-a845-159d753b5a48 0xc00a634b57 0xc00a634b58}] []  [{kube-controller-manager Update v1 2022-08-19 16:04:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccc5f3c0-411c-4d2a-a845-159d753b5a48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-08-19 16:04:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-19 16:04:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4fkzf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4fkzf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-164-144.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:04:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:04:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:04:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:04:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.164.144,PodIP:10.131.1.17,StartTime:2022-08-19 16:04:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 16:04:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://91fc99355892acd0a7de765462253fcea8d28f64e6c58bf9b8f1de26865e5bc7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 19 16:04:57.225: INFO: Pod "test-new-deployment-55df494869-4h4vp" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-4h4vp test-new-deployment-55df494869- deployment-9157  2081619d-f69f-4e8f-b0b5-cb07ed0c64e8 79151 0 2022-08-19 16:04:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-55df494869 ccc5f3c0-411c-4d2a-a845-159d753b5a48 0xc00a634f67 0xc00a634f68}] []  [{kube-controller-manager Update v1 2022-08-19 16:04:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccc5f3c0-411c-4d2a-a845-159d753b5a48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hrvcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hrvcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ffkb8,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 19 16:04:57.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9157" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":356,"completed":203,"skipped":3589,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:04:57.245: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
W0819 16:04:57.278739      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0819 16:05:07.298244      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0819 16:05:07.298260      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 19 16:05:07.298: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 19 16:05:07.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1165" for this suite.

• [SLOW TEST:10.063 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":356,"completed":204,"skipped":3599,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:05:07.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 19 16:05:07.387: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 19 16:05:07.396: INFO: starting watch
STEP: patching
STEP: updating
Aug 19 16:05:07.411: INFO: waiting for watch events with expected annotations
Aug 19 16:05:07.411: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Aug 19 16:05:07.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1720" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":356,"completed":205,"skipped":3728,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:05:07.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-f0039773-8042-4cb9-9717-cb7e74415d06
STEP: Creating a pod to test consume configMaps
W0819 16:05:07.498312      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "configmap-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "configmap-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "configmap-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "configmap-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 16:05:07.498: INFO: Waiting up to 5m0s for pod "pod-configmaps-5f841b3f-6a54-47e2-9214-b15104878df0" in namespace "configmap-396" to be "Succeeded or Failed"
Aug 19 16:05:07.501: INFO: Pod "pod-configmaps-5f841b3f-6a54-47e2-9214-b15104878df0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.903587ms
Aug 19 16:05:09.504: INFO: Pod "pod-configmaps-5f841b3f-6a54-47e2-9214-b15104878df0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00643433s
Aug 19 16:05:11.509: INFO: Pod "pod-configmaps-5f841b3f-6a54-47e2-9214-b15104878df0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010796697s
Aug 19 16:05:13.512: INFO: Pod "pod-configmaps-5f841b3f-6a54-47e2-9214-b15104878df0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014196177s
STEP: Saw pod success
Aug 19 16:05:13.512: INFO: Pod "pod-configmaps-5f841b3f-6a54-47e2-9214-b15104878df0" satisfied condition "Succeeded or Failed"
Aug 19 16:05:13.516: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-configmaps-5f841b3f-6a54-47e2-9214-b15104878df0 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 19 16:05:13.542: INFO: Waiting for pod pod-configmaps-5f841b3f-6a54-47e2-9214-b15104878df0 to disappear
Aug 19 16:05:13.545: INFO: Pod pod-configmaps-5f841b3f-6a54-47e2-9214-b15104878df0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 19 16:05:13.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-396" for this suite.

• [SLOW TEST:6.110 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":206,"skipped":3747,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:05:13.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:05:13.590: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-7d3e6127-d6d9-4596-9481-1694460f09ea
STEP: Creating secret with name s-test-opt-upd-e9fc543c-8469-41c9-8d76-32f1f07b265e
STEP: Creating the pod
Aug 19 16:05:13.642: INFO: The status of Pod pod-projected-secrets-dcdc4c8c-a650-4f2b-8bdb-a6e9d83f67cb is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:05:15.647: INFO: The status of Pod pod-projected-secrets-dcdc4c8c-a650-4f2b-8bdb-a6e9d83f67cb is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:05:17.646: INFO: The status of Pod pod-projected-secrets-dcdc4c8c-a650-4f2b-8bdb-a6e9d83f67cb is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-7d3e6127-d6d9-4596-9481-1694460f09ea
STEP: Updating secret s-test-opt-upd-e9fc543c-8469-41c9-8d76-32f1f07b265e
STEP: Creating secret with name s-test-opt-create-487919bc-04ce-441d-9bcb-d07fefad2e30
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 19 16:06:46.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3140" for this suite.

• [SLOW TEST:92.511 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":207,"skipped":3750,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:06:46.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 19 16:06:46.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-5224 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 19 16:06:46.181: INFO: stderr: ""
Aug 19 16:06:46.181: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Aug 19 16:06:46.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-5224 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Aug 19 16:06:47.132: INFO: stderr: ""
Aug 19 16:06:47.132: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 19 16:06:47.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-5224 delete pods e2e-test-httpd-pod'
Aug 19 16:06:50.269: INFO: stderr: ""
Aug 19 16:06:50.269: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 16:06:50.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5224" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":356,"completed":208,"skipped":3763,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:06:50.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 19 16:06:50.335: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c13fddf-2ea5-43f2-807c-82ad6b0cf0ad" in namespace "downward-api-7236" to be "Succeeded or Failed"
Aug 19 16:06:50.338: INFO: Pod "downwardapi-volume-5c13fddf-2ea5-43f2-807c-82ad6b0cf0ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.822165ms
Aug 19 16:06:52.343: INFO: Pod "downwardapi-volume-5c13fddf-2ea5-43f2-807c-82ad6b0cf0ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008488281s
Aug 19 16:06:54.347: INFO: Pod "downwardapi-volume-5c13fddf-2ea5-43f2-807c-82ad6b0cf0ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012739184s
Aug 19 16:06:56.352: INFO: Pod "downwardapi-volume-5c13fddf-2ea5-43f2-807c-82ad6b0cf0ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017061014s
STEP: Saw pod success
Aug 19 16:06:56.352: INFO: Pod "downwardapi-volume-5c13fddf-2ea5-43f2-807c-82ad6b0cf0ad" satisfied condition "Succeeded or Failed"
Aug 19 16:06:56.355: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-5c13fddf-2ea5-43f2-807c-82ad6b0cf0ad container client-container: <nil>
STEP: delete the pod
Aug 19 16:06:56.374: INFO: Waiting for pod downwardapi-volume-5c13fddf-2ea5-43f2-807c-82ad6b0cf0ad to disappear
Aug 19 16:06:56.377: INFO: Pod downwardapi-volume-5c13fddf-2ea5-43f2-807c-82ad6b0cf0ad no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 19 16:06:56.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7236" for this suite.

• [SLOW TEST:6.105 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":209,"skipped":3791,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:06:56.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:06:56.414: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-aeb2248c-0f4f-4ce2-96a5-93edcc0e4e82
STEP: Creating secret with name s-test-opt-upd-adacbd6d-f19e-4fb2-95a5-fd1d118fcd55
STEP: Creating the pod
Aug 19 16:06:56.478: INFO: The status of Pod pod-secrets-b59892e9-573d-4fe9-90ad-eddba9e2df57 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:06:58.484: INFO: The status of Pod pod-secrets-b59892e9-573d-4fe9-90ad-eddba9e2df57 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:07:00.484: INFO: The status of Pod pod-secrets-b59892e9-573d-4fe9-90ad-eddba9e2df57 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-aeb2248c-0f4f-4ce2-96a5-93edcc0e4e82
STEP: Updating secret s-test-opt-upd-adacbd6d-f19e-4fb2-95a5-fd1d118fcd55
STEP: Creating secret with name s-test-opt-create-a88b70e2-7d5d-42f9-850d-367d0c0ea76a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 19 16:08:22.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6153" for this suite.

• [SLOW TEST:86.499 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":210,"skipped":3803,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:08:22.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-9890
Aug 19 16:08:22.941: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:08:24.945: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Aug 19 16:08:24.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-9890 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 19 16:08:25.097: INFO: rc: 7
Aug 19 16:08:25.112: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 19 16:08:25.115: INFO: Pod kube-proxy-mode-detector no longer exists
Aug 19 16:08:25.115: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-9890 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-9890
STEP: creating replication controller affinity-clusterip-timeout in namespace services-9890
I0819 16:08:25.139622      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9890, replica count: 3
I0819 16:08:28.191049      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 19 16:08:28.196: INFO: Creating new exec pod
Aug 19 16:08:33.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-9890 exec execpod-affinitysxs8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Aug 19 16:08:33.385: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Aug 19 16:08:33.385: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 16:08:33.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-9890 exec execpod-affinitysxs8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.180.39 80'
Aug 19 16:08:33.491: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.180.39 80\nConnection to 172.30.180.39 80 port [tcp/http] succeeded!\n"
Aug 19 16:08:33.491: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 16:08:33.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-9890 exec execpod-affinitysxs8d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.180.39:80/ ; done'
Aug 19 16:08:33.814: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n"
Aug 19 16:08:33.814: INFO: stdout: "\naffinity-clusterip-timeout-pprwf\naffinity-clusterip-timeout-pprwf\naffinity-clusterip-timeout-pprwf\naffinity-clusterip-timeout-pprwf\naffinity-clusterip-timeout-pprwf\naffinity-clusterip-timeout-pprwf\naffinity-clusterip-timeout-pprwf\naffinity-clusterip-timeout-pprwf\naffinity-clusterip-timeout-pprwf\naffinity-clusterip-timeout-pprwf\naffinity-clusterip-timeout-pprwf\naffinity-clusterip-timeout-pprwf\naffinity-clusterip-timeout-pprwf\naffinity-clusterip-timeout-pprwf\naffinity-clusterip-timeout-pprwf\naffinity-clusterip-timeout-pprwf"
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Received response from host: affinity-clusterip-timeout-pprwf
Aug 19 16:08:33.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-9890 exec execpod-affinitysxs8d -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.180.39:80/'
Aug 19 16:08:33.920: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n"
Aug 19 16:08:33.920: INFO: stdout: "affinity-clusterip-timeout-pprwf"
Aug 19 16:08:53.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-9890 exec execpod-affinitysxs8d -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.180.39:80/'
Aug 19 16:08:54.052: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.180.39:80/\n"
Aug 19 16:08:54.052: INFO: stdout: "affinity-clusterip-timeout-n5vk8"
Aug 19 16:08:54.052: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9890, will wait for the garbage collector to delete the pods
Aug 19 16:08:54.124: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.837655ms
Aug 19 16:08:54.225: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.883263ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 16:08:56.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9890" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:33.773 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":211,"skipped":3806,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:08:56.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 19 16:08:56.804: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:08:56.804: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:08:56.804: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:08:56.818: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 16:08:56.818: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:08:57.823: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:08:57.823: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:08:57.823: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:08:57.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 16:08:57.826: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:08:58.822: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:08:58.823: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:08:58.823: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:08:58.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 16:08:58.826: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:08:59.823: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:08:59.823: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:08:59.823: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:08:59.825: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 19 16:08:59.825: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
Aug 19 16:08:59.832: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Aug 19 16:08:59.840: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Aug 19 16:08:59.841: INFO: Observed &DaemonSet event: ADDED
Aug 19 16:08:59.842: INFO: Observed &DaemonSet event: MODIFIED
Aug 19 16:08:59.842: INFO: Observed &DaemonSet event: MODIFIED
Aug 19 16:08:59.842: INFO: Observed &DaemonSet event: MODIFIED
Aug 19 16:08:59.842: INFO: Observed &DaemonSet event: MODIFIED
Aug 19 16:08:59.842: INFO: Found daemon set daemon-set in namespace daemonsets-6475 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 19 16:08:59.842: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Aug 19 16:08:59.850: INFO: Observed &DaemonSet event: ADDED
Aug 19 16:08:59.850: INFO: Observed &DaemonSet event: MODIFIED
Aug 19 16:08:59.850: INFO: Observed &DaemonSet event: MODIFIED
Aug 19 16:08:59.850: INFO: Observed &DaemonSet event: MODIFIED
Aug 19 16:08:59.850: INFO: Observed &DaemonSet event: MODIFIED
Aug 19 16:08:59.850: INFO: Observed daemon set daemon-set in namespace daemonsets-6475 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 19 16:08:59.850: INFO: Observed &DaemonSet event: MODIFIED
Aug 19 16:08:59.850: INFO: Found daemon set daemon-set in namespace daemonsets-6475 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Aug 19 16:08:59.850: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6475, will wait for the garbage collector to delete the pods
Aug 19 16:08:59.917: INFO: Deleting DaemonSet.extensions daemon-set took: 7.38908ms
Aug 19 16:09:00.017: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.206414ms
Aug 19 16:09:02.622: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 16:09:02.622: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 19 16:09:02.626: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"81607"},"items":null}

Aug 19 16:09:02.634: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"81607"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 19 16:09:02.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6475" for this suite.

• [SLOW TEST:6.006 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":356,"completed":212,"skipped":3807,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:09:02.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 19 16:09:02.718: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 19 16:10:02.845: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:10:02.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Aug 19 16:10:06.946: INFO: found a healthy node: ip-10-0-164-144.ec2.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:10:21.022: INFO: pods created so far: [1 1 1]
Aug 19 16:10:21.022: INFO: length of pods created so far: 3
Aug 19 16:10:23.040: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:188
Aug 19 16:10:30.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9104" for this suite.
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Aug 19 16:10:30.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1899" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:87.500 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":356,"completed":213,"skipped":3811,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:10:30.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replication controller my-hostname-basic-8eab5bb1-708a-4147-b80e-251aae1cc0ac
W0819 16:10:30.208030      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-8eab5bb1-708a-4147-b80e-251aae1cc0ac" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-8eab5bb1-708a-4147-b80e-251aae1cc0ac" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-8eab5bb1-708a-4147-b80e-251aae1cc0ac" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-8eab5bb1-708a-4147-b80e-251aae1cc0ac" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 16:10:30.214: INFO: Pod name my-hostname-basic-8eab5bb1-708a-4147-b80e-251aae1cc0ac: Found 0 pods out of 1
Aug 19 16:10:35.218: INFO: Pod name my-hostname-basic-8eab5bb1-708a-4147-b80e-251aae1cc0ac: Found 1 pods out of 1
Aug 19 16:10:35.218: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-8eab5bb1-708a-4147-b80e-251aae1cc0ac" are running
Aug 19 16:10:35.221: INFO: Pod "my-hostname-basic-8eab5bb1-708a-4147-b80e-251aae1cc0ac-dhhmg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-19 16:10:30 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-19 16:10:33 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-19 16:10:33 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-19 16:10:30 +0000 UTC Reason: Message:}])
Aug 19 16:10:35.221: INFO: Trying to dial the pod
Aug 19 16:10:40.234: INFO: Controller my-hostname-basic-8eab5bb1-708a-4147-b80e-251aae1cc0ac: Got expected result from replica 1 [my-hostname-basic-8eab5bb1-708a-4147-b80e-251aae1cc0ac-dhhmg]: "my-hostname-basic-8eab5bb1-708a-4147-b80e-251aae1cc0ac-dhhmg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Aug 19 16:10:40.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5892" for this suite.

• [SLOW TEST:10.077 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":214,"skipped":3821,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:10:40.245: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 16:10:40.501: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 19 16:10:42.517: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 16, 10, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 10, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 10, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 10, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 16:10:45.537: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:10:45.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Registering the custom resource webhook via the AdmissionRegistration API
Aug 19 16:10:46.068: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:10:48.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4835" for this suite.
STEP: Destroying namespace "webhook-4835-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.607 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":356,"completed":215,"skipped":3856,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:10:48.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0819 16:10:48.922311      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 16:10:48.922: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b294f4f3-8b49-43a6-a8e8-9d9f1f68eab8" in namespace "downward-api-4792" to be "Succeeded or Failed"
Aug 19 16:10:48.927: INFO: Pod "downwardapi-volume-b294f4f3-8b49-43a6-a8e8-9d9f1f68eab8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.218376ms
Aug 19 16:10:50.931: INFO: Pod "downwardapi-volume-b294f4f3-8b49-43a6-a8e8-9d9f1f68eab8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009460387s
Aug 19 16:10:52.937: INFO: Pod "downwardapi-volume-b294f4f3-8b49-43a6-a8e8-9d9f1f68eab8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014705479s
Aug 19 16:10:54.942: INFO: Pod "downwardapi-volume-b294f4f3-8b49-43a6-a8e8-9d9f1f68eab8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019697615s
STEP: Saw pod success
Aug 19 16:10:54.942: INFO: Pod "downwardapi-volume-b294f4f3-8b49-43a6-a8e8-9d9f1f68eab8" satisfied condition "Succeeded or Failed"
Aug 19 16:10:54.945: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-b294f4f3-8b49-43a6-a8e8-9d9f1f68eab8 container client-container: <nil>
STEP: delete the pod
Aug 19 16:10:54.971: INFO: Waiting for pod downwardapi-volume-b294f4f3-8b49-43a6-a8e8-9d9f1f68eab8 to disappear
Aug 19 16:10:54.975: INFO: Pod downwardapi-volume-b294f4f3-8b49-43a6-a8e8-9d9f1f68eab8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 19 16:10:54.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4792" for this suite.

• [SLOW TEST:6.133 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":216,"skipped":3874,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:10:54.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:188
Aug 19 16:10:55.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7237" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":356,"completed":217,"skipped":3903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:10:55.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-8707
STEP: creating service affinity-nodeport in namespace services-8707
STEP: creating replication controller affinity-nodeport in namespace services-8707
I0819 16:10:55.124712      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-8707, replica count: 3
I0819 16:10:58.175406      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 19 16:10:58.186: INFO: Creating new exec pod
Aug 19 16:11:03.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-8707 exec execpod-affinitys7q5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Aug 19 16:11:03.345: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 19 16:11:03.345: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 16:11:03.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-8707 exec execpod-affinitys7q5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.233.38 80'
Aug 19 16:11:03.448: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.233.38 80\nConnection to 172.30.233.38 80 port [tcp/http] succeeded!\n"
Aug 19 16:11:03.448: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 16:11:03.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-8707 exec execpod-affinitys7q5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.131.169 32300'
Aug 19 16:11:03.571: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.131.169 32300\nConnection to 10.0.131.169 32300 port [tcp/*] succeeded!\n"
Aug 19 16:11:03.571: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 16:11:03.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-8707 exec execpod-affinitys7q5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.157.99 32300'
Aug 19 16:11:03.674: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.157.99 32300\nConnection to 10.0.157.99 32300 port [tcp/*] succeeded!\n"
Aug 19 16:11:03.674: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 16:11:03.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-8707 exec execpod-affinitys7q5w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.131.169:32300/ ; done'
Aug 19 16:11:03.836: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.131.169:32300/\n"
Aug 19 16:11:03.836: INFO: stdout: "\naffinity-nodeport-25rnw\naffinity-nodeport-25rnw\naffinity-nodeport-25rnw\naffinity-nodeport-25rnw\naffinity-nodeport-25rnw\naffinity-nodeport-25rnw\naffinity-nodeport-25rnw\naffinity-nodeport-25rnw\naffinity-nodeport-25rnw\naffinity-nodeport-25rnw\naffinity-nodeport-25rnw\naffinity-nodeport-25rnw\naffinity-nodeport-25rnw\naffinity-nodeport-25rnw\naffinity-nodeport-25rnw\naffinity-nodeport-25rnw"
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Received response from host: affinity-nodeport-25rnw
Aug 19 16:11:03.836: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-8707, will wait for the garbage collector to delete the pods
Aug 19 16:11:03.909: INFO: Deleting ReplicationController affinity-nodeport took: 6.447083ms
Aug 19 16:11:04.009: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.280158ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 16:11:05.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8707" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:10.824 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":218,"skipped":3964,"failed":0}
SSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:11:05.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Aug 19 16:11:06.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6120" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":219,"skipped":3967,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:11:06.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 19 16:11:06.218: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3746  155cfc18-64a7-4252-aa2a-fba4897a669d 83146 0 2022-08-19 16:11:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-19 16:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 19 16:11:06.218: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3746  155cfc18-64a7-4252-aa2a-fba4897a669d 83148 0 2022-08-19 16:11:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-19 16:11:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 19 16:11:06.218: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3746  155cfc18-64a7-4252-aa2a-fba4897a669d 83152 0 2022-08-19 16:11:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-19 16:11:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 19 16:11:16.249: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3746  155cfc18-64a7-4252-aa2a-fba4897a669d 83287 0 2022-08-19 16:11:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-19 16:11:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 19 16:11:16.249: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3746  155cfc18-64a7-4252-aa2a-fba4897a669d 83288 0 2022-08-19 16:11:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-19 16:11:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 19 16:11:16.249: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3746  155cfc18-64a7-4252-aa2a-fba4897a669d 83289 0 2022-08-19 16:11:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-19 16:11:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Aug 19 16:11:16.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3746" for this suite.

• [SLOW TEST:10.148 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":356,"completed":220,"skipped":3971,"failed":0}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:11:16.260: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:11:16.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:11:19.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1736" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":356,"completed":221,"skipped":3971,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:11:19.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Aug 19 16:11:19.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-653" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":356,"completed":222,"skipped":3995,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:11:19.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Aug 19 16:11:19.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 16:11:29.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:11:52.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7600" for this suite.

• [SLOW TEST:33.414 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":356,"completed":223,"skipped":4017,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:11:52.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 16:11:53.519: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 19 16:11:55.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 16, 11, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 11, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 11, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 11, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 16:11:58.545: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:11:58.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9769-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:12:01.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2652" for this suite.
STEP: Destroying namespace "webhook-2652-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.788 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":356,"completed":224,"skipped":4037,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:12:01.737: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-d431d2e1-917d-4ec9-8168-040b4efce746
STEP: Creating a pod to test consume secrets
Aug 19 16:12:01.924: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b2f39a4b-71ad-46c4-90ff-a09e54bb4abd" in namespace "projected-2484" to be "Succeeded or Failed"
Aug 19 16:12:01.942: INFO: Pod "pod-projected-secrets-b2f39a4b-71ad-46c4-90ff-a09e54bb4abd": Phase="Pending", Reason="", readiness=false. Elapsed: 17.604734ms
Aug 19 16:12:03.947: INFO: Pod "pod-projected-secrets-b2f39a4b-71ad-46c4-90ff-a09e54bb4abd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022577281s
Aug 19 16:12:05.952: INFO: Pod "pod-projected-secrets-b2f39a4b-71ad-46c4-90ff-a09e54bb4abd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027323022s
Aug 19 16:12:07.955: INFO: Pod "pod-projected-secrets-b2f39a4b-71ad-46c4-90ff-a09e54bb4abd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031033534s
STEP: Saw pod success
Aug 19 16:12:07.955: INFO: Pod "pod-projected-secrets-b2f39a4b-71ad-46c4-90ff-a09e54bb4abd" satisfied condition "Succeeded or Failed"
Aug 19 16:12:07.958: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-projected-secrets-b2f39a4b-71ad-46c4-90ff-a09e54bb4abd container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 19 16:12:07.976: INFO: Waiting for pod pod-projected-secrets-b2f39a4b-71ad-46c4-90ff-a09e54bb4abd to disappear
Aug 19 16:12:07.979: INFO: Pod pod-projected-secrets-b2f39a4b-71ad-46c4-90ff-a09e54bb4abd no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 19 16:12:07.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2484" for this suite.

• [SLOW TEST:6.252 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":225,"skipped":4053,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:12:07.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 19 16:12:08.028: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
W0819 16:12:08.062993      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 19 16:12:13.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9479" for this suite.

• [SLOW TEST:5.943 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":356,"completed":226,"skipped":4082,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:12:13.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 16:12:14.697: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 19 16:12:16.708: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 16, 12, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 12, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 12, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 12, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 16:12:19.725: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:12:19.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5241-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:12:22.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1820" for this suite.
STEP: Destroying namespace "webhook-1820-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.979 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":356,"completed":227,"skipped":4093,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:12:22.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 16:12:23.324: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 19 16:12:25.334: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 16, 12, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 12, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 12, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 12, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 16:12:28.350: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:12:40.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3920" for this suite.
STEP: Destroying namespace "webhook-3920-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:17.640 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":356,"completed":228,"skipped":4103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:12:40.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 19 16:12:51.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9774" for this suite.

• [SLOW TEST:11.237 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":356,"completed":229,"skipped":4139,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:12:51.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 19 16:12:51.855: INFO: Waiting up to 5m0s for pod "pod-c9f2c78b-76c4-4397-9aa8-a755fecd0b40" in namespace "emptydir-5483" to be "Succeeded or Failed"
Aug 19 16:12:51.859: INFO: Pod "pod-c9f2c78b-76c4-4397-9aa8-a755fecd0b40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.507658ms
Aug 19 16:12:53.864: INFO: Pod "pod-c9f2c78b-76c4-4397-9aa8-a755fecd0b40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00902747s
Aug 19 16:12:55.869: INFO: Pod "pod-c9f2c78b-76c4-4397-9aa8-a755fecd0b40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014041773s
Aug 19 16:12:57.873: INFO: Pod "pod-c9f2c78b-76c4-4397-9aa8-a755fecd0b40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018595384s
STEP: Saw pod success
Aug 19 16:12:57.873: INFO: Pod "pod-c9f2c78b-76c4-4397-9aa8-a755fecd0b40" satisfied condition "Succeeded or Failed"
Aug 19 16:12:57.876: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-c9f2c78b-76c4-4397-9aa8-a755fecd0b40 container test-container: <nil>
STEP: delete the pod
Aug 19 16:12:57.893: INFO: Waiting for pod pod-c9f2c78b-76c4-4397-9aa8-a755fecd0b40 to disappear
Aug 19 16:12:57.896: INFO: Pod pod-c9f2c78b-76c4-4397-9aa8-a755fecd0b40 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 19 16:12:57.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5483" for this suite.

• [SLOW TEST:6.117 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":230,"skipped":4147,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:12:57.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 19 16:12:57.969: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2ef02d1-1934-4cb4-a197-3b7882086f7c" in namespace "downward-api-7643" to be "Succeeded or Failed"
Aug 19 16:12:57.973: INFO: Pod "downwardapi-volume-b2ef02d1-1934-4cb4-a197-3b7882086f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.386585ms
Aug 19 16:12:59.977: INFO: Pod "downwardapi-volume-b2ef02d1-1934-4cb4-a197-3b7882086f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007897881s
Aug 19 16:13:01.982: INFO: Pod "downwardapi-volume-b2ef02d1-1934-4cb4-a197-3b7882086f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012516512s
Aug 19 16:13:03.989: INFO: Pod "downwardapi-volume-b2ef02d1-1934-4cb4-a197-3b7882086f7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019310013s
STEP: Saw pod success
Aug 19 16:13:03.989: INFO: Pod "downwardapi-volume-b2ef02d1-1934-4cb4-a197-3b7882086f7c" satisfied condition "Succeeded or Failed"
Aug 19 16:13:03.992: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-b2ef02d1-1934-4cb4-a197-3b7882086f7c container client-container: <nil>
STEP: delete the pod
Aug 19 16:13:04.008: INFO: Waiting for pod downwardapi-volume-b2ef02d1-1934-4cb4-a197-3b7882086f7c to disappear
Aug 19 16:13:04.011: INFO: Pod downwardapi-volume-b2ef02d1-1934-4cb4-a197-3b7882086f7c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 19 16:13:04.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7643" for this suite.

• [SLOW TEST:6.116 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":231,"skipped":4149,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:13:04.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Aug 19 16:13:04.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:13:34.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6657" for this suite.

• [SLOW TEST:30.232 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":356,"completed":232,"skipped":4177,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:13:34.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Aug 19 16:13:34.351: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:13:36.358: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:13:38.358: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Aug 19 16:13:38.378: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:13:40.382: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:13:42.383: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 19 16:13:42.402: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 19 16:13:42.406: INFO: Pod pod-with-poststart-http-hook still exists
Aug 19 16:13:44.406: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 19 16:13:44.411: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Aug 19 16:13:44.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5636" for this suite.

• [SLOW TEST:10.169 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":356,"completed":233,"skipped":4244,"failed":0}
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:13:44.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Aug 19 16:13:44.501: INFO: Waiting up to 5m0s for pod "security-context-b724926d-6839-449d-8f82-88b14b776bd5" in namespace "security-context-2350" to be "Succeeded or Failed"
Aug 19 16:13:44.510: INFO: Pod "security-context-b724926d-6839-449d-8f82-88b14b776bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.728156ms
Aug 19 16:13:46.515: INFO: Pod "security-context-b724926d-6839-449d-8f82-88b14b776bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014216991s
Aug 19 16:13:48.520: INFO: Pod "security-context-b724926d-6839-449d-8f82-88b14b776bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019346564s
Aug 19 16:13:50.523: INFO: Pod "security-context-b724926d-6839-449d-8f82-88b14b776bd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022291161s
STEP: Saw pod success
Aug 19 16:13:50.523: INFO: Pod "security-context-b724926d-6839-449d-8f82-88b14b776bd5" satisfied condition "Succeeded or Failed"
Aug 19 16:13:50.526: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod security-context-b724926d-6839-449d-8f82-88b14b776bd5 container test-container: <nil>
STEP: delete the pod
Aug 19 16:13:50.548: INFO: Waiting for pod security-context-b724926d-6839-449d-8f82-88b14b776bd5 to disappear
Aug 19 16:13:50.550: INFO: Pod security-context-b724926d-6839-449d-8f82-88b14b776bd5 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 19 16:13:50.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2350" for this suite.

• [SLOW TEST:6.135 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":234,"skipped":4244,"failed":0}
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:13:50.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3615
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating stateful set ss in namespace statefulset-3615
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3615
Aug 19 16:13:50.625: INFO: Found 0 stateful pods, waiting for 1
Aug 19 16:14:00.631: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 19 16:14:00.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-3615 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 19 16:14:00.740: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 19 16:14:00.740: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 19 16:14:00.740: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 19 16:14:00.745: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 19 16:14:10.755: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 19 16:14:10.755: INFO: Waiting for statefulset status.replicas updated to 0
Aug 19 16:14:10.771: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 19 16:14:10.771: INFO: ss-0  ip-10-0-164-144.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:13:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:14:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:14:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:13:50 +0000 UTC  }]
Aug 19 16:14:10.771: INFO: 
Aug 19 16:14:10.771: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 19 16:14:11.775: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996758969s
Aug 19 16:14:12.781: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99229465s
Aug 19 16:14:13.784: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986806074s
Aug 19 16:14:14.790: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982861344s
Aug 19 16:14:15.797: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978318486s
Aug 19 16:14:16.801: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97072177s
Aug 19 16:14:17.806: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966290933s
Aug 19 16:14:18.814: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.961287616s
Aug 19 16:14:19.818: INFO: Verifying statefulset ss doesn't scale past 3 for another 953.918497ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3615
Aug 19 16:14:20.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-3615 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 19 16:14:20.927: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 19 16:14:20.927: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 19 16:14:20.927: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 19 16:14:20.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-3615 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 19 16:14:21.026: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 19 16:14:21.026: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 19 16:14:21.026: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 19 16:14:21.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-3615 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 19 16:14:21.149: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 19 16:14:21.149: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 19 16:14:21.149: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 19 16:14:21.154: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 19 16:14:21.154: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 19 16:14:21.154: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 19 16:14:21.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-3615 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 19 16:14:21.263: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 19 16:14:21.263: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 19 16:14:21.263: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 19 16:14:21.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-3615 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 19 16:14:21.363: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 19 16:14:21.363: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 19 16:14:21.363: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 19 16:14:21.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-3615 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 19 16:14:21.456: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 19 16:14:21.456: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 19 16:14:21.456: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 19 16:14:21.456: INFO: Waiting for statefulset status.replicas updated to 0
Aug 19 16:14:21.459: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Aug 19 16:14:31.471: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 19 16:14:31.471: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 19 16:14:31.471: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 19 16:14:31.489: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 19 16:14:31.489: INFO: ss-0  ip-10-0-164-144.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:13:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:14:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:14:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:13:50 +0000 UTC  }]
Aug 19 16:14:31.489: INFO: ss-1  ip-10-0-157-99.ec2.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:14:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:14:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:14:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:14:10 +0000 UTC  }]
Aug 19 16:14:31.489: INFO: ss-2  ip-10-0-131-169.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:14:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:14:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:14:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-19 16:14:10 +0000 UTC  }]
Aug 19 16:14:31.489: INFO: 
Aug 19 16:14:31.489: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 19 16:14:32.499: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.996532835s
Aug 19 16:14:33.504: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.986576532s
Aug 19 16:14:34.509: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.980875196s
Aug 19 16:14:35.513: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.975705823s
Aug 19 16:14:36.519: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.97170584s
Aug 19 16:14:37.525: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.965705648s
Aug 19 16:14:38.530: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.959707937s
Aug 19 16:14:39.535: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.954484397s
Aug 19 16:14:40.539: INFO: Verifying statefulset ss doesn't scale past 0 for another 949.713099ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3615
Aug 19 16:14:41.545: INFO: Scaling statefulset ss to 0
Aug 19 16:14:41.554: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 19 16:14:41.557: INFO: Deleting all statefulset in ns statefulset-3615
Aug 19 16:14:41.560: INFO: Scaling statefulset ss to 0
Aug 19 16:14:41.568: INFO: Waiting for statefulset status.replicas updated to 0
Aug 19 16:14:41.571: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 19 16:14:41.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3615" for this suite.

• [SLOW TEST:51.031 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":356,"completed":235,"skipped":4244,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:14:41.590: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Aug 19 16:14:41.687: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Aug 19 16:14:41.706: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 19 16:14:41.706: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Aug 19 16:14:41.733: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 19 16:14:41.733: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Aug 19 16:14:41.753: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 19 16:14:41.753: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Aug 19 16:14:48.819: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:188
Aug 19 16:14:48.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7029" for this suite.

• [SLOW TEST:7.252 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":356,"completed":236,"skipped":4291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:14:48.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:14:48.888: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-7d514494-2dc7-49c7-859b-23887b31dbb0
STEP: Creating configMap with name cm-test-opt-upd-ea477f01-f1cf-4561-ba14-119a74678020
STEP: Creating the pod
Aug 19 16:14:48.950: INFO: The status of Pod pod-projected-configmaps-0d4e06fd-1ed2-45f6-b0fc-9e8cf460c7d6 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:14:50.956: INFO: The status of Pod pod-projected-configmaps-0d4e06fd-1ed2-45f6-b0fc-9e8cf460c7d6 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:14:52.955: INFO: The status of Pod pod-projected-configmaps-0d4e06fd-1ed2-45f6-b0fc-9e8cf460c7d6 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-7d514494-2dc7-49c7-859b-23887b31dbb0
STEP: Updating configmap cm-test-opt-upd-ea477f01-f1cf-4561-ba14-119a74678020
STEP: Creating configMap with name cm-test-opt-create-3eb5c9b2-cdbe-4562-9939-d071622a284a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 19 16:16:25.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7618" for this suite.

• [SLOW TEST:96.540 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":237,"skipped":4337,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:16:25.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Aug 19 16:16:29.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3371" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":356,"completed":238,"skipped":4365,"failed":0}
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:16:29.512: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 19 16:16:34.606: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Aug 19 16:16:34.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-238" for this suite.

• [SLOW TEST:5.119 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":239,"skipped":4370,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:16:34.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
W0819 16:16:34.667275      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 16:16:34.676: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 19 16:16:39.684: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 19 16:16:39.684: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 19 16:16:41.690: INFO: Creating deployment "test-rollover-deployment"
Aug 19 16:16:41.699: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 19 16:16:43.709: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 19 16:16:43.713: INFO: Ensure that both replica sets have 1 created replica
Aug 19 16:16:43.718: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 19 16:16:43.728: INFO: Updating deployment test-rollover-deployment
Aug 19 16:16:43.728: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 19 16:16:45.733: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 19 16:16:45.739: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 19 16:16:45.744: INFO: all replica sets need to contain the pod-template-hash label
Aug 19 16:16:45.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 16, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 19 16:16:47.753: INFO: all replica sets need to contain the pod-template-hash label
Aug 19 16:16:47.753: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 16, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 19 16:16:49.753: INFO: all replica sets need to contain the pod-template-hash label
Aug 19 16:16:49.753: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 16, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 19 16:16:51.755: INFO: all replica sets need to contain the pod-template-hash label
Aug 19 16:16:51.755: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 16, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 19 16:16:53.753: INFO: all replica sets need to contain the pod-template-hash label
Aug 19 16:16:53.753: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 16, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 19 16:16:55.750: INFO: all replica sets need to contain the pod-template-hash label
Aug 19 16:16:55.750: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 16, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 16, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 19 16:16:57.754: INFO: 
Aug 19 16:16:57.754: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 19 16:16:57.763: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7205  f4ce3fe3-a0b4-4965-bee2-1439cb4e201e 87193 2 2022-08-19 16:16:41 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-08-19 16:16:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 16:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0094e05b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-19 16:16:41 +0000 UTC,LastTransitionTime:2022-08-19 16:16:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-77745f886c" has successfully progressed.,LastUpdateTime:2022-08-19 16:16:56 +0000 UTC,LastTransitionTime:2022-08-19 16:16:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 19 16:16:57.765: INFO: New ReplicaSet "test-rollover-deployment-77745f886c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-77745f886c  deployment-7205  09ed1f14-f626-48b6-834d-5ed9c8cfbe1a 87183 2 2022-08-19 16:16:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77745f886c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment f4ce3fe3-a0b4-4965-bee2-1439cb4e201e 0xc0094e0a77 0xc0094e0a78}] []  [{kube-controller-manager Update apps/v1 2022-08-19 16:16:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ce3fe3-a0b4-4965-bee2-1439cb4e201e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 16:16:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 77745f886c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77745f886c] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0094e0b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 19 16:16:57.765: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 19 16:16:57.765: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7205  6c98db99-6a94-49f1-a49c-528c26468419 87192 2 2022-08-19 16:16:34 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment f4ce3fe3-a0b4-4965-bee2-1439cb4e201e 0xc0094e0947 0xc0094e0948}] []  [{e2e.test Update apps/v1 2022-08-19 16:16:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 16:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ce3fe3-a0b4-4965-bee2-1439cb4e201e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-08-19 16:16:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0094e0a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 19 16:16:57.765: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-87f8f6dcf  deployment-7205  3065e281-f874-4a26-8591-e8aef3034c94 87067 2 2022-08-19 16:16:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment f4ce3fe3-a0b4-4965-bee2-1439cb4e201e 0xc0094e0b90 0xc0094e0b91}] []  [{kube-controller-manager Update apps/v1 2022-08-19 16:16:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ce3fe3-a0b4-4965-bee2-1439cb4e201e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 16:16:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 87f8f6dcf,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0094e0c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 19 16:16:57.767: INFO: Pod "test-rollover-deployment-77745f886c-dmzcc" is available:
&Pod{ObjectMeta:{test-rollover-deployment-77745f886c-dmzcc test-rollover-deployment-77745f886c- deployment-7205  78ebab04-0c59-4ee5-93c0-936ea86a7560 87111 0 2022-08-19 16:16:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77745f886c] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.1.54"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.1.54"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-77745f886c 09ed1f14-f626-48b6-834d-5ed9c8cfbe1a 0xc006169177 0xc006169178}] []  [{kube-controller-manager Update v1 2022-08-19 16:16:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09ed1f14-f626-48b6-834d-5ed9c8cfbe1a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-19 16:16:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-08-19 16:16:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gmxpn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gmxpn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-164-144.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c12,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kcpjc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:16:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:16:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:16:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:16:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.164.144,PodIP:10.131.1.54,StartTime:2022-08-19 16:16:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 16:16:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403,ContainerID:cri-o://c2ad1a09a71aa1d2194ff2d8f50445e255940a21f1524be04a75e5dc1fa00915,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 19 16:16:57.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7205" for this suite.

• [SLOW TEST:23.146 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":356,"completed":240,"skipped":4375,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:16:57.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-7f37f52f-dda6-4835-a64a-a7552b10ce54
STEP: Creating a pod to test consume configMaps
Aug 19 16:16:57.857: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ebaab194-cabd-4924-9d75-ab9efa2e62bc" in namespace "projected-7772" to be "Succeeded or Failed"
Aug 19 16:16:57.862: INFO: Pod "pod-projected-configmaps-ebaab194-cabd-4924-9d75-ab9efa2e62bc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.033812ms
Aug 19 16:16:59.869: INFO: Pod "pod-projected-configmaps-ebaab194-cabd-4924-9d75-ab9efa2e62bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012143224s
Aug 19 16:17:01.873: INFO: Pod "pod-projected-configmaps-ebaab194-cabd-4924-9d75-ab9efa2e62bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01666633s
STEP: Saw pod success
Aug 19 16:17:01.873: INFO: Pod "pod-projected-configmaps-ebaab194-cabd-4924-9d75-ab9efa2e62bc" satisfied condition "Succeeded or Failed"
Aug 19 16:17:01.877: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-projected-configmaps-ebaab194-cabd-4924-9d75-ab9efa2e62bc container agnhost-container: <nil>
STEP: delete the pod
Aug 19 16:17:01.893: INFO: Waiting for pod pod-projected-configmaps-ebaab194-cabd-4924-9d75-ab9efa2e62bc to disappear
Aug 19 16:17:01.895: INFO: Pod pod-projected-configmaps-ebaab194-cabd-4924-9d75-ab9efa2e62bc no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 19 16:17:01.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7772" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":241,"skipped":4409,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:17:01.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1412
STEP: creating an pod
Aug 19 16:17:01.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-67 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.36 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 19 16:17:02.012: INFO: stderr: ""
Aug 19 16:17:02.012: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for log generator to start.
Aug 19 16:17:02.012: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 19 16:17:02.012: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-67" to be "running and ready, or succeeded"
Aug 19 16:17:02.017: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.254312ms
Aug 19 16:17:04.023: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011084409s
Aug 19 16:17:06.028: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.016248055s
Aug 19 16:17:06.028: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 19 16:17:06.028: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Aug 19 16:17:06.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-67 logs logs-generator logs-generator'
Aug 19 16:17:06.079: INFO: stderr: ""
Aug 19 16:17:06.079: INFO: stdout: "I0819 16:17:03.875431       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/ts7z 467\nI0819 16:17:04.075414       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/m6x8 547\nI0819 16:17:04.276025       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/qqf6 516\nI0819 16:17:04.476269       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/mmm 349\nI0819 16:17:04.675407       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/lf5 233\nI0819 16:17:04.875530       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/psb 308\nI0819 16:17:05.075826       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/4dlk 574\nI0819 16:17:05.276127       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/fzn7 330\nI0819 16:17:05.475375       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/7h4 273\nI0819 16:17:05.675529       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/ms78 290\nI0819 16:17:05.875826       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/ldb 303\nI0819 16:17:06.076124       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/5x8h 357\n"
STEP: limiting log lines
Aug 19 16:17:06.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-67 logs logs-generator logs-generator --tail=1'
Aug 19 16:17:06.126: INFO: stderr: ""
Aug 19 16:17:06.126: INFO: stdout: "I0819 16:17:06.076124       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/5x8h 357\n"
Aug 19 16:17:06.126: INFO: got output "I0819 16:17:06.076124       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/5x8h 357\n"
STEP: limiting log bytes
Aug 19 16:17:06.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-67 logs logs-generator logs-generator --limit-bytes=1'
Aug 19 16:17:06.171: INFO: stderr: ""
Aug 19 16:17:06.171: INFO: stdout: "I"
Aug 19 16:17:06.171: INFO: got output "I"
STEP: exposing timestamps
Aug 19 16:17:06.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-67 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 19 16:17:06.215: INFO: stderr: ""
Aug 19 16:17:06.215: INFO: stdout: "2022-08-19T16:17:06.076163400Z I0819 16:17:06.076124       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/5x8h 357\n"
Aug 19 16:17:06.215: INFO: got output "2022-08-19T16:17:06.076163400Z I0819 16:17:06.076124       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/5x8h 357\n"
STEP: restricting to a time range
Aug 19 16:17:08.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-67 logs logs-generator logs-generator --since=1s'
Aug 19 16:17:08.764: INFO: stderr: ""
Aug 19 16:17:08.764: INFO: stdout: "I0819 16:17:07.875376       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/h5wz 571\nI0819 16:17:08.075529       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/69n 498\nI0819 16:17:08.275830       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/74vh 266\nI0819 16:17:08.476135       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/x4fg 255\nI0819 16:17:08.675379       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/k9z8 518\n"
Aug 19 16:17:08.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-67 logs logs-generator logs-generator --since=24h'
Aug 19 16:17:08.811: INFO: stderr: ""
Aug 19 16:17:08.811: INFO: stdout: "I0819 16:17:03.875431       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/ts7z 467\nI0819 16:17:04.075414       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/m6x8 547\nI0819 16:17:04.276025       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/qqf6 516\nI0819 16:17:04.476269       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/mmm 349\nI0819 16:17:04.675407       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/lf5 233\nI0819 16:17:04.875530       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/psb 308\nI0819 16:17:05.075826       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/4dlk 574\nI0819 16:17:05.276127       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/fzn7 330\nI0819 16:17:05.475375       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/7h4 273\nI0819 16:17:05.675529       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/ms78 290\nI0819 16:17:05.875826       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/ldb 303\nI0819 16:17:06.076124       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/5x8h 357\nI0819 16:17:06.275368       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/qvxx 519\nI0819 16:17:06.475529       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/ncb9 369\nI0819 16:17:06.675827       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/vs7 444\nI0819 16:17:06.876042       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/2gbs 423\nI0819 16:17:07.076331       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/mwbp 342\nI0819 16:17:07.275528       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/5s7 348\nI0819 16:17:07.475826       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/qbg 241\nI0819 16:17:07.676129       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/2j2v 347\nI0819 16:17:07.875376       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/h5wz 571\nI0819 16:17:08.075529       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/69n 498\nI0819 16:17:08.275830       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/74vh 266\nI0819 16:17:08.476135       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/x4fg 255\nI0819 16:17:08.675379       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/k9z8 518\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1417
Aug 19 16:17:08.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-67 delete pod logs-generator'
Aug 19 16:17:09.473: INFO: stderr: ""
Aug 19 16:17:09.473: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 16:17:09.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-67" for this suite.

• [SLOW TEST:7.582 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1409
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":356,"completed":242,"skipped":4419,"failed":0}
SSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:17:09.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Aug 19 16:17:09.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4472" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":243,"skipped":4425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:17:09.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-05a5a057-ce1c-44a2-8139-6f0f0fcd5af3
STEP: Creating a pod to test consume configMaps
Aug 19 16:17:09.623: INFO: Waiting up to 5m0s for pod "pod-configmaps-1a079e17-8498-4d0c-960c-bd9a8a7e411c" in namespace "configmap-8295" to be "Succeeded or Failed"
Aug 19 16:17:09.640: INFO: Pod "pod-configmaps-1a079e17-8498-4d0c-960c-bd9a8a7e411c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.248977ms
Aug 19 16:17:11.646: INFO: Pod "pod-configmaps-1a079e17-8498-4d0c-960c-bd9a8a7e411c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023204876s
Aug 19 16:17:13.654: INFO: Pod "pod-configmaps-1a079e17-8498-4d0c-960c-bd9a8a7e411c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030812385s
Aug 19 16:17:15.660: INFO: Pod "pod-configmaps-1a079e17-8498-4d0c-960c-bd9a8a7e411c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036884213s
STEP: Saw pod success
Aug 19 16:17:15.660: INFO: Pod "pod-configmaps-1a079e17-8498-4d0c-960c-bd9a8a7e411c" satisfied condition "Succeeded or Failed"
Aug 19 16:17:15.662: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-configmaps-1a079e17-8498-4d0c-960c-bd9a8a7e411c container agnhost-container: <nil>
STEP: delete the pod
Aug 19 16:17:15.679: INFO: Waiting for pod pod-configmaps-1a079e17-8498-4d0c-960c-bd9a8a7e411c to disappear
Aug 19 16:17:15.681: INFO: Pod pod-configmaps-1a079e17-8498-4d0c-960c-bd9a8a7e411c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 19 16:17:15.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8295" for this suite.

• [SLOW TEST:6.122 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":244,"skipped":4460,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:17:15.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-52b03cde-892b-47ef-8c94-2a15a34028b7
STEP: Creating a pod to test consume configMaps
Aug 19 16:17:15.792: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ada7fd55-bc12-45d7-bf71-2fcebd08482c" in namespace "projected-9415" to be "Succeeded or Failed"
Aug 19 16:17:15.796: INFO: Pod "pod-projected-configmaps-ada7fd55-bc12-45d7-bf71-2fcebd08482c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.987637ms
Aug 19 16:17:17.801: INFO: Pod "pod-projected-configmaps-ada7fd55-bc12-45d7-bf71-2fcebd08482c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009014208s
Aug 19 16:17:19.805: INFO: Pod "pod-projected-configmaps-ada7fd55-bc12-45d7-bf71-2fcebd08482c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013047763s
Aug 19 16:17:21.811: INFO: Pod "pod-projected-configmaps-ada7fd55-bc12-45d7-bf71-2fcebd08482c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018793301s
STEP: Saw pod success
Aug 19 16:17:21.811: INFO: Pod "pod-projected-configmaps-ada7fd55-bc12-45d7-bf71-2fcebd08482c" satisfied condition "Succeeded or Failed"
Aug 19 16:17:21.814: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-projected-configmaps-ada7fd55-bc12-45d7-bf71-2fcebd08482c container agnhost-container: <nil>
STEP: delete the pod
Aug 19 16:17:21.834: INFO: Waiting for pod pod-projected-configmaps-ada7fd55-bc12-45d7-bf71-2fcebd08482c to disappear
Aug 19 16:17:21.837: INFO: Pod pod-projected-configmaps-ada7fd55-bc12-45d7-bf71-2fcebd08482c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 19 16:17:21.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9415" for this suite.

• [SLOW TEST:6.153 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":245,"skipped":4466,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:17:21.845: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 19 16:17:37.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6189" for this suite.

• [SLOW TEST:16.166 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":356,"completed":246,"skipped":4480,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:17:38.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test env composition
Aug 19 16:17:38.078: INFO: Waiting up to 5m0s for pod "var-expansion-b5d9a4b8-a1c8-44ae-b491-906217dc251c" in namespace "var-expansion-9079" to be "Succeeded or Failed"
Aug 19 16:17:38.083: INFO: Pod "var-expansion-b5d9a4b8-a1c8-44ae-b491-906217dc251c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.907203ms
Aug 19 16:17:40.087: INFO: Pod "var-expansion-b5d9a4b8-a1c8-44ae-b491-906217dc251c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008899274s
Aug 19 16:17:42.094: INFO: Pod "var-expansion-b5d9a4b8-a1c8-44ae-b491-906217dc251c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016164733s
STEP: Saw pod success
Aug 19 16:17:42.094: INFO: Pod "var-expansion-b5d9a4b8-a1c8-44ae-b491-906217dc251c" satisfied condition "Succeeded or Failed"
Aug 19 16:17:42.097: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod var-expansion-b5d9a4b8-a1c8-44ae-b491-906217dc251c container dapi-container: <nil>
STEP: delete the pod
Aug 19 16:17:42.118: INFO: Waiting for pod var-expansion-b5d9a4b8-a1c8-44ae-b491-906217dc251c to disappear
Aug 19 16:17:42.120: INFO: Pod var-expansion-b5d9a4b8-a1c8-44ae-b491-906217dc251c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 19 16:17:42.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9079" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":356,"completed":247,"skipped":4481,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:17:42.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:17:42.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Creating first CR 
Aug 19 16:17:44.759: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-19T16:17:44Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-19T16:17:44Z]] name:name1 resourceVersion:88012 uid:1a11164c-2830-408f-ac3c-00c1c722c3db] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Aug 19 16:17:54.766: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-19T16:17:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-19T16:17:54Z]] name:name2 resourceVersion:88072 uid:a1225889-783c-4d7c-bf3e-3fca6abce864] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Aug 19 16:18:04.778: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-19T16:17:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-19T16:18:04Z]] name:name1 resourceVersion:88140 uid:1a11164c-2830-408f-ac3c-00c1c722c3db] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Aug 19 16:18:14.790: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-19T16:17:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-19T16:18:14Z]] name:name2 resourceVersion:88193 uid:a1225889-783c-4d7c-bf3e-3fca6abce864] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Aug 19 16:18:24.803: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-19T16:17:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-19T16:18:04Z]] name:name1 resourceVersion:88234 uid:1a11164c-2830-408f-ac3c-00c1c722c3db] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Aug 19 16:18:34.813: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-19T16:17:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-19T16:18:14Z]] name:name2 resourceVersion:88309 uid:a1225889-783c-4d7c-bf3e-3fca6abce864] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:18:45.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2185" for this suite.

• [SLOW TEST:63.208 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":356,"completed":248,"skipped":4487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:18:45.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Aug 19 16:18:45.384: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 19 16:18:51.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2436" for this suite.

• [SLOW TEST:6.348 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":356,"completed":249,"skipped":4511,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:18:51.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2271
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2271
I0819 16:18:51.754826      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2271, replica count: 2
I0819 16:18:54.806314      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 19 16:18:54.806: INFO: Creating new exec pod
Aug 19 16:18:59.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-2271 exec execpod448l8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 19 16:18:59.944: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 19 16:18:59.944: INFO: stdout: ""
Aug 19 16:19:00.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-2271 exec execpod448l8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 19 16:19:01.043: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 19 16:19:01.043: INFO: stdout: "externalname-service-5b5k6"
Aug 19 16:19:01.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-2271 exec execpod448l8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.194.57 80'
Aug 19 16:19:01.149: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.194.57 80\nConnection to 172.30.194.57 80 port [tcp/http] succeeded!\n"
Aug 19 16:19:01.149: INFO: stdout: "externalname-service-ptxsm"
Aug 19 16:19:01.149: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 16:19:01.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2271" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:9.494 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":356,"completed":250,"skipped":4525,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:19:01.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:19:01.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:19:02.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2350" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":356,"completed":251,"skipped":4538,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:19:02.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:19:02.345: INFO: The status of Pod busybox-host-aliasese0d8e44a-b7f8-430a-9db1-f61b714ee473 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:19:04.350: INFO: The status of Pod busybox-host-aliasese0d8e44a-b7f8-430a-9db1-f61b714ee473 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:19:06.350: INFO: The status of Pod busybox-host-aliasese0d8e44a-b7f8-430a-9db1-f61b714ee473 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Aug 19 16:19:06.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9658" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":252,"skipped":4554,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:19:06.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name secret-emptykey-test-1789478e-8e84-406e-a98c-7e914d9090f0
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Aug 19 16:19:06.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5272" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":356,"completed":253,"skipped":4590,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:19:06.423: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-9917
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 19 16:19:06.466: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 19 16:19:06.568: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:19:08.572: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:19:10.575: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:12.572: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:14.575: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:16.574: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:18.573: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:20.578: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:22.575: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:24.577: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:26.572: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:28.576: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 19 16:19:28.580: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 19 16:19:28.586: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 19 16:19:32.637: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 19 16:19:32.637: INFO: Going to poll 10.128.2.121 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 19 16:19:32.639: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.2.121:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9917 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 16:19:32.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 16:19:32.639: INFO: ExecWithOptions: Clientset creation
Aug 19 16:19:32.639: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-9917/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.2.121%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 19 16:19:32.729: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 19 16:19:32.729: INFO: Going to poll 10.129.2.146 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 19 16:19:32.732: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.129.2.146:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9917 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 16:19:32.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 16:19:32.732: INFO: ExecWithOptions: Clientset creation
Aug 19 16:19:32.732: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-9917/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.129.2.146%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 19 16:19:32.836: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 19 16:19:32.836: INFO: Going to poll 10.131.1.64 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 19 16:19:32.839: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.131.1.64:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9917 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 16:19:32.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 16:19:32.840: INFO: ExecWithOptions: Clientset creation
Aug 19 16:19:32.840: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-9917/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.131.1.64%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 19 16:19:32.914: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Aug 19 16:19:32.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9917" for this suite.

• [SLOW TEST:26.504 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":254,"skipped":4598,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:19:32.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-6666
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 19 16:19:32.956: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 19 16:19:33.060: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:19:35.068: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:19:37.068: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:39.090: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:41.068: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:43.066: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:45.066: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:47.065: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:49.071: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:51.085: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:53.064: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 19 16:19:55.065: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 19 16:19:55.070: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 19 16:19:55.074: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 19 16:19:59.102: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 19 16:19:59.102: INFO: Breadth first check of 10.128.2.122 on host 10.0.131.169...
Aug 19 16:19:59.105: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.67:9080/dial?request=hostname&protocol=http&host=10.128.2.122&port=8083&tries=1'] Namespace:pod-network-test-6666 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 16:19:59.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 16:19:59.105: INFO: ExecWithOptions: Clientset creation
Aug 19 16:19:59.105: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-6666/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.1.67%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.2.122%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 19 16:19:59.184: INFO: Waiting for responses: map[]
Aug 19 16:19:59.184: INFO: reached 10.128.2.122 after 0/1 tries
Aug 19 16:19:59.184: INFO: Breadth first check of 10.129.2.147 on host 10.0.157.99...
Aug 19 16:19:59.187: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.67:9080/dial?request=hostname&protocol=http&host=10.129.2.147&port=8083&tries=1'] Namespace:pod-network-test-6666 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 16:19:59.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 16:19:59.187: INFO: ExecWithOptions: Clientset creation
Aug 19 16:19:59.187: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-6666/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.1.67%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.129.2.147%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 19 16:19:59.258: INFO: Waiting for responses: map[]
Aug 19 16:19:59.258: INFO: reached 10.129.2.147 after 0/1 tries
Aug 19 16:19:59.258: INFO: Breadth first check of 10.131.1.66 on host 10.0.164.144...
Aug 19 16:19:59.261: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.67:9080/dial?request=hostname&protocol=http&host=10.131.1.66&port=8083&tries=1'] Namespace:pod-network-test-6666 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 16:19:59.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 16:19:59.261: INFO: ExecWithOptions: Clientset creation
Aug 19 16:19:59.261: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-6666/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.1.67%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.131.1.66%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 19 16:19:59.315: INFO: Waiting for responses: map[]
Aug 19 16:19:59.315: INFO: reached 10.131.1.66 after 0/1 tries
Aug 19 16:19:59.315: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Aug 19 16:19:59.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6666" for this suite.

• [SLOW TEST:26.398 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":356,"completed":255,"skipped":4599,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:19:59.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-fd2000f3-4f0b-4034-bed6-4dcecb7e4da4
STEP: Creating a pod to test consume configMaps
Aug 19 16:19:59.411: INFO: Waiting up to 5m0s for pod "pod-configmaps-1fdb5762-4451-4e24-af68-7168c0936929" in namespace "configmap-6961" to be "Succeeded or Failed"
Aug 19 16:19:59.415: INFO: Pod "pod-configmaps-1fdb5762-4451-4e24-af68-7168c0936929": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891991ms
Aug 19 16:20:01.421: INFO: Pod "pod-configmaps-1fdb5762-4451-4e24-af68-7168c0936929": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009909692s
Aug 19 16:20:03.428: INFO: Pod "pod-configmaps-1fdb5762-4451-4e24-af68-7168c0936929": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016369094s
Aug 19 16:20:05.432: INFO: Pod "pod-configmaps-1fdb5762-4451-4e24-af68-7168c0936929": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020562077s
STEP: Saw pod success
Aug 19 16:20:05.432: INFO: Pod "pod-configmaps-1fdb5762-4451-4e24-af68-7168c0936929" satisfied condition "Succeeded or Failed"
Aug 19 16:20:05.435: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-configmaps-1fdb5762-4451-4e24-af68-7168c0936929 container agnhost-container: <nil>
STEP: delete the pod
Aug 19 16:20:05.455: INFO: Waiting for pod pod-configmaps-1fdb5762-4451-4e24-af68-7168c0936929 to disappear
Aug 19 16:20:05.457: INFO: Pod pod-configmaps-1fdb5762-4451-4e24-af68-7168c0936929 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 19 16:20:05.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6961" for this suite.

• [SLOW TEST:6.139 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":256,"skipped":4610,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:20:05.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:20:05.524: INFO: The status of Pod busybox-scheduling-8a2ce3ac-c2c9-4365-86c8-0007ea638717 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:20:07.529: INFO: The status of Pod busybox-scheduling-8a2ce3ac-c2c9-4365-86c8-0007ea638717 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:20:09.532: INFO: The status of Pod busybox-scheduling-8a2ce3ac-c2c9-4365-86c8-0007ea638717 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Aug 19 16:20:09.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4692" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":356,"completed":257,"skipped":4619,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:20:09.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 19 16:20:26.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5160" for this suite.

• [SLOW TEST:17.098 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":356,"completed":258,"skipped":4630,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:20:26.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 19 16:20:27.004: INFO: Pod name wrapped-volume-race-232459d8-1ed6-4874-bc66-6cd65cf36daa: Found 0 pods out of 5
Aug 19 16:20:32.015: INFO: Pod name wrapped-volume-race-232459d8-1ed6-4874-bc66-6cd65cf36daa: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-232459d8-1ed6-4874-bc66-6cd65cf36daa in namespace emptydir-wrapper-5547, will wait for the garbage collector to delete the pods
Aug 19 16:20:32.094: INFO: Deleting ReplicationController wrapped-volume-race-232459d8-1ed6-4874-bc66-6cd65cf36daa took: 9.851471ms
Aug 19 16:20:32.194: INFO: Terminating ReplicationController wrapped-volume-race-232459d8-1ed6-4874-bc66-6cd65cf36daa pods took: 100.255934ms
STEP: Creating RC which spawns configmap-volume pods
Aug 19 16:20:33.400: INFO: Pod name wrapped-volume-race-16b27f6f-8533-4d88-b4c9-af9b1c0e1ff0: Found 2 pods out of 5
Aug 19 16:20:38.406: INFO: Pod name wrapped-volume-race-16b27f6f-8533-4d88-b4c9-af9b1c0e1ff0: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-16b27f6f-8533-4d88-b4c9-af9b1c0e1ff0 in namespace emptydir-wrapper-5547, will wait for the garbage collector to delete the pods
Aug 19 16:20:38.480: INFO: Deleting ReplicationController wrapped-volume-race-16b27f6f-8533-4d88-b4c9-af9b1c0e1ff0 took: 5.375239ms
Aug 19 16:20:38.580: INFO: Terminating ReplicationController wrapped-volume-race-16b27f6f-8533-4d88-b4c9-af9b1c0e1ff0 pods took: 100.226464ms
STEP: Creating RC which spawns configmap-volume pods
Aug 19 16:20:40.301: INFO: Pod name wrapped-volume-race-e4cd1bc6-ca8c-40c2-85be-29880b0a66d1: Found 0 pods out of 5
Aug 19 16:20:45.311: INFO: Pod name wrapped-volume-race-e4cd1bc6-ca8c-40c2-85be-29880b0a66d1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e4cd1bc6-ca8c-40c2-85be-29880b0a66d1 in namespace emptydir-wrapper-5547, will wait for the garbage collector to delete the pods
Aug 19 16:20:45.390: INFO: Deleting ReplicationController wrapped-volume-race-e4cd1bc6-ca8c-40c2-85be-29880b0a66d1 took: 5.315406ms
Aug 19 16:20:45.490: INFO: Terminating ReplicationController wrapped-volume-race-e4cd1bc6-ca8c-40c2-85be-29880b0a66d1 pods took: 100.759956ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Aug 19 16:20:47.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5547" for this suite.

• [SLOW TEST:20.707 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":356,"completed":259,"skipped":4632,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:20:47.358: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:20:47.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Aug 19 16:20:54.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-7768 --namespace=crd-publish-openapi-7768 create -f -'
Aug 19 16:20:55.233: INFO: stderr: ""
Aug 19 16:20:55.233: INFO: stdout: "e2e-test-crd-publish-openapi-425-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 19 16:20:55.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-7768 --namespace=crd-publish-openapi-7768 delete e2e-test-crd-publish-openapi-425-crds test-cr'
Aug 19 16:20:55.282: INFO: stderr: ""
Aug 19 16:20:55.282: INFO: stdout: "e2e-test-crd-publish-openapi-425-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 19 16:20:55.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-7768 --namespace=crd-publish-openapi-7768 apply -f -'
Aug 19 16:20:56.041: INFO: stderr: ""
Aug 19 16:20:56.041: INFO: stdout: "e2e-test-crd-publish-openapi-425-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 19 16:20:56.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-7768 --namespace=crd-publish-openapi-7768 delete e2e-test-crd-publish-openapi-425-crds test-cr'
Aug 19 16:20:56.124: INFO: stderr: ""
Aug 19 16:20:56.124: INFO: stdout: "e2e-test-crd-publish-openapi-425-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Aug 19 16:20:56.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=crd-publish-openapi-7768 explain e2e-test-crd-publish-openapi-425-crds'
Aug 19 16:20:57.310: INFO: stderr: ""
Aug 19 16:20:57.310: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-425-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:21:03.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7768" for this suite.

• [SLOW TEST:16.315 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":356,"completed":260,"skipped":4656,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:21:03.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a collection of services
Aug 19 16:21:03.711: INFO: Creating e2e-svc-a-w25kk
Aug 19 16:21:03.731: INFO: Creating e2e-svc-b-ln6h8
Aug 19 16:21:03.746: INFO: Creating e2e-svc-c-zbfxc
STEP: deleting service collection
Aug 19 16:21:03.830: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 16:21:03.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2189" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":356,"completed":261,"skipped":4660,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:21:03.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Aug 19 16:21:09.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3267" for this suite.

• [SLOW TEST:6.106 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":356,"completed":262,"skipped":4678,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:21:09.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-3537/configmap-test-be031c14-0009-429a-9f07-82166d153202
STEP: Creating a pod to test consume configMaps
Aug 19 16:21:10.005: INFO: Waiting up to 5m0s for pod "pod-configmaps-067aa0e0-7b39-458b-b2c0-e29ad4bf8d5b" in namespace "configmap-3537" to be "Succeeded or Failed"
Aug 19 16:21:10.016: INFO: Pod "pod-configmaps-067aa0e0-7b39-458b-b2c0-e29ad4bf8d5b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.421933ms
Aug 19 16:21:12.021: INFO: Pod "pod-configmaps-067aa0e0-7b39-458b-b2c0-e29ad4bf8d5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015518596s
Aug 19 16:21:14.025: INFO: Pod "pod-configmaps-067aa0e0-7b39-458b-b2c0-e29ad4bf8d5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019437458s
STEP: Saw pod success
Aug 19 16:21:14.025: INFO: Pod "pod-configmaps-067aa0e0-7b39-458b-b2c0-e29ad4bf8d5b" satisfied condition "Succeeded or Failed"
Aug 19 16:21:14.028: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-configmaps-067aa0e0-7b39-458b-b2c0-e29ad4bf8d5b container env-test: <nil>
STEP: delete the pod
Aug 19 16:21:14.049: INFO: Waiting for pod pod-configmaps-067aa0e0-7b39-458b-b2c0-e29ad4bf8d5b to disappear
Aug 19 16:21:14.052: INFO: Pod pod-configmaps-067aa0e0-7b39-458b-b2c0-e29ad4bf8d5b no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Aug 19 16:21:14.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3537" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":263,"skipped":4701,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:21:14.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Aug 19 16:21:14.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1709" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":264,"skipped":4717,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:21:14.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-b5c80a92-f9fd-4d0c-90cf-728c04ec3f08
STEP: Creating a pod to test consume secrets
Aug 19 16:21:14.341: INFO: Waiting up to 5m0s for pod "pod-secrets-edd849f0-7efc-4b98-885f-c6fc8fa95c3e" in namespace "secrets-9570" to be "Succeeded or Failed"
Aug 19 16:21:14.346: INFO: Pod "pod-secrets-edd849f0-7efc-4b98-885f-c6fc8fa95c3e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.655708ms
Aug 19 16:21:16.349: INFO: Pod "pod-secrets-edd849f0-7efc-4b98-885f-c6fc8fa95c3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007953534s
Aug 19 16:21:18.356: INFO: Pod "pod-secrets-edd849f0-7efc-4b98-885f-c6fc8fa95c3e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014552982s
Aug 19 16:21:20.360: INFO: Pod "pod-secrets-edd849f0-7efc-4b98-885f-c6fc8fa95c3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018452139s
STEP: Saw pod success
Aug 19 16:21:20.360: INFO: Pod "pod-secrets-edd849f0-7efc-4b98-885f-c6fc8fa95c3e" satisfied condition "Succeeded or Failed"
Aug 19 16:21:20.362: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-secrets-edd849f0-7efc-4b98-885f-c6fc8fa95c3e container secret-volume-test: <nil>
STEP: delete the pod
Aug 19 16:21:20.384: INFO: Waiting for pod pod-secrets-edd849f0-7efc-4b98-885f-c6fc8fa95c3e to disappear
Aug 19 16:21:20.387: INFO: Pod pod-secrets-edd849f0-7efc-4b98-885f-c6fc8fa95c3e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 19 16:21:20.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9570" for this suite.

• [SLOW TEST:6.152 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":265,"skipped":4842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:21:20.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating all guestbook components
Aug 19 16:21:20.506: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 19 16:21:20.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1536 create -f -'
Aug 19 16:21:21.386: INFO: stderr: ""
Aug 19 16:21:21.386: INFO: stdout: "service/agnhost-replica created\n"
Aug 19 16:21:21.386: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 19 16:21:21.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1536 create -f -'
Aug 19 16:21:21.570: INFO: stderr: ""
Aug 19 16:21:21.570: INFO: stdout: "service/agnhost-primary created\n"
Aug 19 16:21:21.570: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 19 16:21:21.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1536 create -f -'
Aug 19 16:21:21.747: INFO: stderr: ""
Aug 19 16:21:21.747: INFO: stdout: "service/frontend created\n"
Aug 19 16:21:21.747: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.36
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 19 16:21:21.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1536 create -f -'
Aug 19 16:21:22.566: INFO: stderr: ""
Aug 19 16:21:22.566: INFO: stdout: "deployment.apps/frontend created\n"
Aug 19 16:21:22.566: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.36
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 19 16:21:22.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1536 create -f -'
Aug 19 16:21:23.570: INFO: stderr: ""
Aug 19 16:21:23.570: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 19 16:21:23.571: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.36
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 19 16:21:23.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1536 create -f -'
Aug 19 16:21:23.724: INFO: stderr: ""
Aug 19 16:21:23.724: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Aug 19 16:21:23.724: INFO: Waiting for all frontend pods to be Running.
Aug 19 16:21:28.777: INFO: Waiting for frontend to serve content.
Aug 19 16:21:28.792: INFO: Trying to add a new entry to the guestbook.
Aug 19 16:21:28.804: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 19 16:21:28.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1536 delete --grace-period=0 --force -f -'
Aug 19 16:21:28.892: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 19 16:21:28.892: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Aug 19 16:21:28.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1536 delete --grace-period=0 --force -f -'
Aug 19 16:21:28.958: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 19 16:21:28.958: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Aug 19 16:21:28.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1536 delete --grace-period=0 --force -f -'
Aug 19 16:21:29.022: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 19 16:21:29.022: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 19 16:21:29.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1536 delete --grace-period=0 --force -f -'
Aug 19 16:21:29.070: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 19 16:21:29.070: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 19 16:21:29.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1536 delete --grace-period=0 --force -f -'
Aug 19 16:21:29.130: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 19 16:21:29.130: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Aug 19 16:21:29.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1536 delete --grace-period=0 --force -f -'
Aug 19 16:21:29.172: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 19 16:21:29.172: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 16:21:29.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1536" for this suite.

• [SLOW TEST:8.756 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:340
    should create and stop a working application  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":356,"completed":266,"skipped":4873,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:21:29.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-b34dd126-05c5-4731-a34b-89c030619fad
STEP: Creating a pod to test consume configMaps
Aug 19 16:21:29.254: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-92cc0e66-dfdb-487c-8ad8-fd9551bf2744" in namespace "projected-8592" to be "Succeeded or Failed"
Aug 19 16:21:29.260: INFO: Pod "pod-projected-configmaps-92cc0e66-dfdb-487c-8ad8-fd9551bf2744": Phase="Pending", Reason="", readiness=false. Elapsed: 6.350948ms
Aug 19 16:21:31.267: INFO: Pod "pod-projected-configmaps-92cc0e66-dfdb-487c-8ad8-fd9551bf2744": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012770271s
Aug 19 16:21:33.274: INFO: Pod "pod-projected-configmaps-92cc0e66-dfdb-487c-8ad8-fd9551bf2744": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020277953s
Aug 19 16:21:35.281: INFO: Pod "pod-projected-configmaps-92cc0e66-dfdb-487c-8ad8-fd9551bf2744": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027575098s
STEP: Saw pod success
Aug 19 16:21:35.282: INFO: Pod "pod-projected-configmaps-92cc0e66-dfdb-487c-8ad8-fd9551bf2744" satisfied condition "Succeeded or Failed"
Aug 19 16:21:35.284: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-projected-configmaps-92cc0e66-dfdb-487c-8ad8-fd9551bf2744 container agnhost-container: <nil>
STEP: delete the pod
Aug 19 16:21:35.325: INFO: Waiting for pod pod-projected-configmaps-92cc0e66-dfdb-487c-8ad8-fd9551bf2744 to disappear
Aug 19 16:21:35.329: INFO: Pod pod-projected-configmaps-92cc0e66-dfdb-487c-8ad8-fd9551bf2744 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 19 16:21:35.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8592" for this suite.

• [SLOW TEST:6.159 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":267,"skipped":4909,"failed":0}
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:21:35.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0819 16:21:35.392113      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-user-65534-99a9b45c-c26f-4a8c-b360-6ae6e678126a" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-user-65534-99a9b45c-c26f-4a8c-b360-6ae6e678126a" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "busybox-user-65534-99a9b45c-c26f-4a8c-b360-6ae6e678126a" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 16:21:35.392: INFO: Waiting up to 5m0s for pod "busybox-user-65534-99a9b45c-c26f-4a8c-b360-6ae6e678126a" in namespace "security-context-test-890" to be "Succeeded or Failed"
Aug 19 16:21:35.395: INFO: Pod "busybox-user-65534-99a9b45c-c26f-4a8c-b360-6ae6e678126a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.959549ms
Aug 19 16:21:37.402: INFO: Pod "busybox-user-65534-99a9b45c-c26f-4a8c-b360-6ae6e678126a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010533388s
Aug 19 16:21:39.406: INFO: Pod "busybox-user-65534-99a9b45c-c26f-4a8c-b360-6ae6e678126a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014324265s
Aug 19 16:21:41.413: INFO: Pod "busybox-user-65534-99a9b45c-c26f-4a8c-b360-6ae6e678126a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02106982s
Aug 19 16:21:41.413: INFO: Pod "busybox-user-65534-99a9b45c-c26f-4a8c-b360-6ae6e678126a" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 19 16:21:41.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-890" for this suite.

• [SLOW TEST:6.086 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:52
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":268,"skipped":4909,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:21:41.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0819 16:21:41.489826      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 19 16:22:41.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1256" for this suite.

• [SLOW TEST:60.079 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":356,"completed":269,"skipped":4923,"failed":0}
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:22:41.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Aug 19 16:22:41.581: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:22:43.589: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:22:45.589: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.164.144 on the node which pod1 resides and expect scheduled
Aug 19 16:22:45.605: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:22:47.611: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:22:49.610: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.164.144 but use UDP protocol on the node which pod2 resides
Aug 19 16:22:49.624: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:22:51.631: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:22:53.633: INFO: The status of Pod pod3 is Running (Ready = true)
Aug 19 16:22:53.651: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:22:55.658: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Aug 19 16:22:55.661: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.164.144 http://127.0.0.1:54323/hostname] Namespace:hostport-9272 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 16:22:55.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 16:22:55.662: INFO: ExecWithOptions: Clientset creation
Aug 19 16:22:55.662: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-9272/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.164.144+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.164.144, port: 54323
Aug 19 16:22:55.773: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.164.144:54323/hostname] Namespace:hostport-9272 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 16:22:55.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 16:22:55.773: INFO: ExecWithOptions: Clientset creation
Aug 19 16:22:55.773: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-9272/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.164.144%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.164.144, port: 54323 UDP
Aug 19 16:22:55.867: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.164.144 54323] Namespace:hostport-9272 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 19 16:22:55.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 16:22:55.867: INFO: ExecWithOptions: Clientset creation
Aug 19 16:22:55.867: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-9272/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+10.0.164.144+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:188
Aug 19 16:23:00.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-9272" for this suite.

• [SLOW TEST:19.473 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":356,"completed":270,"skipped":4923,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:23:00.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-34389535-8527-4abd-8716-6659d721c38f
STEP: Creating a pod to test consume configMaps
Aug 19 16:23:01.065: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8dd344ea-a367-404d-a64b-bc62181a2d01" in namespace "projected-2000" to be "Succeeded or Failed"
Aug 19 16:23:01.088: INFO: Pod "pod-projected-configmaps-8dd344ea-a367-404d-a64b-bc62181a2d01": Phase="Pending", Reason="", readiness=false. Elapsed: 23.441562ms
Aug 19 16:23:03.096: INFO: Pod "pod-projected-configmaps-8dd344ea-a367-404d-a64b-bc62181a2d01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031898308s
Aug 19 16:23:05.100: INFO: Pod "pod-projected-configmaps-8dd344ea-a367-404d-a64b-bc62181a2d01": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035877732s
Aug 19 16:23:07.105: INFO: Pod "pod-projected-configmaps-8dd344ea-a367-404d-a64b-bc62181a2d01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040497006s
STEP: Saw pod success
Aug 19 16:23:07.105: INFO: Pod "pod-projected-configmaps-8dd344ea-a367-404d-a64b-bc62181a2d01" satisfied condition "Succeeded or Failed"
Aug 19 16:23:07.115: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-projected-configmaps-8dd344ea-a367-404d-a64b-bc62181a2d01 container agnhost-container: <nil>
STEP: delete the pod
Aug 19 16:23:07.140: INFO: Waiting for pod pod-projected-configmaps-8dd344ea-a367-404d-a64b-bc62181a2d01 to disappear
Aug 19 16:23:07.142: INFO: Pod pod-projected-configmaps-8dd344ea-a367-404d-a64b-bc62181a2d01 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 19 16:23:07.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2000" for this suite.

• [SLOW TEST:6.171 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":271,"skipped":4925,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:23:07.153: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Aug 19 16:23:09.247: INFO: running pods: 0 < 3
Aug 19 16:23:11.253: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Aug 19 16:23:13.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3646" for this suite.

• [SLOW TEST:6.116 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":356,"completed":272,"skipped":4972,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:23:13.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 19 16:23:13.326: INFO: Waiting up to 5m0s for pod "pod-1c6adc3a-6d0b-4266-afbe-c758c651b25c" in namespace "emptydir-9895" to be "Succeeded or Failed"
Aug 19 16:23:13.329: INFO: Pod "pod-1c6adc3a-6d0b-4266-afbe-c758c651b25c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.378781ms
Aug 19 16:23:15.336: INFO: Pod "pod-1c6adc3a-6d0b-4266-afbe-c758c651b25c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010624731s
Aug 19 16:23:17.343: INFO: Pod "pod-1c6adc3a-6d0b-4266-afbe-c758c651b25c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016946619s
Aug 19 16:23:19.349: INFO: Pod "pod-1c6adc3a-6d0b-4266-afbe-c758c651b25c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022928545s
STEP: Saw pod success
Aug 19 16:23:19.349: INFO: Pod "pod-1c6adc3a-6d0b-4266-afbe-c758c651b25c" satisfied condition "Succeeded or Failed"
Aug 19 16:23:19.351: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-1c6adc3a-6d0b-4266-afbe-c758c651b25c container test-container: <nil>
STEP: delete the pod
Aug 19 16:23:19.371: INFO: Waiting for pod pod-1c6adc3a-6d0b-4266-afbe-c758c651b25c to disappear
Aug 19 16:23:19.375: INFO: Pod pod-1c6adc3a-6d0b-4266-afbe-c758c651b25c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 19 16:23:19.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9895" for this suite.

• [SLOW TEST:6.118 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":273,"skipped":4986,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:23:19.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Starting the proxy
Aug 19 16:23:19.419: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-984 proxy --unix-socket=/tmp/kubectl-proxy-unix3087985975/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 16:23:19.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-984" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":356,"completed":274,"skipped":4990,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:23:19.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:23:19.508: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-a18e7226-989f-4b5b-8210-5a9066645cb0
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 19 16:23:23.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5201" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":275,"skipped":5005,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:23:23.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 19 16:23:23.605: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 19 16:23:23.615: INFO: Waiting for terminating namespaces to be deleted...
Aug 19 16:23:23.633: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-131-169.ec2.internal before test
Aug 19 16:23:23.672: INFO: aws-ebs-csi-driver-node-zp4zp from openshift-cluster-csi-drivers started at 2022-08-19 14:38:55 +0000 UTC (3 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container csi-driver ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 19 16:23:23.672: INFO: tuned-7vdfg from openshift-cluster-node-tuning-operator started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container tuned ready: true, restart count 0
Aug 19 16:23:23.672: INFO: downloads-858cc8f4cb-zkm66 from openshift-console started at 2022-08-19 14:47:16 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container download-server ready: true, restart count 0
Aug 19 16:23:23.672: INFO: dns-default-zk2kr from openshift-dns started at 2022-08-19 14:39:21 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container dns ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.672: INFO: node-resolver-ttx27 from openshift-dns started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 19 16:23:23.672: INFO: image-registry-79dbb9c69c-2qmtr from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container registry ready: true, restart count 0
Aug 19 16:23:23.672: INFO: node-ca-qqztn from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container node-ca ready: true, restart count 0
Aug 19 16:23:23.672: INFO: ingress-canary-lr86r from openshift-ingress-canary started at 2022-08-19 14:40:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 19 16:23:23.672: INFO: router-default-7664744558-bndls from openshift-ingress started at 2022-08-19 14:40:03 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container router ready: true, restart count 0
Aug 19 16:23:23.672: INFO: machine-config-daemon-gpvls from openshift-machine-config-operator started at 2022-08-19 14:38:55 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 16:23:23.672: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-08-19 15:46:54 +0000 UTC (6 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container alertmanager ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 16:23:23.672: INFO: node-exporter-tv7s8 from openshift-monitoring started at 2022-08-19 14:41:12 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container node-exporter ready: true, restart count 0
Aug 19 16:23:23.672: INFO: prometheus-adapter-85bd9549d5-h6mlp from openshift-monitoring started at 2022-08-19 14:42:01 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 19 16:23:23.672: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-08-19 15:46:55 +0000 UTC (6 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container prometheus ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 19 16:23:23.672: INFO: prometheus-operator-admission-webhook-6db58c58f7-2922r from openshift-monitoring started at 2022-08-19 14:39:23 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 19 16:23:23.672: INFO: thanos-querier-8559769b94-jjnm2 from openshift-monitoring started at 2022-08-19 15:10:44 +0000 UTC (6 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container thanos-query ready: true, restart count 0
Aug 19 16:23:23.672: INFO: multus-additional-cni-plugins-2fqhw from openshift-multus started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 19 16:23:23.672: INFO: multus-pss88 from openshift-multus started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container kube-multus ready: true, restart count 0
Aug 19 16:23:23.672: INFO: network-metrics-daemon-bkf7f from openshift-multus started at 2022-08-19 14:38:55 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 19 16:23:23.672: INFO: network-check-target-zpzkf from openshift-network-diagnostics started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 19 16:23:23.672: INFO: collect-profiles-27682065-grb4q from openshift-operator-lifecycle-manager started at 2022-08-19 15:45:00 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 19 16:23:23.672: INFO: collect-profiles-27682080-d2bbw from openshift-operator-lifecycle-manager started at 2022-08-19 16:00:00 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 19 16:23:23.672: INFO: collect-profiles-27682095-mbvlr from openshift-operator-lifecycle-manager started at 2022-08-19 16:15:00 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 19 16:23:23.672: INFO: sdn-vjrw2 from openshift-sdn started at 2022-08-19 14:38:55 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container sdn ready: true, restart count 0
Aug 19 16:23:23.672: INFO: sonobuoy from sonobuoy started at 2022-08-19 14:57:28 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 19 16:23:23.672: INFO: sonobuoy-e2e-job-b3deceaf6b87402a from sonobuoy started at 2022-08-19 14:57:31 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container e2e ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 16:23:23.672: INFO: sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-57h77 from sonobuoy started at 2022-08-19 14:57:31 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.672: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 19 16:23:23.672: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-157-99.ec2.internal before test
Aug 19 16:23:23.698: INFO: aws-ebs-csi-driver-node-sknln from openshift-cluster-csi-drivers started at 2022-08-19 14:39:05 +0000 UTC (3 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container csi-driver ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 19 16:23:23.698: INFO: tuned-ttgq4 from openshift-cluster-node-tuning-operator started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container tuned ready: true, restart count 0
Aug 19 16:23:23.698: INFO: downloads-858cc8f4cb-57s2s from openshift-console started at 2022-08-19 15:46:52 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container download-server ready: true, restart count 0
Aug 19 16:23:23.698: INFO: dns-default-m65sc from openshift-dns started at 2022-08-19 14:40:04 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container dns ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.698: INFO: node-resolver-dzbsb from openshift-dns started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 19 16:23:23.698: INFO: image-registry-79dbb9c69c-bbrd6 from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container registry ready: true, restart count 0
Aug 19 16:23:23.698: INFO: node-ca-2jbtx from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container node-ca ready: true, restart count 0
Aug 19 16:23:23.698: INFO: ingress-canary-cmqqt from openshift-ingress-canary started at 2022-08-19 14:40:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 19 16:23:23.698: INFO: router-default-7664744558-px7pp from openshift-ingress started at 2022-08-19 15:10:43 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container router ready: true, restart count 0
Aug 19 16:23:23.698: INFO: machine-config-daemon-zvdcx from openshift-machine-config-operator started at 2022-08-19 14:39:05 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 16:23:23.698: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-08-19 14:48:20 +0000 UTC (6 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container alertmanager ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 16:23:23.698: INFO: kube-state-metrics-9f5df78c9-s6s4x from openshift-monitoring started at 2022-08-19 14:41:11 +0000 UTC (3 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 19 16:23:23.698: INFO: node-exporter-zw8vw from openshift-monitoring started at 2022-08-19 14:41:11 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container node-exporter ready: true, restart count 0
Aug 19 16:23:23.698: INFO: openshift-state-metrics-6c88b54494-79p9n from openshift-monitoring started at 2022-08-19 14:41:11 +0000 UTC (3 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 19 16:23:23.698: INFO: prometheus-adapter-85bd9549d5-nnh58 from openshift-monitoring started at 2022-08-19 14:42:01 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 19 16:23:23.698: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-08-19 14:47:30 +0000 UTC (6 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container prometheus ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 19 16:23:23.698: INFO: prometheus-operator-admission-webhook-6db58c58f7-khv8v from openshift-monitoring started at 2022-08-19 15:10:44 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 19 16:23:23.698: INFO: telemeter-client-86d58945d5-4tqj6 from openshift-monitoring started at 2022-08-19 14:42:06 +0000 UTC (3 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container reload ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 19 16:23:23.698: INFO: thanos-querier-8559769b94-2pf4w from openshift-monitoring started at 2022-08-19 14:41:18 +0000 UTC (6 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container thanos-query ready: true, restart count 0
Aug 19 16:23:23.698: INFO: multus-additional-cni-plugins-n26hm from openshift-multus started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 19 16:23:23.698: INFO: multus-w6shh from openshift-multus started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container kube-multus ready: true, restart count 0
Aug 19 16:23:23.698: INFO: network-metrics-daemon-jhrx8 from openshift-multus started at 2022-08-19 14:39:05 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 19 16:23:23.698: INFO: network-check-source-5c64cf6958-bl5pp from openshift-network-diagnostics started at 2022-08-19 15:46:52 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 19 16:23:23.698: INFO: network-check-target-xmjhc from openshift-network-diagnostics started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 19 16:23:23.698: INFO: sdn-m6b96 from openshift-sdn started at 2022-08-19 14:39:05 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container sdn ready: true, restart count 0
Aug 19 16:23:23.698: INFO: sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-fh9k8 from sonobuoy started at 2022-08-19 14:57:32 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.698: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 19 16:23:23.698: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-164-144.ec2.internal before test
Aug 19 16:23:23.720: INFO: pod-configmaps-6d8c111c-1fe6-442c-8543-3b2ddd2ea887 from configmap-5201 started at 2022-08-19 16:23:19 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.720: INFO: 	Container agnhost-container ready: true, restart count 0
Aug 19 16:23:23.720: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
Aug 19 16:23:23.720: INFO: aws-ebs-csi-driver-node-l9xm2 from openshift-cluster-csi-drivers started at 2022-08-19 14:38:49 +0000 UTC (3 container statuses recorded)
Aug 19 16:23:23.720: INFO: 	Container csi-driver ready: true, restart count 0
Aug 19 16:23:23.720: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Aug 19 16:23:23.720: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 19 16:23:23.720: INFO: tuned-f4rpl from openshift-cluster-node-tuning-operator started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.720: INFO: 	Container tuned ready: true, restart count 0
Aug 19 16:23:23.720: INFO: dns-default-jpsrv from openshift-dns started at 2022-08-19 15:47:18 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.720: INFO: 	Container dns ready: true, restart count 0
Aug 19 16:23:23.720: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.720: INFO: node-resolver-pmmzs from openshift-dns started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.720: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 19 16:23:23.720: INFO: node-ca-xt2pz from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.720: INFO: 	Container node-ca ready: true, restart count 0
Aug 19 16:23:23.720: INFO: ingress-canary-97q7v from openshift-ingress-canary started at 2022-08-19 15:47:18 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.720: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 19 16:23:23.720: INFO: machine-config-daemon-xz9st from openshift-machine-config-operator started at 2022-08-19 14:38:49 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.720: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 19 16:23:23.720: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 16:23:23.720: INFO: node-exporter-64n9p from openshift-monitoring started at 2022-08-19 14:41:12 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.720: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.720: INFO: 	Container node-exporter ready: true, restart count 0
Aug 19 16:23:23.720: INFO: multus-85nv9 from openshift-multus started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.720: INFO: 	Container kube-multus ready: true, restart count 0
Aug 19 16:23:23.720: INFO: multus-additional-cni-plugins-9kc7x from openshift-multus started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.720: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 19 16:23:23.720: INFO: network-metrics-daemon-skfmf from openshift-multus started at 2022-08-19 14:38:49 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.720: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.720: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 19 16:23:23.720: INFO: network-check-target-vklmp from openshift-network-diagnostics started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 16:23:23.720: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 19 16:23:23.720: INFO: sdn-zfgs6 from openshift-sdn started at 2022-08-19 14:38:49 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.720: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:23:23.720: INFO: 	Container sdn ready: true, restart count 0
Aug 19 16:23:23.720: INFO: sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-x4rlh from sonobuoy started at 2022-08-19 14:57:32 +0000 UTC (2 container statuses recorded)
Aug 19 16:23:23.720: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 16:23:23.720: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.170ccb95181c40e1], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 node(s) didn't match Pod's node affinity/selector. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Aug 19 16:23:24.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2661" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":356,"completed":276,"skipped":5035,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:23:24.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-tgr5
STEP: Creating a pod to test atomic-volume-subpath
Aug 19 16:23:24.895: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tgr5" in namespace "subpath-2472" to be "Succeeded or Failed"
Aug 19 16:23:24.908: INFO: Pod "pod-subpath-test-configmap-tgr5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.342342ms
Aug 19 16:23:26.912: INFO: Pod "pod-subpath-test-configmap-tgr5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016813012s
Aug 19 16:23:28.922: INFO: Pod "pod-subpath-test-configmap-tgr5": Phase="Running", Reason="", readiness=true. Elapsed: 4.026851553s
Aug 19 16:23:30.927: INFO: Pod "pod-subpath-test-configmap-tgr5": Phase="Running", Reason="", readiness=true. Elapsed: 6.031536755s
Aug 19 16:23:32.935: INFO: Pod "pod-subpath-test-configmap-tgr5": Phase="Running", Reason="", readiness=true. Elapsed: 8.03970836s
Aug 19 16:23:34.943: INFO: Pod "pod-subpath-test-configmap-tgr5": Phase="Running", Reason="", readiness=true. Elapsed: 10.047079545s
Aug 19 16:23:36.947: INFO: Pod "pod-subpath-test-configmap-tgr5": Phase="Running", Reason="", readiness=true. Elapsed: 12.051538598s
Aug 19 16:23:38.960: INFO: Pod "pod-subpath-test-configmap-tgr5": Phase="Running", Reason="", readiness=true. Elapsed: 14.064703027s
Aug 19 16:23:40.965: INFO: Pod "pod-subpath-test-configmap-tgr5": Phase="Running", Reason="", readiness=true. Elapsed: 16.069325954s
Aug 19 16:23:42.971: INFO: Pod "pod-subpath-test-configmap-tgr5": Phase="Running", Reason="", readiness=true. Elapsed: 18.075698214s
Aug 19 16:23:44.979: INFO: Pod "pod-subpath-test-configmap-tgr5": Phase="Running", Reason="", readiness=true. Elapsed: 20.083585662s
Aug 19 16:23:46.985: INFO: Pod "pod-subpath-test-configmap-tgr5": Phase="Running", Reason="", readiness=true. Elapsed: 22.089205308s
Aug 19 16:23:48.990: INFO: Pod "pod-subpath-test-configmap-tgr5": Phase="Running", Reason="", readiness=false. Elapsed: 24.094414491s
Aug 19 16:23:50.996: INFO: Pod "pod-subpath-test-configmap-tgr5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.100193727s
STEP: Saw pod success
Aug 19 16:23:50.996: INFO: Pod "pod-subpath-test-configmap-tgr5" satisfied condition "Succeeded or Failed"
Aug 19 16:23:50.999: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-subpath-test-configmap-tgr5 container test-container-subpath-configmap-tgr5: <nil>
STEP: delete the pod
Aug 19 16:23:51.021: INFO: Waiting for pod pod-subpath-test-configmap-tgr5 to disappear
Aug 19 16:23:51.023: INFO: Pod pod-subpath-test-configmap-tgr5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tgr5
Aug 19 16:23:51.023: INFO: Deleting pod "pod-subpath-test-configmap-tgr5" in namespace "subpath-2472"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Aug 19 16:23:51.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2472" for this suite.

• [SLOW TEST:26.208 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","total":356,"completed":277,"skipped":5035,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:23:51.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 19 16:23:51.111: INFO: Waiting up to 5m0s for pod "pod-ce9494aa-6a4a-418c-89bb-f20fb2f4bd81" in namespace "emptydir-6946" to be "Succeeded or Failed"
Aug 19 16:23:51.127: INFO: Pod "pod-ce9494aa-6a4a-418c-89bb-f20fb2f4bd81": Phase="Pending", Reason="", readiness=false. Elapsed: 15.339705ms
Aug 19 16:23:53.132: INFO: Pod "pod-ce9494aa-6a4a-418c-89bb-f20fb2f4bd81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020944717s
Aug 19 16:23:55.141: INFO: Pod "pod-ce9494aa-6a4a-418c-89bb-f20fb2f4bd81": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029336794s
Aug 19 16:23:57.147: INFO: Pod "pod-ce9494aa-6a4a-418c-89bb-f20fb2f4bd81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035324158s
STEP: Saw pod success
Aug 19 16:23:57.147: INFO: Pod "pod-ce9494aa-6a4a-418c-89bb-f20fb2f4bd81" satisfied condition "Succeeded or Failed"
Aug 19 16:23:57.149: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-ce9494aa-6a4a-418c-89bb-f20fb2f4bd81 container test-container: <nil>
STEP: delete the pod
Aug 19 16:23:57.168: INFO: Waiting for pod pod-ce9494aa-6a4a-418c-89bb-f20fb2f4bd81 to disappear
Aug 19 16:23:57.172: INFO: Pod pod-ce9494aa-6a4a-418c-89bb-f20fb2f4bd81 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 19 16:23:57.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6946" for this suite.

• [SLOW TEST:6.147 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":278,"skipped":5040,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:23:57.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
W0819 16:23:57.218185      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Aug 19 16:23:57.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8428" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":356,"completed":279,"skipped":5054,"failed":0}
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:23:57.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Aug 19 16:23:57.395: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:23:59.399: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:24:01.403: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Aug 19 16:24:01.420: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:24:03.425: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:24:05.427: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Aug 19 16:24:05.439: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 19 16:24:05.441: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 19 16:24:07.441: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 19 16:24:07.446: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Aug 19 16:24:07.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6955" for this suite.

• [SLOW TEST:10.188 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":356,"completed":280,"skipped":5058,"failed":0}
SS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:24:07.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
Aug 19 16:24:07.518: INFO: created test-event-1
Aug 19 16:24:07.524: INFO: created test-event-2
Aug 19 16:24:07.545: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Aug 19 16:24:07.548: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Aug 19 16:24:07.617: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Aug 19 16:24:07.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7216" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":356,"completed":281,"skipped":5060,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:24:07.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8997
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8997
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8997
Aug 19 16:24:07.714: INFO: Found 0 stateful pods, waiting for 1
Aug 19 16:24:17.721: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 19 16:24:17.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-8997 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 19 16:24:17.845: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 19 16:24:17.845: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 19 16:24:17.845: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 19 16:24:17.848: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 19 16:24:27.856: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 19 16:24:27.856: INFO: Waiting for statefulset status.replicas updated to 0
Aug 19 16:24:27.871: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999856s
Aug 19 16:24:28.876: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996967704s
Aug 19 16:24:29.881: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992513725s
Aug 19 16:24:30.885: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987624182s
Aug 19 16:24:31.891: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982966123s
Aug 19 16:24:32.896: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977414794s
Aug 19 16:24:33.900: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972618806s
Aug 19 16:24:34.905: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.967957389s
Aug 19 16:24:35.910: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.962967596s
Aug 19 16:24:36.916: INFO: Verifying statefulset ss doesn't scale past 1 for another 957.968336ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8997
Aug 19 16:24:37.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-8997 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 19 16:24:38.049: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 19 16:24:38.049: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 19 16:24:38.049: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 19 16:24:38.052: INFO: Found 1 stateful pods, waiting for 3
Aug 19 16:24:48.060: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 19 16:24:48.060: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 19 16:24:48.060: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 19 16:24:48.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-8997 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 19 16:24:48.186: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 19 16:24:48.186: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 19 16:24:48.186: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 19 16:24:48.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-8997 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 19 16:24:48.302: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 19 16:24:48.302: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 19 16:24:48.302: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 19 16:24:48.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-8997 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 19 16:24:48.423: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 19 16:24:48.423: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 19 16:24:48.423: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 19 16:24:48.423: INFO: Waiting for statefulset status.replicas updated to 0
Aug 19 16:24:48.427: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Aug 19 16:24:58.436: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 19 16:24:58.436: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 19 16:24:58.436: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 19 16:24:58.450: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999986s
Aug 19 16:24:59.456: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995955967s
Aug 19 16:25:00.460: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990702323s
Aug 19 16:25:01.466: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985703587s
Aug 19 16:25:02.478: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980682355s
Aug 19 16:25:03.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968697198s
Aug 19 16:25:04.490: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.96160675s
Aug 19 16:25:05.495: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.956754158s
Aug 19 16:25:06.501: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.950692995s
Aug 19 16:25:07.506: INFO: Verifying statefulset ss doesn't scale past 3 for another 944.711422ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8997
Aug 19 16:25:08.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-8997 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 19 16:25:08.635: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 19 16:25:08.635: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 19 16:25:08.635: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 19 16:25:08.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-8997 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 19 16:25:08.754: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 19 16:25:08.754: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 19 16:25:08.754: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 19 16:25:08.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=statefulset-8997 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 19 16:25:08.870: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 19 16:25:08.870: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 19 16:25:08.870: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 19 16:25:08.870: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 19 16:25:18.891: INFO: Deleting all statefulset in ns statefulset-8997
Aug 19 16:25:18.894: INFO: Scaling statefulset ss to 0
Aug 19 16:25:18.907: INFO: Waiting for statefulset status.replicas updated to 0
Aug 19 16:25:18.909: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 19 16:25:18.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8997" for this suite.

• [SLOW TEST:71.298 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":356,"completed":282,"skipped":5061,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:25:18.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Aug 19 16:25:18.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2113 create -f -'
Aug 19 16:25:19.138: INFO: stderr: ""
Aug 19 16:25:19.138: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 19 16:25:20.142: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 19 16:25:20.142: INFO: Found 0 / 1
Aug 19 16:25:21.143: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 19 16:25:21.143: INFO: Found 0 / 1
Aug 19 16:25:22.143: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 19 16:25:22.143: INFO: Found 1 / 1
Aug 19 16:25:22.143: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 19 16:25:22.145: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 19 16:25:22.145: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 19 16:25:22.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-2113 patch pod agnhost-primary-5zbcx -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 19 16:25:22.202: INFO: stderr: ""
Aug 19 16:25:22.202: INFO: stdout: "pod/agnhost-primary-5zbcx patched\n"
STEP: checking annotations
Aug 19 16:25:22.205: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 19 16:25:22.205: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 16:25:22.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2113" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":356,"completed":283,"skipped":5085,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:25:22.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: reading a file in the container
Aug 19 16:25:26.298: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3182 pod-service-account-33224854-704e-4056-8da0-1e37e548bfbd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 19 16:25:26.414: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3182 pod-service-account-33224854-704e-4056-8da0-1e37e548bfbd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 19 16:25:26.519: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3182 pod-service-account-33224854-704e-4056-8da0-1e37e548bfbd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Aug 19 16:25:26.634: INFO: Got root ca configmap in namespace "svcaccounts-3182"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 19 16:25:26.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3182" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":356,"completed":284,"skipped":5119,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:25:26.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Aug 19 16:25:26.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 16:25:33.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:25:56.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2318" for this suite.

• [SLOW TEST:30.173 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":356,"completed":285,"skipped":5147,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:25:56.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 16:25:57.222: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 19 16:25:59.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 16, 25, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 25, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 25, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 25, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 16:26:02.252: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:26:02.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8773" for this suite.
STEP: Destroying namespace "webhook-8773-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.610 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":356,"completed":286,"skipped":5160,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:26:02.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 16:26:02.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4634" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":356,"completed":287,"skipped":5176,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:26:02.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:26:02.669: INFO: The status of Pod pod-secrets-beaf8dcc-9e02-4af5-a1df-a5d52d804062 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:26:04.673: INFO: The status of Pod pod-secrets-beaf8dcc-9e02-4af5-a1df-a5d52d804062 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:26:06.674: INFO: The status of Pod pod-secrets-beaf8dcc-9e02-4af5-a1df-a5d52d804062 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Aug 19 16:26:06.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3784" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":356,"completed":288,"skipped":5182,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:26:06.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-7121/configmap-test-b90f5043-ca89-4dfe-8d1d-98a3d852ff27
STEP: Creating a pod to test consume configMaps
Aug 19 16:26:06.803: INFO: Waiting up to 5m0s for pod "pod-configmaps-7930f0e6-f1ab-44e8-b18d-857fd8db7f02" in namespace "configmap-7121" to be "Succeeded or Failed"
Aug 19 16:26:06.811: INFO: Pod "pod-configmaps-7930f0e6-f1ab-44e8-b18d-857fd8db7f02": Phase="Pending", Reason="", readiness=false. Elapsed: 7.081468ms
Aug 19 16:26:08.814: INFO: Pod "pod-configmaps-7930f0e6-f1ab-44e8-b18d-857fd8db7f02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010914345s
Aug 19 16:26:10.819: INFO: Pod "pod-configmaps-7930f0e6-f1ab-44e8-b18d-857fd8db7f02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015006874s
STEP: Saw pod success
Aug 19 16:26:10.819: INFO: Pod "pod-configmaps-7930f0e6-f1ab-44e8-b18d-857fd8db7f02" satisfied condition "Succeeded or Failed"
Aug 19 16:26:10.822: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-configmaps-7930f0e6-f1ab-44e8-b18d-857fd8db7f02 container env-test: <nil>
STEP: delete the pod
Aug 19 16:26:10.846: INFO: Waiting for pod pod-configmaps-7930f0e6-f1ab-44e8-b18d-857fd8db7f02 to disappear
Aug 19 16:26:10.848: INFO: Pod pod-configmaps-7930f0e6-f1ab-44e8-b18d-857fd8db7f02 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Aug 19 16:26:10.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7121" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":356,"completed":289,"skipped":5191,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:26:10.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Aug 19 16:26:10.943: INFO: The status of Pod labelsupdate4386f0ff-5a67-42d0-965e-461b47d50d4a is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:26:12.947: INFO: The status of Pod labelsupdate4386f0ff-5a67-42d0-965e-461b47d50d4a is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:26:14.948: INFO: The status of Pod labelsupdate4386f0ff-5a67-42d0-965e-461b47d50d4a is Running (Ready = true)
Aug 19 16:26:15.477: INFO: Successfully updated pod "labelsupdate4386f0ff-5a67-42d0-965e-461b47d50d4a"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 19 16:26:17.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4508" for this suite.

• [SLOW TEST:6.643 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":290,"skipped":5193,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:26:17.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-1bfbcc77-11c3-4154-b575-dcf2223fb922 in namespace container-probe-1445
Aug 19 16:26:21.574: INFO: Started pod liveness-1bfbcc77-11c3-4154-b575-dcf2223fb922 in namespace container-probe-1445
STEP: checking the pod's current state and verifying that restartCount is present
Aug 19 16:26:21.577: INFO: Initial restart count of pod liveness-1bfbcc77-11c3-4154-b575-dcf2223fb922 is 0
Aug 19 16:26:39.639: INFO: Restart count of pod container-probe-1445/liveness-1bfbcc77-11c3-4154-b575-dcf2223fb922 is now 1 (18.061671112s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 19 16:26:39.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1445" for this suite.

• [SLOW TEST:22.162 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":291,"skipped":5208,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:26:39.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0819 16:26:45.733422      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0819 16:26:45.733441      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 19 16:26:45.733: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 19 16:26:45.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1375" for this suite.

• [SLOW TEST:6.080 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":356,"completed":292,"skipped":5227,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:26:45.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 19 16:26:58.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5467" for this suite.

• [SLOW TEST:13.125 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":356,"completed":293,"skipped":5238,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:26:58.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 19 16:26:58.947: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:26:58.947: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:26:58.947: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:26:58.953: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 16:26:58.953: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:26:59.958: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:26:59.958: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:26:59.958: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:26:59.963: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 16:26:59.963: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:27:00.959: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:00.959: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:00.959: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:00.962: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 16:27:00.962: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:27:01.958: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:01.958: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:01.958: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:01.961: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 19 16:27:01.961: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 19 16:27:01.978: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:01.978: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:01.978: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:01.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 19 16:27:01.981: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:27:02.986: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:02.986: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:02.986: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:02.990: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 19 16:27:02.990: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:27:03.987: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:03.987: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:03.987: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:03.991: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 19 16:27:03.991: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:27:04.987: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:04.987: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:04.987: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:04.990: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 19 16:27:04.990: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:27:05.986: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:05.986: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:05.986: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:05.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 19 16:27:05.989: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:27:06.986: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:06.986: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:06.986: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:06.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 19 16:27:06.989: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:27:07.988: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:07.988: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:07.988: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:27:07.991: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 19 16:27:07.991: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6484, will wait for the garbage collector to delete the pods
Aug 19 16:27:08.054: INFO: Deleting DaemonSet.extensions daemon-set took: 7.125566ms
Aug 19 16:27:08.155: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.29665ms
Aug 19 16:27:10.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 16:27:10.559: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 19 16:27:10.562: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"97026"},"items":null}

Aug 19 16:27:10.565: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"97026"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 19 16:27:10.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6484" for this suite.

• [SLOW TEST:11.717 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":356,"completed":294,"skipped":5244,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:27:10.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
W0819 16:27:10.652253      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 16:27:10.656: INFO: The status of Pod annotationupdate18192fc1-5a08-4cc8-9557-0d47f1fd3718 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:27:12.662: INFO: The status of Pod annotationupdate18192fc1-5a08-4cc8-9557-0d47f1fd3718 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:27:14.661: INFO: The status of Pod annotationupdate18192fc1-5a08-4cc8-9557-0d47f1fd3718 is Running (Ready = true)
Aug 19 16:27:15.187: INFO: Successfully updated pod "annotationupdate18192fc1-5a08-4cc8-9557-0d47f1fd3718"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 19 16:27:17.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3558" for this suite.

• [SLOW TEST:6.632 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":295,"skipped":5255,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:27:17.219: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 19 16:27:17.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4517" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":296,"skipped":5288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:27:17.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 19 16:27:17.361: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 19 16:28:17.518: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Aug 19 16:28:17.563: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 19 16:28:17.584: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 19 16:28:17.626: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 19 16:28:17.641: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 19 16:28:17.670: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 19 16:28:17.685: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
W0819 16:28:27.719939      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "critical-pod" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "critical-pod" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "critical-pod" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "critical-pod" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Aug 19 16:28:33.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3097" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:76.519 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":356,"completed":297,"skipped":5315,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:28:33.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:28:33.892: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-3309fa45-df7b-42e8-9ee7-4e1c89afe3d8" in namespace "security-context-test-5462" to be "Succeeded or Failed"
Aug 19 16:28:33.903: INFO: Pod "busybox-readonly-false-3309fa45-df7b-42e8-9ee7-4e1c89afe3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.178565ms
Aug 19 16:28:35.907: INFO: Pod "busybox-readonly-false-3309fa45-df7b-42e8-9ee7-4e1c89afe3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01433567s
Aug 19 16:28:37.910: INFO: Pod "busybox-readonly-false-3309fa45-df7b-42e8-9ee7-4e1c89afe3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017864861s
Aug 19 16:28:39.915: INFO: Pod "busybox-readonly-false-3309fa45-df7b-42e8-9ee7-4e1c89afe3d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022149493s
Aug 19 16:28:39.915: INFO: Pod "busybox-readonly-false-3309fa45-df7b-42e8-9ee7-4e1c89afe3d8" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 19 16:28:39.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5462" for this suite.

• [SLOW TEST:6.099 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:173
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":356,"completed":298,"skipped":5343,"failed":0}
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:28:39.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:28:39.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 19 16:28:39.990: INFO: The status of Pod pod-exec-websocket-9488043f-0c18-4aa1-badf-d1c597104c19 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:28:41.995: INFO: The status of Pod pod-exec-websocket-9488043f-0c18-4aa1-badf-d1c597104c19 is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:28:43.994: INFO: The status of Pod pod-exec-websocket-9488043f-0c18-4aa1-badf-d1c597104c19 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 19 16:28:44.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5342" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":356,"completed":299,"skipped":5343,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:28:44.059: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 19 16:28:44.115: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a2cff49-1f8e-402c-b8c8-c24add5d8097" in namespace "downward-api-722" to be "Succeeded or Failed"
Aug 19 16:28:44.118: INFO: Pod "downwardapi-volume-5a2cff49-1f8e-402c-b8c8-c24add5d8097": Phase="Pending", Reason="", readiness=false. Elapsed: 3.592566ms
Aug 19 16:28:46.123: INFO: Pod "downwardapi-volume-5a2cff49-1f8e-402c-b8c8-c24add5d8097": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008486317s
Aug 19 16:28:48.127: INFO: Pod "downwardapi-volume-5a2cff49-1f8e-402c-b8c8-c24add5d8097": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01247844s
Aug 19 16:28:50.132: INFO: Pod "downwardapi-volume-5a2cff49-1f8e-402c-b8c8-c24add5d8097": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017247123s
STEP: Saw pod success
Aug 19 16:28:50.132: INFO: Pod "downwardapi-volume-5a2cff49-1f8e-402c-b8c8-c24add5d8097" satisfied condition "Succeeded or Failed"
Aug 19 16:28:50.135: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-5a2cff49-1f8e-402c-b8c8-c24add5d8097 container client-container: <nil>
STEP: delete the pod
Aug 19 16:28:50.158: INFO: Waiting for pod downwardapi-volume-5a2cff49-1f8e-402c-b8c8-c24add5d8097 to disappear
Aug 19 16:28:50.161: INFO: Pod downwardapi-volume-5a2cff49-1f8e-402c-b8c8-c24add5d8097 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 19 16:28:50.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-722" for this suite.

• [SLOW TEST:6.114 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":300,"skipped":5346,"failed":0}
SSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:28:50.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-projected-all-test-volume-bd5d0a64-d32a-4fda-9918-ad59f7f924b4
STEP: Creating secret with name secret-projected-all-test-volume-46294c59-76c5-4cfa-b9f4-7213e9adf198
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 19 16:28:50.291: INFO: Waiting up to 5m0s for pod "projected-volume-3ffdb98f-e10a-47a9-b080-f192ece152f6" in namespace "projected-4751" to be "Succeeded or Failed"
Aug 19 16:28:50.296: INFO: Pod "projected-volume-3ffdb98f-e10a-47a9-b080-f192ece152f6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.036608ms
Aug 19 16:28:52.300: INFO: Pod "projected-volume-3ffdb98f-e10a-47a9-b080-f192ece152f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008940025s
Aug 19 16:28:54.304: INFO: Pod "projected-volume-3ffdb98f-e10a-47a9-b080-f192ece152f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012912532s
Aug 19 16:28:56.311: INFO: Pod "projected-volume-3ffdb98f-e10a-47a9-b080-f192ece152f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019748402s
STEP: Saw pod success
Aug 19 16:28:56.311: INFO: Pod "projected-volume-3ffdb98f-e10a-47a9-b080-f192ece152f6" satisfied condition "Succeeded or Failed"
Aug 19 16:28:56.314: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod projected-volume-3ffdb98f-e10a-47a9-b080-f192ece152f6 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 19 16:28:56.335: INFO: Waiting for pod projected-volume-3ffdb98f-e10a-47a9-b080-f192ece152f6 to disappear
Aug 19 16:28:56.338: INFO: Pod projected-volume-3ffdb98f-e10a-47a9-b080-f192ece152f6 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:188
Aug 19 16:28:56.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4751" for this suite.

• [SLOW TEST:6.174 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":356,"completed":301,"skipped":5349,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:28:56.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:28:56.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:29:02.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4233" for this suite.

• [SLOW TEST:6.657 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":356,"completed":302,"skipped":5395,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:29:03.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 19 16:29:03.185: INFO: Waiting up to 5m0s for pod "pod-683e7044-f919-4013-a580-5182d7f89ef8" in namespace "emptydir-5079" to be "Succeeded or Failed"
Aug 19 16:29:03.204: INFO: Pod "pod-683e7044-f919-4013-a580-5182d7f89ef8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.815347ms
Aug 19 16:29:05.209: INFO: Pod "pod-683e7044-f919-4013-a580-5182d7f89ef8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023673053s
Aug 19 16:29:07.214: INFO: Pod "pod-683e7044-f919-4013-a580-5182d7f89ef8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02851143s
Aug 19 16:29:09.218: INFO: Pod "pod-683e7044-f919-4013-a580-5182d7f89ef8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032823744s
STEP: Saw pod success
Aug 19 16:29:09.218: INFO: Pod "pod-683e7044-f919-4013-a580-5182d7f89ef8" satisfied condition "Succeeded or Failed"
Aug 19 16:29:09.221: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-683e7044-f919-4013-a580-5182d7f89ef8 container test-container: <nil>
STEP: delete the pod
Aug 19 16:29:09.241: INFO: Waiting for pod pod-683e7044-f919-4013-a580-5182d7f89ef8 to disappear
Aug 19 16:29:09.244: INFO: Pod pod-683e7044-f919-4013-a580-5182d7f89ef8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 19 16:29:09.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5079" for this suite.

• [SLOW TEST:6.250 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":303,"skipped":5432,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:29:09.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 19 16:29:09.323: INFO: Waiting up to 5m0s for pod "pod-ab2af9fc-627f-49db-b5c3-66e52177746a" in namespace "emptydir-6453" to be "Succeeded or Failed"
Aug 19 16:29:09.331: INFO: Pod "pod-ab2af9fc-627f-49db-b5c3-66e52177746a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.612841ms
Aug 19 16:29:11.336: INFO: Pod "pod-ab2af9fc-627f-49db-b5c3-66e52177746a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012930223s
Aug 19 16:29:13.340: INFO: Pod "pod-ab2af9fc-627f-49db-b5c3-66e52177746a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017270466s
Aug 19 16:29:15.345: INFO: Pod "pod-ab2af9fc-627f-49db-b5c3-66e52177746a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022121557s
STEP: Saw pod success
Aug 19 16:29:15.345: INFO: Pod "pod-ab2af9fc-627f-49db-b5c3-66e52177746a" satisfied condition "Succeeded or Failed"
Aug 19 16:29:15.348: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-ab2af9fc-627f-49db-b5c3-66e52177746a container test-container: <nil>
STEP: delete the pod
Aug 19 16:29:15.372: INFO: Waiting for pod pod-ab2af9fc-627f-49db-b5c3-66e52177746a to disappear
Aug 19 16:29:15.376: INFO: Pod pod-ab2af9fc-627f-49db-b5c3-66e52177746a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 19 16:29:15.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6453" for this suite.

• [SLOW TEST:6.133 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":304,"skipped":5435,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:29:15.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Aug 19 16:29:15.451: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 19 16:29:15.451: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 19 16:29:15.472: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 19 16:29:15.472: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 19 16:29:15.492: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 19 16:29:15.492: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 19 16:29:15.543: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 19 16:29:15.543: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 19 16:29:17.707: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 19 16:29:17.707: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 19 16:29:18.360: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Aug 19 16:29:18.369: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Aug 19 16:29:18.370: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0
Aug 19 16:29:18.370: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0
Aug 19 16:29:18.370: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0
Aug 19 16:29:18.370: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0
Aug 19 16:29:18.370: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0
Aug 19 16:29:18.370: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0
Aug 19 16:29:18.370: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0
Aug 19 16:29:18.370: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 0
Aug 19 16:29:18.370: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1
Aug 19 16:29:18.370: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1
Aug 19 16:29:18.370: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2
Aug 19 16:29:18.370: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2
Aug 19 16:29:18.370: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2
Aug 19 16:29:18.370: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2
Aug 19 16:29:18.381: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2
Aug 19 16:29:18.381: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2
Aug 19 16:29:18.397: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2
Aug 19 16:29:18.397: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2
Aug 19 16:29:18.409: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1
Aug 19 16:29:18.409: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1
Aug 19 16:29:18.422: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1
Aug 19 16:29:18.423: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1
Aug 19 16:29:21.398: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2
Aug 19 16:29:21.398: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2
Aug 19 16:29:21.417: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1
STEP: listing Deployments
Aug 19 16:29:21.423: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Aug 19 16:29:21.432: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Aug 19 16:29:21.440: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 19 16:29:21.445: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 19 16:29:21.462: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 19 16:29:21.475: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 19 16:29:21.491: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 19 16:29:21.499: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 19 16:29:24.389: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 19 16:29:24.408: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 19 16:29:24.427: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 19 16:29:26.761: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Aug 19 16:29:26.792: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1
Aug 19 16:29:26.792: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1
Aug 19 16:29:26.792: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1
Aug 19 16:29:26.792: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1
Aug 19 16:29:26.792: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1
Aug 19 16:29:26.792: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 1
Aug 19 16:29:26.792: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2
Aug 19 16:29:26.792: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2
Aug 19 16:29:26.792: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 2
Aug 19 16:29:26.792: INFO: observed Deployment test-deployment in namespace deployment-3052 with ReadyReplicas 3
STEP: deleting the Deployment
Aug 19 16:29:26.801: INFO: observed event type MODIFIED
Aug 19 16:29:26.801: INFO: observed event type MODIFIED
Aug 19 16:29:26.801: INFO: observed event type MODIFIED
Aug 19 16:29:26.801: INFO: observed event type MODIFIED
Aug 19 16:29:26.801: INFO: observed event type MODIFIED
Aug 19 16:29:26.801: INFO: observed event type MODIFIED
Aug 19 16:29:26.801: INFO: observed event type MODIFIED
Aug 19 16:29:26.801: INFO: observed event type MODIFIED
Aug 19 16:29:26.801: INFO: observed event type MODIFIED
Aug 19 16:29:26.801: INFO: observed event type MODIFIED
Aug 19 16:29:26.801: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 19 16:29:26.805: INFO: Log out all the ReplicaSets if there is no deployment created
Aug 19 16:29:26.808: INFO: ReplicaSet "test-deployment-6bdc46c995":
&ReplicaSet{ObjectMeta:{test-deployment-6bdc46c995  deployment-3052  512002ae-7846-48d6-89ac-2fbc22091909 98822 3 2022-08-19 16:29:15 +0000 UTC <nil> <nil> map[pod-template-hash:6bdc46c995 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 2d4244b9-c34c-4c0d-89e4-16b574ed1cef 0xc005df5ae7 0xc005df5ae8}] []  [{kube-controller-manager Update apps/v1 2022-08-19 16:29:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d4244b9-c34c-4c0d-89e4-16b574ed1cef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 16:29:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6bdc46c995,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6bdc46c995 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005df5c10 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 19 16:29:26.811: INFO: ReplicaSet "test-deployment-74c6dd549b":
&ReplicaSet{ObjectMeta:{test-deployment-74c6dd549b  deployment-3052  0ae2d1b4-5c3e-4021-9038-2c446812c2de 98928 2 2022-08-19 16:29:21 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 2d4244b9-c34c-4c0d-89e4-16b574ed1cef 0xc005df5ca7 0xc005df5ca8}] []  [{kube-controller-manager Update apps/v1 2022-08-19 16:29:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d4244b9-c34c-4c0d-89e4-16b574ed1cef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 16:29:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 74c6dd549b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005df5d30 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Aug 19 16:29:26.814: INFO: pod: "test-deployment-74c6dd549b-9lbnx":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-9lbnx test-deployment-74c6dd549b- deployment-3052  ce4282a1-87e5-40e4-98a4-1454a3768184 98927 0 2022-08-19 16:29:24 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.199"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.199"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-74c6dd549b 0ae2d1b4-5c3e-4021-9038-2c446812c2de 0xc002488517 0xc002488518}] []  [{kube-controller-manager Update v1 2022-08-19 16:29:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ae2d1b4-5c3e-4021-9038-2c446812c2de\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-08-19 16:29:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-19 16:29:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.199\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2sgq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2sgq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-157-99.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nzcvd,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:29:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:29:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:29:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.157.99,PodIP:10.129.2.199,StartTime:2022-08-19 16:29:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 16:29:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://bc4ab8921bb0b184e61d0a50f58f76d7d84369005505cfa6945a7a50d04ad77a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.199,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 19 16:29:26.814: INFO: pod: "test-deployment-74c6dd549b-wkbpd":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-wkbpd test-deployment-74c6dd549b- deployment-3052  4bbb278c-e7ee-4f29-981e-388a3b78676c 98890 0 2022-08-19 16:29:21 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.1.145"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.1.145"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-74c6dd549b 0ae2d1b4-5c3e-4021-9038-2c446812c2de 0xc002488767 0xc002488768}] []  [{kube-controller-manager Update v1 2022-08-19 16:29:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ae2d1b4-5c3e-4021-9038-2c446812c2de\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-08-19 16:29:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-19 16:29:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w5qkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w5qkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-164-144.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nzcvd,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:29:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:29:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:29:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.164.144,PodIP:10.131.1.145,StartTime:2022-08-19 16:29:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 16:29:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://cc2dd5e90cb96dea81742ae7c16ac2921e38923071e57c8aaff65ed56a01b4ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 19 16:29:26.814: INFO: ReplicaSet "test-deployment-84b949bdfc":
&ReplicaSet{ObjectMeta:{test-deployment-84b949bdfc  deployment-3052  050e0f79-8b9d-4e3e-b590-d4554853122b 98936 4 2022-08-19 16:29:18 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 2d4244b9-c34c-4c0d-89e4-16b574ed1cef 0xc005df5dc7 0xc005df5dc8}] []  [{kube-controller-manager Update apps/v1 2022-08-19 16:29:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d4244b9-c34c-4c0d-89e4-16b574ed1cef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-19 16:29:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 84b949bdfc,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.7 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005df5f20 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 19 16:29:26.818: INFO: pod: "test-deployment-84b949bdfc-qc9pr":
&Pod{ObjectMeta:{test-deployment-84b949bdfc-qc9pr test-deployment-84b949bdfc- deployment-3052  51c8a030-1b59-46f9-9f20-882543149995 98932 0 2022-08-19 16:29:18 +0000 UTC 2022-08-19 16:29:27 +0000 UTC 0xc009e85af8 map[pod-template-hash:84b949bdfc test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.1.144"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.1.144"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-84b949bdfc 050e0f79-8b9d-4e3e-b590-d4554853122b 0xc009e85b37 0xc009e85b38}] []  [{kube-controller-manager Update v1 2022-08-19 16:29:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"050e0f79-8b9d-4e3e-b590-d4554853122b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-08-19 16:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-19 16:29:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bvkwq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.7,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bvkwq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-164-144.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nzcvd,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:29:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-19 16:29:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.164.144,PodIP:10.131.1.144,StartTime:2022-08-19 16:29:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-19 16:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.7,ImageID:k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c,ContainerID:cri-o://9380c60dda8950512813cf863adb0da5c92c90265146f50e608df66205952d12,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 19 16:29:26.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3052" for this suite.

• [SLOW TEST:11.441 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":356,"completed":305,"skipped":5442,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:29:26.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-0782afb0-a20c-4c3e-963a-33dc030509ff
STEP: Creating a pod to test consume configMaps
Aug 19 16:29:26.897: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3a7de883-fe95-4166-9529-26221d1bb3df" in namespace "projected-1915" to be "Succeeded or Failed"
Aug 19 16:29:26.900: INFO: Pod "pod-projected-configmaps-3a7de883-fe95-4166-9529-26221d1bb3df": Phase="Pending", Reason="", readiness=false. Elapsed: 3.368603ms
Aug 19 16:29:28.906: INFO: Pod "pod-projected-configmaps-3a7de883-fe95-4166-9529-26221d1bb3df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008895803s
Aug 19 16:29:30.910: INFO: Pod "pod-projected-configmaps-3a7de883-fe95-4166-9529-26221d1bb3df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013525628s
Aug 19 16:29:32.914: INFO: Pod "pod-projected-configmaps-3a7de883-fe95-4166-9529-26221d1bb3df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017389471s
STEP: Saw pod success
Aug 19 16:29:32.914: INFO: Pod "pod-projected-configmaps-3a7de883-fe95-4166-9529-26221d1bb3df" satisfied condition "Succeeded or Failed"
Aug 19 16:29:32.924: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-projected-configmaps-3a7de883-fe95-4166-9529-26221d1bb3df container agnhost-container: <nil>
STEP: delete the pod
Aug 19 16:29:32.951: INFO: Waiting for pod pod-projected-configmaps-3a7de883-fe95-4166-9529-26221d1bb3df to disappear
Aug 19 16:29:32.954: INFO: Pod pod-projected-configmaps-3a7de883-fe95-4166-9529-26221d1bb3df no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 19 16:29:32.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1915" for this suite.

• [SLOW TEST:6.137 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":306,"skipped":5471,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:29:32.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:29:32.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:29:33.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3977" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":356,"completed":307,"skipped":5472,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:29:33.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 19 16:29:33.804: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 19 16:30:33.958: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Aug 19 16:30:33.987: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 19 16:30:34.002: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 19 16:30:34.027: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 19 16:30:34.039: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 19 16:30:34.059: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 19 16:30:34.083: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Aug 19 16:30:42.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2473" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:68.575 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":356,"completed":308,"skipped":5517,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:30:42.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Aug 19 16:31:02.463: INFO: EndpointSlice for Service endpointslice-770/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Aug 19 16:31:12.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-770" for this suite.

• [SLOW TEST:30.259 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":356,"completed":309,"skipped":5558,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:31:12.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 19 16:31:12.516: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 19 16:31:12.533: INFO: Waiting for terminating namespaces to be deleted...
Aug 19 16:31:12.538: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-131-169.ec2.internal before test
Aug 19 16:31:12.563: INFO: aws-ebs-csi-driver-node-zp4zp from openshift-cluster-csi-drivers started at 2022-08-19 14:38:55 +0000 UTC (3 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container csi-driver ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 19 16:31:12.563: INFO: tuned-7vdfg from openshift-cluster-node-tuning-operator started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container tuned ready: true, restart count 0
Aug 19 16:31:12.563: INFO: downloads-858cc8f4cb-zkm66 from openshift-console started at 2022-08-19 14:47:16 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container download-server ready: true, restart count 0
Aug 19 16:31:12.563: INFO: dns-default-zk2kr from openshift-dns started at 2022-08-19 14:39:21 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container dns ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.563: INFO: node-resolver-ttx27 from openshift-dns started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 19 16:31:12.563: INFO: image-registry-79dbb9c69c-2qmtr from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container registry ready: true, restart count 0
Aug 19 16:31:12.563: INFO: node-ca-qqztn from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container node-ca ready: true, restart count 0
Aug 19 16:31:12.563: INFO: ingress-canary-lr86r from openshift-ingress-canary started at 2022-08-19 14:40:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 19 16:31:12.563: INFO: router-default-7664744558-bndls from openshift-ingress started at 2022-08-19 14:40:03 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container router ready: true, restart count 0
Aug 19 16:31:12.563: INFO: machine-config-daemon-gpvls from openshift-machine-config-operator started at 2022-08-19 14:38:55 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 16:31:12.563: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-08-19 15:46:54 +0000 UTC (6 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container alertmanager ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 16:31:12.563: INFO: node-exporter-tv7s8 from openshift-monitoring started at 2022-08-19 14:41:12 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container node-exporter ready: true, restart count 0
Aug 19 16:31:12.563: INFO: prometheus-adapter-85bd9549d5-h6mlp from openshift-monitoring started at 2022-08-19 14:42:01 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 19 16:31:12.563: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-08-19 15:46:55 +0000 UTC (6 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container prometheus ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 19 16:31:12.563: INFO: prometheus-operator-admission-webhook-6db58c58f7-2922r from openshift-monitoring started at 2022-08-19 14:39:23 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 19 16:31:12.563: INFO: thanos-querier-8559769b94-jjnm2 from openshift-monitoring started at 2022-08-19 15:10:44 +0000 UTC (6 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container thanos-query ready: true, restart count 0
Aug 19 16:31:12.563: INFO: multus-additional-cni-plugins-2fqhw from openshift-multus started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 19 16:31:12.563: INFO: multus-pss88 from openshift-multus started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container kube-multus ready: true, restart count 0
Aug 19 16:31:12.563: INFO: network-metrics-daemon-bkf7f from openshift-multus started at 2022-08-19 14:38:55 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 19 16:31:12.563: INFO: network-check-target-zpzkf from openshift-network-diagnostics started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 19 16:31:12.563: INFO: collect-profiles-27682080-d2bbw from openshift-operator-lifecycle-manager started at 2022-08-19 16:00:00 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 19 16:31:12.563: INFO: collect-profiles-27682095-mbvlr from openshift-operator-lifecycle-manager started at 2022-08-19 16:15:00 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 19 16:31:12.563: INFO: collect-profiles-27682110-fr5jb from openshift-operator-lifecycle-manager started at 2022-08-19 16:30:00 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 19 16:31:12.563: INFO: sdn-vjrw2 from openshift-sdn started at 2022-08-19 14:38:55 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container sdn ready: true, restart count 0
Aug 19 16:31:12.563: INFO: sonobuoy from sonobuoy started at 2022-08-19 14:57:28 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 19 16:31:12.563: INFO: sonobuoy-e2e-job-b3deceaf6b87402a from sonobuoy started at 2022-08-19 14:57:31 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container e2e ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 16:31:12.563: INFO: sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-57h77 from sonobuoy started at 2022-08-19 14:57:31 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.563: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 19 16:31:12.563: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-157-99.ec2.internal before test
Aug 19 16:31:12.591: INFO: aws-ebs-csi-driver-node-sknln from openshift-cluster-csi-drivers started at 2022-08-19 14:39:05 +0000 UTC (3 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container csi-driver ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 19 16:31:12.591: INFO: tuned-ttgq4 from openshift-cluster-node-tuning-operator started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container tuned ready: true, restart count 0
Aug 19 16:31:12.591: INFO: downloads-858cc8f4cb-57s2s from openshift-console started at 2022-08-19 15:46:52 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container download-server ready: true, restart count 0
Aug 19 16:31:12.591: INFO: dns-default-m65sc from openshift-dns started at 2022-08-19 14:40:04 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container dns ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.591: INFO: node-resolver-dzbsb from openshift-dns started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 19 16:31:12.591: INFO: image-registry-79dbb9c69c-bbrd6 from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container registry ready: true, restart count 0
Aug 19 16:31:12.591: INFO: node-ca-2jbtx from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container node-ca ready: true, restart count 0
Aug 19 16:31:12.591: INFO: ingress-canary-cmqqt from openshift-ingress-canary started at 2022-08-19 14:40:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 19 16:31:12.591: INFO: router-default-7664744558-px7pp from openshift-ingress started at 2022-08-19 15:10:43 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container router ready: true, restart count 0
Aug 19 16:31:12.591: INFO: machine-config-daemon-zvdcx from openshift-machine-config-operator started at 2022-08-19 14:39:05 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 16:31:12.591: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-08-19 14:48:20 +0000 UTC (6 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container alertmanager ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 16:31:12.591: INFO: kube-state-metrics-9f5df78c9-s6s4x from openshift-monitoring started at 2022-08-19 14:41:11 +0000 UTC (3 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 19 16:31:12.591: INFO: node-exporter-zw8vw from openshift-monitoring started at 2022-08-19 14:41:11 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container node-exporter ready: true, restart count 0
Aug 19 16:31:12.591: INFO: openshift-state-metrics-6c88b54494-79p9n from openshift-monitoring started at 2022-08-19 14:41:11 +0000 UTC (3 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 19 16:31:12.591: INFO: prometheus-adapter-85bd9549d5-nnh58 from openshift-monitoring started at 2022-08-19 14:42:01 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 19 16:31:12.591: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-08-19 14:47:30 +0000 UTC (6 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container prometheus ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 19 16:31:12.591: INFO: prometheus-operator-admission-webhook-6db58c58f7-khv8v from openshift-monitoring started at 2022-08-19 15:10:44 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 19 16:31:12.591: INFO: telemeter-client-86d58945d5-4tqj6 from openshift-monitoring started at 2022-08-19 14:42:06 +0000 UTC (3 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container reload ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 19 16:31:12.591: INFO: thanos-querier-8559769b94-2pf4w from openshift-monitoring started at 2022-08-19 14:41:18 +0000 UTC (6 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container thanos-query ready: true, restart count 0
Aug 19 16:31:12.591: INFO: multus-additional-cni-plugins-n26hm from openshift-multus started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 19 16:31:12.591: INFO: multus-w6shh from openshift-multus started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container kube-multus ready: true, restart count 0
Aug 19 16:31:12.591: INFO: network-metrics-daemon-jhrx8 from openshift-multus started at 2022-08-19 14:39:05 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 19 16:31:12.591: INFO: network-check-source-5c64cf6958-bl5pp from openshift-network-diagnostics started at 2022-08-19 15:46:52 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 19 16:31:12.591: INFO: network-check-target-xmjhc from openshift-network-diagnostics started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 19 16:31:12.591: INFO: sdn-m6b96 from openshift-sdn started at 2022-08-19 14:39:05 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container sdn ready: true, restart count 0
Aug 19 16:31:12.591: INFO: sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-fh9k8 from sonobuoy started at 2022-08-19 14:57:32 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.591: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 19 16:31:12.591: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-164-144.ec2.internal before test
Aug 19 16:31:12.614: INFO: pod1 from endpointslice-770 started at 2022-08-19 16:30:42 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container container1 ready: true, restart count 0
Aug 19 16:31:12.614: INFO: pod2 from endpointslice-770 started at 2022-08-19 16:30:42 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container container1 ready: true, restart count 0
Aug 19 16:31:12.614: INFO: aws-ebs-csi-driver-node-l9xm2 from openshift-cluster-csi-drivers started at 2022-08-19 14:38:49 +0000 UTC (3 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container csi-driver ready: true, restart count 0
Aug 19 16:31:12.614: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Aug 19 16:31:12.614: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 19 16:31:12.614: INFO: tuned-f4rpl from openshift-cluster-node-tuning-operator started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container tuned ready: true, restart count 0
Aug 19 16:31:12.614: INFO: dns-default-jpsrv from openshift-dns started at 2022-08-19 15:47:18 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container dns ready: true, restart count 0
Aug 19 16:31:12.614: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.614: INFO: node-resolver-pmmzs from openshift-dns started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 19 16:31:12.614: INFO: node-ca-xt2pz from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container node-ca ready: true, restart count 0
Aug 19 16:31:12.614: INFO: ingress-canary-97q7v from openshift-ingress-canary started at 2022-08-19 15:47:18 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 19 16:31:12.614: INFO: machine-config-daemon-xz9st from openshift-machine-config-operator started at 2022-08-19 14:38:49 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 19 16:31:12.614: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 16:31:12.614: INFO: node-exporter-64n9p from openshift-monitoring started at 2022-08-19 14:41:12 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.614: INFO: 	Container node-exporter ready: true, restart count 0
Aug 19 16:31:12.614: INFO: multus-85nv9 from openshift-multus started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container kube-multus ready: true, restart count 0
Aug 19 16:31:12.614: INFO: multus-additional-cni-plugins-9kc7x from openshift-multus started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 19 16:31:12.614: INFO: network-metrics-daemon-skfmf from openshift-multus started at 2022-08-19 14:38:49 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.614: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 19 16:31:12.614: INFO: network-check-target-vklmp from openshift-network-diagnostics started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 19 16:31:12.614: INFO: sdn-zfgs6 from openshift-sdn started at 2022-08-19 14:38:49 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:31:12.614: INFO: 	Container sdn ready: true, restart count 0
Aug 19 16:31:12.614: INFO: sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-x4rlh from sonobuoy started at 2022-08-19 14:57:32 +0000 UTC (2 container statuses recorded)
Aug 19 16:31:12.614: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 16:31:12.614: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
STEP: verifying the node has the label node ip-10-0-131-169.ec2.internal
STEP: verifying the node has the label node ip-10-0-157-99.ec2.internal
STEP: verifying the node has the label node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod pod1 requesting resource cpu=0m on Node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod pod2 requesting resource cpu=0m on Node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod aws-ebs-csi-driver-node-l9xm2 requesting resource cpu=30m on Node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod aws-ebs-csi-driver-node-sknln requesting resource cpu=30m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod aws-ebs-csi-driver-node-zp4zp requesting resource cpu=30m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod tuned-7vdfg requesting resource cpu=10m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod tuned-f4rpl requesting resource cpu=10m on Node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod tuned-ttgq4 requesting resource cpu=10m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod downloads-858cc8f4cb-57s2s requesting resource cpu=10m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod downloads-858cc8f4cb-zkm66 requesting resource cpu=10m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod dns-default-jpsrv requesting resource cpu=60m on Node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod dns-default-m65sc requesting resource cpu=60m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod dns-default-zk2kr requesting resource cpu=60m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod node-resolver-dzbsb requesting resource cpu=5m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod node-resolver-pmmzs requesting resource cpu=5m on Node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod node-resolver-ttx27 requesting resource cpu=5m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod image-registry-79dbb9c69c-2qmtr requesting resource cpu=100m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod image-registry-79dbb9c69c-bbrd6 requesting resource cpu=100m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod node-ca-2jbtx requesting resource cpu=10m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod node-ca-qqztn requesting resource cpu=10m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod node-ca-xt2pz requesting resource cpu=10m on Node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod ingress-canary-97q7v requesting resource cpu=10m on Node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod ingress-canary-cmqqt requesting resource cpu=10m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod ingress-canary-lr86r requesting resource cpu=10m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod router-default-7664744558-bndls requesting resource cpu=100m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod router-default-7664744558-px7pp requesting resource cpu=100m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod machine-config-daemon-gpvls requesting resource cpu=40m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod machine-config-daemon-xz9st requesting resource cpu=40m on Node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod machine-config-daemon-zvdcx requesting resource cpu=40m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod kube-state-metrics-9f5df78c9-s6s4x requesting resource cpu=4m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod node-exporter-64n9p requesting resource cpu=9m on Node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod node-exporter-tv7s8 requesting resource cpu=9m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod node-exporter-zw8vw requesting resource cpu=9m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod openshift-state-metrics-6c88b54494-79p9n requesting resource cpu=3m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod prometheus-adapter-85bd9549d5-h6mlp requesting resource cpu=1m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod prometheus-adapter-85bd9549d5-nnh58 requesting resource cpu=1m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod prometheus-operator-admission-webhook-6db58c58f7-2922r requesting resource cpu=5m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod prometheus-operator-admission-webhook-6db58c58f7-khv8v requesting resource cpu=5m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod telemeter-client-86d58945d5-4tqj6 requesting resource cpu=3m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod thanos-querier-8559769b94-2pf4w requesting resource cpu=15m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod thanos-querier-8559769b94-jjnm2 requesting resource cpu=15m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod multus-85nv9 requesting resource cpu=10m on Node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod multus-additional-cni-plugins-2fqhw requesting resource cpu=10m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod multus-additional-cni-plugins-9kc7x requesting resource cpu=10m on Node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod multus-additional-cni-plugins-n26hm requesting resource cpu=10m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod multus-pss88 requesting resource cpu=10m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod multus-w6shh requesting resource cpu=10m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod network-metrics-daemon-bkf7f requesting resource cpu=20m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod network-metrics-daemon-jhrx8 requesting resource cpu=20m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod network-metrics-daemon-skfmf requesting resource cpu=20m on Node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod network-check-source-5c64cf6958-bl5pp requesting resource cpu=10m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod network-check-target-vklmp requesting resource cpu=10m on Node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod network-check-target-xmjhc requesting resource cpu=10m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod network-check-target-zpzkf requesting resource cpu=10m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod sdn-m6b96 requesting resource cpu=110m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod sdn-vjrw2 requesting resource cpu=110m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod sdn-zfgs6 requesting resource cpu=110m on Node ip-10-0-164-144.ec2.internal
Aug 19 16:31:12.764: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod sonobuoy-e2e-job-b3deceaf6b87402a requesting resource cpu=0m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-57h77 requesting resource cpu=0m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.764: INFO: Pod sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-fh9k8 requesting resource cpu=0m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.764: INFO: Pod sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-x4rlh requesting resource cpu=0m on Node ip-10-0-164-144.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
Aug 19 16:31:12.764: INFO: Creating a pod which consumes cpu=1995m on Node ip-10-0-131-169.ec2.internal
Aug 19 16:31:12.787: INFO: Creating a pod which consumes cpu=1981m on Node ip-10-0-157-99.ec2.internal
Aug 19 16:31:12.805: INFO: Creating a pod which consumes cpu=2216m on Node ip-10-0-164-144.ec2.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-82fad2d7-b2ce-4e7b-9644-1747ce378211.170ccc0249f901a4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2562/filler-pod-82fad2d7-b2ce-4e7b-9644-1747ce378211 to ip-10-0-131-169.ec2.internal by ip-10-0-155-87]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-82fad2d7-b2ce-4e7b-9644-1747ce378211.170ccc02bca39b07], Reason = [AddedInterface], Message = [Add eth0 [10.128.2.172/23] from openshift-sdn]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-82fad2d7-b2ce-4e7b-9644-1747ce378211.170ccc02be43f67c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-82fad2d7-b2ce-4e7b-9644-1747ce378211.170ccc02c5762655], Reason = [Created], Message = [Created container filler-pod-82fad2d7-b2ce-4e7b-9644-1747ce378211]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-82fad2d7-b2ce-4e7b-9644-1747ce378211.170ccc02c669c270], Reason = [Started], Message = [Started container filler-pod-82fad2d7-b2ce-4e7b-9644-1747ce378211]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f551e13d-b362-41ab-bd4b-fdf03e9e7a95.170ccc024c06de46], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2562/filler-pod-f551e13d-b362-41ab-bd4b-fdf03e9e7a95 to ip-10-0-164-144.ec2.internal by ip-10-0-155-87]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f551e13d-b362-41ab-bd4b-fdf03e9e7a95.170ccc02de48267a], Reason = [AddedInterface], Message = [Add eth0 [10.131.1.151/23] from openshift-sdn]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f551e13d-b362-41ab-bd4b-fdf03e9e7a95.170ccc02dfc1265b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f551e13d-b362-41ab-bd4b-fdf03e9e7a95.170ccc02e59a4ace], Reason = [Created], Message = [Created container filler-pod-f551e13d-b362-41ab-bd4b-fdf03e9e7a95]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f551e13d-b362-41ab-bd4b-fdf03e9e7a95.170ccc02e6697f32], Reason = [Started], Message = [Started container filler-pod-f551e13d-b362-41ab-bd4b-fdf03e9e7a95]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa089419-219c-44ed-a777-9ebea7cb6cde.170ccc024afa40e2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2562/filler-pod-fa089419-219c-44ed-a777-9ebea7cb6cde to ip-10-0-157-99.ec2.internal by ip-10-0-155-87]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa089419-219c-44ed-a777-9ebea7cb6cde.170ccc02db5ccdda], Reason = [AddedInterface], Message = [Add eth0 [10.129.2.202/23] from openshift-sdn]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa089419-219c-44ed-a777-9ebea7cb6cde.170ccc02dd206c84], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa089419-219c-44ed-a777-9ebea7cb6cde.170ccc02e3ff9d67], Reason = [Created], Message = [Created container filler-pod-fa089419-219c-44ed-a777-9ebea7cb6cde]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa089419-219c-44ed-a777-9ebea7cb6cde.170ccc02e5a67db8], Reason = [Started], Message = [Started container filler-pod-fa089419-219c-44ed-a777-9ebea7cb6cde]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.170ccc033bedc6c0], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 5 Insufficient cpu. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.]
STEP: removing the label node off the node ip-10-0-164-144.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-131-169.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-157-99.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Aug 19 16:31:17.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2562" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:5.523 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":356,"completed":310,"skipped":5615,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:31:18.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Aug 19 16:31:20.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8678" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","total":356,"completed":311,"skipped":5656,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:31:20.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Aug 19 16:31:20.225: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Aug 19 16:31:20.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3360" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":356,"completed":312,"skipped":5667,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:31:20.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 19 16:31:20.666: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 19 16:31:22.677: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 16, 31, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 31, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 31, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 31, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 19 16:31:24.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 19, 16, 31, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 31, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 19, 16, 31, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 19, 16, 31, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 19 16:31:27.692: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:31:27.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6025" for this suite.
STEP: Destroying namespace "webhook-6025-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.697 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":356,"completed":313,"skipped":5669,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:31:27.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:31:28.139: INFO: created pod
Aug 19 16:31:28.140: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1320" to be "Succeeded or Failed"
Aug 19 16:31:28.160: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 20.961215ms
Aug 19 16:31:30.164: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024444778s
Aug 19 16:31:32.169: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029021172s
Aug 19 16:31:34.175: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035830132s
STEP: Saw pod success
Aug 19 16:31:34.175: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Aug 19 16:32:04.179: INFO: polling logs
Aug 19 16:32:04.194: INFO: Pod logs: 
I0819 16:31:29.769502       1 log.go:195] OK: Got token
I0819 16:31:29.769524       1 log.go:195] validating with in-cluster discovery
I0819 16:31:29.769811       1 log.go:195] OK: got issuer https://kubernetes.default.svc
I0819 16:31:29.769833       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1320:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1660927288, NotBefore:1660926688, IssuedAt:1660926688, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1320", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b1e173b0-f4a7-4e49-8775-d430a82be536"}}}
I0819 16:31:29.778556       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0819 16:31:29.788201       1 log.go:195] OK: Validated signature on JWT
I0819 16:31:29.788259       1 log.go:195] OK: Got valid claims from token!
I0819 16:31:29.788282       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1320:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1660927288, NotBefore:1660926688, IssuedAt:1660926688, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1320", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b1e173b0-f4a7-4e49-8775-d430a82be536"}}}

Aug 19 16:32:04.194: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 19 16:32:04.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1320" for this suite.

• [SLOW TEST:36.228 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":356,"completed":314,"skipped":5739,"failed":0}
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:32:04.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's command
Aug 19 16:32:04.289: INFO: Waiting up to 5m0s for pod "var-expansion-4e26c9f7-75b7-4198-8601-4ee7fa4ac708" in namespace "var-expansion-9927" to be "Succeeded or Failed"
Aug 19 16:32:04.299: INFO: Pod "var-expansion-4e26c9f7-75b7-4198-8601-4ee7fa4ac708": Phase="Pending", Reason="", readiness=false. Elapsed: 10.759005ms
Aug 19 16:32:06.303: INFO: Pod "var-expansion-4e26c9f7-75b7-4198-8601-4ee7fa4ac708": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01436864s
Aug 19 16:32:08.306: INFO: Pod "var-expansion-4e26c9f7-75b7-4198-8601-4ee7fa4ac708": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017549619s
Aug 19 16:32:10.315: INFO: Pod "var-expansion-4e26c9f7-75b7-4198-8601-4ee7fa4ac708": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025874836s
STEP: Saw pod success
Aug 19 16:32:10.315: INFO: Pod "var-expansion-4e26c9f7-75b7-4198-8601-4ee7fa4ac708" satisfied condition "Succeeded or Failed"
Aug 19 16:32:10.331: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod var-expansion-4e26c9f7-75b7-4198-8601-4ee7fa4ac708 container dapi-container: <nil>
STEP: delete the pod
Aug 19 16:32:10.375: INFO: Waiting for pod var-expansion-4e26c9f7-75b7-4198-8601-4ee7fa4ac708 to disappear
Aug 19 16:32:10.382: INFO: Pod var-expansion-4e26c9f7-75b7-4198-8601-4ee7fa4ac708 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 19 16:32:10.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9927" for this suite.

• [SLOW TEST:6.200 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":356,"completed":315,"skipped":5739,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:32:10.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-741dd6c0-9469-4f91-ab47-a6fabbe3fe87
STEP: Creating a pod to test consume secrets
Aug 19 16:32:10.585: INFO: Waiting up to 5m0s for pod "pod-secrets-f749b115-6103-4dd1-8153-d20eeb91a41f" in namespace "secrets-5493" to be "Succeeded or Failed"
Aug 19 16:32:10.589: INFO: Pod "pod-secrets-f749b115-6103-4dd1-8153-d20eeb91a41f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.204361ms
Aug 19 16:32:12.593: INFO: Pod "pod-secrets-f749b115-6103-4dd1-8153-d20eeb91a41f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007962616s
Aug 19 16:32:14.598: INFO: Pod "pod-secrets-f749b115-6103-4dd1-8153-d20eeb91a41f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012928008s
Aug 19 16:32:16.602: INFO: Pod "pod-secrets-f749b115-6103-4dd1-8153-d20eeb91a41f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017676819s
STEP: Saw pod success
Aug 19 16:32:16.602: INFO: Pod "pod-secrets-f749b115-6103-4dd1-8153-d20eeb91a41f" satisfied condition "Succeeded or Failed"
Aug 19 16:32:16.606: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-secrets-f749b115-6103-4dd1-8153-d20eeb91a41f container secret-volume-test: <nil>
STEP: delete the pod
Aug 19 16:32:16.625: INFO: Waiting for pod pod-secrets-f749b115-6103-4dd1-8153-d20eeb91a41f to disappear
Aug 19 16:32:16.628: INFO: Pod pod-secrets-f749b115-6103-4dd1-8153-d20eeb91a41f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 19 16:32:16.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5493" for this suite.

• [SLOW TEST:6.226 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":316,"skipped":5753,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:32:16.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching services
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 16:32:16.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3413" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":356,"completed":317,"skipped":5769,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:32:16.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 19 16:32:16.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-627" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":356,"completed":318,"skipped":5776,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:32:16.845: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-a46ad2b5-34f3-4dbd-b745-db2e4de54849
STEP: Creating a pod to test consume secrets
Aug 19 16:32:16.901: INFO: Waiting up to 5m0s for pod "pod-secrets-7039a755-b507-428c-aec0-0a6225a4b8d1" in namespace "secrets-9874" to be "Succeeded or Failed"
Aug 19 16:32:16.914: INFO: Pod "pod-secrets-7039a755-b507-428c-aec0-0a6225a4b8d1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.389698ms
Aug 19 16:32:18.919: INFO: Pod "pod-secrets-7039a755-b507-428c-aec0-0a6225a4b8d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018084557s
Aug 19 16:32:20.923: INFO: Pod "pod-secrets-7039a755-b507-428c-aec0-0a6225a4b8d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022402578s
Aug 19 16:32:22.928: INFO: Pod "pod-secrets-7039a755-b507-428c-aec0-0a6225a4b8d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026820952s
STEP: Saw pod success
Aug 19 16:32:22.928: INFO: Pod "pod-secrets-7039a755-b507-428c-aec0-0a6225a4b8d1" satisfied condition "Succeeded or Failed"
Aug 19 16:32:22.931: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-secrets-7039a755-b507-428c-aec0-0a6225a4b8d1 container secret-volume-test: <nil>
STEP: delete the pod
Aug 19 16:32:22.951: INFO: Waiting for pod pod-secrets-7039a755-b507-428c-aec0-0a6225a4b8d1 to disappear
Aug 19 16:32:22.953: INFO: Pod pod-secrets-7039a755-b507-428c-aec0-0a6225a4b8d1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 19 16:32:22.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9874" for this suite.

• [SLOW TEST:6.118 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":319,"skipped":5808,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:32:22.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-8bee3b4e-771b-4d52-be69-e54885a2cd52
STEP: Creating a pod to test consume configMaps
Aug 19 16:32:23.106: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fad22a49-94bb-4b1e-ba3a-7b5add83058d" in namespace "projected-1934" to be "Succeeded or Failed"
Aug 19 16:32:23.121: INFO: Pod "pod-projected-configmaps-fad22a49-94bb-4b1e-ba3a-7b5add83058d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.516558ms
Aug 19 16:32:25.125: INFO: Pod "pod-projected-configmaps-fad22a49-94bb-4b1e-ba3a-7b5add83058d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019119261s
Aug 19 16:32:27.128: INFO: Pod "pod-projected-configmaps-fad22a49-94bb-4b1e-ba3a-7b5add83058d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022645697s
Aug 19 16:32:29.133: INFO: Pod "pod-projected-configmaps-fad22a49-94bb-4b1e-ba3a-7b5add83058d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027044139s
STEP: Saw pod success
Aug 19 16:32:29.133: INFO: Pod "pod-projected-configmaps-fad22a49-94bb-4b1e-ba3a-7b5add83058d" satisfied condition "Succeeded or Failed"
Aug 19 16:32:29.136: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-projected-configmaps-fad22a49-94bb-4b1e-ba3a-7b5add83058d container agnhost-container: <nil>
STEP: delete the pod
Aug 19 16:32:29.155: INFO: Waiting for pod pod-projected-configmaps-fad22a49-94bb-4b1e-ba3a-7b5add83058d to disappear
Aug 19 16:32:29.158: INFO: Pod pod-projected-configmaps-fad22a49-94bb-4b1e-ba3a-7b5add83058d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 19 16:32:29.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1934" for this suite.

• [SLOW TEST:6.206 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":320,"skipped":5808,"failed":0}
SSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:32:29.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:32:29.227: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-1d1c084f-8bab-457a-b6a4-cf4a908fc9a9" in namespace "security-context-test-60" to be "Succeeded or Failed"
Aug 19 16:32:29.232: INFO: Pod "alpine-nnp-false-1d1c084f-8bab-457a-b6a4-cf4a908fc9a9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.433564ms
Aug 19 16:32:31.237: INFO: Pod "alpine-nnp-false-1d1c084f-8bab-457a-b6a4-cf4a908fc9a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01043275s
Aug 19 16:32:33.241: INFO: Pod "alpine-nnp-false-1d1c084f-8bab-457a-b6a4-cf4a908fc9a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014517712s
Aug 19 16:32:35.247: INFO: Pod "alpine-nnp-false-1d1c084f-8bab-457a-b6a4-cf4a908fc9a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020063952s
Aug 19 16:32:37.250: INFO: Pod "alpine-nnp-false-1d1c084f-8bab-457a-b6a4-cf4a908fc9a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.023605152s
Aug 19 16:32:37.250: INFO: Pod "alpine-nnp-false-1d1c084f-8bab-457a-b6a4-cf4a908fc9a9" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 19 16:32:37.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-60" for this suite.

• [SLOW TEST:8.097 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:298
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":321,"skipped":5812,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:32:37.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-855
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:32:37.330: INFO: Found 0 stateful pods, waiting for 1
Aug 19 16:32:47.335: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Aug 19 16:32:47.357: INFO: Found 1 stateful pods, waiting for 2
Aug 19 16:32:57.363: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 19 16:32:57.363: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 19 16:32:57.384: INFO: Deleting all statefulset in ns statefulset-855
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 19 16:32:57.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-855" for this suite.

• [SLOW TEST:20.144 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":356,"completed":322,"skipped":5850,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:32:57.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 19 16:32:57.446: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 19 16:32:57.463: INFO: Waiting for terminating namespaces to be deleted...
Aug 19 16:32:57.470: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-131-169.ec2.internal before test
Aug 19 16:32:57.496: INFO: aws-ebs-csi-driver-node-zp4zp from openshift-cluster-csi-drivers started at 2022-08-19 14:38:55 +0000 UTC (3 container statuses recorded)
Aug 19 16:32:57.496: INFO: 	Container csi-driver ready: true, restart count 0
Aug 19 16:32:57.496: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Aug 19 16:32:57.496: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 19 16:32:57.496: INFO: tuned-7vdfg from openshift-cluster-node-tuning-operator started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.496: INFO: 	Container tuned ready: true, restart count 0
Aug 19 16:32:57.496: INFO: downloads-858cc8f4cb-zkm66 from openshift-console started at 2022-08-19 14:47:16 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.496: INFO: 	Container download-server ready: true, restart count 0
Aug 19 16:32:57.496: INFO: dns-default-zk2kr from openshift-dns started at 2022-08-19 14:39:21 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.496: INFO: 	Container dns ready: true, restart count 0
Aug 19 16:32:57.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.496: INFO: node-resolver-ttx27 from openshift-dns started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.496: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 19 16:32:57.496: INFO: image-registry-79dbb9c69c-2qmtr from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.496: INFO: 	Container registry ready: true, restart count 0
Aug 19 16:32:57.496: INFO: node-ca-qqztn from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.496: INFO: 	Container node-ca ready: true, restart count 0
Aug 19 16:32:57.496: INFO: ingress-canary-lr86r from openshift-ingress-canary started at 2022-08-19 14:40:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.496: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 19 16:32:57.496: INFO: router-default-7664744558-bndls from openshift-ingress started at 2022-08-19 14:40:03 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.496: INFO: 	Container router ready: true, restart count 0
Aug 19 16:32:57.496: INFO: machine-config-daemon-gpvls from openshift-machine-config-operator started at 2022-08-19 14:38:55 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.496: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 19 16:32:57.496: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 16:32:57.496: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-08-19 15:46:54 +0000 UTC (6 container statuses recorded)
Aug 19 16:32:57.496: INFO: 	Container alertmanager ready: true, restart count 0
Aug 19 16:32:57.496: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 19 16:32:57.496: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 16:32:57.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.496: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 19 16:32:57.496: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 16:32:57.496: INFO: node-exporter-tv7s8 from openshift-monitoring started at 2022-08-19 14:41:12 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.496: INFO: 	Container node-exporter ready: true, restart count 0
Aug 19 16:32:57.496: INFO: prometheus-adapter-85bd9549d5-h6mlp from openshift-monitoring started at 2022-08-19 14:42:01 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.497: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 19 16:32:57.497: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-08-19 15:46:55 +0000 UTC (6 container statuses recorded)
Aug 19 16:32:57.497: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 16:32:57.497: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.497: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 19 16:32:57.497: INFO: 	Container prometheus ready: true, restart count 0
Aug 19 16:32:57.497: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 19 16:32:57.497: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 19 16:32:57.497: INFO: prometheus-operator-admission-webhook-6db58c58f7-2922r from openshift-monitoring started at 2022-08-19 14:39:23 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.497: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 19 16:32:57.497: INFO: thanos-querier-8559769b94-jjnm2 from openshift-monitoring started at 2022-08-19 15:10:44 +0000 UTC (6 container statuses recorded)
Aug 19 16:32:57.497: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.497: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 19 16:32:57.497: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 19 16:32:57.497: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 16:32:57.497: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 16:32:57.497: INFO: 	Container thanos-query ready: true, restart count 0
Aug 19 16:32:57.497: INFO: multus-additional-cni-plugins-2fqhw from openshift-multus started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.497: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 19 16:32:57.497: INFO: multus-pss88 from openshift-multus started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.497: INFO: 	Container kube-multus ready: true, restart count 0
Aug 19 16:32:57.497: INFO: network-metrics-daemon-bkf7f from openshift-multus started at 2022-08-19 14:38:55 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.497: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.497: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 19 16:32:57.497: INFO: network-check-target-zpzkf from openshift-network-diagnostics started at 2022-08-19 14:38:55 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.497: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 19 16:32:57.497: INFO: collect-profiles-27682080-d2bbw from openshift-operator-lifecycle-manager started at 2022-08-19 16:00:00 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.497: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 19 16:32:57.497: INFO: collect-profiles-27682095-mbvlr from openshift-operator-lifecycle-manager started at 2022-08-19 16:15:00 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.497: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 19 16:32:57.497: INFO: collect-profiles-27682110-fr5jb from openshift-operator-lifecycle-manager started at 2022-08-19 16:30:00 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.497: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 19 16:32:57.497: INFO: sdn-vjrw2 from openshift-sdn started at 2022-08-19 14:38:55 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.497: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.497: INFO: 	Container sdn ready: true, restart count 0
Aug 19 16:32:57.497: INFO: sonobuoy from sonobuoy started at 2022-08-19 14:57:28 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.497: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 19 16:32:57.497: INFO: sonobuoy-e2e-job-b3deceaf6b87402a from sonobuoy started at 2022-08-19 14:57:31 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.497: INFO: 	Container e2e ready: true, restart count 0
Aug 19 16:32:57.497: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 16:32:57.497: INFO: sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-57h77 from sonobuoy started at 2022-08-19 14:57:31 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.497: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 16:32:57.497: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 19 16:32:57.497: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-157-99.ec2.internal before test
Aug 19 16:32:57.528: INFO: aws-ebs-csi-driver-node-sknln from openshift-cluster-csi-drivers started at 2022-08-19 14:39:05 +0000 UTC (3 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container csi-driver ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 19 16:32:57.528: INFO: tuned-ttgq4 from openshift-cluster-node-tuning-operator started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container tuned ready: true, restart count 0
Aug 19 16:32:57.528: INFO: downloads-858cc8f4cb-57s2s from openshift-console started at 2022-08-19 15:46:52 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container download-server ready: true, restart count 0
Aug 19 16:32:57.528: INFO: dns-default-m65sc from openshift-dns started at 2022-08-19 14:40:04 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container dns ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.528: INFO: node-resolver-dzbsb from openshift-dns started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 19 16:32:57.528: INFO: image-registry-79dbb9c69c-bbrd6 from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container registry ready: true, restart count 0
Aug 19 16:32:57.528: INFO: node-ca-2jbtx from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container node-ca ready: true, restart count 0
Aug 19 16:32:57.528: INFO: ingress-canary-cmqqt from openshift-ingress-canary started at 2022-08-19 14:40:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 19 16:32:57.528: INFO: router-default-7664744558-px7pp from openshift-ingress started at 2022-08-19 15:10:43 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container router ready: true, restart count 0
Aug 19 16:32:57.528: INFO: machine-config-daemon-zvdcx from openshift-machine-config-operator started at 2022-08-19 14:39:05 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 16:32:57.528: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-08-19 14:48:20 +0000 UTC (6 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container alertmanager ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 16:32:57.528: INFO: kube-state-metrics-9f5df78c9-s6s4x from openshift-monitoring started at 2022-08-19 14:41:11 +0000 UTC (3 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 19 16:32:57.528: INFO: node-exporter-zw8vw from openshift-monitoring started at 2022-08-19 14:41:11 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container node-exporter ready: true, restart count 0
Aug 19 16:32:57.528: INFO: openshift-state-metrics-6c88b54494-79p9n from openshift-monitoring started at 2022-08-19 14:41:11 +0000 UTC (3 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 19 16:32:57.528: INFO: prometheus-adapter-85bd9549d5-nnh58 from openshift-monitoring started at 2022-08-19 14:42:01 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 19 16:32:57.528: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-08-19 14:47:30 +0000 UTC (6 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container config-reloader ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container prometheus ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 19 16:32:57.528: INFO: prometheus-operator-admission-webhook-6db58c58f7-khv8v from openshift-monitoring started at 2022-08-19 15:10:44 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 19 16:32:57.528: INFO: telemeter-client-86d58945d5-4tqj6 from openshift-monitoring started at 2022-08-19 14:42:06 +0000 UTC (3 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container reload ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 19 16:32:57.528: INFO: thanos-querier-8559769b94-2pf4w from openshift-monitoring started at 2022-08-19 14:41:18 +0000 UTC (6 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container thanos-query ready: true, restart count 0
Aug 19 16:32:57.528: INFO: multus-additional-cni-plugins-n26hm from openshift-multus started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 19 16:32:57.528: INFO: multus-w6shh from openshift-multus started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container kube-multus ready: true, restart count 0
Aug 19 16:32:57.528: INFO: network-metrics-daemon-jhrx8 from openshift-multus started at 2022-08-19 14:39:05 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 19 16:32:57.528: INFO: network-check-source-5c64cf6958-bl5pp from openshift-network-diagnostics started at 2022-08-19 15:46:52 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 19 16:32:57.528: INFO: network-check-target-xmjhc from openshift-network-diagnostics started at 2022-08-19 14:39:05 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 19 16:32:57.528: INFO: sdn-m6b96 from openshift-sdn started at 2022-08-19 14:39:05 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container sdn ready: true, restart count 0
Aug 19 16:32:57.528: INFO: sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-fh9k8 from sonobuoy started at 2022-08-19 14:57:32 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 19 16:32:57.528: INFO: test-ss-1 from statefulset-855 started at 2022-08-19 16:32:47 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.528: INFO: 	Container test-ss ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 	Container webserver ready: true, restart count 0
Aug 19 16:32:57.528: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-164-144.ec2.internal before test
Aug 19 16:32:57.544: INFO: aws-ebs-csi-driver-node-l9xm2 from openshift-cluster-csi-drivers started at 2022-08-19 14:38:49 +0000 UTC (3 container statuses recorded)
Aug 19 16:32:57.544: INFO: 	Container csi-driver ready: true, restart count 0
Aug 19 16:32:57.545: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Aug 19 16:32:57.545: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Aug 19 16:32:57.545: INFO: tuned-f4rpl from openshift-cluster-node-tuning-operator started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.545: INFO: 	Container tuned ready: true, restart count 0
Aug 19 16:32:57.545: INFO: dns-default-jpsrv from openshift-dns started at 2022-08-19 15:47:18 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.545: INFO: 	Container dns ready: true, restart count 0
Aug 19 16:32:57.545: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.545: INFO: node-resolver-pmmzs from openshift-dns started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.545: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 19 16:32:57.545: INFO: node-ca-xt2pz from openshift-image-registry started at 2022-08-19 14:41:04 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.545: INFO: 	Container node-ca ready: true, restart count 0
Aug 19 16:32:57.545: INFO: ingress-canary-97q7v from openshift-ingress-canary started at 2022-08-19 15:47:18 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.545: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 19 16:32:57.545: INFO: machine-config-daemon-xz9st from openshift-machine-config-operator started at 2022-08-19 14:38:49 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.545: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 19 16:32:57.545: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 19 16:32:57.545: INFO: node-exporter-64n9p from openshift-monitoring started at 2022-08-19 14:41:12 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.545: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.545: INFO: 	Container node-exporter ready: true, restart count 0
Aug 19 16:32:57.545: INFO: multus-85nv9 from openshift-multus started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.545: INFO: 	Container kube-multus ready: true, restart count 0
Aug 19 16:32:57.545: INFO: multus-additional-cni-plugins-9kc7x from openshift-multus started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.545: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 19 16:32:57.545: INFO: network-metrics-daemon-skfmf from openshift-multus started at 2022-08-19 14:38:49 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.545: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.545: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 19 16:32:57.545: INFO: network-check-target-vklmp from openshift-network-diagnostics started at 2022-08-19 14:38:49 +0000 UTC (1 container statuses recorded)
Aug 19 16:32:57.545: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 19 16:32:57.545: INFO: sdn-zfgs6 from openshift-sdn started at 2022-08-19 14:38:49 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.545: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 19 16:32:57.545: INFO: 	Container sdn ready: true, restart count 0
Aug 19 16:32:57.545: INFO: sonobuoy-systemd-logs-daemon-set-58df1eb835d2411f-x4rlh from sonobuoy started at 2022-08-19 14:57:32 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.545: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 19 16:32:57.545: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 19 16:32:57.545: INFO: test-ss-0 from statefulset-855 started at 2022-08-19 16:32:50 +0000 UTC (2 container statuses recorded)
Aug 19 16:32:57.545: INFO: 	Container test-ss ready: true, restart count 0
Aug 19 16:32:57.545: INFO: 	Container webserver ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1e624b43-871f-47a4-bc96-cb73a58bc437 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.164.144 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-1e624b43-871f-47a4-bc96-cb73a58bc437 off the node ip-10-0-164-144.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1e624b43-871f-47a4-bc96-cb73a58bc437
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Aug 19 16:38:05.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4580" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:308.305 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":356,"completed":323,"skipped":5919,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:38:05.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7428.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7428.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7428.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7428.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7428.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7428.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7428.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7428.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7428.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7428.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7428.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7428.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 168.25.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.25.168_udp@PTR;check="$$(dig +tcp +noall +answer +search 168.25.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.25.168_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7428.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7428.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7428.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7428.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7428.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7428.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7428.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7428.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7428.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7428.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7428.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7428.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 168.25.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.25.168_udp@PTR;check="$$(dig +tcp +noall +answer +search 168.25.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.25.168_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 19 16:38:09.886: INFO: Unable to read wheezy_udp@dns-test-service.dns-7428.svc.cluster.local from pod dns-7428/dns-test-4f3b3d09-5a99-4336-a032-6b54c08f974a: the server could not find the requested resource (get pods dns-test-4f3b3d09-5a99-4336-a032-6b54c08f974a)
Aug 19 16:38:09.889: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7428.svc.cluster.local from pod dns-7428/dns-test-4f3b3d09-5a99-4336-a032-6b54c08f974a: the server could not find the requested resource (get pods dns-test-4f3b3d09-5a99-4336-a032-6b54c08f974a)
Aug 19 16:38:09.949: INFO: Lookups using dns-7428/dns-test-4f3b3d09-5a99-4336-a032-6b54c08f974a failed for: [wheezy_udp@dns-test-service.dns-7428.svc.cluster.local wheezy_tcp@dns-test-service.dns-7428.svc.cluster.local]

Aug 19 16:38:15.009: INFO: DNS probes using dns-7428/dns-test-4f3b3d09-5a99-4336-a032-6b54c08f974a succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 19 16:38:15.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7428" for this suite.

• [SLOW TEST:9.362 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":356,"completed":324,"skipped":5937,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:38:15.079: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Aug 19 16:38:15.159: INFO: Waiting up to 5m0s for pod "downward-api-88483204-364a-4d13-8d61-829ccce61940" in namespace "downward-api-3576" to be "Succeeded or Failed"
Aug 19 16:38:15.168: INFO: Pod "downward-api-88483204-364a-4d13-8d61-829ccce61940": Phase="Pending", Reason="", readiness=false. Elapsed: 8.226233ms
Aug 19 16:38:17.171: INFO: Pod "downward-api-88483204-364a-4d13-8d61-829ccce61940": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011989535s
Aug 19 16:38:19.176: INFO: Pod "downward-api-88483204-364a-4d13-8d61-829ccce61940": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016436114s
Aug 19 16:38:21.180: INFO: Pod "downward-api-88483204-364a-4d13-8d61-829ccce61940": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020521651s
STEP: Saw pod success
Aug 19 16:38:21.180: INFO: Pod "downward-api-88483204-364a-4d13-8d61-829ccce61940" satisfied condition "Succeeded or Failed"
Aug 19 16:38:21.183: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downward-api-88483204-364a-4d13-8d61-829ccce61940 container dapi-container: <nil>
STEP: delete the pod
Aug 19 16:38:21.210: INFO: Waiting for pod downward-api-88483204-364a-4d13-8d61-829ccce61940 to disappear
Aug 19 16:38:21.213: INFO: Pod downward-api-88483204-364a-4d13-8d61-829ccce61940 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Aug 19 16:38:21.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3576" for this suite.

• [SLOW TEST:6.145 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":356,"completed":325,"skipped":5942,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:38:21.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-1868
STEP: creating service affinity-clusterip-transition in namespace services-1868
STEP: creating replication controller affinity-clusterip-transition in namespace services-1868
I0819 16:38:21.286007      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1868, replica count: 3
I0819 16:38:24.336654      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0819 16:38:27.337488      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 19 16:38:27.343: INFO: Creating new exec pod
Aug 19 16:38:32.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-1868 exec execpod-affinityl9hzh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Aug 19 16:38:32.477: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 19 16:38:32.477: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 16:38:32.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-1868 exec execpod-affinityl9hzh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.137.14 80'
Aug 19 16:38:32.569: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.137.14 80\nConnection to 172.30.137.14 80 port [tcp/http] succeeded!\n"
Aug 19 16:38:32.569: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 19 16:38:32.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-1868 exec execpod-affinityl9hzh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.137.14:80/ ; done'
Aug 19 16:38:32.737: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n"
Aug 19 16:38:32.737: INFO: stdout: "\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-78nmq\naffinity-clusterip-transition-bg65x\naffinity-clusterip-transition-bg65x\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-78nmq\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-bg65x\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-78nmq\naffinity-clusterip-transition-78nmq\naffinity-clusterip-transition-78nmq\naffinity-clusterip-transition-bg65x\naffinity-clusterip-transition-78nmq\naffinity-clusterip-transition-bg65x"
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-78nmq
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-bg65x
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-bg65x
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-78nmq
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-bg65x
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-78nmq
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-78nmq
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-78nmq
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-bg65x
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-78nmq
Aug 19 16:38:32.737: INFO: Received response from host: affinity-clusterip-transition-bg65x
Aug 19 16:38:32.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=services-1868 exec execpod-affinityl9hzh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.137.14:80/ ; done'
Aug 19 16:38:32.903: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.137.14:80/\n"
Aug 19 16:38:32.903: INFO: stdout: "\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5\naffinity-clusterip-transition-frbl5"
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Received response from host: affinity-clusterip-transition-frbl5
Aug 19 16:38:32.903: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1868, will wait for the garbage collector to delete the pods
Aug 19 16:38:32.976: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.698421ms
Aug 19 16:38:33.077: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.476482ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 19 16:38:35.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1868" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:14.282 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":326,"skipped":5964,"failed":0}
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:38:35.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:38:37.577: INFO: Deleting pod "var-expansion-5834d03a-f496-4a4b-ad75-d398470804ba" in namespace "var-expansion-4182"
Aug 19 16:38:37.585: INFO: Wait up to 5m0s for pod "var-expansion-5834d03a-f496-4a4b-ad75-d398470804ba" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 19 16:38:41.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4182" for this suite.

• [SLOW TEST:6.097 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":356,"completed":327,"skipped":5964,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:38:41.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 19 16:39:09.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7440" for this suite.

• [SLOW TEST:28.083 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":356,"completed":328,"skipped":5976,"failed":0}
SSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:39:09.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override arguments
Aug 19 16:39:09.744: INFO: Waiting up to 5m0s for pod "client-containers-5cb3a515-58a2-4b32-9cf0-0c4d6b41e103" in namespace "containers-8661" to be "Succeeded or Failed"
Aug 19 16:39:09.762: INFO: Pod "client-containers-5cb3a515-58a2-4b32-9cf0-0c4d6b41e103": Phase="Pending", Reason="", readiness=false. Elapsed: 17.943937ms
Aug 19 16:39:11.768: INFO: Pod "client-containers-5cb3a515-58a2-4b32-9cf0-0c4d6b41e103": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024227701s
Aug 19 16:39:13.772: INFO: Pod "client-containers-5cb3a515-58a2-4b32-9cf0-0c4d6b41e103": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028405339s
Aug 19 16:39:15.776: INFO: Pod "client-containers-5cb3a515-58a2-4b32-9cf0-0c4d6b41e103": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032107861s
STEP: Saw pod success
Aug 19 16:39:15.776: INFO: Pod "client-containers-5cb3a515-58a2-4b32-9cf0-0c4d6b41e103" satisfied condition "Succeeded or Failed"
Aug 19 16:39:15.779: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod client-containers-5cb3a515-58a2-4b32-9cf0-0c4d6b41e103 container agnhost-container: <nil>
STEP: delete the pod
Aug 19 16:39:15.801: INFO: Waiting for pod client-containers-5cb3a515-58a2-4b32-9cf0-0c4d6b41e103 to disappear
Aug 19 16:39:15.804: INFO: Pod client-containers-5cb3a515-58a2-4b32-9cf0-0c4d6b41e103 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Aug 19 16:39:15.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8661" for this suite.

• [SLOW TEST:6.128 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","total":356,"completed":329,"skipped":5982,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:39:15.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0819 16:39:15.865326      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 19 16:39:15.865: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7da062ad-763f-4d4f-8a5f-9577819aa6ab" in namespace "projected-5319" to be "Succeeded or Failed"
Aug 19 16:39:15.868: INFO: Pod "downwardapi-volume-7da062ad-763f-4d4f-8a5f-9577819aa6ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.751332ms
Aug 19 16:39:17.872: INFO: Pod "downwardapi-volume-7da062ad-763f-4d4f-8a5f-9577819aa6ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007069218s
Aug 19 16:39:19.878: INFO: Pod "downwardapi-volume-7da062ad-763f-4d4f-8a5f-9577819aa6ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012593206s
Aug 19 16:39:21.883: INFO: Pod "downwardapi-volume-7da062ad-763f-4d4f-8a5f-9577819aa6ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018123353s
STEP: Saw pod success
Aug 19 16:39:21.883: INFO: Pod "downwardapi-volume-7da062ad-763f-4d4f-8a5f-9577819aa6ab" satisfied condition "Succeeded or Failed"
Aug 19 16:39:21.886: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-7da062ad-763f-4d4f-8a5f-9577819aa6ab container client-container: <nil>
STEP: delete the pod
Aug 19 16:39:21.906: INFO: Waiting for pod downwardapi-volume-7da062ad-763f-4d4f-8a5f-9577819aa6ab to disappear
Aug 19 16:39:21.909: INFO: Pod downwardapi-volume-7da062ad-763f-4d4f-8a5f-9577819aa6ab no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 19 16:39:21.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5319" for this suite.

• [SLOW TEST:6.105 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":356,"completed":330,"skipped":6000,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:39:21.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 19 16:39:21.973: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 19 16:39:26.977: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Aug 19 16:39:28.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8975" for this suite.

• [SLOW TEST:6.092 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":356,"completed":331,"skipped":6008,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:39:28.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-df4ab36f-7666-497a-a675-cd731ae49a18 in namespace container-probe-5979
Aug 19 16:39:32.065: INFO: Started pod busybox-df4ab36f-7666-497a-a675-cd731ae49a18 in namespace container-probe-5979
STEP: checking the pod's current state and verifying that restartCount is present
Aug 19 16:39:32.068: INFO: Initial restart count of pod busybox-df4ab36f-7666-497a-a675-cd731ae49a18 is 0
Aug 19 16:40:20.201: INFO: Restart count of pod container-probe-5979/busybox-df4ab36f-7666-497a-a675-cd731ae49a18 is now 1 (48.132650668s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 19 16:40:20.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5979" for this suite.

• [SLOW TEST:52.216 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":332,"skipped":6019,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:40:20.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Aug 19 16:40:20.342: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:40:22.347: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:40:24.347: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Aug 19 16:40:24.372: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:40:26.376: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 19 16:40:28.378: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 19 16:40:28.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 19 16:40:28.398: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 19 16:40:30.399: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 19 16:40:30.403: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 19 16:40:32.399: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 19 16:40:32.403: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Aug 19 16:40:32.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7628" for this suite.

• [SLOW TEST:12.187 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":356,"completed":333,"skipped":6056,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:40:32.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Aug 19 16:40:32.459: INFO: PodSpec: initContainers in spec.initContainers
Aug 19 16:41:17.703: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2d8ccdcf-47cc-46ab-8dd8-b44fa03585db", GenerateName:"", Namespace:"init-container-9088", SelfLink:"", UID:"1c1b2d3d-5774-42c0-954d-8741dd0c3ffb", ResourceVersion:"105722", Generation:0, CreationTimestamp:time.Date(2022, time.August, 19, 16, 40, 32, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"459738442"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.1.175\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.1.175\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 19, 16, 40, 32, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003cd5128), Subresource:""}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 19, 16, 40, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003cd5158), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 19, 16, 40, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003cd5188), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-htjt9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00d25ca40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-htjt9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0030945a0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-htjt9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003094660), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.7", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-htjt9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003094540), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005f700f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-164-144.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002080a80), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005f701b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005f701f0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005f7020c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc005f70230), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004447b90), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 19, 16, 40, 32, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 19, 16, 40, 32, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 19, 16, 40, 32, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 19, 16, 40, 32, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.164.144", PodIP:"10.131.1.175", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.131.1.175"}}, StartTime:time.Date(2022, time.August, 19, 16, 40, 32, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002080b60)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002080bd0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://722d8440f6037a5514baf8578931b6fa1a02278c3cff256bc74bcaf5d360f150", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00d25cac0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00d25caa0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.7", ImageID:"", ContainerID:"", Started:(*bool)(0xc005f702ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 19 16:41:17.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9088" for this suite.

• [SLOW TEST:45.301 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":356,"completed":334,"skipped":6073,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:41:17.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 19 16:41:17.806: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a2aa167-e2bb-4c0d-a25b-1e56824e8632" in namespace "projected-4917" to be "Succeeded or Failed"
Aug 19 16:41:17.811: INFO: Pod "downwardapi-volume-7a2aa167-e2bb-4c0d-a25b-1e56824e8632": Phase="Pending", Reason="", readiness=false. Elapsed: 4.172361ms
Aug 19 16:41:19.815: INFO: Pod "downwardapi-volume-7a2aa167-e2bb-4c0d-a25b-1e56824e8632": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009042976s
Aug 19 16:41:21.819: INFO: Pod "downwardapi-volume-7a2aa167-e2bb-4c0d-a25b-1e56824e8632": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01300511s
STEP: Saw pod success
Aug 19 16:41:21.819: INFO: Pod "downwardapi-volume-7a2aa167-e2bb-4c0d-a25b-1e56824e8632" satisfied condition "Succeeded or Failed"
Aug 19 16:41:21.823: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-7a2aa167-e2bb-4c0d-a25b-1e56824e8632 container client-container: <nil>
STEP: delete the pod
Aug 19 16:41:21.842: INFO: Waiting for pod downwardapi-volume-7a2aa167-e2bb-4c0d-a25b-1e56824e8632 to disappear
Aug 19 16:41:21.844: INFO: Pod downwardapi-volume-7a2aa167-e2bb-4c0d-a25b-1e56824e8632 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 19 16:41:21.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4917" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":335,"skipped":6081,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:41:21.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-0bc18212-dd8d-4f98-b65e-98ecec5b3bde
STEP: Creating a pod to test consume configMaps
Aug 19 16:41:21.919: INFO: Waiting up to 5m0s for pod "pod-configmaps-eba083d9-3363-4d66-ab64-c5719817b5fa" in namespace "configmap-453" to be "Succeeded or Failed"
Aug 19 16:41:21.928: INFO: Pod "pod-configmaps-eba083d9-3363-4d66-ab64-c5719817b5fa": Phase="Pending", Reason="", readiness=false. Elapsed: 9.195412ms
Aug 19 16:41:23.931: INFO: Pod "pod-configmaps-eba083d9-3363-4d66-ab64-c5719817b5fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011978603s
Aug 19 16:41:25.935: INFO: Pod "pod-configmaps-eba083d9-3363-4d66-ab64-c5719817b5fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016194533s
STEP: Saw pod success
Aug 19 16:41:25.935: INFO: Pod "pod-configmaps-eba083d9-3363-4d66-ab64-c5719817b5fa" satisfied condition "Succeeded or Failed"
Aug 19 16:41:25.938: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-configmaps-eba083d9-3363-4d66-ab64-c5719817b5fa container agnhost-container: <nil>
STEP: delete the pod
Aug 19 16:41:25.956: INFO: Waiting for pod pod-configmaps-eba083d9-3363-4d66-ab64-c5719817b5fa to disappear
Aug 19 16:41:25.963: INFO: Pod pod-configmaps-eba083d9-3363-4d66-ab64-c5719817b5fa no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 19 16:41:25.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-453" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":336,"skipped":6157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:41:25.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Aug 19 16:41:31.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8734" for this suite.

• [SLOW TEST:5.505 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":356,"completed":337,"skipped":6186,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:41:31.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-f3fde204-3587-477d-997a-60f43acd8a96
STEP: Creating a pod to test consume configMaps
Aug 19 16:41:31.600: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d74d06b8-4f32-496c-be36-c0823d5a36b4" in namespace "projected-179" to be "Succeeded or Failed"
Aug 19 16:41:31.608: INFO: Pod "pod-projected-configmaps-d74d06b8-4f32-496c-be36-c0823d5a36b4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.44015ms
Aug 19 16:41:33.627: INFO: Pod "pod-projected-configmaps-d74d06b8-4f32-496c-be36-c0823d5a36b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026543565s
Aug 19 16:41:35.631: INFO: Pod "pod-projected-configmaps-d74d06b8-4f32-496c-be36-c0823d5a36b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031185991s
Aug 19 16:41:37.635: INFO: Pod "pod-projected-configmaps-d74d06b8-4f32-496c-be36-c0823d5a36b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034828812s
STEP: Saw pod success
Aug 19 16:41:37.635: INFO: Pod "pod-projected-configmaps-d74d06b8-4f32-496c-be36-c0823d5a36b4" satisfied condition "Succeeded or Failed"
Aug 19 16:41:37.639: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod pod-projected-configmaps-d74d06b8-4f32-496c-be36-c0823d5a36b4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 19 16:41:37.654: INFO: Waiting for pod pod-projected-configmaps-d74d06b8-4f32-496c-be36-c0823d5a36b4 to disappear
Aug 19 16:41:37.658: INFO: Pod pod-projected-configmaps-d74d06b8-4f32-496c-be36-c0823d5a36b4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 19 16:41:37.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-179" for this suite.

• [SLOW TEST:6.189 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":338,"skipped":6224,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:41:37.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Aug 19 16:41:37.727: INFO: Waiting up to 5m0s for pod "downward-api-98699f96-6fdd-4e6a-8175-42cb2e1f70f9" in namespace "downward-api-3392" to be "Succeeded or Failed"
Aug 19 16:41:37.733: INFO: Pod "downward-api-98699f96-6fdd-4e6a-8175-42cb2e1f70f9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.359315ms
Aug 19 16:41:39.737: INFO: Pod "downward-api-98699f96-6fdd-4e6a-8175-42cb2e1f70f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009522247s
Aug 19 16:41:41.741: INFO: Pod "downward-api-98699f96-6fdd-4e6a-8175-42cb2e1f70f9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013306328s
Aug 19 16:41:43.745: INFO: Pod "downward-api-98699f96-6fdd-4e6a-8175-42cb2e1f70f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018100647s
STEP: Saw pod success
Aug 19 16:41:43.746: INFO: Pod "downward-api-98699f96-6fdd-4e6a-8175-42cb2e1f70f9" satisfied condition "Succeeded or Failed"
Aug 19 16:41:43.749: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downward-api-98699f96-6fdd-4e6a-8175-42cb2e1f70f9 container dapi-container: <nil>
STEP: delete the pod
Aug 19 16:41:43.772: INFO: Waiting for pod downward-api-98699f96-6fdd-4e6a-8175-42cb2e1f70f9 to disappear
Aug 19 16:41:43.775: INFO: Pod downward-api-98699f96-6fdd-4e6a-8175-42cb2e1f70f9 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Aug 19 16:41:43.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3392" for this suite.

• [SLOW TEST:6.117 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":356,"completed":339,"skipped":6258,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:41:43.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: creating
W0819 16:41:43.825985      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: getting
STEP: listing
STEP: watching
Aug 19 16:41:43.832: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 19 16:41:43.835: INFO: starting watch
STEP: patching
STEP: updating
Aug 19 16:41:43.850: INFO: waiting for watch events with expected annotations
Aug 19 16:41:43.850: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Aug 19 16:41:43.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5237" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":356,"completed":340,"skipped":6281,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:41:43.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 19 16:41:43.972: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf48e2f7-159d-4cd4-9ed0-3ac9aab072a2" in namespace "projected-6727" to be "Succeeded or Failed"
Aug 19 16:41:43.980: INFO: Pod "downwardapi-volume-bf48e2f7-159d-4cd4-9ed0-3ac9aab072a2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.384885ms
Aug 19 16:41:45.984: INFO: Pod "downwardapi-volume-bf48e2f7-159d-4cd4-9ed0-3ac9aab072a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011879776s
Aug 19 16:41:47.988: INFO: Pod "downwardapi-volume-bf48e2f7-159d-4cd4-9ed0-3ac9aab072a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015903005s
Aug 19 16:41:49.993: INFO: Pod "downwardapi-volume-bf48e2f7-159d-4cd4-9ed0-3ac9aab072a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021372845s
STEP: Saw pod success
Aug 19 16:41:49.993: INFO: Pod "downwardapi-volume-bf48e2f7-159d-4cd4-9ed0-3ac9aab072a2" satisfied condition "Succeeded or Failed"
Aug 19 16:41:49.997: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-bf48e2f7-159d-4cd4-9ed0-3ac9aab072a2 container client-container: <nil>
STEP: delete the pod
Aug 19 16:41:50.016: INFO: Waiting for pod downwardapi-volume-bf48e2f7-159d-4cd4-9ed0-3ac9aab072a2 to disappear
Aug 19 16:41:50.019: INFO: Pod downwardapi-volume-bf48e2f7-159d-4cd4-9ed0-3ac9aab072a2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 19 16:41:50.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6727" for this suite.

• [SLOW TEST:6.116 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":341,"skipped":6298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:41:50.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:41:50.060: INFO: Creating pod...
Aug 19 16:41:54.104: INFO: Creating service...
Aug 19 16:41:54.117: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5947/pods/agnhost/proxy/some/path/with/DELETE
Aug 19 16:41:54.123: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 19 16:41:54.123: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5947/pods/agnhost/proxy/some/path/with/GET
Aug 19 16:41:54.130: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 19 16:41:54.130: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5947/pods/agnhost/proxy/some/path/with/HEAD
Aug 19 16:41:54.134: INFO: http.Client request:HEAD | StatusCode:200
Aug 19 16:41:54.134: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5947/pods/agnhost/proxy/some/path/with/OPTIONS
Aug 19 16:41:54.140: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 19 16:41:54.140: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5947/pods/agnhost/proxy/some/path/with/PATCH
Aug 19 16:41:54.147: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 19 16:41:54.147: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5947/pods/agnhost/proxy/some/path/with/POST
Aug 19 16:41:54.151: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 19 16:41:54.151: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5947/pods/agnhost/proxy/some/path/with/PUT
Aug 19 16:41:54.154: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 19 16:41:54.154: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5947/services/test-service/proxy/some/path/with/DELETE
Aug 19 16:41:54.160: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 19 16:41:54.160: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5947/services/test-service/proxy/some/path/with/GET
Aug 19 16:41:54.166: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 19 16:41:54.166: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5947/services/test-service/proxy/some/path/with/HEAD
Aug 19 16:41:54.171: INFO: http.Client request:HEAD | StatusCode:200
Aug 19 16:41:54.171: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5947/services/test-service/proxy/some/path/with/OPTIONS
Aug 19 16:41:54.176: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 19 16:41:54.176: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5947/services/test-service/proxy/some/path/with/PATCH
Aug 19 16:41:54.183: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 19 16:41:54.183: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5947/services/test-service/proxy/some/path/with/POST
Aug 19 16:41:54.189: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 19 16:41:54.189: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5947/services/test-service/proxy/some/path/with/PUT
Aug 19 16:41:54.199: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Aug 19 16:41:54.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5947" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":356,"completed":342,"skipped":6330,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:41:54.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 19 16:41:54.312: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7757addf-3632-417d-aefd-ab9ee19c709d" in namespace "projected-9683" to be "Succeeded or Failed"
Aug 19 16:41:54.316: INFO: Pod "downwardapi-volume-7757addf-3632-417d-aefd-ab9ee19c709d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039881ms
Aug 19 16:41:56.320: INFO: Pod "downwardapi-volume-7757addf-3632-417d-aefd-ab9ee19c709d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008023105s
Aug 19 16:41:58.326: INFO: Pod "downwardapi-volume-7757addf-3632-417d-aefd-ab9ee19c709d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013267648s
Aug 19 16:42:00.330: INFO: Pod "downwardapi-volume-7757addf-3632-417d-aefd-ab9ee19c709d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018050106s
STEP: Saw pod success
Aug 19 16:42:00.330: INFO: Pod "downwardapi-volume-7757addf-3632-417d-aefd-ab9ee19c709d" satisfied condition "Succeeded or Failed"
Aug 19 16:42:00.333: INFO: Trying to get logs from node ip-10-0-164-144.ec2.internal pod downwardapi-volume-7757addf-3632-417d-aefd-ab9ee19c709d container client-container: <nil>
STEP: delete the pod
Aug 19 16:42:00.350: INFO: Waiting for pod downwardapi-volume-7757addf-3632-417d-aefd-ab9ee19c709d to disappear
Aug 19 16:42:00.353: INFO: Pod downwardapi-volume-7757addf-3632-417d-aefd-ab9ee19c709d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 19 16:42:00.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9683" for this suite.

• [SLOW TEST:6.154 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":343,"skipped":6339,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:42:00.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:42:00.431: INFO: Create a RollingUpdate DaemonSet
Aug 19 16:42:00.439: INFO: Check that daemon pods launch on every node of the cluster
Aug 19 16:42:00.445: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:00.445: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:00.445: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:00.450: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 16:42:00.450: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:42:01.455: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:01.455: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:01.455: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:01.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 16:42:01.459: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:42:02.455: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:02.455: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:02.455: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:02.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 16:42:02.459: INFO: Node ip-10-0-131-169.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:42:03.458: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:03.458: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:03.458: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:03.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 19 16:42:03.464: INFO: Node ip-10-0-164-144.ec2.internal is running 0 daemon pod, expected 1
Aug 19 16:42:04.456: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:04.456: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:04.456: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:04.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 19 16:42:04.459: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Aug 19 16:42:04.459: INFO: Update the DaemonSet to trigger a rollout
Aug 19 16:42:04.484: INFO: Updating DaemonSet daemon-set
Aug 19 16:42:07.498: INFO: Roll back the DaemonSet before rollout is complete
Aug 19 16:42:07.506: INFO: Updating DaemonSet daemon-set
Aug 19 16:42:07.506: INFO: Make sure DaemonSet rollback is complete
Aug 19 16:42:07.512: INFO: Wrong image for pod: daemon-set-fwwrf. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Aug 19 16:42:07.512: INFO: Pod daemon-set-fwwrf is not available
Aug 19 16:42:07.516: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:07.516: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:07.516: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:08.527: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:08.527: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:08.527: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:09.525: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:09.525: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:09.525: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:10.521: INFO: Pod daemon-set-9cz4m is not available
Aug 19 16:42:10.527: INFO: DaemonSet pods can't tolerate node ip-10-0-140-208.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:10.527: INFO: DaemonSet pods can't tolerate node ip-10-0-155-87.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 19 16:42:10.527: INFO: DaemonSet pods can't tolerate node ip-10-0-160-76.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4402, will wait for the garbage collector to delete the pods
Aug 19 16:42:10.601: INFO: Deleting DaemonSet.extensions daemon-set took: 8.985763ms
Aug 19 16:42:10.701: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.251718ms
Aug 19 16:42:13.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 19 16:42:13.606: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 19 16:42:13.609: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"106890"},"items":null}

Aug 19 16:42:13.612: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"106890"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 19 16:42:13.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4402" for this suite.

• [SLOW TEST:13.282 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":356,"completed":344,"skipped":6342,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:42:13.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Aug 19 16:42:13.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Aug 19 16:42:35.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
Aug 19 16:42:41.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 19 16:43:04.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7062" for this suite.

• [SLOW TEST:51.208 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":356,"completed":345,"skipped":6367,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:43:04.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1540
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 19 16:43:04.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-787 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Aug 19 16:43:04.947: INFO: stderr: ""
Aug 19 16:43:04.947: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1544
Aug 19 16:43:04.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-787 delete pods e2e-test-httpd-pod'
Aug 19 16:43:08.932: INFO: stderr: ""
Aug 19 16:43:08.932: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 16:43:08.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-787" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":356,"completed":346,"skipped":6376,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:43:08.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating api versions
Aug 19 16:43:08.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1601 api-versions'
Aug 19 16:43:09.004: INFO: stderr: ""
Aug 19 16:43:09.004: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloud.network.openshift.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.openshift.io/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 16:43:09.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1601" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":356,"completed":347,"skipped":6376,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:43:09.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 19 16:43:20.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9870" for this suite.

• [SLOW TEST:11.088 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":356,"completed":348,"skipped":6378,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:43:20.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap that has name configmap-test-emptyKey-8a65c99a-4ad8-426a-a62e-a11acad8f16a
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Aug 19 16:43:20.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5728" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":356,"completed":349,"skipped":6403,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:43:20.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Aug 19 16:43:50.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8279" for this suite.

• [SLOW TEST:30.430 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":356,"completed":350,"skipped":6483,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:43:50.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Aug 19 16:43:50.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1231 create -f -'
Aug 19 16:43:51.445: INFO: stderr: ""
Aug 19 16:43:51.445: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 19 16:43:51.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1231 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 19 16:43:51.496: INFO: stderr: ""
Aug 19 16:43:51.496: INFO: stdout: "update-demo-nautilus-2lrjj update-demo-nautilus-wpchr "
Aug 19 16:43:51.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1231 get pods update-demo-nautilus-2lrjj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 19 16:43:51.537: INFO: stderr: ""
Aug 19 16:43:51.537: INFO: stdout: ""
Aug 19 16:43:51.537: INFO: update-demo-nautilus-2lrjj is created but not running
Aug 19 16:43:56.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1231 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 19 16:43:56.582: INFO: stderr: ""
Aug 19 16:43:56.583: INFO: stdout: "update-demo-nautilus-2lrjj update-demo-nautilus-wpchr "
Aug 19 16:43:56.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1231 get pods update-demo-nautilus-2lrjj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 19 16:43:56.622: INFO: stderr: ""
Aug 19 16:43:56.622: INFO: stdout: "true"
Aug 19 16:43:56.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1231 get pods update-demo-nautilus-2lrjj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 19 16:43:56.661: INFO: stderr: ""
Aug 19 16:43:56.661: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 19 16:43:56.661: INFO: validating pod update-demo-nautilus-2lrjj
Aug 19 16:43:56.667: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 19 16:43:56.667: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 19 16:43:56.667: INFO: update-demo-nautilus-2lrjj is verified up and running
Aug 19 16:43:56.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1231 get pods update-demo-nautilus-wpchr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 19 16:43:56.708: INFO: stderr: ""
Aug 19 16:43:56.708: INFO: stdout: "true"
Aug 19 16:43:56.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1231 get pods update-demo-nautilus-wpchr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 19 16:43:56.747: INFO: stderr: ""
Aug 19 16:43:56.747: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 19 16:43:56.747: INFO: validating pod update-demo-nautilus-wpchr
Aug 19 16:43:56.753: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 19 16:43:56.753: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 19 16:43:56.753: INFO: update-demo-nautilus-wpchr is verified up and running
STEP: using delete to clean up resources
Aug 19 16:43:56.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1231 delete --grace-period=0 --force -f -'
Aug 19 16:43:56.796: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 19 16:43:56.796: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 19 16:43:56.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1231 get rc,svc -l name=update-demo --no-headers'
Aug 19 16:43:56.845: INFO: stderr: "No resources found in kubectl-1231 namespace.\n"
Aug 19 16:43:56.845: INFO: stdout: ""
Aug 19 16:43:56.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1441835109 --namespace=kubectl-1231 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 19 16:43:56.892: INFO: stderr: ""
Aug 19 16:43:56.892: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 19 16:43:56.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1231" for this suite.

• [SLOW TEST:6.316 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should create and stop a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":356,"completed":351,"skipped":6483,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:43:56.904: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8484.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8484.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8484.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8484.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
W0819 16:43:56.955406      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 19 16:44:00.996: INFO: DNS probes using dns-8484/dns-test-f4dcb42a-8ec9-42b0-a85f-db8662563ffc succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 19 16:44:01.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8484" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","total":356,"completed":352,"skipped":6494,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:44:01.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 19 16:44:08.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8209" for this suite.

• [SLOW TEST:7.056 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":356,"completed":353,"skipped":6501,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:44:08.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:44:08.192: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"90f1dae0-54fc-4a81-9fac-b91f4b363279", Controller:(*bool)(0xc009180562), BlockOwnerDeletion:(*bool)(0xc009180563)}}
Aug 19 16:44:08.204: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f3cc76ec-3a7c-4186-af3e-6b513e4c0d30", Controller:(*bool)(0xc0091808c6), BlockOwnerDeletion:(*bool)(0xc0091808c7)}}
Aug 19 16:44:08.212: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ba0681af-073e-40fb-b967-a123f00e6d8f", Controller:(*bool)(0xc009180d92), BlockOwnerDeletion:(*bool)(0xc009180d93)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 19 16:44:13.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6713" for this suite.

• [SLOW TEST:5.166 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":356,"completed":354,"skipped":6518,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:44:13.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
Aug 19 16:44:13.364: INFO: Endpoints addresses: [10.0.140.208 10.0.155.87 10.0.160.76] , ports: [6443]
Aug 19 16:44:13.364: INFO: EndpointSlices addresses: [10.0.140.208 10.0.155.87 10.0.160.76] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Aug 19 16:44:13.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-185" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":356,"completed":355,"skipped":6554,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 19 16:44:13.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1441835109
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
W0819 16:44:13.486150      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 19 16:44:19.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-46" for this suite.

• [SLOW TEST:6.153 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":356,"skipped":6567,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSAug 19 16:44:19.537: INFO: Running AfterSuite actions on all nodes
Aug 19 16:44:19.537: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func19.2
Aug 19 16:44:19.537: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Aug 19 16:44:19.537: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Aug 19 16:44:19.537: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Aug 19 16:44:19.537: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Aug 19 16:44:19.537: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Aug 19 16:44:19.537: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Aug 19 16:44:19.537: INFO: Running AfterSuite actions on node 1
Aug 19 16:44:19.537: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":356,"completed":356,"skipped":6609,"failed":0}

Ran 356 of 6965 Specs in 6399.058 seconds
SUCCESS! -- 356 Passed | 0 Failed | 0 Pending | 6609 Skipped
PASS

Ginkgo ran 1 suite in 1h46m40.244114344s
Test Suite Passed
