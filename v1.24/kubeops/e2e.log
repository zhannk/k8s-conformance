I0320 07:27:32.825364      23 e2e.go:129] Starting e2e run "68c446fa-1a5b-4cef-a49f-90076ccfe47a" on Ginkgo node 1
{"msg":"Test Suite starting","total":356,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1679297252 - Will randomize all specs
Will run 356 of 6973 specs

Mar 20 07:27:34.276: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:27:34.278: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 20 07:27:34.310: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 20 07:27:34.352: INFO: 45 / 45 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 20 07:27:34.352: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Mar 20 07:27:34.352: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 20 07:27:34.359: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar 20 07:27:34.359: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'config-update' (0 seconds elapsed)
Mar 20 07:27:34.359: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds' (0 seconds elapsed)
Mar 20 07:27:34.359: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar 20 07:27:34.359: INFO: e2e test version: v1.24.8
Mar 20 07:27:34.361: INFO: kube-apiserver version: v1.24.8
Mar 20 07:27:34.361: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:27:34.365: INFO: Cluster IP family: ipv4
S
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:27:34.365: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-probe
Mar 20 07:27:34.408: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
W0320 07:27:34.408173      23 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-6e0f9664-a3f1-4f1f-a5d1-9080463a496c in namespace container-probe-4847
Mar 20 07:27:36.440: INFO: Started pod busybox-6e0f9664-a3f1-4f1f-a5d1-9080463a496c in namespace container-probe-4847
STEP: checking the pod's current state and verifying that restartCount is present
Mar 20 07:27:36.443: INFO: Initial restart count of pod busybox-6e0f9664-a3f1-4f1f-a5d1-9080463a496c is 0
Mar 20 07:28:26.730: INFO: Restart count of pod container-probe-4847/busybox-6e0f9664-a3f1-4f1f-a5d1-9080463a496c is now 1 (50.286653823s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar 20 07:28:26.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4847" for this suite.

• [SLOW TEST:52.389 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":1,"skipped":1,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:28:26.754: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-de420e47-00e3-4c64-a064-36d14728e040
STEP: Creating a pod to test consume secrets
Mar 20 07:28:26.800: INFO: Waiting up to 5m0s for pod "pod-secrets-dbe115f0-14cc-4cba-a47f-6d9c3f035337" in namespace "secrets-9618" to be "Succeeded or Failed"
Mar 20 07:28:26.803: INFO: Pod "pod-secrets-dbe115f0-14cc-4cba-a47f-6d9c3f035337": Phase="Pending", Reason="", readiness=false. Elapsed: 2.923162ms
Mar 20 07:28:28.813: INFO: Pod "pod-secrets-dbe115f0-14cc-4cba-a47f-6d9c3f035337": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012886238s
Mar 20 07:28:30.822: INFO: Pod "pod-secrets-dbe115f0-14cc-4cba-a47f-6d9c3f035337": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022167516s
STEP: Saw pod success
Mar 20 07:28:30.822: INFO: Pod "pod-secrets-dbe115f0-14cc-4cba-a47f-6d9c3f035337" satisfied condition "Succeeded or Failed"
Mar 20 07:28:30.826: INFO: Trying to get logs from node env016ar130-worker01 pod pod-secrets-dbe115f0-14cc-4cba-a47f-6d9c3f035337 container secret-volume-test: <nil>
STEP: delete the pod
Mar 20 07:28:30.864: INFO: Waiting for pod pod-secrets-dbe115f0-14cc-4cba-a47f-6d9c3f035337 to disappear
Mar 20 07:28:30.868: INFO: Pod pod-secrets-dbe115f0-14cc-4cba-a47f-6d9c3f035337 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar 20 07:28:30.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9618" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":2,"skipped":11,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:28:30.881: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4879
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4879
I0320 07:28:30.952745      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4879, replica count: 2
Mar 20 07:28:34.008: INFO: Creating new exec pod
I0320 07:28:34.008358      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 20 07:28:37.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-4879 exec execpodrg9p2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 20 07:28:37.318: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 20 07:28:37.318: INFO: stdout: ""
Mar 20 07:28:38.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-4879 exec execpodrg9p2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 20 07:28:38.493: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 20 07:28:38.493: INFO: stdout: "externalname-service-tjqz6"
Mar 20 07:28:38.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-4879 exec execpodrg9p2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.224.181 80'
Mar 20 07:28:38.636: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.224.181 80\nConnection to 192.168.224.181 80 port [tcp/http] succeeded!\n"
Mar 20 07:28:38.636: INFO: stdout: "externalname-service-xnxtl"
Mar 20 07:28:38.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-4879 exec execpodrg9p2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.10.71 32381'
Mar 20 07:28:38.795: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.10.71 32381\nConnection to 10.2.10.71 32381 port [tcp/*] succeeded!\n"
Mar 20 07:28:38.795: INFO: stdout: "externalname-service-xnxtl"
Mar 20 07:28:38.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-4879 exec execpodrg9p2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.10.73 32381'
Mar 20 07:28:38.954: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.10.73 32381\nConnection to 10.2.10.73 32381 port [tcp/*] succeeded!\n"
Mar 20 07:28:38.954: INFO: stdout: "externalname-service-tjqz6"
Mar 20 07:28:38.954: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 07:28:38.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4879" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:8.104 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":356,"completed":3,"skipped":15,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:28:38.985: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 20 07:28:39.016: INFO: Waiting up to 5m0s for pod "pod-65149fc8-cf35-4f64-b60b-0d2478ae064b" in namespace "emptydir-2158" to be "Succeeded or Failed"
Mar 20 07:28:39.019: INFO: Pod "pod-65149fc8-cf35-4f64-b60b-0d2478ae064b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.688612ms
Mar 20 07:28:41.033: INFO: Pod "pod-65149fc8-cf35-4f64-b60b-0d2478ae064b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017048387s
Mar 20 07:28:43.041: INFO: Pod "pod-65149fc8-cf35-4f64-b60b-0d2478ae064b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024795205s
STEP: Saw pod success
Mar 20 07:28:43.041: INFO: Pod "pod-65149fc8-cf35-4f64-b60b-0d2478ae064b" satisfied condition "Succeeded or Failed"
Mar 20 07:28:43.044: INFO: Trying to get logs from node env016ar130-worker02 pod pod-65149fc8-cf35-4f64-b60b-0d2478ae064b container test-container: <nil>
STEP: delete the pod
Mar 20 07:28:43.089: INFO: Waiting for pod pod-65149fc8-cf35-4f64-b60b-0d2478ae064b to disappear
Mar 20 07:28:43.092: INFO: Pod pod-65149fc8-cf35-4f64-b60b-0d2478ae064b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar 20 07:28:43.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2158" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":4,"skipped":23,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:28:43.101: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 07:28:43.472: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 07:28:46.494: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:28:46.503: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9084-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:28:49.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1785" for this suite.
STEP: Destroying namespace "webhook-1785-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.575 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":356,"completed":5,"skipped":39,"failed":0}
SS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:28:49.676: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Mar 20 07:28:51.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7592" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","total":356,"completed":6,"skipped":41,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:28:51.756: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:28:51.792: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar 20 07:28:53.845: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Mar 20 07:28:54.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4749" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":356,"completed":7,"skipped":82,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:28:54.872: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-790aecb1-0c2e-4f90-b169-214795886ded
STEP: Creating a pod to test consume configMaps
Mar 20 07:28:54.913: INFO: Waiting up to 5m0s for pod "pod-configmaps-f988fc91-a127-4710-81a1-b37d73cd9015" in namespace "configmap-5924" to be "Succeeded or Failed"
Mar 20 07:28:54.915: INFO: Pod "pod-configmaps-f988fc91-a127-4710-81a1-b37d73cd9015": Phase="Pending", Reason="", readiness=false. Elapsed: 2.336334ms
Mar 20 07:28:56.923: INFO: Pod "pod-configmaps-f988fc91-a127-4710-81a1-b37d73cd9015": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010701966s
Mar 20 07:28:58.936: INFO: Pod "pod-configmaps-f988fc91-a127-4710-81a1-b37d73cd9015": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023323704s
STEP: Saw pod success
Mar 20 07:28:58.936: INFO: Pod "pod-configmaps-f988fc91-a127-4710-81a1-b37d73cd9015" satisfied condition "Succeeded or Failed"
Mar 20 07:28:58.940: INFO: Trying to get logs from node env016ar130-worker01 pod pod-configmaps-f988fc91-a127-4710-81a1-b37d73cd9015 container agnhost-container: <nil>
STEP: delete the pod
Mar 20 07:28:58.963: INFO: Waiting for pod pod-configmaps-f988fc91-a127-4710-81a1-b37d73cd9015 to disappear
Mar 20 07:28:58.966: INFO: Pod pod-configmaps-f988fc91-a127-4710-81a1-b37d73cd9015 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar 20 07:28:58.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5924" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":8,"skipped":87,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:28:58.977: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Deleting RuntimeClass runtimeclass-2740-delete-me
STEP: Waiting for the RuntimeClass to disappear
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Mar 20 07:28:59.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2740" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":9,"skipped":122,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:28:59.036: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-37be2280-c1bf-477a-b232-f6a61971d62b
STEP: Creating configMap with name cm-test-opt-upd-b5883ed9-e6cc-420c-b3f9-4aec8556eb8f
STEP: Creating the pod
Mar 20 07:28:59.102: INFO: The status of Pod pod-projected-configmaps-46aa52fc-1fcf-4acc-acbc-f31a9f9f25ed is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:29:01.117: INFO: The status of Pod pod-projected-configmaps-46aa52fc-1fcf-4acc-acbc-f31a9f9f25ed is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-37be2280-c1bf-477a-b232-f6a61971d62b
STEP: Updating configmap cm-test-opt-upd-b5883ed9-e6cc-420c-b3f9-4aec8556eb8f
STEP: Creating configMap with name cm-test-opt-create-415eb742-18ee-4dd3-b69c-370216bba27b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar 20 07:29:03.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5207" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":10,"skipped":140,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:29:03.211: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar 20 07:29:31.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3595" for this suite.

• [SLOW TEST:28.180 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":356,"completed":11,"skipped":143,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:29:31.392: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Mar 20 07:29:31.421: INFO: namespace kubectl-2283
Mar 20 07:29:31.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-2283 create -f -'
Mar 20 07:29:32.661: INFO: stderr: ""
Mar 20 07:29:32.661: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 20 07:29:33.670: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 20 07:29:33.670: INFO: Found 0 / 1
Mar 20 07:29:34.669: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 20 07:29:34.669: INFO: Found 1 / 1
Mar 20 07:29:34.669: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 20 07:29:34.672: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 20 07:29:34.672: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 20 07:29:34.672: INFO: wait on agnhost-primary startup in kubectl-2283 
Mar 20 07:29:34.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-2283 logs agnhost-primary-g7876 agnhost-primary'
Mar 20 07:29:34.782: INFO: stderr: ""
Mar 20 07:29:34.782: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar 20 07:29:34.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-2283 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar 20 07:29:34.871: INFO: stderr: ""
Mar 20 07:29:34.871: INFO: stdout: "service/rm2 exposed\n"
Mar 20 07:29:34.876: INFO: Service rm2 in namespace kubectl-2283 found.
STEP: exposing service
Mar 20 07:29:36.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-2283 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar 20 07:29:36.974: INFO: stderr: ""
Mar 20 07:29:36.974: INFO: stdout: "service/rm3 exposed\n"
Mar 20 07:29:36.982: INFO: Service rm3 in namespace kubectl-2283 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 07:29:38.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2283" for this suite.

• [SLOW TEST:7.620 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1249
    should create services for rc  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":356,"completed":12,"skipped":153,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:29:39.012: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 07:29:39.644: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 07:29:42.732: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:29:42.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8375" for this suite.
STEP: Destroying namespace "webhook-8375-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":356,"completed":13,"skipped":161,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:29:42.881: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override all
Mar 20 07:29:42.918: INFO: Waiting up to 5m0s for pod "client-containers-e11b421e-fc74-4fe7-a2fb-8406d4e9da22" in namespace "containers-9163" to be "Succeeded or Failed"
Mar 20 07:29:42.921: INFO: Pod "client-containers-e11b421e-fc74-4fe7-a2fb-8406d4e9da22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.977049ms
Mar 20 07:29:44.929: INFO: Pod "client-containers-e11b421e-fc74-4fe7-a2fb-8406d4e9da22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011111168s
Mar 20 07:29:46.940: INFO: Pod "client-containers-e11b421e-fc74-4fe7-a2fb-8406d4e9da22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022111134s
STEP: Saw pod success
Mar 20 07:29:46.940: INFO: Pod "client-containers-e11b421e-fc74-4fe7-a2fb-8406d4e9da22" satisfied condition "Succeeded or Failed"
Mar 20 07:29:46.944: INFO: Trying to get logs from node env016ar130-worker02 pod client-containers-e11b421e-fc74-4fe7-a2fb-8406d4e9da22 container agnhost-container: <nil>
STEP: delete the pod
Mar 20 07:29:46.962: INFO: Waiting for pod client-containers-e11b421e-fc74-4fe7-a2fb-8406d4e9da22 to disappear
Mar 20 07:29:46.966: INFO: Pod client-containers-e11b421e-fc74-4fe7-a2fb-8406d4e9da22 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Mar 20 07:29:46.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9163" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":356,"completed":14,"skipped":178,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:29:46.975: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 20 07:29:47.028: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 20 07:29:47.042: INFO: Waiting for terminating namespaces to be deleted...
Mar 20 07:29:47.047: INFO: 
Logging pods the apiserver thinks is on node env016ar130-worker01 before test
Mar 20 07:29:47.071: INFO: calico-node-hdjmf from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container calico-node ready: true, restart count 0
Mar 20 07:29:47.071: INFO: config-update-4j7v2 from kube-system started at 2023-03-17 13:36:30 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container config-update ready: true, restart count 0
Mar 20 07:29:47.071: INFO: kube-multus-ds-w25rw from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container kube-multus ready: true, restart count 0
Mar 20 07:29:47.071: INFO: kube-proxy-sjh4k from kube-system started at 2023-03-16 12:25:26 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 20 07:29:47.071: INFO: static-lb-env016ar130-worker01 from kube-system started at 2023-03-16 12:25:52 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container static-lb ready: true, restart count 0
Mar 20 07:29:47.071: INFO: alertmanager-prometheus-kube-prometheus-alertmanager-0 from kubeops started at 2023-03-17 13:36:23 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container alertmanager ready: true, restart count 1
Mar 20 07:29:47.071: INFO: 	Container config-reloader ready: true, restart count 0
Mar 20 07:29:47.071: INFO: csi-cephfsplugin-rwcrw from kubeops started at 2023-03-17 13:36:14 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 07:29:47.071: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 07:29:47.071: INFO: csi-rbdplugin-bhv5r from kubeops started at 2023-03-17 13:36:14 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 07:29:47.071: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 07:29:47.071: INFO: filebeat-filebeat-msn75 from kubeops started at 2023-03-17 13:36:14 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container filebeat ready: true, restart count 0
Mar 20 07:29:47.071: INFO: prometheus-prometheus-kube-prometheus-prometheus-0 from kubeops started at 2023-03-17 13:36:26 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container config-reloader ready: true, restart count 0
Mar 20 07:29:47.071: INFO: 	Container prometheus ready: true, restart count 0
Mar 20 07:29:47.071: INFO: prometheus-prometheus-node-exporter-s7blz from kubeops started at 2023-03-17 13:36:14 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container node-exporter ready: true, restart count 0
Mar 20 07:29:47.071: INFO: rook-ceph-crashcollector-env016ar130-worker01-766b6f9754-skgq8 from kubeops started at 2023-03-17 13:36:14 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container ceph-crash ready: true, restart count 0
Mar 20 07:29:47.071: INFO: rook-ceph-mon-c-78d58bd46b-k77fj from kubeops started at 2023-03-17 13:36:14 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 07:29:47.071: INFO: 	Container mon ready: true, restart count 0
Mar 20 07:29:47.071: INFO: rook-ceph-osd-0-c77fcd474-dprvw from kubeops started at 2023-03-17 13:36:14 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 07:29:47.071: INFO: 	Container osd ready: true, restart count 0
Mar 20 07:29:47.071: INFO: rook-ceph-osd-prepare-env016ar130-worker01-mgg4n from kubeops started at 2023-03-20 06:41:18 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container provision ready: false, restart count 0
Mar 20 07:29:47.071: INFO: rook-discover-hgkbz from kubeops started at 2023-03-17 13:36:14 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container rook-discover ready: true, restart count 0
Mar 20 07:29:47.071: INFO: sonobuoy from sonobuoy started at 2023-03-20 07:27:30 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 20 07:29:47.071: INFO: sonobuoy-e2e-job-5ef492aa84d840a2 from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container e2e ready: true, restart count 0
Mar 20 07:29:47.071: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 07:29:47.071: INFO: sonobuoy-systemd-logs-daemon-set-3a399369495241ac-j79q9 from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.071: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 07:29:47.071: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 20 07:29:47.071: INFO: 
Logging pods the apiserver thinks is on node env016ar130-worker02 before test
Mar 20 07:29:47.097: INFO: calico-node-bphlv from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container calico-node ready: true, restart count 0
Mar 20 07:29:47.097: INFO: config-update-9h6xg from kube-system started at 2023-03-16 14:15:50 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container config-update ready: true, restart count 0
Mar 20 07:29:47.097: INFO: kube-multus-ds-9xqdf from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container kube-multus ready: true, restart count 0
Mar 20 07:29:47.097: INFO: kube-proxy-dk5kb from kube-system started at 2023-03-16 12:25:25 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 20 07:29:47.097: INFO: static-lb-env016ar130-worker02 from kube-system started at 2023-03-16 12:25:53 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container static-lb ready: true, restart count 0
Mar 20 07:29:47.097: INFO: csi-cephfsplugin-gd4zn from kubeops started at 2023-03-16 14:15:32 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 07:29:47.097: INFO: csi-cephfsplugin-provisioner-8f66f988-nr7c7 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (5 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 07:29:47.097: INFO: csi-rbdplugin-mkm94 from kubeops started at 2023-03-16 14:15:05 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 07:29:47.097: INFO: csi-rbdplugin-provisioner-7bb4c8b9c7-h8jb6 from kubeops started at 2023-03-16 15:03:27 +0000 UTC (5 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 07:29:47.097: INFO: filebeat-filebeat-pf2vz from kubeops started at 2023-03-16 14:15:59 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container filebeat ready: true, restart count 0
Mar 20 07:29:47.097: INFO: gatekeeper-audit-749874bd85-gbfdv from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container manager ready: true, restart count 0
Mar 20 07:29:47.097: INFO: gatekeeper-controller-manager-768fd8789c-s4xkm from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container manager ready: true, restart count 0
Mar 20 07:29:47.097: INFO: gatekeeper-controller-manager-768fd8789c-w7sxj from kubeops started at 2023-03-16 15:03:30 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container manager ready: true, restart count 0
Mar 20 07:29:47.097: INFO: gatekeeper-controller-manager-768fd8789c-wgpj5 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container manager ready: true, restart count 0
Mar 20 07:29:47.097: INFO: harbor-chartmuseum-7df8df844b-8fds2 from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container chartmuseum ready: true, restart count 0
Mar 20 07:29:47.097: INFO: harbor-jobservice-7b746648f6-x2kd8 from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container jobservice ready: true, restart count 1
Mar 20 07:29:47.097: INFO: harbor-notary-signer-c9db4c94d-fs46b from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container notary-signer ready: true, restart count 0
Mar 20 07:29:47.097: INFO: harbor-notary-signer-c9db4c94d-zqpzz from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container notary-signer ready: true, restart count 0
Mar 20 07:29:47.097: INFO: harbor-portal-dbbbb4456-7hb5f from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container portal ready: true, restart count 0
Mar 20 07:29:47.097: INFO: harbor-registry-b66c45c5f-lq8vr from kubeops started at 2023-03-16 14:14:52 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container registry ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container registryctl ready: true, restart count 0
Mar 20 07:29:47.097: INFO: harbor-trivy-1 from kubeops started at 2023-03-16 14:16:07 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container trivy ready: true, restart count 0
Mar 20 07:29:47.097: INFO: opensearch-cluster-master-0 from kubeops started at 2023-03-16 14:16:53 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container opensearch ready: true, restart count 0
Mar 20 07:29:47.097: INFO: opensearch-cluster-master-2 from kubeops started at 2023-03-17 12:48:10 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container opensearch ready: true, restart count 0
Mar 20 07:29:47.097: INFO: prometheus-prometheus-node-exporter-rxbn4 from kubeops started at 2023-03-16 14:15:38 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container node-exporter ready: true, restart count 0
Mar 20 07:29:47.097: INFO: rook-ceph-crashcollector-env016ar130-worker02-799c88cbcf-9ktc7 from kubeops started at 2023-03-16 15:03:28 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container ceph-crash ready: true, restart count 0
Mar 20 07:29:47.097: INFO: rook-ceph-mds-myfs-a-85c5fc87d-gj7d4 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container mds ready: true, restart count 0
Mar 20 07:29:47.097: INFO: rook-ceph-mgr-a-857696f864-4gjb2 from kubeops started at 2023-03-16 15:03:30 +0000 UTC (3 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container mgr ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container watch-active ready: true, restart count 0
Mar 20 07:29:47.097: INFO: rook-ceph-mon-a-cc8d49d77-sgj2c from kubeops started at 2023-03-16 14:15:59 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container mon ready: true, restart count 0
Mar 20 07:29:47.097: INFO: rook-ceph-osd-2-557dc55864-679xk from kubeops started at 2023-03-16 14:15:54 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container osd ready: true, restart count 0
Mar 20 07:29:47.097: INFO: rook-ceph-osd-prepare-env016ar130-worker02-9qkbd from kubeops started at 2023-03-20 06:41:22 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container provision ready: false, restart count 0
Mar 20 07:29:47.097: INFO: rook-ceph-tools-59d749577f-chcwh from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Mar 20 07:29:47.097: INFO: rook-discover-x9lwv from kubeops started at 2023-03-16 14:15:44 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container rook-discover ready: true, restart count 0
Mar 20 07:29:47.097: INFO: sonobuoy-systemd-logs-daemon-set-3a399369495241ac-swzbr from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.097: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 20 07:29:47.097: INFO: 
Logging pods the apiserver thinks is on node env016ar130-worker03 before test
Mar 20 07:29:47.120: INFO: calico-node-cpqql from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.120: INFO: 	Container calico-node ready: true, restart count 0
Mar 20 07:29:47.120: INFO: config-update-lj4n7 from kube-system started at 2023-03-16 12:35:49 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.120: INFO: 	Container config-update ready: true, restart count 0
Mar 20 07:29:47.120: INFO: kube-multus-ds-xcsml from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.120: INFO: 	Container kube-multus ready: true, restart count 0
Mar 20 07:29:47.120: INFO: kube-proxy-hxh8h from kube-system started at 2023-03-16 12:25:31 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.121: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 20 07:29:47.121: INFO: static-lb-env016ar130-worker03 from kube-system started at 2023-03-16 12:25:53 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.121: INFO: 	Container static-lb ready: true, restart count 0
Mar 20 07:29:47.121: INFO: csi-cephfsplugin-dpmqw from kubeops started at 2023-03-16 12:41:07 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.121: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 07:29:47.121: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 07:29:47.121: INFO: csi-cephfsplugin-provisioner-8f66f988-hg4ds from kubeops started at 2023-03-16 14:14:52 +0000 UTC (5 container statuses recorded)
Mar 20 07:29:47.121: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 07:29:47.121: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 07:29:47.121: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 07:29:47.121: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 07:29:47.121: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 07:29:47.121: INFO: csi-rbdplugin-fhk4v from kubeops started at 2023-03-16 12:41:07 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.121: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 07:29:47.121: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 07:29:47.121: INFO: csi-rbdplugin-provisioner-7bb4c8b9c7-z7z8d from kubeops started at 2023-03-16 12:41:07 +0000 UTC (5 container statuses recorded)
Mar 20 07:29:47.121: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 07:29:47.121: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 07:29:47.121: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 07:29:47.121: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 07:29:47.121: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 07:29:47.121: INFO: filebeat-filebeat-hbpls from kubeops started at 2023-03-16 12:56:38 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.121: INFO: 	Container filebeat ready: true, restart count 0
Mar 20 07:29:47.121: INFO: harbor-chartmuseum-7df8df844b-7zqxs from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.121: INFO: 	Container chartmuseum ready: true, restart count 0
Mar 20 07:29:47.121: INFO: harbor-chartmuseum-7df8df844b-hks64 from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.121: INFO: 	Container chartmuseum ready: true, restart count 0
Mar 20 07:29:47.121: INFO: harbor-core-d54995cdd-6c49r from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.121: INFO: 	Container core ready: true, restart count 0
Mar 20 07:29:47.121: INFO: harbor-core-d54995cdd-brrck from kubeops started at 2023-03-16 15:03:27 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.121: INFO: 	Container core ready: true, restart count 1
Mar 20 07:29:47.121: INFO: harbor-core-d54995cdd-n4l9h from kubeops started at 2023-03-16 15:03:28 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.121: INFO: 	Container core ready: true, restart count 0
Mar 20 07:29:47.121: INFO: harbor-jobservice-7b746648f6-cv6bz from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.121: INFO: 	Container jobservice ready: true, restart count 1
Mar 20 07:29:47.121: INFO: harbor-jobservice-7b746648f6-l24wk from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.121: INFO: 	Container jobservice ready: true, restart count 4
Mar 20 07:29:47.122: INFO: harbor-nginx-f8c975ff6-2bwcd from kubeops started at 2023-03-16 12:54:06 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container nginx ready: true, restart count 0
Mar 20 07:29:47.122: INFO: harbor-nginx-f8c975ff6-5hv2g from kubeops started at 2023-03-16 15:03:27 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container nginx ready: true, restart count 0
Mar 20 07:29:47.122: INFO: harbor-nginx-f8c975ff6-hllb9 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container nginx ready: true, restart count 0
Mar 20 07:29:47.122: INFO: harbor-notary-server-89fd67fc6-67cdb from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container notary-server ready: true, restart count 2
Mar 20 07:29:47.122: INFO: harbor-notary-server-89fd67fc6-7h95b from kubeops started at 2023-03-16 12:54:06 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container notary-server ready: true, restart count 0
Mar 20 07:29:47.122: INFO: harbor-notary-server-89fd67fc6-kn77l from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container notary-server ready: true, restart count 2
Mar 20 07:29:47.122: INFO: harbor-notary-signer-c9db4c94d-txwpf from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container notary-signer ready: true, restart count 2
Mar 20 07:29:47.122: INFO: harbor-portal-dbbbb4456-gqbx6 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container portal ready: true, restart count 0
Mar 20 07:29:47.122: INFO: harbor-portal-dbbbb4456-qgsgw from kubeops started at 2023-03-16 12:54:06 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container portal ready: true, restart count 0
Mar 20 07:29:47.122: INFO: harbor-registry-b66c45c5f-d5q94 from kubeops started at 2023-03-16 12:54:07 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container registry ready: true, restart count 0
Mar 20 07:29:47.122: INFO: 	Container registryctl ready: true, restart count 0
Mar 20 07:29:47.122: INFO: harbor-registry-b66c45c5f-fr9g4 from kubeops started at 2023-03-16 15:03:27 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container registry ready: true, restart count 0
Mar 20 07:29:47.122: INFO: 	Container registryctl ready: true, restart count 0
Mar 20 07:29:47.122: INFO: harbor-trivy-0 from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container trivy ready: true, restart count 0
Mar 20 07:29:47.122: INFO: harbor-trivy-2 from kubeops started at 2023-03-16 12:55:51 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container trivy ready: true, restart count 0
Mar 20 07:29:47.122: INFO: logstash-logstash-0 from kubeops started at 2023-03-16 12:56:41 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container logstash ready: true, restart count 1
Mar 20 07:29:47.122: INFO: opensearch-cluster-master-1 from kubeops started at 2023-03-16 12:56:36 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container opensearch ready: true, restart count 0
Mar 20 07:29:47.122: INFO: opensearch-dashboards-69f44df846-zmxg9 from kubeops started at 2023-03-16 12:56:36 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container dashboards ready: true, restart count 0
Mar 20 07:29:47.122: INFO: postgres-7f6cc6d46c-cdpmp from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container postgres ready: true, restart count 0
Mar 20 07:29:47.122: INFO: prometheus-grafana-5c58fc9dbb-gj98n from kubeops started at 2023-03-16 12:55:16 +0000 UTC (3 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container grafana ready: true, restart count 0
Mar 20 07:29:47.122: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Mar 20 07:29:47.122: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Mar 20 07:29:47.122: INFO: prometheus-kube-prometheus-operator-76b748b4b7-hgsmt from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Mar 20 07:29:47.122: INFO: prometheus-kube-state-metrics-85655df84d-qsk5t from kubeops started at 2023-03-16 12:55:16 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 20 07:29:47.122: INFO: prometheus-prometheus-node-exporter-sqpkb from kubeops started at 2023-03-16 12:55:16 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container node-exporter ready: true, restart count 0
Mar 20 07:29:47.122: INFO: redis-6fdbc8bc6c-cj4g2 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container redis ready: true, restart count 0
Mar 20 07:29:47.122: INFO: rook-ceph-crashcollector-env016ar130-worker03-68f58d9f4b-pbp4c from kubeops started at 2023-03-16 12:44:17 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container ceph-crash ready: true, restart count 0
Mar 20 07:29:47.122: INFO: rook-ceph-mds-myfs-b-767cb4cfc5-p5f4k from kubeops started at 2023-03-16 12:44:16 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 07:29:47.122: INFO: 	Container mds ready: true, restart count 0
Mar 20 07:29:47.122: INFO: rook-ceph-mgr-b-69f6d8d6c-8klhf from kubeops started at 2023-03-16 12:43:36 +0000 UTC (3 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 07:29:47.122: INFO: 	Container mgr ready: true, restart count 0
Mar 20 07:29:47.122: INFO: 	Container watch-active ready: true, restart count 0
Mar 20 07:29:47.122: INFO: rook-ceph-mon-b-7b5485875c-fjr22 from kubeops started at 2023-03-16 12:43:15 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 07:29:47.122: INFO: 	Container mon ready: true, restart count 0
Mar 20 07:29:47.122: INFO: rook-ceph-operator-f6f75855b-lt27v from kubeops started at 2023-03-16 12:39:01 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.122: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Mar 20 07:29:47.122: INFO: rook-ceph-osd-1-755cf9d57c-j6jcs from kubeops started at 2023-03-16 12:44:07 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.123: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 07:29:47.123: INFO: 	Container osd ready: true, restart count 0
Mar 20 07:29:47.123: INFO: rook-ceph-osd-prepare-env016ar130-worker03-cqqzt from kubeops started at 2023-03-20 06:41:25 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.123: INFO: 	Container provision ready: false, restart count 0
Mar 20 07:29:47.123: INFO: rook-discover-wj92q from kubeops started at 2023-03-16 12:39:43 +0000 UTC (1 container statuses recorded)
Mar 20 07:29:47.123: INFO: 	Container rook-discover ready: true, restart count 0
Mar 20 07:29:47.123: INFO: sonobuoy-systemd-logs-daemon-set-3a399369495241ac-zkwlr from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 07:29:47.123: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 07:29:47.123: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.174e10138aec825a], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 node(s) didn't match Pod's node affinity/selector. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Mar 20 07:29:48.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5215" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":356,"completed":15,"skipped":179,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:29:48.213: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 07:29:48.254: INFO: Waiting up to 5m0s for pod "downwardapi-volume-782d9e5b-516f-48a3-a407-f0e31dd60aca" in namespace "downward-api-1082" to be "Succeeded or Failed"
Mar 20 07:29:48.257: INFO: Pod "downwardapi-volume-782d9e5b-516f-48a3-a407-f0e31dd60aca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.986202ms
Mar 20 07:29:50.274: INFO: Pod "downwardapi-volume-782d9e5b-516f-48a3-a407-f0e31dd60aca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019254232s
Mar 20 07:29:52.279: INFO: Pod "downwardapi-volume-782d9e5b-516f-48a3-a407-f0e31dd60aca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025033839s
STEP: Saw pod success
Mar 20 07:29:52.279: INFO: Pod "downwardapi-volume-782d9e5b-516f-48a3-a407-f0e31dd60aca" satisfied condition "Succeeded or Failed"
Mar 20 07:29:52.283: INFO: Trying to get logs from node env016ar130-worker02 pod downwardapi-volume-782d9e5b-516f-48a3-a407-f0e31dd60aca container client-container: <nil>
STEP: delete the pod
Mar 20 07:29:52.360: INFO: Waiting for pod downwardapi-volume-782d9e5b-516f-48a3-a407-f0e31dd60aca to disappear
Mar 20 07:29:52.364: INFO: Pod downwardapi-volume-782d9e5b-516f-48a3-a407-f0e31dd60aca no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar 20 07:29:52.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1082" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":16,"skipped":192,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:29:52.377: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Mar 20 07:29:52.428: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:29:54.438: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.2.10.72 on the node which pod1 resides and expect scheduled
Mar 20 07:29:54.455: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:29:56.464: INFO: The status of Pod pod2 is Running (Ready = false)
Mar 20 07:29:58.463: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.2.10.72 but use UDP protocol on the node which pod2 resides
Mar 20 07:29:58.480: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:30:00.496: INFO: The status of Pod pod3 is Running (Ready = true)
Mar 20 07:30:00.513: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:30:02.521: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Mar 20 07:30:02.525: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.2.10.72 http://127.0.0.1:54323/hostname] Namespace:hostport-7453 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:30:02.525: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:30:02.526: INFO: ExecWithOptions: Clientset creation
Mar 20 07:30:02.526: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/hostport-7453/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.2.10.72+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.2.10.72, port: 54323
Mar 20 07:30:02.629: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.2.10.72:54323/hostname] Namespace:hostport-7453 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:30:02.629: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:30:02.630: INFO: ExecWithOptions: Clientset creation
Mar 20 07:30:02.631: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/hostport-7453/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.2.10.72%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.2.10.72, port: 54323 UDP
Mar 20 07:30:02.725: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.2.10.72 54323] Namespace:hostport-7453 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:30:02.725: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:30:02.726: INFO: ExecWithOptions: Clientset creation
Mar 20 07:30:02.726: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/hostport-7453/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+10.2.10.72+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:188
Mar 20 07:30:07.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-7453" for this suite.

• [SLOW TEST:15.466 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":356,"completed":17,"skipped":206,"failed":0}
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:30:07.843: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 20 07:30:07.965: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar 20 07:30:07.969: INFO: starting watch
STEP: patching
STEP: updating
Mar 20 07:30:07.988: INFO: waiting for watch events with expected annotations
Mar 20 07:30:07.988: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:188
Mar 20 07:30:08.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5073" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":356,"completed":18,"skipped":206,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:30:08.038: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-7f4e696b-a407-460d-908a-da3d4735f08f
STEP: Creating a pod to test consume secrets
Mar 20 07:30:08.083: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2c091669-0e9a-44c3-bd29-f312a135851d" in namespace "projected-2356" to be "Succeeded or Failed"
Mar 20 07:30:08.085: INFO: Pod "pod-projected-secrets-2c091669-0e9a-44c3-bd29-f312a135851d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.413387ms
Mar 20 07:30:10.094: INFO: Pod "pod-projected-secrets-2c091669-0e9a-44c3-bd29-f312a135851d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011628328s
Mar 20 07:30:12.102: INFO: Pod "pod-projected-secrets-2c091669-0e9a-44c3-bd29-f312a135851d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019508393s
STEP: Saw pod success
Mar 20 07:30:12.102: INFO: Pod "pod-projected-secrets-2c091669-0e9a-44c3-bd29-f312a135851d" satisfied condition "Succeeded or Failed"
Mar 20 07:30:12.107: INFO: Trying to get logs from node env016ar130-worker01 pod pod-projected-secrets-2c091669-0e9a-44c3-bd29-f312a135851d container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 20 07:30:12.130: INFO: Waiting for pod pod-projected-secrets-2c091669-0e9a-44c3-bd29-f312a135851d to disappear
Mar 20 07:30:12.133: INFO: Pod pod-projected-secrets-2c091669-0e9a-44c3-bd29-f312a135851d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Mar 20 07:30:12.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2356" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":19,"skipped":209,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:30:12.145: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating all guestbook components
Mar 20 07:30:12.173: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar 20 07:30:12.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5569 create -f -'
Mar 20 07:30:13.363: INFO: stderr: ""
Mar 20 07:30:13.363: INFO: stdout: "service/agnhost-replica created\n"
Mar 20 07:30:13.363: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar 20 07:30:13.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5569 create -f -'
Mar 20 07:30:14.503: INFO: stderr: ""
Mar 20 07:30:14.503: INFO: stdout: "service/agnhost-primary created\n"
Mar 20 07:30:14.503: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 20 07:30:14.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5569 create -f -'
Mar 20 07:30:14.697: INFO: stderr: ""
Mar 20 07:30:14.697: INFO: stdout: "service/frontend created\n"
Mar 20 07:30:14.697: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 20 07:30:14.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5569 create -f -'
Mar 20 07:30:14.914: INFO: stderr: ""
Mar 20 07:30:14.914: INFO: stdout: "deployment.apps/frontend created\n"
Mar 20 07:30:14.915: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 20 07:30:14.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5569 create -f -'
Mar 20 07:30:15.107: INFO: stderr: ""
Mar 20 07:30:15.107: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar 20 07:30:15.107: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 20 07:30:15.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5569 create -f -'
Mar 20 07:30:15.390: INFO: stderr: ""
Mar 20 07:30:15.390: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Mar 20 07:30:15.390: INFO: Waiting for all frontend pods to be Running.
Mar 20 07:30:20.442: INFO: Waiting for frontend to serve content.
Mar 20 07:30:20.460: INFO: Trying to add a new entry to the guestbook.
Mar 20 07:30:20.472: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar 20 07:30:20.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5569 delete --grace-period=0 --force -f -'
Mar 20 07:30:20.585: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 20 07:30:20.585: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Mar 20 07:30:20.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5569 delete --grace-period=0 --force -f -'
Mar 20 07:30:20.661: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 20 07:30:20.661: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar 20 07:30:20.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5569 delete --grace-period=0 --force -f -'
Mar 20 07:30:20.729: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 20 07:30:20.729: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 20 07:30:20.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5569 delete --grace-period=0 --force -f -'
Mar 20 07:30:20.797: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 20 07:30:20.797: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 20 07:30:20.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5569 delete --grace-period=0 --force -f -'
Mar 20 07:30:20.866: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 20 07:30:20.866: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar 20 07:30:20.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5569 delete --grace-period=0 --force -f -'
Mar 20 07:30:20.944: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 20 07:30:20.944: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 07:30:20.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5569" for this suite.

• [SLOW TEST:8.816 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:340
    should create and stop a working application  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":356,"completed":20,"skipped":228,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:30:20.961: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar 20 07:30:21.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2948" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":356,"completed":21,"skipped":274,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:30:21.028: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name secret-emptykey-test-e605b8f5-482c-4f32-8362-51843827a656
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Mar 20 07:30:21.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3501" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":356,"completed":22,"skipped":282,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:30:21.069: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Mar 20 07:30:21.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6514" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":23,"skipped":296,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:30:21.223: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Mar 20 07:30:21.267: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:30:23.277: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 20 07:30:24.314: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Mar 20 07:30:25.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1206" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":356,"completed":24,"skipped":338,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:30:25.366: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:30:25.394: INFO: Creating deployment "test-recreate-deployment"
Mar 20 07:30:25.401: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 20 07:30:25.406: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Mar 20 07:30:27.419: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 20 07:30:27.421: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 20 07:30:27.439: INFO: Updating deployment test-recreate-deployment
Mar 20 07:30:27.439: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 20 07:30:27.559: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-250  671dd707-6bda-4d90-a0e7-004757d925a4 802524 2 2023-03-20 07:30:25 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-03-20 07:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 07:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ced0f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-20 07:30:27 +0000 UTC,LastTransitionTime:2023-03-20 07:30:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cd8586fc7" is progressing.,LastUpdateTime:2023-03-20 07:30:27 +0000 UTC,LastTransitionTime:2023-03-20 07:30:25 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 20 07:30:27.562: INFO: New ReplicaSet "test-recreate-deployment-cd8586fc7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cd8586fc7  deployment-250  6cf5f124-c5e4-4a63-9f19-75df25d42005 802523 1 2023-03-20 07:30:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 671dd707-6bda-4d90-a0e7-004757d925a4 0xc003d7aaa0 0xc003d7aaa1}] []  [{kube-controller-manager Update apps/v1 2023-03-20 07:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"671dd707-6bda-4d90-a0e7-004757d925a4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 07:30:27 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cd8586fc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d7ab38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 20 07:30:27.562: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 20 07:30:27.562: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-845d658455  deployment-250  7377dcf6-648a-404b-953e-30a89b6fc14f 802513 2 2023-03-20 07:30:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 671dd707-6bda-4d90-a0e7-004757d925a4 0xc003d7a987 0xc003d7a988}] []  [{kube-controller-manager Update apps/v1 2023-03-20 07:30:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"671dd707-6bda-4d90-a0e7-004757d925a4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 07:30:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 845d658455,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d7aa38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 20 07:30:27.566: INFO: Pod "test-recreate-deployment-cd8586fc7-hv5s2" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cd8586fc7-hv5s2 test-recreate-deployment-cd8586fc7- deployment-250  dfc6d63d-4f46-4e90-94a3-66c859a1a1ac 802525 0 2023-03-20 07:30:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cd8586fc7 6cf5f124-c5e4-4a63-9f19-75df25d42005 0xc003ced490 0xc003ced491}] []  [{kube-controller-manager Update v1 2023-03-20 07:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cf5f124-c5e4-4a63-9f19-75df25d42005\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:30:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jtzpl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jtzpl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:30:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:30:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:30:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:30:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.71,PodIP:,StartTime:2023-03-20 07:30:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar 20 07:30:27.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-250" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":25,"skipped":340,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:30:27.574: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Mar 20 07:30:27.623: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:30:29.636: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Mar 20 07:30:29.661: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:30:31.675: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 20 07:30:31.715: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 20 07:30:31.719: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 20 07:30:33.720: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 20 07:30:33.733: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 20 07:30:35.720: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 20 07:30:35.734: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Mar 20 07:30:35.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1608" for this suite.

• [SLOW TEST:8.182 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":356,"completed":26,"skipped":348,"failed":0}
SSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:30:35.756: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Mar 20 07:30:41.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4363" for this suite.

• [SLOW TEST:6.132 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":356,"completed":27,"skipped":352,"failed":0}
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:30:41.889: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:30:41.926: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-e7ea4fd4-1dfb-4299-8a43-d839e65ad723" in namespace "security-context-test-1594" to be "Succeeded or Failed"
Mar 20 07:30:41.929: INFO: Pod "alpine-nnp-false-e7ea4fd4-1dfb-4299-8a43-d839e65ad723": Phase="Pending", Reason="", readiness=false. Elapsed: 2.701051ms
Mar 20 07:30:43.938: INFO: Pod "alpine-nnp-false-e7ea4fd4-1dfb-4299-8a43-d839e65ad723": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011525724s
Mar 20 07:30:45.953: INFO: Pod "alpine-nnp-false-e7ea4fd4-1dfb-4299-8a43-d839e65ad723": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026538608s
Mar 20 07:30:45.953: INFO: Pod "alpine-nnp-false-e7ea4fd4-1dfb-4299-8a43-d839e65ad723" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Mar 20 07:30:45.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1594" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":28,"skipped":352,"failed":0}

------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:30:45.975: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Mar 20 07:30:46.014: INFO: Waiting up to 5m0s for pod "security-context-dec7dded-88a9-4b1b-8a63-92f6d5cc463f" in namespace "security-context-4520" to be "Succeeded or Failed"
Mar 20 07:30:46.018: INFO: Pod "security-context-dec7dded-88a9-4b1b-8a63-92f6d5cc463f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.64308ms
Mar 20 07:30:48.031: INFO: Pod "security-context-dec7dded-88a9-4b1b-8a63-92f6d5cc463f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017104493s
Mar 20 07:30:50.039: INFO: Pod "security-context-dec7dded-88a9-4b1b-8a63-92f6d5cc463f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024884224s
STEP: Saw pod success
Mar 20 07:30:50.039: INFO: Pod "security-context-dec7dded-88a9-4b1b-8a63-92f6d5cc463f" satisfied condition "Succeeded or Failed"
Mar 20 07:30:50.043: INFO: Trying to get logs from node env016ar130-worker01 pod security-context-dec7dded-88a9-4b1b-8a63-92f6d5cc463f container test-container: <nil>
STEP: delete the pod
Mar 20 07:30:50.060: INFO: Waiting for pod security-context-dec7dded-88a9-4b1b-8a63-92f6d5cc463f to disappear
Mar 20 07:30:50.064: INFO: Pod security-context-dec7dded-88a9-4b1b-8a63-92f6d5cc463f no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Mar 20 07:30:50.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4520" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":29,"skipped":352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:30:50.076: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:30:50.109: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Mar 20 07:30:55.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-1360 --namespace=crd-publish-openapi-1360 create -f -'
Mar 20 07:30:56.131: INFO: stderr: ""
Mar 20 07:30:56.131: INFO: stdout: "e2e-test-crd-publish-openapi-2744-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 20 07:30:56.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-1360 --namespace=crd-publish-openapi-1360 delete e2e-test-crd-publish-openapi-2744-crds test-cr'
Mar 20 07:30:56.210: INFO: stderr: ""
Mar 20 07:30:56.210: INFO: stdout: "e2e-test-crd-publish-openapi-2744-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 20 07:30:56.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-1360 --namespace=crd-publish-openapi-1360 apply -f -'
Mar 20 07:30:57.034: INFO: stderr: ""
Mar 20 07:30:57.034: INFO: stdout: "e2e-test-crd-publish-openapi-2744-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 20 07:30:57.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-1360 --namespace=crd-publish-openapi-1360 delete e2e-test-crd-publish-openapi-2744-crds test-cr'
Mar 20 07:30:57.111: INFO: stderr: ""
Mar 20 07:30:57.111: INFO: stdout: "e2e-test-crd-publish-openapi-2744-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar 20 07:30:57.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-1360 explain e2e-test-crd-publish-openapi-2744-crds'
Mar 20 07:30:57.327: INFO: stderr: ""
Mar 20 07:30:57.327: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2744-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:31:02.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1360" for this suite.

• [SLOW TEST:12.223 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":356,"completed":30,"skipped":375,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:31:02.299: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 07:31:02.627: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 07:31:05.654: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:31:05.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6791" for this suite.
STEP: Destroying namespace "webhook-6791-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":356,"completed":31,"skipped":428,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:31:05.758: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2500
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-2500
Mar 20 07:31:05.790: INFO: Found 0 stateful pods, waiting for 1
Mar 20 07:31:15.807: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Mar 20 07:31:15.832: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Mar 20 07:31:15.841: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Mar 20 07:31:15.844: INFO: Observed &StatefulSet event: ADDED
Mar 20 07:31:15.844: INFO: Found Statefulset ss in namespace statefulset-2500 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 20 07:31:15.844: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Mar 20 07:31:15.844: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 20 07:31:15.848: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Mar 20 07:31:15.850: INFO: Observed &StatefulSet event: ADDED
Mar 20 07:31:15.850: INFO: Observed Statefulset ss in namespace statefulset-2500 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 20 07:31:15.850: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 20 07:31:15.850: INFO: Deleting all statefulset in ns statefulset-2500
Mar 20 07:31:15.853: INFO: Scaling statefulset ss to 0
Mar 20 07:31:25.880: INFO: Waiting for statefulset status.replicas updated to 0
Mar 20 07:31:25.890: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar 20 07:31:25.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2500" for this suite.

• [SLOW TEST:20.224 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":356,"completed":32,"skipped":447,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:31:25.983: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Mar 20 07:31:32.089: INFO: 80 pods remaining
Mar 20 07:31:32.089: INFO: 80 pods has nil DeletionTimestamp
Mar 20 07:31:32.089: INFO: 
Mar 20 07:31:33.057: INFO: 71 pods remaining
Mar 20 07:31:33.057: INFO: 71 pods has nil DeletionTimestamp
Mar 20 07:31:33.057: INFO: 
Mar 20 07:31:34.065: INFO: 59 pods remaining
Mar 20 07:31:34.065: INFO: 59 pods has nil DeletionTimestamp
Mar 20 07:31:34.065: INFO: 
Mar 20 07:31:35.058: INFO: 40 pods remaining
Mar 20 07:31:35.058: INFO: 40 pods has nil DeletionTimestamp
Mar 20 07:31:35.058: INFO: 
Mar 20 07:31:36.064: INFO: 31 pods remaining
Mar 20 07:31:36.064: INFO: 31 pods has nil DeletionTimestamp
Mar 20 07:31:36.064: INFO: 
Mar 20 07:31:37.055: INFO: 19 pods remaining
Mar 20 07:31:37.055: INFO: 19 pods has nil DeletionTimestamp
Mar 20 07:31:37.055: INFO: 
STEP: Gathering metrics
Mar 20 07:31:38.094: INFO: The status of Pod kube-controller-manager-env016ar130-master03 is Running (Ready = true)
Mar 20 07:31:38.169: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Mar 20 07:31:38.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6207" for this suite.

• [SLOW TEST:12.196 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":356,"completed":33,"skipped":478,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:31:38.179: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 07:31:38.489: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 20 07:31:40.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 20, 7, 31, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 7, 31, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 7, 31, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 7, 31, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 20 07:31:42.512: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 20, 7, 31, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 7, 31, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 7, 31, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 7, 31, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 20 07:31:44.506: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 20, 7, 31, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 7, 31, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 7, 31, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 7, 31, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 07:31:47.529: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:31:47.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6397" for this suite.
STEP: Destroying namespace "webhook-6397-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:9.469 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":356,"completed":34,"skipped":497,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:31:47.648: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-8334
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 20 07:31:47.682: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 20 07:31:47.722: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:31:49.729: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 07:31:51.730: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 07:31:53.734: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 07:31:55.758: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 07:31:57.734: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 07:31:59.738: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 20 07:31:59.745: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 20 07:31:59.751: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 20 07:32:01.820: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 20 07:32:01.820: INFO: Going to poll 192.168.90.68 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 20 07:32:01.823: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.90.68:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8334 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:32:01.823: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:32:01.824: INFO: ExecWithOptions: Clientset creation
Mar 20 07:32:01.824: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/pod-network-test-8334/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.90.68%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 20 07:32:01.930: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 20 07:32:01.930: INFO: Going to poll 192.168.12.20 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 20 07:32:01.935: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.12.20:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8334 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:32:01.935: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:32:01.936: INFO: ExecWithOptions: Clientset creation
Mar 20 07:32:01.936: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/pod-network-test-8334/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.12.20%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 20 07:32:02.040: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 20 07:32:02.040: INFO: Going to poll 192.168.71.172 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 20 07:32:02.045: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.71.172:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8334 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:32:02.045: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:32:02.045: INFO: ExecWithOptions: Clientset creation
Mar 20 07:32:02.045: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/pod-network-test-8334/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.71.172%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 20 07:32:02.135: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Mar 20 07:32:02.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8334" for this suite.

• [SLOW TEST:14.501 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":35,"skipped":499,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:32:02.149: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 07:32:02.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84054ede-7972-4355-9f98-b2da56b6dd17" in namespace "downward-api-8261" to be "Succeeded or Failed"
Mar 20 07:32:02.186: INFO: Pod "downwardapi-volume-84054ede-7972-4355-9f98-b2da56b6dd17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.633469ms
Mar 20 07:32:04.195: INFO: Pod "downwardapi-volume-84054ede-7972-4355-9f98-b2da56b6dd17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012054175s
Mar 20 07:32:06.204: INFO: Pod "downwardapi-volume-84054ede-7972-4355-9f98-b2da56b6dd17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020290396s
STEP: Saw pod success
Mar 20 07:32:06.204: INFO: Pod "downwardapi-volume-84054ede-7972-4355-9f98-b2da56b6dd17" satisfied condition "Succeeded or Failed"
Mar 20 07:32:06.207: INFO: Trying to get logs from node env016ar130-worker01 pod downwardapi-volume-84054ede-7972-4355-9f98-b2da56b6dd17 container client-container: <nil>
STEP: delete the pod
Mar 20 07:32:06.233: INFO: Waiting for pod downwardapi-volume-84054ede-7972-4355-9f98-b2da56b6dd17 to disappear
Mar 20 07:32:06.237: INFO: Pod downwardapi-volume-84054ede-7972-4355-9f98-b2da56b6dd17 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar 20 07:32:06.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8261" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":36,"skipped":503,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:32:06.247: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name projected-secret-test-a69cae71-d6d1-4031-92a9-aae5e9d7a688
STEP: Creating a pod to test consume secrets
Mar 20 07:32:06.285: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-89ea489a-6fa7-465f-baa4-c52bb8d48d06" in namespace "projected-8901" to be "Succeeded or Failed"
Mar 20 07:32:06.288: INFO: Pod "pod-projected-secrets-89ea489a-6fa7-465f-baa4-c52bb8d48d06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.561273ms
Mar 20 07:32:08.312: INFO: Pod "pod-projected-secrets-89ea489a-6fa7-465f-baa4-c52bb8d48d06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02663726s
Mar 20 07:32:10.327: INFO: Pod "pod-projected-secrets-89ea489a-6fa7-465f-baa4-c52bb8d48d06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042188843s
STEP: Saw pod success
Mar 20 07:32:10.327: INFO: Pod "pod-projected-secrets-89ea489a-6fa7-465f-baa4-c52bb8d48d06" satisfied condition "Succeeded or Failed"
Mar 20 07:32:10.330: INFO: Trying to get logs from node env016ar130-worker02 pod pod-projected-secrets-89ea489a-6fa7-465f-baa4-c52bb8d48d06 container secret-volume-test: <nil>
STEP: delete the pod
Mar 20 07:32:10.361: INFO: Waiting for pod pod-projected-secrets-89ea489a-6fa7-465f-baa4-c52bb8d48d06 to disappear
Mar 20 07:32:10.364: INFO: Pod pod-projected-secrets-89ea489a-6fa7-465f-baa4-c52bb8d48d06 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Mar 20 07:32:10.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8901" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":37,"skipped":512,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:32:10.373: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Mar 20 07:32:10.435: INFO: The status of Pod labelsupdate2460d250-dc53-48dd-8bd0-0289665b374b is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:32:12.444: INFO: The status of Pod labelsupdate2460d250-dc53-48dd-8bd0-0289665b374b is Running (Ready = true)
Mar 20 07:32:12.977: INFO: Successfully updated pod "labelsupdate2460d250-dc53-48dd-8bd0-0289665b374b"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar 20 07:32:17.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-240" for this suite.

• [SLOW TEST:6.653 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":38,"skipped":528,"failed":0}
SSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:32:17.026: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-d4ed17cc-debf-433a-9b29-d680ca0cd94d in namespace container-probe-9091
Mar 20 07:32:19.070: INFO: Started pod liveness-d4ed17cc-debf-433a-9b29-d680ca0cd94d in namespace container-probe-9091
STEP: checking the pod's current state and verifying that restartCount is present
Mar 20 07:32:19.073: INFO: Initial restart count of pod liveness-d4ed17cc-debf-433a-9b29-d680ca0cd94d is 0
Mar 20 07:32:39.173: INFO: Restart count of pod container-probe-9091/liveness-d4ed17cc-debf-433a-9b29-d680ca0cd94d is now 1 (20.100719988s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar 20 07:32:39.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9091" for this suite.

• [SLOW TEST:22.179 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":39,"skipped":535,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:32:39.205: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-3419
STEP: creating service affinity-clusterip-transition in namespace services-3419
STEP: creating replication controller affinity-clusterip-transition in namespace services-3419
I0320 07:32:39.251252      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3419, replica count: 3
I0320 07:32:42.301763      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 20 07:32:42.316: INFO: Creating new exec pod
Mar 20 07:32:45.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-3419 exec execpod-affinitycw5nt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Mar 20 07:32:45.499: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar 20 07:32:45.499: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 07:32:45.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-3419 exec execpod-affinitycw5nt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.206.176 80'
Mar 20 07:32:45.679: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.206.176 80\nConnection to 192.168.206.176 80 port [tcp/http] succeeded!\n"
Mar 20 07:32:45.679: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 07:32:45.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-3419 exec execpod-affinitycw5nt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.206.176:80/ ; done'
Mar 20 07:32:45.909: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n"
Mar 20 07:32:45.909: INFO: stdout: "\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-p6772\naffinity-clusterip-transition-6nvl8\naffinity-clusterip-transition-6nvl8\naffinity-clusterip-transition-p6772\naffinity-clusterip-transition-6nvl8\naffinity-clusterip-transition-6nvl8\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-6nvl8\naffinity-clusterip-transition-p6772\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-p6772\naffinity-clusterip-transition-p6772\naffinity-clusterip-transition-p6772\naffinity-clusterip-transition-6nvl8\naffinity-clusterip-transition-p6772"
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-p6772
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-6nvl8
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-6nvl8
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-p6772
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-6nvl8
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-6nvl8
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-6nvl8
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-p6772
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-p6772
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-p6772
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-p6772
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-6nvl8
Mar 20 07:32:45.909: INFO: Received response from host: affinity-clusterip-transition-p6772
Mar 20 07:32:45.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-3419 exec execpod-affinitycw5nt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.206.176:80/ ; done'
Mar 20 07:32:46.143: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.176:80/\n"
Mar 20 07:32:46.143: INFO: stdout: "\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-nf7qk\naffinity-clusterip-transition-nf7qk"
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Received response from host: affinity-clusterip-transition-nf7qk
Mar 20 07:32:46.143: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3419, will wait for the garbage collector to delete the pods
Mar 20 07:32:46.225: INFO: Deleting ReplicationController affinity-clusterip-transition took: 8.384043ms
Mar 20 07:32:46.325: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.872689ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 07:32:48.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3419" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.050 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":40,"skipped":582,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:32:48.255: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar 20 07:33:04.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7665" for this suite.

• [SLOW TEST:16.220 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":356,"completed":41,"skipped":603,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:33:04.477: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 07:33:04.508: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd5409a1-80e6-4cff-b84b-b0291e8d856f" in namespace "downward-api-4546" to be "Succeeded or Failed"
Mar 20 07:33:04.511: INFO: Pod "downwardapi-volume-bd5409a1-80e6-4cff-b84b-b0291e8d856f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.672354ms
Mar 20 07:33:06.524: INFO: Pod "downwardapi-volume-bd5409a1-80e6-4cff-b84b-b0291e8d856f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015713124s
Mar 20 07:33:08.536: INFO: Pod "downwardapi-volume-bd5409a1-80e6-4cff-b84b-b0291e8d856f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027992168s
STEP: Saw pod success
Mar 20 07:33:08.537: INFO: Pod "downwardapi-volume-bd5409a1-80e6-4cff-b84b-b0291e8d856f" satisfied condition "Succeeded or Failed"
Mar 20 07:33:08.541: INFO: Trying to get logs from node env016ar130-worker01 pod downwardapi-volume-bd5409a1-80e6-4cff-b84b-b0291e8d856f container client-container: <nil>
STEP: delete the pod
Mar 20 07:33:08.564: INFO: Waiting for pod downwardapi-volume-bd5409a1-80e6-4cff-b84b-b0291e8d856f to disappear
Mar 20 07:33:08.566: INFO: Pod downwardapi-volume-bd5409a1-80e6-4cff-b84b-b0291e8d856f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar 20 07:33:08.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4546" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":42,"skipped":645,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:33:08.575: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-431
Mar 20 07:33:08.602: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:33:10.614: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar 20 07:33:10.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-431 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 20 07:33:10.823: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 20 07:33:10.823: INFO: stdout: "iptables"
Mar 20 07:33:10.823: INFO: proxyMode: iptables
Mar 20 07:33:10.840: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 20 07:33:10.842: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-431
STEP: creating replication controller affinity-nodeport-timeout in namespace services-431
I0320 07:33:10.901851      23 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-431, replica count: 3
I0320 07:33:13.953161      23 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 20 07:33:13.962: INFO: Creating new exec pod
Mar 20 07:33:17.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-431 exec execpod-affinity62zxj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Mar 20 07:33:17.204: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar 20 07:33:17.204: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 07:33:17.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-431 exec execpod-affinity62zxj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.172.179 80'
Mar 20 07:33:17.382: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.172.179 80\nConnection to 192.168.172.179 80 port [tcp/http] succeeded!\n"
Mar 20 07:33:17.382: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 07:33:17.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-431 exec execpod-affinity62zxj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.10.71 31460'
Mar 20 07:33:17.564: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.10.71 31460\nConnection to 10.2.10.71 31460 port [tcp/*] succeeded!\n"
Mar 20 07:33:17.564: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 07:33:17.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-431 exec execpod-affinity62zxj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.10.72 31460'
Mar 20 07:33:17.736: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.10.72 31460\nConnection to 10.2.10.72 31460 port [tcp/*] succeeded!\n"
Mar 20 07:33:17.736: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 07:33:17.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-431 exec execpod-affinity62zxj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.10.71:31460/ ; done'
Mar 20 07:33:18.039: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n"
Mar 20 07:33:18.039: INFO: stdout: "\naffinity-nodeport-timeout-cl2f4\naffinity-nodeport-timeout-cl2f4\naffinity-nodeport-timeout-cl2f4\naffinity-nodeport-timeout-cl2f4\naffinity-nodeport-timeout-cl2f4\naffinity-nodeport-timeout-cl2f4\naffinity-nodeport-timeout-cl2f4\naffinity-nodeport-timeout-cl2f4\naffinity-nodeport-timeout-cl2f4\naffinity-nodeport-timeout-cl2f4\naffinity-nodeport-timeout-cl2f4\naffinity-nodeport-timeout-cl2f4\naffinity-nodeport-timeout-cl2f4\naffinity-nodeport-timeout-cl2f4\naffinity-nodeport-timeout-cl2f4\naffinity-nodeport-timeout-cl2f4"
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Received response from host: affinity-nodeport-timeout-cl2f4
Mar 20 07:33:18.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-431 exec execpod-affinity62zxj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.2.10.71:31460/'
Mar 20 07:33:18.210: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n"
Mar 20 07:33:18.210: INFO: stdout: "affinity-nodeport-timeout-cl2f4"
Mar 20 07:33:38.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-431 exec execpod-affinity62zxj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.2.10.71:31460/'
Mar 20 07:33:38.380: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.2.10.71:31460/\n"
Mar 20 07:33:38.380: INFO: stdout: "affinity-nodeport-timeout-xd9zn"
Mar 20 07:33:38.380: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-431, will wait for the garbage collector to delete the pods
Mar 20 07:33:38.458: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.826794ms
Mar 20 07:33:38.559: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.664294ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 07:33:40.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-431" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:31.907 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":43,"skipped":651,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:33:40.482: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-dd551a7a-e302-4ba5-9876-58a2caadc169
STEP: Creating the pod
Mar 20 07:33:40.517: INFO: The status of Pod pod-configmaps-88cb9e75-1503-485b-bd62-6e506ecbb5ca is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:33:42.530: INFO: The status of Pod pod-configmaps-88cb9e75-1503-485b-bd62-6e506ecbb5ca is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-dd551a7a-e302-4ba5-9876-58a2caadc169
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar 20 07:33:44.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3126" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":44,"skipped":652,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:33:44.593: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Mar 20 07:33:44.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-2671 create -f -'
Mar 20 07:33:45.477: INFO: stderr: ""
Mar 20 07:33:45.477: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 20 07:33:46.485: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 20 07:33:46.485: INFO: Found 0 / 1
Mar 20 07:33:47.484: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 20 07:33:47.484: INFO: Found 1 / 1
Mar 20 07:33:47.484: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 20 07:33:47.488: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 20 07:33:47.488: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 20 07:33:47.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-2671 patch pod agnhost-primary-cghcb -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 20 07:33:47.591: INFO: stderr: ""
Mar 20 07:33:47.591: INFO: stdout: "pod/agnhost-primary-cghcb patched\n"
STEP: checking annotations
Mar 20 07:33:47.596: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 20 07:33:47.596: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 07:33:47.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2671" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":356,"completed":45,"skipped":673,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:33:47.608: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar 20 07:33:47.632: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar 20 07:34:05.169: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:34:10.214: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:34:25.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8897" for this suite.

• [SLOW TEST:38.324 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":356,"completed":46,"skipped":681,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:34:25.932: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8407.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8407.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8407.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8407.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 20 07:34:30.028: INFO: DNS probes using dns-8407/dns-test-a963e907-c4b8-49c2-aa39-07743ed0ec17 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar 20 07:34:30.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8407" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","total":356,"completed":47,"skipped":689,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:34:30.073: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-9d1dd2e9-65c4-4835-a297-6f92ed6c4024
STEP: Creating secret with name s-test-opt-upd-b348a957-5c8e-4d43-b116-6f69a2e5fe36
STEP: Creating the pod
Mar 20 07:34:30.131: INFO: The status of Pod pod-secrets-02661fc5-eeae-4bd3-b62e-394c859bb2e3 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:34:32.137: INFO: The status of Pod pod-secrets-02661fc5-eeae-4bd3-b62e-394c859bb2e3 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:34:34.141: INFO: The status of Pod pod-secrets-02661fc5-eeae-4bd3-b62e-394c859bb2e3 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-9d1dd2e9-65c4-4835-a297-6f92ed6c4024
STEP: Updating secret s-test-opt-upd-b348a957-5c8e-4d43-b116-6f69a2e5fe36
STEP: Creating secret with name s-test-opt-create-0a0d9d81-9d89-4a8a-bad7-5577bee2b431
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar 20 07:36:04.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3743" for this suite.

• [SLOW TEST:94.669 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":48,"skipped":709,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:36:04.742: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:36:04.774: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:36:05.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6789" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":356,"completed":49,"skipped":715,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:36:05.356: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 20 07:36:05.389: INFO: Waiting up to 5m0s for pod "pod-facd017c-e1fc-4740-914b-832edc5ad9c3" in namespace "emptydir-3959" to be "Succeeded or Failed"
Mar 20 07:36:05.392: INFO: Pod "pod-facd017c-e1fc-4740-914b-832edc5ad9c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.194342ms
Mar 20 07:36:07.404: INFO: Pod "pod-facd017c-e1fc-4740-914b-832edc5ad9c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015513041s
Mar 20 07:36:09.421: INFO: Pod "pod-facd017c-e1fc-4740-914b-832edc5ad9c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032325578s
STEP: Saw pod success
Mar 20 07:36:09.421: INFO: Pod "pod-facd017c-e1fc-4740-914b-832edc5ad9c3" satisfied condition "Succeeded or Failed"
Mar 20 07:36:09.425: INFO: Trying to get logs from node env016ar130-worker02 pod pod-facd017c-e1fc-4740-914b-832edc5ad9c3 container test-container: <nil>
STEP: delete the pod
Mar 20 07:36:09.445: INFO: Waiting for pod pod-facd017c-e1fc-4740-914b-832edc5ad9c3 to disappear
Mar 20 07:36:09.447: INFO: Pod pod-facd017c-e1fc-4740-914b-832edc5ad9c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar 20 07:36:09.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3959" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":50,"skipped":728,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:36:09.458: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 07:36:10.028: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 07:36:13.051: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:36:13.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4336" for this suite.
STEP: Destroying namespace "webhook-4336-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":356,"completed":51,"skipped":780,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:36:13.137: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Mar 20 07:36:13.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9255" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":356,"completed":52,"skipped":816,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:36:13.204: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 07:36:13.420: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 07:36:16.448: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:36:16.452: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:36:19.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6823" for this suite.
STEP: Destroying namespace "webhook-6823-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.500 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":356,"completed":53,"skipped":816,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:36:19.704: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:36:19.730: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:36:20.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4499" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":356,"completed":54,"skipped":828,"failed":0}

------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:36:20.777: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 20 07:36:24.858: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Mar 20 07:36:24.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8666" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":356,"completed":55,"skipped":828,"failed":0}

------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:36:24.883: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Mar 20 07:36:26.939: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Mar 20 07:36:31.039: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Mar 20 07:36:33.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7591" for this suite.

• [SLOW TEST:8.214 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":356,"completed":56,"skipped":828,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:36:33.097: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-8877
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 20 07:36:33.124: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 20 07:36:33.170: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:36:35.180: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 07:36:37.178: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 07:36:39.178: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 07:36:41.187: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 07:36:43.180: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 07:36:45.179: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 07:36:47.181: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 07:36:49.184: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 07:36:51.184: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 07:36:53.178: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 07:36:55.183: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 20 07:36:55.190: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 20 07:36:55.196: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 20 07:36:57.229: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 20 07:36:57.229: INFO: Breadth first check of 192.168.90.76 on host 10.2.10.71...
Mar 20 07:36:57.232: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.58:9080/dial?request=hostname&protocol=http&host=192.168.90.76&port=8083&tries=1'] Namespace:pod-network-test-8877 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:36:57.233: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:36:57.234: INFO: ExecWithOptions: Clientset creation
Mar 20 07:36:57.234: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/pod-network-test-8877/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.58%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.90.76%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 20 07:36:57.331: INFO: Waiting for responses: map[]
Mar 20 07:36:57.331: INFO: reached 192.168.90.76 after 0/1 tries
Mar 20 07:36:57.331: INFO: Breadth first check of 192.168.12.5 on host 10.2.10.72...
Mar 20 07:36:57.336: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.58:9080/dial?request=hostname&protocol=http&host=192.168.12.5&port=8083&tries=1'] Namespace:pod-network-test-8877 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:36:57.336: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:36:57.337: INFO: ExecWithOptions: Clientset creation
Mar 20 07:36:57.337: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/pod-network-test-8877/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.58%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.12.5%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 20 07:36:57.429: INFO: Waiting for responses: map[]
Mar 20 07:36:57.429: INFO: reached 192.168.12.5 after 0/1 tries
Mar 20 07:36:57.429: INFO: Breadth first check of 192.168.71.176 on host 10.2.10.73...
Mar 20 07:36:57.434: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.58:9080/dial?request=hostname&protocol=http&host=192.168.71.176&port=8083&tries=1'] Namespace:pod-network-test-8877 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:36:57.434: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:36:57.435: INFO: ExecWithOptions: Clientset creation
Mar 20 07:36:57.435: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/pod-network-test-8877/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.58%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.71.176%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 20 07:36:57.517: INFO: Waiting for responses: map[]
Mar 20 07:36:57.517: INFO: reached 192.168.71.176 after 0/1 tries
Mar 20 07:36:57.517: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Mar 20 07:36:57.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8877" for this suite.

• [SLOW TEST:24.445 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":356,"completed":57,"skipped":837,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:36:57.542: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 20 07:36:57.589: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8415  23750161-81e3-40e4-a004-7b5713950742 807202 0 2023-03-20 07:36:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-03-20 07:36:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 20 07:36:57.589: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8415  23750161-81e3-40e4-a004-7b5713950742 807203 0 2023-03-20 07:36:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-03-20 07:36:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 20 07:36:57.602: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8415  23750161-81e3-40e4-a004-7b5713950742 807204 0 2023-03-20 07:36:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-03-20 07:36:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 20 07:36:57.603: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8415  23750161-81e3-40e4-a004-7b5713950742 807205 0 2023-03-20 07:36:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-03-20 07:36:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Mar 20 07:36:57.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8415" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":356,"completed":58,"skipped":845,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:36:57.616: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:36:57.653: INFO: The status of Pod busybox-scheduling-8c205c91-85a4-4a86-93f4-a235eb895c28 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:36:59.669: INFO: The status of Pod busybox-scheduling-8c205c91-85a4-4a86-93f4-a235eb895c28 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Mar 20 07:36:59.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1474" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":356,"completed":59,"skipped":862,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:36:59.703: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 20 07:36:59.744: INFO: Waiting up to 5m0s for pod "pod-b820a26e-83d3-42e1-be60-a167b6b3d833" in namespace "emptydir-1717" to be "Succeeded or Failed"
Mar 20 07:36:59.748: INFO: Pod "pod-b820a26e-83d3-42e1-be60-a167b6b3d833": Phase="Pending", Reason="", readiness=false. Elapsed: 3.348428ms
Mar 20 07:37:01.757: INFO: Pod "pod-b820a26e-83d3-42e1-be60-a167b6b3d833": Phase="Running", Reason="", readiness=true. Elapsed: 2.01314028s
Mar 20 07:37:03.769: INFO: Pod "pod-b820a26e-83d3-42e1-be60-a167b6b3d833": Phase="Running", Reason="", readiness=false. Elapsed: 4.025149122s
Mar 20 07:37:05.780: INFO: Pod "pod-b820a26e-83d3-42e1-be60-a167b6b3d833": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035292443s
STEP: Saw pod success
Mar 20 07:37:05.780: INFO: Pod "pod-b820a26e-83d3-42e1-be60-a167b6b3d833" satisfied condition "Succeeded or Failed"
Mar 20 07:37:05.784: INFO: Trying to get logs from node env016ar130-worker01 pod pod-b820a26e-83d3-42e1-be60-a167b6b3d833 container test-container: <nil>
STEP: delete the pod
Mar 20 07:37:05.807: INFO: Waiting for pod pod-b820a26e-83d3-42e1-be60-a167b6b3d833 to disappear
Mar 20 07:37:05.810: INFO: Pod pod-b820a26e-83d3-42e1-be60-a167b6b3d833 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar 20 07:37:05.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1717" for this suite.

• [SLOW TEST:6.118 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":60,"skipped":902,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:37:05.822: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5461
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5461
STEP: creating replication controller externalsvc in namespace services-5461
I0320 07:37:05.893221      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5461, replica count: 2
I0320 07:37:08.946211      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar 20 07:37:08.974: INFO: Creating new exec pod
Mar 20 07:37:11.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-5461 exec execpodf5qqg -- /bin/sh -x -c nslookup nodeport-service.services-5461.svc.cluster.local'
Mar 20 07:37:11.189: INFO: stderr: "+ nslookup nodeport-service.services-5461.svc.cluster.local\n"
Mar 20 07:37:11.189: INFO: stdout: "Server:\t\t192.168.128.10\nAddress:\t192.168.128.10#53\n\nnodeport-service.services-5461.svc.cluster.local\tcanonical name = externalsvc.services-5461.svc.cluster.local.\nName:\texternalsvc.services-5461.svc.cluster.local\nAddress: 192.168.182.124\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5461, will wait for the garbage collector to delete the pods
Mar 20 07:37:11.258: INFO: Deleting ReplicationController externalsvc took: 12.171619ms
Mar 20 07:37:11.359: INFO: Terminating ReplicationController externalsvc pods took: 100.499879ms
Mar 20 07:37:13.282: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 07:37:13.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5461" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:7.485 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":356,"completed":61,"skipped":932,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:37:13.307: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Starting the proxy
Mar 20 07:37:13.330: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-4575 proxy --unix-socket=/tmp/kubectl-proxy-unix1335919059/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 07:37:13.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4575" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":356,"completed":62,"skipped":962,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:37:13.385: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar 20 07:37:13.410: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:37:18.917: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:37:34.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8822" for this suite.

• [SLOW TEST:21.003 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":356,"completed":63,"skipped":967,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:37:34.389: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod with failed condition
STEP: updating the pod
Mar 20 07:39:34.983: INFO: Successfully updated pod "var-expansion-17f4df9e-1976-4d6f-ba57-a5088075fc88"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Mar 20 07:39:37.002: INFO: Deleting pod "var-expansion-17f4df9e-1976-4d6f-ba57-a5088075fc88" in namespace "var-expansion-164"
Mar 20 07:39:37.011: INFO: Wait up to 5m0s for pod "var-expansion-17f4df9e-1976-4d6f-ba57-a5088075fc88" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar 20 07:40:09.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-164" for this suite.

• [SLOW TEST:154.655 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":356,"completed":64,"skipped":977,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:40:09.044: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Mar 20 07:40:09.097: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Mar 20 07:40:09.123: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Mar 20 07:40:09.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4633" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":356,"completed":65,"skipped":1008,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:40:09.156: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Mar 20 07:40:09.192: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 20 07:40:14.202: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Mar 20 07:40:14.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4908" for this suite.

• [SLOW TEST:5.079 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":356,"completed":66,"skipped":1009,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:40:14.236: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 20 07:40:16.302: INFO: DNS probes using dns-7577/dns-test-1648b637-79ac-4e26-987a-34be0f54dca6 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar 20 07:40:16.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7577" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":356,"completed":67,"skipped":1036,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:40:16.329: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Mar 20 07:40:16.373: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 20 07:40:16.374: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 20 07:40:16.383: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 20 07:40:16.383: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 20 07:40:16.395: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 20 07:40:16.395: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 20 07:40:16.408: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 20 07:40:16.408: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 20 07:40:17.583: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 20 07:40:17.583: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 20 07:40:17.835: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Mar 20 07:40:17.858: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Mar 20 07:40:17.859: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0
Mar 20 07:40:17.859: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0
Mar 20 07:40:17.860: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0
Mar 20 07:40:17.860: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0
Mar 20 07:40:17.860: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0
Mar 20 07:40:17.860: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0
Mar 20 07:40:17.860: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0
Mar 20 07:40:17.860: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 0
Mar 20 07:40:17.860: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1
Mar 20 07:40:17.860: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1
Mar 20 07:40:17.860: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2
Mar 20 07:40:17.860: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2
Mar 20 07:40:17.860: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2
Mar 20 07:40:17.860: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2
Mar 20 07:40:17.867: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2
Mar 20 07:40:17.867: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2
Mar 20 07:40:17.882: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2
Mar 20 07:40:17.882: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2
Mar 20 07:40:17.888: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1
Mar 20 07:40:17.888: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1
Mar 20 07:40:17.895: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1
Mar 20 07:40:17.895: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1
Mar 20 07:40:19.596: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2
Mar 20 07:40:19.596: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2
Mar 20 07:40:19.610: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1
STEP: listing Deployments
Mar 20 07:40:19.621: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Mar 20 07:40:19.637: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Mar 20 07:40:19.644: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 20 07:40:19.652: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 20 07:40:19.666: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 20 07:40:19.673: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 20 07:40:19.691: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 20 07:40:20.888: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 20 07:40:20.899: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Mar 20 07:40:20.917: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 20 07:40:20.937: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 20 07:40:22.617: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Mar 20 07:40:22.646: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1
Mar 20 07:40:22.646: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1
Mar 20 07:40:22.646: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1
Mar 20 07:40:22.647: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1
Mar 20 07:40:22.647: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 1
Mar 20 07:40:22.647: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2
Mar 20 07:40:22.647: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 3
Mar 20 07:40:22.647: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2
Mar 20 07:40:22.647: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 2
Mar 20 07:40:22.647: INFO: observed Deployment test-deployment in namespace deployment-2172 with ReadyReplicas 3
STEP: deleting the Deployment
Mar 20 07:40:22.657: INFO: observed event type MODIFIED
Mar 20 07:40:22.658: INFO: observed event type MODIFIED
Mar 20 07:40:22.658: INFO: observed event type MODIFIED
Mar 20 07:40:22.658: INFO: observed event type MODIFIED
Mar 20 07:40:22.658: INFO: observed event type MODIFIED
Mar 20 07:40:22.658: INFO: observed event type MODIFIED
Mar 20 07:40:22.658: INFO: observed event type MODIFIED
Mar 20 07:40:22.658: INFO: observed event type MODIFIED
Mar 20 07:40:22.658: INFO: observed event type MODIFIED
Mar 20 07:40:22.658: INFO: observed event type MODIFIED
Mar 20 07:40:22.658: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 20 07:40:22.661: INFO: Log out all the ReplicaSets if there is no deployment created
Mar 20 07:40:22.666: INFO: ReplicaSet "test-deployment-6b48c869b6":
&ReplicaSet{ObjectMeta:{test-deployment-6b48c869b6  deployment-2172  a691c926-0d5d-4f99-b8ea-681c05c386ba 808183 3 2023-03-20 07:40:16 +0000 UTC <nil> <nil> map[pod-template-hash:6b48c869b6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 6310f4c0-913a-45de-8d3b-2fe41db58365 0xc0044b8937 0xc0044b8938}] []  [{kube-controller-manager Update apps/v1 2023-03-20 07:40:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6310f4c0-913a-45de-8d3b-2fe41db58365\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 07:40:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6b48c869b6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6b48c869b6 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044b89c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar 20 07:40:22.668: INFO: ReplicaSet "test-deployment-74c6dd549b":
&ReplicaSet{ObjectMeta:{test-deployment-74c6dd549b  deployment-2172  c00d7973-fdfd-4596-8ecf-c27a9014158d 808296 2 2023-03-20 07:40:19 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 6310f4c0-913a-45de-8d3b-2fe41db58365 0xc0044b8a27 0xc0044b8a28}] []  [{kube-controller-manager Update apps/v1 2023-03-20 07:40:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6310f4c0-913a-45de-8d3b-2fe41db58365\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 07:40:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 74c6dd549b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044b8ab0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Mar 20 07:40:22.680: INFO: pod: "test-deployment-74c6dd549b-np2xd":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-np2xd test-deployment-74c6dd549b- deployment-2172  13d7f4fb-7797-4c49-8bbd-a69eac3b5682 808310 0 2023-03-20 07:40:19 +0000 UTC 2023-03-20 07:40:23 +0000 UTC 0xc0044b8d38 map[pod-template-hash:74c6dd549b test-deployment-static:true] map[cni.projectcalico.org/podIP:192.168.12.34/32 cni.projectcalico.org/podIPs:192.168.12.34/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.34"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.34"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-74c6dd549b c00d7973-fdfd-4596-8ecf-c27a9014158d 0xc0044b8d87 0xc0044b8d88}] []  [{kube-controller-manager Update v1 2023-03-20 07:40:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c00d7973-fdfd-4596-8ecf-c27a9014158d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-20 07:40:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-20 07:40:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-03-20 07:40:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9gl2p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9gl2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:40:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:40:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:40:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:40:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.72,PodIP:192.168.12.34,StartTime:2023-03-20 07:40:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-20 07:40:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://871948d95d024e318ed497005533fb9c2d82bc5e7b4b21462f4cf48565127d0e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 20 07:40:22.680: INFO: pod: "test-deployment-74c6dd549b-wxcxw":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-wxcxw test-deployment-74c6dd549b- deployment-2172  978972b6-011e-4788-9e7a-ad9012b33ae0 808295 0 2023-03-20 07:40:20 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[cni.projectcalico.org/podIP:192.168.90.104/32 cni.projectcalico.org/podIPs:192.168.90.104/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.104"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.104"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-74c6dd549b c00d7973-fdfd-4596-8ecf-c27a9014158d 0xc0044b8fa7 0xc0044b8fa8}] []  [{kube-controller-manager Update v1 2023-03-20 07:40:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c00d7973-fdfd-4596-8ecf-c27a9014158d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-20 07:40:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:40:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-20 07:40:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.90.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r4t4l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r4t4l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:40:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:40:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:40:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:40:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.71,PodIP:192.168.90.104,StartTime:2023-03-20 07:40:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-20 07:40:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1270f105d46b55a01fbcab288937f68ab54105f05c1e6a8fecbbc2716165251d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.90.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar 20 07:40:22.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2172" for this suite.

• [SLOW TEST:6.360 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":356,"completed":68,"skipped":1051,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:40:22.690: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:40:22.714: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Mar 20 07:40:27.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-1764 --namespace=crd-publish-openapi-1764 create -f -'
Mar 20 07:40:28.693: INFO: stderr: ""
Mar 20 07:40:28.693: INFO: stdout: "e2e-test-crd-publish-openapi-5634-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 20 07:40:28.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-1764 --namespace=crd-publish-openapi-1764 delete e2e-test-crd-publish-openapi-5634-crds test-cr'
Mar 20 07:40:28.777: INFO: stderr: ""
Mar 20 07:40:28.777: INFO: stdout: "e2e-test-crd-publish-openapi-5634-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 20 07:40:28.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-1764 --namespace=crd-publish-openapi-1764 apply -f -'
Mar 20 07:40:29.549: INFO: stderr: ""
Mar 20 07:40:29.549: INFO: stdout: "e2e-test-crd-publish-openapi-5634-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 20 07:40:29.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-1764 --namespace=crd-publish-openapi-1764 delete e2e-test-crd-publish-openapi-5634-crds test-cr'
Mar 20 07:40:29.635: INFO: stderr: ""
Mar 20 07:40:29.635: INFO: stdout: "e2e-test-crd-publish-openapi-5634-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 20 07:40:29.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-1764 explain e2e-test-crd-publish-openapi-5634-crds'
Mar 20 07:40:29.866: INFO: stderr: ""
Mar 20 07:40:29.866: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5634-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:40:34.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1764" for this suite.

• [SLOW TEST:12.179 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":356,"completed":69,"skipped":1076,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:40:34.869: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Mar 20 07:40:34.902: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:40:55.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4981" for this suite.

• [SLOW TEST:20.618 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":356,"completed":70,"skipped":1082,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:40:55.487: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-b64cc0e0-8efd-4a7b-8d61-917752f406a5
STEP: Creating a pod to test consume secrets
Mar 20 07:40:55.540: INFO: Waiting up to 5m0s for pod "pod-secrets-a1c08c15-82c1-4381-85ee-0e67402a3d06" in namespace "secrets-8283" to be "Succeeded or Failed"
Mar 20 07:40:55.543: INFO: Pod "pod-secrets-a1c08c15-82c1-4381-85ee-0e67402a3d06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.675933ms
Mar 20 07:40:57.549: INFO: Pod "pod-secrets-a1c08c15-82c1-4381-85ee-0e67402a3d06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00855653s
Mar 20 07:40:59.558: INFO: Pod "pod-secrets-a1c08c15-82c1-4381-85ee-0e67402a3d06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017373362s
STEP: Saw pod success
Mar 20 07:40:59.558: INFO: Pod "pod-secrets-a1c08c15-82c1-4381-85ee-0e67402a3d06" satisfied condition "Succeeded or Failed"
Mar 20 07:40:59.561: INFO: Trying to get logs from node env016ar130-worker02 pod pod-secrets-a1c08c15-82c1-4381-85ee-0e67402a3d06 container secret-volume-test: <nil>
STEP: delete the pod
Mar 20 07:40:59.598: INFO: Waiting for pod pod-secrets-a1c08c15-82c1-4381-85ee-0e67402a3d06 to disappear
Mar 20 07:40:59.601: INFO: Pod pod-secrets-a1c08c15-82c1-4381-85ee-0e67402a3d06 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar 20 07:40:59.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8283" for this suite.
STEP: Destroying namespace "secret-namespace-7255" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":356,"completed":71,"skipped":1098,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:40:59.614: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-36/configmap-test-e81e9728-359a-47e2-b3e9-d5718d43b832
STEP: Creating a pod to test consume configMaps
Mar 20 07:40:59.652: INFO: Waiting up to 5m0s for pod "pod-configmaps-9dc1b42b-cd34-4bfb-add0-725630161563" in namespace "configmap-36" to be "Succeeded or Failed"
Mar 20 07:40:59.655: INFO: Pod "pod-configmaps-9dc1b42b-cd34-4bfb-add0-725630161563": Phase="Pending", Reason="", readiness=false. Elapsed: 2.647047ms
Mar 20 07:41:01.665: INFO: Pod "pod-configmaps-9dc1b42b-cd34-4bfb-add0-725630161563": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012927222s
Mar 20 07:41:03.680: INFO: Pod "pod-configmaps-9dc1b42b-cd34-4bfb-add0-725630161563": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027503954s
STEP: Saw pod success
Mar 20 07:41:03.680: INFO: Pod "pod-configmaps-9dc1b42b-cd34-4bfb-add0-725630161563" satisfied condition "Succeeded or Failed"
Mar 20 07:41:03.683: INFO: Trying to get logs from node env016ar130-worker02 pod pod-configmaps-9dc1b42b-cd34-4bfb-add0-725630161563 container env-test: <nil>
STEP: delete the pod
Mar 20 07:41:03.699: INFO: Waiting for pod pod-configmaps-9dc1b42b-cd34-4bfb-add0-725630161563 to disappear
Mar 20 07:41:03.702: INFO: Pod pod-configmaps-9dc1b42b-cd34-4bfb-add0-725630161563 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Mar 20 07:41:03.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-36" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":72,"skipped":1117,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:41:03.710: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Mar 20 07:41:16.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7215" for this suite.
STEP: Destroying namespace "nsdeletetest-9420" for this suite.
Mar 20 07:41:16.816: INFO: Namespace nsdeletetest-9420 was already deleted
STEP: Destroying namespace "nsdeletetest-7139" for this suite.

• [SLOW TEST:13.109 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":356,"completed":73,"skipped":1142,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:41:16.819: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 07:41:16.847: INFO: Waiting up to 5m0s for pod "downwardapi-volume-907b7f07-35a3-4106-b56f-1a33a370cec0" in namespace "downward-api-7351" to be "Succeeded or Failed"
Mar 20 07:41:16.849: INFO: Pod "downwardapi-volume-907b7f07-35a3-4106-b56f-1a33a370cec0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.599676ms
Mar 20 07:41:18.859: INFO: Pod "downwardapi-volume-907b7f07-35a3-4106-b56f-1a33a370cec0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011649155s
Mar 20 07:41:20.871: INFO: Pod "downwardapi-volume-907b7f07-35a3-4106-b56f-1a33a370cec0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023677245s
STEP: Saw pod success
Mar 20 07:41:20.871: INFO: Pod "downwardapi-volume-907b7f07-35a3-4106-b56f-1a33a370cec0" satisfied condition "Succeeded or Failed"
Mar 20 07:41:20.875: INFO: Trying to get logs from node env016ar130-worker02 pod downwardapi-volume-907b7f07-35a3-4106-b56f-1a33a370cec0 container client-container: <nil>
STEP: delete the pod
Mar 20 07:41:20.948: INFO: Waiting for pod downwardapi-volume-907b7f07-35a3-4106-b56f-1a33a370cec0 to disappear
Mar 20 07:41:20.953: INFO: Pod downwardapi-volume-907b7f07-35a3-4106-b56f-1a33a370cec0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar 20 07:41:20.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7351" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":356,"completed":74,"skipped":1148,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:41:20.963: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 20 07:41:20.994: INFO: Waiting up to 5m0s for pod "pod-697a80eb-a3b1-4674-b21f-866189cbc812" in namespace "emptydir-8873" to be "Succeeded or Failed"
Mar 20 07:41:20.998: INFO: Pod "pod-697a80eb-a3b1-4674-b21f-866189cbc812": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066364ms
Mar 20 07:41:23.009: INFO: Pod "pod-697a80eb-a3b1-4674-b21f-866189cbc812": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014962293s
Mar 20 07:41:25.022: INFO: Pod "pod-697a80eb-a3b1-4674-b21f-866189cbc812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027593406s
STEP: Saw pod success
Mar 20 07:41:25.022: INFO: Pod "pod-697a80eb-a3b1-4674-b21f-866189cbc812" satisfied condition "Succeeded or Failed"
Mar 20 07:41:25.026: INFO: Trying to get logs from node env016ar130-worker01 pod pod-697a80eb-a3b1-4674-b21f-866189cbc812 container test-container: <nil>
STEP: delete the pod
Mar 20 07:41:25.057: INFO: Waiting for pod pod-697a80eb-a3b1-4674-b21f-866189cbc812 to disappear
Mar 20 07:41:25.059: INFO: Pod pod-697a80eb-a3b1-4674-b21f-866189cbc812 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar 20 07:41:25.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8873" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":75,"skipped":1151,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:41:25.068: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-a3856405-33f6-4a5b-9c69-77546070d5e5
STEP: Creating a pod to test consume secrets
Mar 20 07:41:25.153: INFO: Waiting up to 5m0s for pod "pod-secrets-e3fc738b-a95a-4ed2-853c-e15d09f1a9a5" in namespace "secrets-4152" to be "Succeeded or Failed"
Mar 20 07:41:25.157: INFO: Pod "pod-secrets-e3fc738b-a95a-4ed2-853c-e15d09f1a9a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.247707ms
Mar 20 07:41:27.161: INFO: Pod "pod-secrets-e3fc738b-a95a-4ed2-853c-e15d09f1a9a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007451296s
Mar 20 07:41:29.171: INFO: Pod "pod-secrets-e3fc738b-a95a-4ed2-853c-e15d09f1a9a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017154824s
STEP: Saw pod success
Mar 20 07:41:29.171: INFO: Pod "pod-secrets-e3fc738b-a95a-4ed2-853c-e15d09f1a9a5" satisfied condition "Succeeded or Failed"
Mar 20 07:41:29.174: INFO: Trying to get logs from node env016ar130-worker01 pod pod-secrets-e3fc738b-a95a-4ed2-853c-e15d09f1a9a5 container secret-volume-test: <nil>
STEP: delete the pod
Mar 20 07:41:29.191: INFO: Waiting for pod pod-secrets-e3fc738b-a95a-4ed2-853c-e15d09f1a9a5 to disappear
Mar 20 07:41:29.194: INFO: Pod pod-secrets-e3fc738b-a95a-4ed2-853c-e15d09f1a9a5 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar 20 07:41:29.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4152" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":76,"skipped":1159,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:41:29.202: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1412
STEP: creating an pod
Mar 20 07:41:29.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-3924 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.39 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 20 07:41:29.306: INFO: stderr: ""
Mar 20 07:41:29.306: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for log generator to start.
Mar 20 07:41:29.306: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 20 07:41:29.306: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3924" to be "running and ready, or succeeded"
Mar 20 07:41:29.310: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.34567ms
Mar 20 07:41:31.320: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.013913182s
Mar 20 07:41:31.320: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 20 07:41:31.320: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar 20 07:41:31.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-3924 logs logs-generator logs-generator'
Mar 20 07:41:31.418: INFO: stderr: ""
Mar 20 07:41:31.418: INFO: stdout: "I0320 07:41:30.229605       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/2gvp 424\nI0320 07:41:30.430130       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/pps2 241\nI0320 07:41:30.630494       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/mklj 554\nI0320 07:41:30.829800       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/4kcn 470\nI0320 07:41:31.030037       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/4hq 423\nI0320 07:41:31.230347       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/6nd 373\n"
STEP: limiting log lines
Mar 20 07:41:31.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-3924 logs logs-generator logs-generator --tail=1'
Mar 20 07:41:31.498: INFO: stderr: ""
Mar 20 07:41:31.498: INFO: stdout: "I0320 07:41:31.430681       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/47th 433\n"
Mar 20 07:41:31.498: INFO: got output "I0320 07:41:31.430681       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/47th 433\n"
STEP: limiting log bytes
Mar 20 07:41:31.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-3924 logs logs-generator logs-generator --limit-bytes=1'
Mar 20 07:41:31.577: INFO: stderr: ""
Mar 20 07:41:31.577: INFO: stdout: "I"
Mar 20 07:41:31.577: INFO: got output "I"
STEP: exposing timestamps
Mar 20 07:41:31.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-3924 logs logs-generator logs-generator --tail=1 --timestamps'
Mar 20 07:41:31.663: INFO: stderr: ""
Mar 20 07:41:31.663: INFO: stdout: "2023-03-20T08:41:31.630209900+01:00 I0320 07:41:31.630047       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/mxfs 386\n"
Mar 20 07:41:31.663: INFO: got output "2023-03-20T08:41:31.630209900+01:00 I0320 07:41:31.630047       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/mxfs 386\n"
STEP: restricting to a time range
Mar 20 07:41:34.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-3924 logs logs-generator logs-generator --since=1s'
Mar 20 07:41:34.260: INFO: stderr: ""
Mar 20 07:41:34.260: INFO: stdout: "I0320 07:41:33.429768       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/6mfl 510\nI0320 07:41:33.630193       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/jzn6 247\nI0320 07:41:33.829809       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/rmwv 275\nI0320 07:41:34.030262       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/hgjd 309\nI0320 07:41:34.230664       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/npm 218\n"
Mar 20 07:41:34.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-3924 logs logs-generator logs-generator --since=24h'
Mar 20 07:41:34.335: INFO: stderr: ""
Mar 20 07:41:34.335: INFO: stdout: "I0320 07:41:30.229605       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/2gvp 424\nI0320 07:41:30.430130       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/pps2 241\nI0320 07:41:30.630494       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/mklj 554\nI0320 07:41:30.829800       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/4kcn 470\nI0320 07:41:31.030037       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/4hq 423\nI0320 07:41:31.230347       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/6nd 373\nI0320 07:41:31.430681       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/47th 433\nI0320 07:41:31.630047       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/mxfs 386\nI0320 07:41:31.830461       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/wx4c 374\nI0320 07:41:32.029754       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/ppz6 215\nI0320 07:41:32.230146       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/bgs 277\nI0320 07:41:32.429711       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/8dwp 277\nI0320 07:41:32.630343       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/qmq6 324\nI0320 07:41:32.829689       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/mnp4 253\nI0320 07:41:33.030045       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/kmw 249\nI0320 07:41:33.230431       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/f92 333\nI0320 07:41:33.429768       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/6mfl 510\nI0320 07:41:33.630193       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/jzn6 247\nI0320 07:41:33.829809       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/rmwv 275\nI0320 07:41:34.030262       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/hgjd 309\nI0320 07:41:34.230664       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/npm 218\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1417
Mar 20 07:41:34.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-3924 delete pod logs-generator'
Mar 20 07:41:35.215: INFO: stderr: ""
Mar 20 07:41:35.215: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 07:41:35.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3924" for this suite.

• [SLOW TEST:6.023 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1409
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":356,"completed":77,"skipped":1214,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:41:35.225: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1540
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Mar 20 07:41:35.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-3934 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Mar 20 07:41:35.305: INFO: stderr: ""
Mar 20 07:41:35.305: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1544
Mar 20 07:41:35.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-3934 delete pods e2e-test-httpd-pod'
Mar 20 07:41:37.227: INFO: stderr: ""
Mar 20 07:41:37.227: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 07:41:37.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3934" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":356,"completed":78,"skipped":1226,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:41:37.239: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 07:41:37.270: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a6eb0fe-f42e-4710-b0dd-aae39079ecca" in namespace "projected-8866" to be "Succeeded or Failed"
Mar 20 07:41:37.272: INFO: Pod "downwardapi-volume-1a6eb0fe-f42e-4710-b0dd-aae39079ecca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.536233ms
Mar 20 07:41:39.285: INFO: Pod "downwardapi-volume-1a6eb0fe-f42e-4710-b0dd-aae39079ecca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014922252s
Mar 20 07:41:41.296: INFO: Pod "downwardapi-volume-1a6eb0fe-f42e-4710-b0dd-aae39079ecca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026273612s
STEP: Saw pod success
Mar 20 07:41:41.296: INFO: Pod "downwardapi-volume-1a6eb0fe-f42e-4710-b0dd-aae39079ecca" satisfied condition "Succeeded or Failed"
Mar 20 07:41:41.299: INFO: Trying to get logs from node env016ar130-worker01 pod downwardapi-volume-1a6eb0fe-f42e-4710-b0dd-aae39079ecca container client-container: <nil>
STEP: delete the pod
Mar 20 07:41:41.315: INFO: Waiting for pod downwardapi-volume-1a6eb0fe-f42e-4710-b0dd-aae39079ecca to disappear
Mar 20 07:41:41.317: INFO: Pod downwardapi-volume-1a6eb0fe-f42e-4710-b0dd-aae39079ecca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar 20 07:41:41.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8866" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":356,"completed":79,"skipped":1230,"failed":0}

------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:41:41.326: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Mar 20 07:41:41.347: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Mar 20 07:41:47.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6500" for this suite.

• [SLOW TEST:5.967 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":356,"completed":80,"skipped":1230,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:41:47.293: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:41:47.311: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 20 07:41:47.319: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 20 07:41:52.328: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 20 07:41:52.328: INFO: Creating deployment "test-rolling-update-deployment"
Mar 20 07:41:52.334: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 20 07:41:52.338: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 20 07:41:54.354: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 20 07:41:54.357: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 20 07:41:54.364: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-251  ec9384d9-fc3a-47fd-ad52-4e326b518e2a 809083 1 2023-03-20 07:41:52 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2023-03-20 07:41:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 07:41:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005fb5538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-20 07:41:52 +0000 UTC,LastTransitionTime:2023-03-20 07:41:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67c8f74c6c" has successfully progressed.,LastUpdateTime:2023-03-20 07:41:54 +0000 UTC,LastTransitionTime:2023-03-20 07:41:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 20 07:41:54.367: INFO: New ReplicaSet "test-rolling-update-deployment-67c8f74c6c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c  deployment-251  fa647ad5-8a28-414d-968b-3170239272fe 809073 1 2023-03-20 07:41:52 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment ec9384d9-fc3a-47fd-ad52-4e326b518e2a 0xc005fb5d67 0xc005fb5d68}] []  [{kube-controller-manager Update apps/v1 2023-03-20 07:41:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec9384d9-fc3a-47fd-ad52-4e326b518e2a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 07:41:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67c8f74c6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005fb5e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 20 07:41:54.367: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 20 07:41:54.367: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-251  54ac3e6b-bba4-4d88-9f7f-e27c35568e3c 809081 2 2023-03-20 07:41:47 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment ec9384d9-fc3a-47fd-ad52-4e326b518e2a 0xc005fb5b47 0xc005fb5b48}] []  [{e2e.test Update apps/v1 2023-03-20 07:41:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 07:41:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec9384d9-fc3a-47fd-ad52-4e326b518e2a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-20 07:41:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005fb5ca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 20 07:41:54.370: INFO: Pod "test-rolling-update-deployment-67c8f74c6c-chxng" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c-chxng test-rolling-update-deployment-67c8f74c6c- deployment-251  c5468394-33ac-49df-95e3-8c7b95dbd53a 809072 0 2023-03-20 07:41:52 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[cni.projectcalico.org/podIP:192.168.90.82/32 cni.projectcalico.org/podIPs:192.168.90.82/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.82"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.82"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rolling-update-deployment-67c8f74c6c fa647ad5-8a28-414d-968b-3170239272fe 0xc00600c517 0xc00600c518}] []  [{calico Update v1 2023-03-20 07:41:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 07:41:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa647ad5-8a28-414d-968b-3170239272fe\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-03-20 07:41:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-20 07:41:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.90.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c9tt2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c9tt2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:41:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:41:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:41:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.71,PodIP:192.168.90.82,StartTime:2023-03-20 07:41:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-20 07:41:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://e39c1d98fcf0bc19f07f4804638c859c77498499b1f3cbe649afb329ab36370a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.90.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar 20 07:41:54.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-251" for this suite.

• [SLOW TEST:7.086 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":81,"skipped":1264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:41:54.380: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar 20 07:41:54.401: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:41:59.383: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:42:17.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8346" for this suite.

• [SLOW TEST:23.490 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":356,"completed":82,"skipped":1341,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:42:17.870: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 07:42:17.921: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba14204a-6182-44bc-9232-353bb88a0860" in namespace "projected-650" to be "Succeeded or Failed"
Mar 20 07:42:17.924: INFO: Pod "downwardapi-volume-ba14204a-6182-44bc-9232-353bb88a0860": Phase="Pending", Reason="", readiness=false. Elapsed: 3.069629ms
Mar 20 07:42:19.936: INFO: Pod "downwardapi-volume-ba14204a-6182-44bc-9232-353bb88a0860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014778656s
Mar 20 07:42:21.943: INFO: Pod "downwardapi-volume-ba14204a-6182-44bc-9232-353bb88a0860": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021686812s
STEP: Saw pod success
Mar 20 07:42:21.943: INFO: Pod "downwardapi-volume-ba14204a-6182-44bc-9232-353bb88a0860" satisfied condition "Succeeded or Failed"
Mar 20 07:42:21.946: INFO: Trying to get logs from node env016ar130-worker02 pod downwardapi-volume-ba14204a-6182-44bc-9232-353bb88a0860 container client-container: <nil>
STEP: delete the pod
Mar 20 07:42:21.965: INFO: Waiting for pod downwardapi-volume-ba14204a-6182-44bc-9232-353bb88a0860 to disappear
Mar 20 07:42:21.968: INFO: Pod downwardapi-volume-ba14204a-6182-44bc-9232-353bb88a0860 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar 20 07:42:21.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-650" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":83,"skipped":1352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:42:21.979: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 07:42:22.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3cb2425-73e6-4e9a-9633-a53d782d7dca" in namespace "projected-320" to be "Succeeded or Failed"
Mar 20 07:42:22.027: INFO: Pod "downwardapi-volume-d3cb2425-73e6-4e9a-9633-a53d782d7dca": Phase="Pending", Reason="", readiness=false. Elapsed: 7.760227ms
Mar 20 07:42:24.038: INFO: Pod "downwardapi-volume-d3cb2425-73e6-4e9a-9633-a53d782d7dca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018436995s
Mar 20 07:42:26.045: INFO: Pod "downwardapi-volume-d3cb2425-73e6-4e9a-9633-a53d782d7dca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025373801s
STEP: Saw pod success
Mar 20 07:42:26.045: INFO: Pod "downwardapi-volume-d3cb2425-73e6-4e9a-9633-a53d782d7dca" satisfied condition "Succeeded or Failed"
Mar 20 07:42:26.049: INFO: Trying to get logs from node env016ar130-worker02 pod downwardapi-volume-d3cb2425-73e6-4e9a-9633-a53d782d7dca container client-container: <nil>
STEP: delete the pod
Mar 20 07:42:26.071: INFO: Waiting for pod downwardapi-volume-d3cb2425-73e6-4e9a-9633-a53d782d7dca to disappear
Mar 20 07:42:26.074: INFO: Pod downwardapi-volume-d3cb2425-73e6-4e9a-9633-a53d782d7dca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar 20 07:42:26.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-320" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":84,"skipped":1385,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:42:26.107: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-40c35887-d407-4739-bba7-4cfec7889e36
STEP: Creating a pod to test consume secrets
Mar 20 07:42:26.173: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ab8da131-d96a-4c10-b8a5-15d563bdd620" in namespace "projected-5697" to be "Succeeded or Failed"
Mar 20 07:42:26.183: INFO: Pod "pod-projected-secrets-ab8da131-d96a-4c10-b8a5-15d563bdd620": Phase="Pending", Reason="", readiness=false. Elapsed: 10.108633ms
Mar 20 07:42:28.189: INFO: Pod "pod-projected-secrets-ab8da131-d96a-4c10-b8a5-15d563bdd620": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016227705s
Mar 20 07:42:30.201: INFO: Pod "pod-projected-secrets-ab8da131-d96a-4c10-b8a5-15d563bdd620": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028150285s
STEP: Saw pod success
Mar 20 07:42:30.201: INFO: Pod "pod-projected-secrets-ab8da131-d96a-4c10-b8a5-15d563bdd620" satisfied condition "Succeeded or Failed"
Mar 20 07:42:30.204: INFO: Trying to get logs from node env016ar130-worker01 pod pod-projected-secrets-ab8da131-d96a-4c10-b8a5-15d563bdd620 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 20 07:42:30.235: INFO: Waiting for pod pod-projected-secrets-ab8da131-d96a-4c10-b8a5-15d563bdd620 to disappear
Mar 20 07:42:30.240: INFO: Pod pod-projected-secrets-ab8da131-d96a-4c10-b8a5-15d563bdd620 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Mar 20 07:42:30.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5697" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":85,"skipped":1399,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:42:30.250: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:188
Mar 20 07:42:30.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-533" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":356,"completed":86,"skipped":1416,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:42:30.302: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:188
Mar 20 07:42:30.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-4099" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":356,"completed":87,"skipped":1460,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:42:30.394: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7474.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7474.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7474.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7474.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7474.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7474.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7474.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7474.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7474.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7474.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 239.178.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.178.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.178.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.178.239_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7474.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7474.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7474.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7474.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7474.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7474.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7474.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7474.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7474.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7474.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 239.178.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.178.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.178.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.178.239_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 20 07:42:32.492: INFO: Unable to read wheezy_udp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:32.495: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:32.498: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:32.501: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:32.515: INFO: Unable to read jessie_udp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:32.518: INFO: Unable to read jessie_tcp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:32.521: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:32.524: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:32.536: INFO: Lookups using dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301 failed for: [wheezy_udp@dns-test-service.dns-7474.svc.cluster.local wheezy_tcp@dns-test-service.dns-7474.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local jessie_udp@dns-test-service.dns-7474.svc.cluster.local jessie_tcp@dns-test-service.dns-7474.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local]

Mar 20 07:42:37.541: INFO: Unable to read wheezy_udp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:37.544: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:37.547: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:37.549: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:37.559: INFO: Unable to read jessie_udp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:37.562: INFO: Unable to read jessie_tcp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:37.565: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:37.568: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:37.579: INFO: Lookups using dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301 failed for: [wheezy_udp@dns-test-service.dns-7474.svc.cluster.local wheezy_tcp@dns-test-service.dns-7474.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local jessie_udp@dns-test-service.dns-7474.svc.cluster.local jessie_tcp@dns-test-service.dns-7474.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local]

Mar 20 07:42:42.543: INFO: Unable to read wheezy_udp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:42.548: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:42.552: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:42.557: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:42.575: INFO: Unable to read jessie_udp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:42.578: INFO: Unable to read jessie_tcp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:42.581: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:42.584: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:42.594: INFO: Lookups using dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301 failed for: [wheezy_udp@dns-test-service.dns-7474.svc.cluster.local wheezy_tcp@dns-test-service.dns-7474.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local jessie_udp@dns-test-service.dns-7474.svc.cluster.local jessie_tcp@dns-test-service.dns-7474.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local]

Mar 20 07:42:47.543: INFO: Unable to read wheezy_udp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:47.547: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:47.551: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:47.555: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:47.574: INFO: Unable to read jessie_udp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:47.578: INFO: Unable to read jessie_tcp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:47.581: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:47.584: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:47.597: INFO: Lookups using dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301 failed for: [wheezy_udp@dns-test-service.dns-7474.svc.cluster.local wheezy_tcp@dns-test-service.dns-7474.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local jessie_udp@dns-test-service.dns-7474.svc.cluster.local jessie_tcp@dns-test-service.dns-7474.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local]

Mar 20 07:42:52.542: INFO: Unable to read wheezy_udp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:52.547: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:52.551: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:52.555: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:52.572: INFO: Unable to read jessie_udp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:52.575: INFO: Unable to read jessie_tcp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:52.578: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:52.581: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:52.593: INFO: Lookups using dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301 failed for: [wheezy_udp@dns-test-service.dns-7474.svc.cluster.local wheezy_tcp@dns-test-service.dns-7474.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local jessie_udp@dns-test-service.dns-7474.svc.cluster.local jessie_tcp@dns-test-service.dns-7474.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local]

Mar 20 07:42:57.544: INFO: Unable to read wheezy_udp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:57.549: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:57.552: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:57.555: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:57.568: INFO: Unable to read jessie_udp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:57.571: INFO: Unable to read jessie_tcp@dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:57.573: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:57.576: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local from pod dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301: the server could not find the requested resource (get pods dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301)
Mar 20 07:42:57.585: INFO: Lookups using dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301 failed for: [wheezy_udp@dns-test-service.dns-7474.svc.cluster.local wheezy_tcp@dns-test-service.dns-7474.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local jessie_udp@dns-test-service.dns-7474.svc.cluster.local jessie_tcp@dns-test-service.dns-7474.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7474.svc.cluster.local]

Mar 20 07:43:02.592: INFO: DNS probes using dns-7474/dns-test-2a62ba03-c186-4edb-b2a9-693b109cf301 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar 20 07:43:02.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7474" for this suite.

• [SLOW TEST:32.270 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":356,"completed":88,"skipped":1470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:43:02.664: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's args
Mar 20 07:43:02.704: INFO: Waiting up to 5m0s for pod "var-expansion-9d94bd80-0c64-462c-b82a-0a5727f8b377" in namespace "var-expansion-1300" to be "Succeeded or Failed"
Mar 20 07:43:02.708: INFO: Pod "var-expansion-9d94bd80-0c64-462c-b82a-0a5727f8b377": Phase="Pending", Reason="", readiness=false. Elapsed: 4.263707ms
Mar 20 07:43:04.716: INFO: Pod "var-expansion-9d94bd80-0c64-462c-b82a-0a5727f8b377": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012115147s
Mar 20 07:43:06.723: INFO: Pod "var-expansion-9d94bd80-0c64-462c-b82a-0a5727f8b377": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019117941s
STEP: Saw pod success
Mar 20 07:43:06.723: INFO: Pod "var-expansion-9d94bd80-0c64-462c-b82a-0a5727f8b377" satisfied condition "Succeeded or Failed"
Mar 20 07:43:06.727: INFO: Trying to get logs from node env016ar130-worker01 pod var-expansion-9d94bd80-0c64-462c-b82a-0a5727f8b377 container dapi-container: <nil>
STEP: delete the pod
Mar 20 07:43:06.750: INFO: Waiting for pod var-expansion-9d94bd80-0c64-462c-b82a-0a5727f8b377 to disappear
Mar 20 07:43:06.756: INFO: Pod var-expansion-9d94bd80-0c64-462c-b82a-0a5727f8b377 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar 20 07:43:06.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1300" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":356,"completed":89,"skipped":1506,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:43:06.767: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Mar 20 07:43:06.832: INFO: Waiting up to 5m0s for pod "downward-api-d001cbb0-b4db-475c-badd-789313af6f79" in namespace "downward-api-9707" to be "Succeeded or Failed"
Mar 20 07:43:06.836: INFO: Pod "downward-api-d001cbb0-b4db-475c-badd-789313af6f79": Phase="Pending", Reason="", readiness=false. Elapsed: 3.678765ms
Mar 20 07:43:08.850: INFO: Pod "downward-api-d001cbb0-b4db-475c-badd-789313af6f79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017709029s
Mar 20 07:43:10.866: INFO: Pod "downward-api-d001cbb0-b4db-475c-badd-789313af6f79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033113593s
STEP: Saw pod success
Mar 20 07:43:10.866: INFO: Pod "downward-api-d001cbb0-b4db-475c-badd-789313af6f79" satisfied condition "Succeeded or Failed"
Mar 20 07:43:10.869: INFO: Trying to get logs from node env016ar130-worker02 pod downward-api-d001cbb0-b4db-475c-badd-789313af6f79 container dapi-container: <nil>
STEP: delete the pod
Mar 20 07:43:10.889: INFO: Waiting for pod downward-api-d001cbb0-b4db-475c-badd-789313af6f79 to disappear
Mar 20 07:43:10.892: INFO: Pod downward-api-d001cbb0-b4db-475c-badd-789313af6f79 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Mar 20 07:43:10.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9707" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":356,"completed":90,"skipped":1524,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:43:10.902: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 20 07:43:10.963: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:43:10.964: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:43:10.964: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:43:10.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 07:43:10.967: INFO: Node env016ar130-worker01 is running 0 daemon pod, expected 1
Mar 20 07:43:11.973: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:43:11.974: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:43:11.974: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:43:11.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 07:43:11.977: INFO: Node env016ar130-worker01 is running 0 daemon pod, expected 1
Mar 20 07:43:12.981: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:43:12.981: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:43:12.981: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:43:12.985: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 20 07:43:12.985: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Mar 20 07:43:13.011: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"809644"},"items":null}

Mar 20 07:43:13.015: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"809644"},"items":[{"metadata":{"name":"daemon-set-2wqzf","generateName":"daemon-set-","namespace":"daemonsets-2261","uid":"5d1c624f-607f-4f55-8e38-e9adb867f04c","resourceVersion":"809642","creationTimestamp":"2023-03-20T07:43:10Z","deletionTimestamp":"2023-03-20T07:43:43Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/podIP":"192.168.90.75/32","cni.projectcalico.org/podIPs":"192.168.90.75/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.90.75\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.90.75\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a55e27a5-bd59-4731-8dde-ffe74fd511bc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-20T07:43:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a55e27a5-bd59-4731-8dde-ffe74fd511bc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-20T07:43:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-03-20T07:43:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-20T07:43:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.90.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bcqsc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bcqsc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"env016ar130-worker01","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["env016ar130-worker01"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-20T07:43:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-20T07:43:12Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-20T07:43:12Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-20T07:43:10Z"}],"hostIP":"10.2.10.71","podIP":"192.168.90.75","podIPs":[{"ip":"192.168.90.75"}],"startTime":"2023-03-20T07:43:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-20T07:43:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://ef6ec81fe17e308c18a3e2ee1c1013747201789443e8494b0c6cc9005c7145eb","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-cbkgt","generateName":"daemon-set-","namespace":"daemonsets-2261","uid":"9ad72473-0526-4408-87c3-285663296af7","resourceVersion":"809643","creationTimestamp":"2023-03-20T07:43:10Z","deletionTimestamp":"2023-03-20T07:43:43Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/podIP":"192.168.71.177/32","cni.projectcalico.org/podIPs":"192.168.71.177/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.71.177\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.71.177\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a55e27a5-bd59-4731-8dde-ffe74fd511bc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-20T07:43:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a55e27a5-bd59-4731-8dde-ffe74fd511bc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-20T07:43:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-03-20T07:43:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-20T07:43:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.71.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gzglm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gzglm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"env016ar130-worker03","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["env016ar130-worker03"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-20T07:43:11Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-20T07:43:12Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-20T07:43:12Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-20T07:43:10Z"}],"hostIP":"10.2.10.73","podIP":"192.168.71.177","podIPs":[{"ip":"192.168.71.177"}],"startTime":"2023-03-20T07:43:11Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-20T07:43:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://baf4bc2bc8cc34a8422da20de6551bd6d252380c637a6e03bc6b386e2e0b42bd","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-wz2xz","generateName":"daemon-set-","namespace":"daemonsets-2261","uid":"3a6b99d5-5530-41db-84c5-c252142fd6c0","resourceVersion":"809644","creationTimestamp":"2023-03-20T07:43:10Z","deletionTimestamp":"2023-03-20T07:43:43Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/podIP":"192.168.12.16/32","cni.projectcalico.org/podIPs":"192.168.12.16/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.12.16\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.12.16\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a55e27a5-bd59-4731-8dde-ffe74fd511bc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-20T07:43:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a55e27a5-bd59-4731-8dde-ffe74fd511bc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-20T07:43:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-03-20T07:43:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-20T07:43:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4lh2v","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4lh2v","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"env016ar130-worker02","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["env016ar130-worker02"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-20T07:43:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-20T07:43:12Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-20T07:43:12Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-20T07:43:10Z"}],"hostIP":"10.2.10.72","podIP":"192.168.12.16","podIPs":[{"ip":"192.168.12.16"}],"startTime":"2023-03-20T07:43:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-20T07:43:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://4c1e5a4e9b9d57875b450df9dd87767a522c8e83bcb91eef22ca00d5d1d6d591","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Mar 20 07:43:13.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2261" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":356,"completed":91,"skipped":1574,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:43:13.068: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 20 07:43:13.118: INFO: Waiting up to 5m0s for pod "pod-9c620f62-8e8b-47a0-bbdc-8246c9b43f6f" in namespace "emptydir-6227" to be "Succeeded or Failed"
Mar 20 07:43:13.121: INFO: Pod "pod-9c620f62-8e8b-47a0-bbdc-8246c9b43f6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.728899ms
Mar 20 07:43:15.130: INFO: Pod "pod-9c620f62-8e8b-47a0-bbdc-8246c9b43f6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012091106s
Mar 20 07:43:17.138: INFO: Pod "pod-9c620f62-8e8b-47a0-bbdc-8246c9b43f6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020476553s
STEP: Saw pod success
Mar 20 07:43:17.138: INFO: Pod "pod-9c620f62-8e8b-47a0-bbdc-8246c9b43f6f" satisfied condition "Succeeded or Failed"
Mar 20 07:43:17.143: INFO: Trying to get logs from node env016ar130-worker01 pod pod-9c620f62-8e8b-47a0-bbdc-8246c9b43f6f container test-container: <nil>
STEP: delete the pod
Mar 20 07:43:17.217: INFO: Waiting for pod pod-9c620f62-8e8b-47a0-bbdc-8246c9b43f6f to disappear
Mar 20 07:43:17.222: INFO: Pod pod-9c620f62-8e8b-47a0-bbdc-8246c9b43f6f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar 20 07:43:17.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6227" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":92,"skipped":1583,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:43:17.235: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 07:43:17.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7253" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":356,"completed":93,"skipped":1671,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:43:17.320: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 07:43:17.782: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 07:43:20.812: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:43:20.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3931" for this suite.
STEP: Destroying namespace "webhook-3931-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":356,"completed":94,"skipped":1713,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:43:20.956: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 07:43:20.993: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b34e259-e37b-4301-a98a-9e9f374b431a" in namespace "downward-api-3041" to be "Succeeded or Failed"
Mar 20 07:43:21.000: INFO: Pod "downwardapi-volume-0b34e259-e37b-4301-a98a-9e9f374b431a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.697715ms
Mar 20 07:43:23.009: INFO: Pod "downwardapi-volume-0b34e259-e37b-4301-a98a-9e9f374b431a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015966627s
Mar 20 07:43:25.024: INFO: Pod "downwardapi-volume-0b34e259-e37b-4301-a98a-9e9f374b431a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030860368s
STEP: Saw pod success
Mar 20 07:43:25.024: INFO: Pod "downwardapi-volume-0b34e259-e37b-4301-a98a-9e9f374b431a" satisfied condition "Succeeded or Failed"
Mar 20 07:43:25.028: INFO: Trying to get logs from node env016ar130-worker02 pod downwardapi-volume-0b34e259-e37b-4301-a98a-9e9f374b431a container client-container: <nil>
STEP: delete the pod
Mar 20 07:43:25.048: INFO: Waiting for pod downwardapi-volume-0b34e259-e37b-4301-a98a-9e9f374b431a to disappear
Mar 20 07:43:25.051: INFO: Pod downwardapi-volume-0b34e259-e37b-4301-a98a-9e9f374b431a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar 20 07:43:25.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3041" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":95,"skipped":1727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:43:25.063: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Mar 20 07:43:27.121: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5188 PodName:pod-sharedvolume-5de972d6-6d08-4576-884c-24551678bfec ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:43:27.121: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:43:27.122: INFO: ExecWithOptions: Clientset creation
Mar 20 07:43:27.122: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/emptydir-5188/pods/pod-sharedvolume-5de972d6-6d08-4576-884c-24551678bfec/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Mar 20 07:43:27.233: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar 20 07:43:27.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5188" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":356,"completed":96,"skipped":1771,"failed":0}

------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:43:27.254: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar 20 07:44:27.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6522" for this suite.

• [SLOW TEST:60.070 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":356,"completed":97,"skipped":1771,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:44:27.324: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 20 07:44:27.370: INFO: Waiting up to 5m0s for pod "pod-3dc6ea2e-2a7a-48d5-b400-e3b4b6d12f3c" in namespace "emptydir-2434" to be "Succeeded or Failed"
Mar 20 07:44:27.373: INFO: Pod "pod-3dc6ea2e-2a7a-48d5-b400-e3b4b6d12f3c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.319303ms
Mar 20 07:44:29.386: INFO: Pod "pod-3dc6ea2e-2a7a-48d5-b400-e3b4b6d12f3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016722293s
Mar 20 07:44:31.400: INFO: Pod "pod-3dc6ea2e-2a7a-48d5-b400-e3b4b6d12f3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030289488s
STEP: Saw pod success
Mar 20 07:44:31.400: INFO: Pod "pod-3dc6ea2e-2a7a-48d5-b400-e3b4b6d12f3c" satisfied condition "Succeeded or Failed"
Mar 20 07:44:31.404: INFO: Trying to get logs from node env016ar130-worker02 pod pod-3dc6ea2e-2a7a-48d5-b400-e3b4b6d12f3c container test-container: <nil>
STEP: delete the pod
Mar 20 07:44:31.424: INFO: Waiting for pod pod-3dc6ea2e-2a7a-48d5-b400-e3b4b6d12f3c to disappear
Mar 20 07:44:31.427: INFO: Pod pod-3dc6ea2e-2a7a-48d5-b400-e3b4b6d12f3c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar 20 07:44:31.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2434" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":98,"skipped":1781,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:44:31.438: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Service
STEP: watching for the Service to be added
Mar 20 07:44:31.486: INFO: Found Service test-service-drmfp in namespace services-3288 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar 20 07:44:31.486: INFO: Service test-service-drmfp created
STEP: Getting /status
Mar 20 07:44:31.490: INFO: Service test-service-drmfp has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Mar 20 07:44:31.501: INFO: observed Service test-service-drmfp in namespace services-3288 with annotations: map[] & LoadBalancer: {[]}
Mar 20 07:44:31.501: INFO: Found Service test-service-drmfp in namespace services-3288 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar 20 07:44:31.501: INFO: Service test-service-drmfp has service status patched
STEP: updating the ServiceStatus
Mar 20 07:44:31.513: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Mar 20 07:44:31.515: INFO: Observed Service test-service-drmfp in namespace services-3288 with annotations: map[] & Conditions: {[]}
Mar 20 07:44:31.515: INFO: Observed event: &Service{ObjectMeta:{test-service-drmfp  services-3288  4e6da082-8b98-4a58-a6a2-610ea87c7da1 810122 0 2023-03-20 07:44:31 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2023-03-20 07:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-20 07:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:192.168.173.213,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[192.168.173.213],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar 20 07:44:31.515: INFO: Found Service test-service-drmfp in namespace services-3288 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 20 07:44:31.515: INFO: Service test-service-drmfp has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Mar 20 07:44:31.528: INFO: observed Service test-service-drmfp in namespace services-3288 with labels: map[test-service-static:true]
Mar 20 07:44:31.528: INFO: observed Service test-service-drmfp in namespace services-3288 with labels: map[test-service-static:true]
Mar 20 07:44:31.528: INFO: observed Service test-service-drmfp in namespace services-3288 with labels: map[test-service-static:true]
Mar 20 07:44:31.528: INFO: Found Service test-service-drmfp in namespace services-3288 with labels: map[test-service:patched test-service-static:true]
Mar 20 07:44:31.528: INFO: Service test-service-drmfp patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Mar 20 07:44:31.539: INFO: Observed event: ADDED
Mar 20 07:44:31.539: INFO: Observed event: MODIFIED
Mar 20 07:44:31.539: INFO: Observed event: MODIFIED
Mar 20 07:44:31.539: INFO: Observed event: MODIFIED
Mar 20 07:44:31.540: INFO: Found Service test-service-drmfp in namespace services-3288 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar 20 07:44:31.540: INFO: Service test-service-drmfp deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 07:44:31.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3288" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":356,"completed":99,"skipped":1802,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:44:31.549: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 07:44:31.840: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 07:44:34.863: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:44:34.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9968" for this suite.
STEP: Destroying namespace "webhook-9968-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":356,"completed":100,"skipped":1809,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:44:34.973: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:44:34.999: INFO: Creating ReplicaSet my-hostname-basic-1fdcdeca-c062-49ed-bd3f-47c0306ce778
Mar 20 07:44:35.013: INFO: Pod name my-hostname-basic-1fdcdeca-c062-49ed-bd3f-47c0306ce778: Found 0 pods out of 1
Mar 20 07:44:40.024: INFO: Pod name my-hostname-basic-1fdcdeca-c062-49ed-bd3f-47c0306ce778: Found 1 pods out of 1
Mar 20 07:44:40.024: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1fdcdeca-c062-49ed-bd3f-47c0306ce778" is running
Mar 20 07:44:40.028: INFO: Pod "my-hostname-basic-1fdcdeca-c062-49ed-bd3f-47c0306ce778-dq7z6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-20 07:44:35 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-20 07:44:35 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-20 07:44:35 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-20 07:44:35 +0000 UTC Reason: Message:}])
Mar 20 07:44:40.028: INFO: Trying to dial the pod
Mar 20 07:44:45.051: INFO: Controller my-hostname-basic-1fdcdeca-c062-49ed-bd3f-47c0306ce778: Got expected result from replica 1 [my-hostname-basic-1fdcdeca-c062-49ed-bd3f-47c0306ce778-dq7z6]: "my-hostname-basic-1fdcdeca-c062-49ed-bd3f-47c0306ce778-dq7z6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Mar 20 07:44:45.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3589" for this suite.

• [SLOW TEST:10.095 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":101,"skipped":1819,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:44:45.068: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:44:45.134: INFO: Got root ca configmap in namespace "svcaccounts-2386"
Mar 20 07:44:45.139: INFO: Deleted root ca configmap in namespace "svcaccounts-2386"
STEP: waiting for a new root ca configmap created
Mar 20 07:44:45.646: INFO: Recreated root ca configmap in namespace "svcaccounts-2386"
Mar 20 07:44:45.657: INFO: Updated root ca configmap in namespace "svcaccounts-2386"
STEP: waiting for the root ca configmap reconciled
Mar 20 07:44:46.166: INFO: Reconciled root ca configmap in namespace "svcaccounts-2386"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Mar 20 07:44:46.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2386" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":356,"completed":102,"skipped":1821,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:44:46.181: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Mar 20 07:44:46.218: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:45:11.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9591" for this suite.

• [SLOW TEST:25.481 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":356,"completed":103,"skipped":1821,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:45:11.662: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Mar 20 07:45:11.705: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:45:13.717: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Mar 20 07:45:13.737: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:45:15.751: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 20 07:45:15.754: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4037 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:45:15.754: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:45:15.755: INFO: ExecWithOptions: Clientset creation
Mar 20 07:45:15.755: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4037/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 20 07:45:15.858: INFO: Exec stderr: ""
Mar 20 07:45:15.858: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4037 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:45:15.858: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:45:15.860: INFO: ExecWithOptions: Clientset creation
Mar 20 07:45:15.860: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4037/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 20 07:45:15.958: INFO: Exec stderr: ""
Mar 20 07:45:15.958: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4037 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:45:15.958: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:45:15.959: INFO: ExecWithOptions: Clientset creation
Mar 20 07:45:15.959: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4037/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 20 07:45:16.055: INFO: Exec stderr: ""
Mar 20 07:45:16.055: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4037 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:45:16.055: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:45:16.056: INFO: ExecWithOptions: Clientset creation
Mar 20 07:45:16.056: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4037/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 20 07:45:16.147: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 20 07:45:16.147: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4037 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:45:16.147: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:45:16.148: INFO: ExecWithOptions: Clientset creation
Mar 20 07:45:16.148: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4037/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 20 07:45:16.250: INFO: Exec stderr: ""
Mar 20 07:45:16.250: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4037 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:45:16.250: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:45:16.251: INFO: ExecWithOptions: Clientset creation
Mar 20 07:45:16.251: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4037/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 20 07:45:16.330: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 20 07:45:16.330: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4037 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:45:16.330: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:45:16.330: INFO: ExecWithOptions: Clientset creation
Mar 20 07:45:16.331: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4037/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 20 07:45:16.414: INFO: Exec stderr: ""
Mar 20 07:45:16.414: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4037 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:45:16.414: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:45:16.415: INFO: ExecWithOptions: Clientset creation
Mar 20 07:45:16.416: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4037/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 20 07:45:16.502: INFO: Exec stderr: ""
Mar 20 07:45:16.502: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4037 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:45:16.502: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:45:16.503: INFO: ExecWithOptions: Clientset creation
Mar 20 07:45:16.503: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4037/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 20 07:45:16.595: INFO: Exec stderr: ""
Mar 20 07:45:16.595: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4037 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:45:16.595: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:45:16.596: INFO: ExecWithOptions: Clientset creation
Mar 20 07:45:16.596: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4037/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 20 07:45:16.682: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:188
Mar 20 07:45:16.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4037" for this suite.

• [SLOW TEST:5.031 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":104,"skipped":1831,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:45:16.696: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Mar 20 07:45:16.721: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 20 07:46:16.846: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:46:16.849: INFO: Starting informer...
STEP: Starting pods...
Mar 20 07:46:17.072: INFO: Pod1 is running on env016ar130-worker01. Tainting Node
Mar 20 07:46:19.301: INFO: Pod2 is running on env016ar130-worker01. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar 20 07:46:27.771: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 20 07:46:45.125: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:188
Mar 20 07:46:45.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5558" for this suite.

• [SLOW TEST:88.495 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":356,"completed":105,"skipped":1900,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:46:45.191: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: reading a file in the container
Mar 20 07:46:57.271: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6896 pod-service-account-ff51119b-f4b1-466c-8fb2-01745301d5d7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar 20 07:46:57.446: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6896 pod-service-account-ff51119b-f4b1-466c-8fb2-01745301d5d7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar 20 07:46:57.621: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6896 pod-service-account-ff51119b-f4b1-466c-8fb2-01745301d5d7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Mar 20 07:46:57.780: INFO: Got root ca configmap in namespace "svcaccounts-6896"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Mar 20 07:46:57.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6896" for this suite.

• [SLOW TEST:12.604 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":356,"completed":106,"skipped":1932,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:46:57.796: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-843ec184-8cbb-45be-944f-4318a815781f
STEP: Creating a pod to test consume configMaps
Mar 20 07:46:57.886: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eb2abf24-d4bd-4e5a-8c83-12863310634d" in namespace "projected-5266" to be "Succeeded or Failed"
Mar 20 07:46:57.890: INFO: Pod "pod-projected-configmaps-eb2abf24-d4bd-4e5a-8c83-12863310634d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.143501ms
Mar 20 07:46:59.897: INFO: Pod "pod-projected-configmaps-eb2abf24-d4bd-4e5a-8c83-12863310634d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010722442s
Mar 20 07:47:01.904: INFO: Pod "pod-projected-configmaps-eb2abf24-d4bd-4e5a-8c83-12863310634d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017743297s
Mar 20 07:47:03.914: INFO: Pod "pod-projected-configmaps-eb2abf24-d4bd-4e5a-8c83-12863310634d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027402722s
Mar 20 07:47:05.923: INFO: Pod "pod-projected-configmaps-eb2abf24-d4bd-4e5a-8c83-12863310634d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036994872s
Mar 20 07:47:07.937: INFO: Pod "pod-projected-configmaps-eb2abf24-d4bd-4e5a-8c83-12863310634d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.050842146s
STEP: Saw pod success
Mar 20 07:47:07.937: INFO: Pod "pod-projected-configmaps-eb2abf24-d4bd-4e5a-8c83-12863310634d" satisfied condition "Succeeded or Failed"
Mar 20 07:47:07.941: INFO: Trying to get logs from node env016ar130-worker01 pod pod-projected-configmaps-eb2abf24-d4bd-4e5a-8c83-12863310634d container agnhost-container: <nil>
STEP: delete the pod
Mar 20 07:47:07.971: INFO: Waiting for pod pod-projected-configmaps-eb2abf24-d4bd-4e5a-8c83-12863310634d to disappear
Mar 20 07:47:07.973: INFO: Pod pod-projected-configmaps-eb2abf24-d4bd-4e5a-8c83-12863310634d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar 20 07:47:07.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5266" for this suite.

• [SLOW TEST:10.186 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":107,"skipped":1938,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:47:07.981: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-3e1abfcb-cd2f-458e-9ed0-f85edfd4a045
STEP: Creating a pod to test consume configMaps
Mar 20 07:47:08.059: INFO: Waiting up to 5m0s for pod "pod-configmaps-c229830b-a146-49f4-bf97-f6a62d804612" in namespace "configmap-9080" to be "Succeeded or Failed"
Mar 20 07:47:08.062: INFO: Pod "pod-configmaps-c229830b-a146-49f4-bf97-f6a62d804612": Phase="Pending", Reason="", readiness=false. Elapsed: 3.769525ms
Mar 20 07:47:10.075: INFO: Pod "pod-configmaps-c229830b-a146-49f4-bf97-f6a62d804612": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01601302s
Mar 20 07:47:12.087: INFO: Pod "pod-configmaps-c229830b-a146-49f4-bf97-f6a62d804612": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027930229s
STEP: Saw pod success
Mar 20 07:47:12.087: INFO: Pod "pod-configmaps-c229830b-a146-49f4-bf97-f6a62d804612" satisfied condition "Succeeded or Failed"
Mar 20 07:47:12.090: INFO: Trying to get logs from node env016ar130-worker02 pod pod-configmaps-c229830b-a146-49f4-bf97-f6a62d804612 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 20 07:47:12.121: INFO: Waiting for pod pod-configmaps-c229830b-a146-49f4-bf97-f6a62d804612 to disappear
Mar 20 07:47:12.125: INFO: Pod pod-configmaps-c229830b-a146-49f4-bf97-f6a62d804612 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar 20 07:47:12.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9080" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":108,"skipped":1944,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:47:12.134: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting the proxy server
Mar 20 07:47:12.156: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-9880 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 07:47:12.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9880" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":356,"completed":109,"skipped":1951,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:47:12.217: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-a02d6638-49bb-4b6f-ab81-8a4b2b416553 in namespace container-probe-2334
Mar 20 07:47:14.257: INFO: Started pod liveness-a02d6638-49bb-4b6f-ab81-8a4b2b416553 in namespace container-probe-2334
STEP: checking the pod's current state and verifying that restartCount is present
Mar 20 07:47:14.261: INFO: Initial restart count of pod liveness-a02d6638-49bb-4b6f-ab81-8a4b2b416553 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar 20 07:51:15.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2334" for this suite.

• [SLOW TEST:243.351 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":356,"completed":110,"skipped":1970,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:51:15.570: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Mar 20 07:51:15.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-244" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":356,"completed":111,"skipped":2026,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:51:15.627: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:51:15.669: INFO: created pod pod-service-account-defaultsa
Mar 20 07:51:15.669: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 20 07:51:15.675: INFO: created pod pod-service-account-mountsa
Mar 20 07:51:15.675: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 20 07:51:15.685: INFO: created pod pod-service-account-nomountsa
Mar 20 07:51:15.685: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 20 07:51:15.693: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 20 07:51:15.693: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 20 07:51:15.702: INFO: created pod pod-service-account-mountsa-mountspec
Mar 20 07:51:15.702: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 20 07:51:15.768: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 20 07:51:15.768: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 20 07:51:15.779: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 20 07:51:15.779: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 20 07:51:15.786: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 20 07:51:15.786: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 20 07:51:15.793: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 20 07:51:15.793: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Mar 20 07:51:15.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6264" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":356,"completed":112,"skipped":2046,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:51:15.803: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensure pods equal to paralellism count is attached to the job
STEP: patching /status
STEP: updating /status
STEP: get /status
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Mar 20 07:51:21.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-359" for this suite.

• [SLOW TEST:6.086 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","total":356,"completed":113,"skipped":2073,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:51:21.890: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 07:51:22.275: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 07:51:25.304: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:51:25.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5172" for this suite.
STEP: Destroying namespace "webhook-5172-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":356,"completed":114,"skipped":2084,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:51:25.400: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar 20 07:51:25.434: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3129  f08e3c03-2f12-4893-b38c-ea63adee3304 812383 0 2023-03-20 07:51:25 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2023-03-20 07:51:25 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62sgs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62sgs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:25.440: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:51:27.451: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Mar 20 07:51:27.451: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3129 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:51:27.451: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:51:27.452: INFO: ExecWithOptions: Clientset creation
Mar 20 07:51:27.452: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/dns-3129/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod...
Mar 20 07:51:27.566: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3129 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 07:51:27.566: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 07:51:27.566: INFO: ExecWithOptions: Clientset creation
Mar 20 07:51:27.567: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/dns-3129/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 20 07:51:27.658: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar 20 07:51:27.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3129" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":356,"completed":115,"skipped":2102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:51:27.686: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  test/e2e/framework/framework.go:652
STEP: Request ServerVersion
STEP: Confirm major version
Mar 20 07:51:27.733: INFO: Major version: 1
STEP: Confirm minor version
Mar 20 07:51:27.733: INFO: cleanMinorVersion: 24
Mar 20 07:51:27.733: INFO: Minor version: 24
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:188
Mar 20 07:51:27.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-7231" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":356,"completed":116,"skipped":2143,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:51:27.743: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1334
STEP: creating the pod
Mar 20 07:51:27.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-400 create -f -'
Mar 20 07:51:28.583: INFO: stderr: ""
Mar 20 07:51:28.583: INFO: stdout: "pod/pause created\n"
Mar 20 07:51:28.583: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 20 07:51:28.583: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-400" to be "running and ready"
Mar 20 07:51:28.587: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.262925ms
Mar 20 07:51:30.591: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007975045s
Mar 20 07:51:30.591: INFO: Pod "pause" satisfied condition "running and ready"
Mar 20 07:51:30.591: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 20 07:51:30.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-400 label pods pause testing-label=testing-label-value'
Mar 20 07:51:30.671: INFO: stderr: ""
Mar 20 07:51:30.671: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 20 07:51:30.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-400 get pod pause -L testing-label'
Mar 20 07:51:30.745: INFO: stderr: ""
Mar 20 07:51:30.745: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 20 07:51:30.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-400 label pods pause testing-label-'
Mar 20 07:51:30.885: INFO: stderr: ""
Mar 20 07:51:30.885: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 20 07:51:30.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-400 get pod pause -L testing-label'
Mar 20 07:51:30.965: INFO: stderr: ""
Mar 20 07:51:30.965: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Mar 20 07:51:30.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-400 delete --grace-period=0 --force -f -'
Mar 20 07:51:31.042: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 20 07:51:31.042: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 20 07:51:31.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-400 get rc,svc -l name=pause --no-headers'
Mar 20 07:51:31.119: INFO: stderr: "No resources found in kubectl-400 namespace.\n"
Mar 20 07:51:31.119: INFO: stdout: ""
Mar 20 07:51:31.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-400 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 20 07:51:31.176: INFO: stderr: ""
Mar 20 07:51:31.176: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 07:51:31.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-400" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":356,"completed":117,"skipped":2159,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:51:31.191: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:51:31.218: INFO: Creating deployment "webserver-deployment"
Mar 20 07:51:31.225: INFO: Waiting for observed generation 1
Mar 20 07:51:33.236: INFO: Waiting for all required pods to come up
Mar 20 07:51:33.242: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 20 07:51:35.260: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 20 07:51:35.265: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 20 07:51:35.277: INFO: Updating deployment webserver-deployment
Mar 20 07:51:35.277: INFO: Waiting for observed generation 2
Mar 20 07:51:37.286: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 20 07:51:37.290: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 20 07:51:37.293: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 20 07:51:37.302: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 20 07:51:37.302: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 20 07:51:37.304: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 20 07:51:37.309: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 20 07:51:37.309: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 20 07:51:37.324: INFO: Updating deployment webserver-deployment
Mar 20 07:51:37.324: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 20 07:51:37.329: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 20 07:51:39.337: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 20 07:51:39.342: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6600  ba4f4806-ce02-4605-82ef-269be2bcadd5 812877 3 2023-03-20 07:51:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003936108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-20 07:51:37 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-57ccb67bb8" is progressing.,LastUpdateTime:2023-03-20 07:51:37 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 20 07:51:39.344: INFO: New ReplicaSet "webserver-deployment-57ccb67bb8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-57ccb67bb8  deployment-6600  fc1516ec-79f0-48a0-bc42-c86e185d10c7 812864 3 2023-03-20 07:51:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment ba4f4806-ce02-4605-82ef-269be2bcadd5 0xc005d1da17 0xc005d1da18}] []  [{kube-controller-manager Update apps/v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba4f4806-ce02-4605-82ef-269be2bcadd5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 57ccb67bb8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d1dab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 20 07:51:39.344: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 20 07:51:39.345: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-55df494869  deployment-6600  52bb23bc-fb74-412b-9861-09407e04666a 812870 3 2023-03-20 07:51:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment ba4f4806-ce02-4605-82ef-269be2bcadd5 0xc005d1d927 0xc005d1d928}] []  [{kube-controller-manager Update apps/v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba4f4806-ce02-4605-82ef-269be2bcadd5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 07:51:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d1d9b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 20 07:51:39.350: INFO: Pod "webserver-deployment-55df494869-4cqfm" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-4cqfm webserver-deployment-55df494869- deployment-6600  ef53b548-ccc0-40aa-b9fd-6eff8de790f3 812865 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc0087e8c87 0xc0087e8c88}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8slzp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8slzp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.72,PodIP:,StartTime:2023-03-20 07:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.350: INFO: Pod "webserver-deployment-55df494869-4z8qs" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-4z8qs webserver-deployment-55df494869- deployment-6600  b9dd3048-c7b3-438a-b38d-e5c95400c0f5 812620 0 2023-03-20 07:51:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.90.91/32 cni.projectcalico.org/podIPs:192.168.90.91/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.91"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.91"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc0087e8e60 0xc0087e8e61}] []  [{calico Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-20 07:51:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.90.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vx6b9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vx6b9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.71,PodIP:192.168.90.91,StartTime:2023-03-20 07:51:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-20 07:51:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cf1daa218fe9fdde03e5420b55e6d6bad87a73999eb2f7b0ab29185372689c05,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.90.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.350: INFO: Pod "webserver-deployment-55df494869-5m6gk" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-5m6gk webserver-deployment-55df494869- deployment-6600  5d2bc5dc-7230-4a3b-bb63-97eeaf4620c8 812913 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.12.8/32 cni.projectcalico.org/podIPs:192.168.12.8/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.8"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.8"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc0087e9090 0xc0087e9091}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zrplp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zrplp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.72,PodIP:,StartTime:2023-03-20 07:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.351: INFO: Pod "webserver-deployment-55df494869-87xlt" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-87xlt webserver-deployment-55df494869- deployment-6600  acc524ed-6a6c-46e0-bd70-2e841a1fd2b2 812922 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.71.183/32 cni.projectcalico.org/podIPs:192.168.71.183/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.183"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.183"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc0087e9290 0xc0087e9291}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gvhkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gvhkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.73,PodIP:,StartTime:2023-03-20 07:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.351: INFO: Pod "webserver-deployment-55df494869-8jvcj" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-8jvcj webserver-deployment-55df494869- deployment-6600  d0a45da4-d91c-4903-8f25-b5ffe34397ab 812647 0 2023-03-20 07:51:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.12.19/32 cni.projectcalico.org/podIPs:192.168.12.19/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.19"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.19"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc0087e94a0 0xc0087e94a1}] []  [{calico Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-20 07:51:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kprjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kprjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.72,PodIP:192.168.12.19,StartTime:2023-03-20 07:51:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-20 07:51:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a9af0b53be734dfee1efc458f1b7a7d27ebe4ffbb4de00bae7f5a76626fe4d35,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.351: INFO: Pod "webserver-deployment-55df494869-bmh74" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-bmh74 webserver-deployment-55df494869- deployment-6600  c2159d98-9235-44b3-88c7-a9a1e8a6f8a6 812945 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.90.65/32 cni.projectcalico.org/podIPs:192.168.90.65/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.65"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.65"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc0087e96d0 0xc0087e96d1}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drkh9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drkh9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.351: INFO: Pod "webserver-deployment-55df494869-bz5dz" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-bz5dz webserver-deployment-55df494869- deployment-6600  e7ac3545-f69e-436b-9341-f85b9b454c6f 813010 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.12.5/32 cni.projectcalico.org/podIPs:192.168.12.5/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.5"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.5"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc0087e9880 0xc0087e9881}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-20 07:51:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:51:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9dhjc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9dhjc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.72,PodIP:,StartTime:2023-03-20 07:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.351: INFO: Pod "webserver-deployment-55df494869-d96bz" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-d96bz webserver-deployment-55df494869- deployment-6600  44c25f6d-bee9-4e46-b552-ff734254e278 812846 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc0087e9a70 0xc0087e9a71}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2cnqn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2cnqn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.351: INFO: Pod "webserver-deployment-55df494869-dqnh8" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-dqnh8 webserver-deployment-55df494869- deployment-6600  14f562e6-3d3b-4c83-92a9-443c915401ff 812640 0 2023-03-20 07:51:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.71.179/32 cni.projectcalico.org/podIPs:192.168.71.179/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.179"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.179"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc0087e9bc0 0xc0087e9bc1}] []  [{calico Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-20 07:51:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.71.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bpc7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bpc7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.73,PodIP:192.168.71.179,StartTime:2023-03-20 07:51:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-20 07:51:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5eccbf7914665ada86055fde94ec343f427f6a3df8d96fc30c04753a23aacb2d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.71.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.352: INFO: Pod "webserver-deployment-55df494869-ffh28" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-ffh28 webserver-deployment-55df494869- deployment-6600  514e1750-7d46-4adf-87b6-37815d3104a8 812666 0 2023-03-20 07:51:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.90.102/32 cni.projectcalico.org/podIPs:192.168.90.102/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.102"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.102"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc0087e9dd0 0xc0087e9dd1}] []  [{calico Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-20 07:51:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.90.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcnpw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcnpw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.71,PodIP:192.168.90.102,StartTime:2023-03-20 07:51:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-20 07:51:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9808304a966c67d6b57459add78585fd63558575b5247b68942467277d367fdb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.90.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.352: INFO: Pod "webserver-deployment-55df494869-gw66b" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-gw66b webserver-deployment-55df494869- deployment-6600  cf752b1e-6555-48bd-9c6d-c9aa79ef6bc7 812943 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.12.56/32 cni.projectcalico.org/podIPs:192.168.12.56/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.56"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.56"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc00869e000 0xc00869e001}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mmmk6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mmmk6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.72,PodIP:,StartTime:2023-03-20 07:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.352: INFO: Pod "webserver-deployment-55df494869-j9nwc" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-j9nwc webserver-deployment-55df494869- deployment-6600  42fe7f53-6ca0-4c0c-b4dd-54fcee361294 812968 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.90.108/32 cni.projectcalico.org/podIPs:192.168.90.108/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.108"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.108"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc00869e1f0 0xc00869e1f1}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xvhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xvhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.71,PodIP:,StartTime:2023-03-20 07:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.352: INFO: Pod "webserver-deployment-55df494869-l626l" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-l626l webserver-deployment-55df494869- deployment-6600  ea8c92f4-b06b-42f2-870e-e4e0afa2c02d 812652 0 2023-03-20 07:51:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.12.52/32 cni.projectcalico.org/podIPs:192.168.12.52/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.52"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.52"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc00869e400 0xc00869e401}] []  [{calico Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-20 07:51:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-452m9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-452m9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.72,PodIP:192.168.12.52,StartTime:2023-03-20 07:51:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-20 07:51:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4978922c67cb37d90b80c10e1df1786dba97b230f7011b5be5643932c985f3f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.352: INFO: Pod "webserver-deployment-55df494869-mrgw5" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-mrgw5 webserver-deployment-55df494869- deployment-6600  9e5f2a14-84d1-4e9f-8b10-4802b4491467 812975 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.71.187/32 cni.projectcalico.org/podIPs:192.168.71.187/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.187"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.187"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc00869e610 0xc00869e611}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jfnsl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jfnsl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.73,PodIP:,StartTime:2023-03-20 07:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.353: INFO: Pod "webserver-deployment-55df494869-q9g2n" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-q9g2n webserver-deployment-55df494869- deployment-6600  f82aee1c-6dda-4775-8503-a7dfbee9533e 812642 0 2023-03-20 07:51:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.12.29/32 cni.projectcalico.org/podIPs:192.168.12.29/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.29"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.29"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc00869e820 0xc00869e821}] []  [{calico Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-20 07:51:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67fts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67fts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.72,PodIP:192.168.12.29,StartTime:2023-03-20 07:51:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-20 07:51:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://deb5e50f5b42a9a9202ad66df0819e397afc52fc1613379091e4cffad1472635,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.353: INFO: Pod "webserver-deployment-55df494869-rfjll" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-rfjll webserver-deployment-55df494869- deployment-6600  146c2587-b148-4c27-8eb5-bc69e879ef78 812637 0 2023-03-20 07:51:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.71.181/32 cni.projectcalico.org/podIPs:192.168.71.181/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.181"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.181"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc00869ea30 0xc00869ea31}] []  [{calico Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-20 07:51:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.71.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jsj5c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jsj5c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.73,PodIP:192.168.71.181,StartTime:2023-03-20 07:51:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-20 07:51:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6b7595abf4797c8d02ea29413e99ebe8adecd084a344458ba003a13a278f12d4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.71.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.353: INFO: Pod "webserver-deployment-55df494869-slqhb" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-slqhb webserver-deployment-55df494869- deployment-6600  0151724f-a935-4b39-b123-a2e182b1d4a8 812876 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc00869ec40 0xc00869ec41}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-snbxk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-snbxk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.73,PodIP:,StartTime:2023-03-20 07:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.353: INFO: Pod "webserver-deployment-55df494869-t2gsw" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-t2gsw webserver-deployment-55df494869- deployment-6600  74c8f9d8-f1c0-4219-a22d-15f8a4d54de7 812933 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.90.70/32 cni.projectcalico.org/podIPs:192.168.90.70/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.70"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.70"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc00869ee10 0xc00869ee11}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r5t6j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r5t6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.71,PodIP:,StartTime:2023-03-20 07:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.353: INFO: Pod "webserver-deployment-55df494869-wcbn2" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-wcbn2 webserver-deployment-55df494869- deployment-6600  a33584d8-410d-4162-889f-518e72fcf06b 812644 0 2023-03-20 07:51:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.71.184/32 cni.projectcalico.org/podIPs:192.168.71.184/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.184"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.184"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc00869f000 0xc00869f001}] []  [{calico Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-03-20 07:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-20 07:51:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.71.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9hwsm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9hwsm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.73,PodIP:192.168.71.184,StartTime:2023-03-20 07:51:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-20 07:51:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://fee9ff1ebde8b8b0b1fc6c7fd1da5a12df78f561612522745de1cddd852b667e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.71.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.353: INFO: Pod "webserver-deployment-55df494869-xwb4z" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-xwb4z webserver-deployment-55df494869- deployment-6600  f0bed1e8-3ed1-4f77-a04b-e59ce944cc83 813009 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.90.95/32 cni.projectcalico.org/podIPs:192.168.90.95/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.95"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.95"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-55df494869 52bb23bc-fb74-412b-9861-09407e04666a 0xc00869f240 0xc00869f241}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52bb23bc-fb74-412b-9861-09407e04666a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-20 07:51:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:51:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qwljc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qwljc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.353: INFO: Pod "webserver-deployment-57ccb67bb8-5wfxl" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-5wfxl webserver-deployment-57ccb67bb8- deployment-6600  e8432045-8505-490e-bf51-230741cdb81f 812837 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 fc1516ec-79f0-48a0-bc42-c86e185d10c7 0xc00869f3d0 0xc00869f3d1}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc1516ec-79f0-48a0-bc42-c86e185d10c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wmxnf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wmxnf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.354: INFO: Pod "webserver-deployment-57ccb67bb8-7q8tm" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-7q8tm webserver-deployment-57ccb67bb8- deployment-6600  cb592a4d-7948-4214-8275-03b1f467185c 812758 0 2023-03-20 07:51:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/podIP:192.168.90.125/32 cni.projectcalico.org/podIPs:192.168.90.125/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.125"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.125"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 fc1516ec-79f0-48a0-bc42-c86e185d10c7 0xc00869f540 0xc00869f541}] []  [{calico Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc1516ec-79f0-48a0-bc42-c86e185d10c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-stxvd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-stxvd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.71,PodIP:,StartTime:2023-03-20 07:51:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.354: INFO: Pod "webserver-deployment-57ccb67bb8-978jn" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-978jn webserver-deployment-57ccb67bb8- deployment-6600  19d963bb-7737-4daf-a8cd-42ef43b7bcd3 812855 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 fc1516ec-79f0-48a0-bc42-c86e185d10c7 0xc00869f750 0xc00869f751}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc1516ec-79f0-48a0-bc42-c86e185d10c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jgcbk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jgcbk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.354: INFO: Pod "webserver-deployment-57ccb67bb8-9smsp" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-9smsp webserver-deployment-57ccb67bb8- deployment-6600  c193a6b2-cfce-4ea8-9f1b-ed0dfd471a9e 812948 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/podIP:192.168.71.182/32 cni.projectcalico.org/podIPs:192.168.71.182/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.182"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.182"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 fc1516ec-79f0-48a0-bc42-c86e185d10c7 0xc00869f8b0 0xc00869f8b1}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc1516ec-79f0-48a0-bc42-c86e185d10c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-szhlf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-szhlf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.73,PodIP:,StartTime:2023-03-20 07:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.354: INFO: Pod "webserver-deployment-57ccb67bb8-bzkz9" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-bzkz9 webserver-deployment-57ccb67bb8- deployment-6600  d926c1b0-c0d4-4c6f-beef-42be83f3c4fe 812760 0 2023-03-20 07:51:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/podIP:192.168.71.180/32 cni.projectcalico.org/podIPs:192.168.71.180/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.180"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.180"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 fc1516ec-79f0-48a0-bc42-c86e185d10c7 0xc00869fac0 0xc00869fac1}] []  [{calico Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc1516ec-79f0-48a0-bc42-c86e185d10c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-97rqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-97rqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.73,PodIP:,StartTime:2023-03-20 07:51:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.354: INFO: Pod "webserver-deployment-57ccb67bb8-cm26c" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-cm26c webserver-deployment-57ccb67bb8- deployment-6600  dbf84649-96ee-41fa-a78e-3c95cf4793ad 812767 0 2023-03-20 07:51:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/podIP:192.168.12.7/32 cni.projectcalico.org/podIPs:192.168.12.7/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.7"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.7"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 fc1516ec-79f0-48a0-bc42-c86e185d10c7 0xc00869fcf0 0xc00869fcf1}] []  [{calico Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc1516ec-79f0-48a0-bc42-c86e185d10c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kwb44,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kwb44,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.72,PodIP:,StartTime:2023-03-20 07:51:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.354: INFO: Pod "webserver-deployment-57ccb67bb8-j5sgk" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-j5sgk webserver-deployment-57ccb67bb8- deployment-6600  d5b16d6e-a774-41c7-bf30-7d9be4e53a8c 813013 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/podIP:192.168.71.188/32 cni.projectcalico.org/podIPs:192.168.71.188/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.188"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.188"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 fc1516ec-79f0-48a0-bc42-c86e185d10c7 0xc00869ff00 0xc00869ff01}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc1516ec-79f0-48a0-bc42-c86e185d10c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-20 07:51:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:51:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mmc8p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mmc8p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.73,PodIP:,StartTime:2023-03-20 07:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.355: INFO: Pod "webserver-deployment-57ccb67bb8-jp9hd" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-jp9hd webserver-deployment-57ccb67bb8- deployment-6600  01380606-d29c-4372-88ac-b524cac0be74 812763 0 2023-03-20 07:51:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/podIP:192.168.90.127/32 cni.projectcalico.org/podIPs:192.168.90.127/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.127"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.127"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 fc1516ec-79f0-48a0-bc42-c86e185d10c7 0xc003174330 0xc003174331}] []  [{calico Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc1516ec-79f0-48a0-bc42-c86e185d10c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djxnm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djxnm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.71,PodIP:,StartTime:2023-03-20 07:51:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.355: INFO: Pod "webserver-deployment-57ccb67bb8-rbzh7" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-rbzh7 webserver-deployment-57ccb67bb8- deployment-6600  f997649e-3c94-4785-923d-6d255d839248 812914 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/podIP:192.168.90.114/32 cni.projectcalico.org/podIPs:192.168.90.114/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.114"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.114"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 fc1516ec-79f0-48a0-bc42-c86e185d10c7 0xc0031748f0 0xc0031748f1}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc1516ec-79f0-48a0-bc42-c86e185d10c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5bhl5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5bhl5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.71,PodIP:,StartTime:2023-03-20 07:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.355: INFO: Pod "webserver-deployment-57ccb67bb8-t9qmw" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-t9qmw webserver-deployment-57ccb67bb8- deployment-6600  f1774241-cf3e-48b0-92a5-0bd273bfe5c6 812985 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/podIP:192.168.71.186/32 cni.projectcalico.org/podIPs:192.168.71.186/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.186"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.71.186"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 fc1516ec-79f0-48a0-bc42-c86e185d10c7 0xc003174d00 0xc003174d01}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc1516ec-79f0-48a0-bc42-c86e185d10c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2gmgz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2gmgz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.73,PodIP:,StartTime:2023-03-20 07:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.355: INFO: Pod "webserver-deployment-57ccb67bb8-twrmw" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-twrmw webserver-deployment-57ccb67bb8- deployment-6600  b88598a7-b5f6-4003-9b2b-f9c7bd8db1c7 812755 0 2023-03-20 07:51:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/podIP:192.168.12.13/32 cni.projectcalico.org/podIPs:192.168.12.13/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.13"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.13"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 fc1516ec-79f0-48a0-bc42-c86e185d10c7 0xc0031753b0 0xc0031753b1}] []  [{calico Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc1516ec-79f0-48a0-bc42-c86e185d10c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-03-20 07:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gqlcz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gqlcz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.72,PodIP:,StartTime:2023-03-20 07:51:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.355: INFO: Pod "webserver-deployment-57ccb67bb8-wrndj" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-wrndj webserver-deployment-57ccb67bb8- deployment-6600  0bc065f6-eb41-496f-b2cd-bc0d16895e8f 812982 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/podIP:192.168.12.58/32 cni.projectcalico.org/podIPs:192.168.12.58/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.58"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.58"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 fc1516ec-79f0-48a0-bc42-c86e185d10c7 0xc0031755f0 0xc0031755f1}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc1516ec-79f0-48a0-bc42-c86e185d10c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49gmh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49gmh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.72,PodIP:,StartTime:2023-03-20 07:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 07:51:39.355: INFO: Pod "webserver-deployment-57ccb67bb8-x6kpz" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-x6kpz webserver-deployment-57ccb67bb8- deployment-6600  44fd29c5-8330-4065-b684-0dd0e6ae2b82 812971 0 2023-03-20 07:51:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/podIP:192.168.12.48/32 cni.projectcalico.org/podIPs:192.168.12.48/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.48"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.48"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 fc1516ec-79f0-48a0-bc42-c86e185d10c7 0xc003175830 0xc003175831}] []  [{kube-controller-manager Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc1516ec-79f0-48a0-bc42-c86e185d10c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-20 07:51:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-20 07:51:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l4wfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l4wfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 07:51:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.72,PodIP:,StartTime:2023-03-20 07:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar 20 07:51:39.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6600" for this suite.

• [SLOW TEST:8.172 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":356,"completed":118,"skipped":2177,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:51:39.364: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-lvlq
STEP: Creating a pod to test atomic-volume-subpath
Mar 20 07:51:39.401: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lvlq" in namespace "subpath-3766" to be "Succeeded or Failed"
Mar 20 07:51:39.404: INFO: Pod "pod-subpath-test-configmap-lvlq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.264824ms
Mar 20 07:51:41.416: INFO: Pod "pod-subpath-test-configmap-lvlq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015213051s
Mar 20 07:51:43.421: INFO: Pod "pod-subpath-test-configmap-lvlq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02028304s
Mar 20 07:51:45.438: INFO: Pod "pod-subpath-test-configmap-lvlq": Phase="Running", Reason="", readiness=true. Elapsed: 6.036645751s
Mar 20 07:51:47.451: INFO: Pod "pod-subpath-test-configmap-lvlq": Phase="Running", Reason="", readiness=true. Elapsed: 8.050175805s
Mar 20 07:51:49.463: INFO: Pod "pod-subpath-test-configmap-lvlq": Phase="Running", Reason="", readiness=true. Elapsed: 10.061863082s
Mar 20 07:51:51.472: INFO: Pod "pod-subpath-test-configmap-lvlq": Phase="Running", Reason="", readiness=true. Elapsed: 12.071276058s
Mar 20 07:51:53.481: INFO: Pod "pod-subpath-test-configmap-lvlq": Phase="Running", Reason="", readiness=true. Elapsed: 14.080054735s
Mar 20 07:51:55.495: INFO: Pod "pod-subpath-test-configmap-lvlq": Phase="Running", Reason="", readiness=true. Elapsed: 16.093654597s
Mar 20 07:51:57.500: INFO: Pod "pod-subpath-test-configmap-lvlq": Phase="Running", Reason="", readiness=true. Elapsed: 18.098840979s
Mar 20 07:51:59.508: INFO: Pod "pod-subpath-test-configmap-lvlq": Phase="Running", Reason="", readiness=true. Elapsed: 20.106837399s
Mar 20 07:52:01.519: INFO: Pod "pod-subpath-test-configmap-lvlq": Phase="Running", Reason="", readiness=true. Elapsed: 22.117903438s
Mar 20 07:52:03.531: INFO: Pod "pod-subpath-test-configmap-lvlq": Phase="Running", Reason="", readiness=false. Elapsed: 24.129928548s
Mar 20 07:52:05.542: INFO: Pod "pod-subpath-test-configmap-lvlq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.140735316s
STEP: Saw pod success
Mar 20 07:52:05.542: INFO: Pod "pod-subpath-test-configmap-lvlq" satisfied condition "Succeeded or Failed"
Mar 20 07:52:05.544: INFO: Trying to get logs from node env016ar130-worker02 pod pod-subpath-test-configmap-lvlq container test-container-subpath-configmap-lvlq: <nil>
STEP: delete the pod
Mar 20 07:52:05.569: INFO: Waiting for pod pod-subpath-test-configmap-lvlq to disappear
Mar 20 07:52:05.577: INFO: Pod pod-subpath-test-configmap-lvlq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lvlq
Mar 20 07:52:05.577: INFO: Deleting pod "pod-subpath-test-configmap-lvlq" in namespace "subpath-3766"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Mar 20 07:52:05.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3766" for this suite.

• [SLOW TEST:26.222 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","total":356,"completed":119,"skipped":2230,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:52:05.587: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Mar 20 07:52:11.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3618" for this suite.
STEP: Destroying namespace "nsdeletetest-3991" for this suite.
Mar 20 07:52:11.688: INFO: Namespace nsdeletetest-3991 was already deleted
STEP: Destroying namespace "nsdeletetest-4786" for this suite.

• [SLOW TEST:6.105 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":356,"completed":120,"skipped":2287,"failed":0}
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:52:11.692: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating secret secrets-7613/secret-test-0b041e0c-22f0-4cf0-ab9c-5869d236969b
STEP: Creating a pod to test consume secrets
Mar 20 07:52:11.730: INFO: Waiting up to 5m0s for pod "pod-configmaps-665ec231-739b-42d4-b574-9451b51a41f0" in namespace "secrets-7613" to be "Succeeded or Failed"
Mar 20 07:52:11.734: INFO: Pod "pod-configmaps-665ec231-739b-42d4-b574-9451b51a41f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.322972ms
Mar 20 07:52:13.746: INFO: Pod "pod-configmaps-665ec231-739b-42d4-b574-9451b51a41f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016107379s
Mar 20 07:52:15.759: INFO: Pod "pod-configmaps-665ec231-739b-42d4-b574-9451b51a41f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028597981s
STEP: Saw pod success
Mar 20 07:52:15.759: INFO: Pod "pod-configmaps-665ec231-739b-42d4-b574-9451b51a41f0" satisfied condition "Succeeded or Failed"
Mar 20 07:52:15.762: INFO: Trying to get logs from node env016ar130-worker01 pod pod-configmaps-665ec231-739b-42d4-b574-9451b51a41f0 container env-test: <nil>
STEP: delete the pod
Mar 20 07:52:15.794: INFO: Waiting for pod pod-configmaps-665ec231-739b-42d4-b574-9451b51a41f0 to disappear
Mar 20 07:52:15.797: INFO: Pod pod-configmaps-665ec231-739b-42d4-b574-9451b51a41f0 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Mar 20 07:52:15.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7613" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":121,"skipped":2287,"failed":0}

------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:52:15.806: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching services
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 07:52:15.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3710" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":356,"completed":122,"skipped":2287,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:52:15.839: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 07:52:16.194: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 07:52:19.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:52:19.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9549" for this suite.
STEP: Destroying namespace "webhook-9549-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":356,"completed":123,"skipped":2291,"failed":0}
SSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:52:19.422: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-25ed6126-eecd-41b6-b488-e56252bb59c0 in namespace container-probe-2603
Mar 20 07:52:21.463: INFO: Started pod liveness-25ed6126-eecd-41b6-b488-e56252bb59c0 in namespace container-probe-2603
STEP: checking the pod's current state and verifying that restartCount is present
Mar 20 07:52:21.465: INFO: Initial restart count of pod liveness-25ed6126-eecd-41b6-b488-e56252bb59c0 is 0
Mar 20 07:52:41.552: INFO: Restart count of pod container-probe-2603/liveness-25ed6126-eecd-41b6-b488-e56252bb59c0 is now 1 (20.087013618s elapsed)
Mar 20 07:53:01.657: INFO: Restart count of pod container-probe-2603/liveness-25ed6126-eecd-41b6-b488-e56252bb59c0 is now 2 (40.192000274s elapsed)
Mar 20 07:53:21.764: INFO: Restart count of pod container-probe-2603/liveness-25ed6126-eecd-41b6-b488-e56252bb59c0 is now 3 (1m0.298661537s elapsed)
Mar 20 07:53:41.862: INFO: Restart count of pod container-probe-2603/liveness-25ed6126-eecd-41b6-b488-e56252bb59c0 is now 4 (1m20.396549442s elapsed)
Mar 20 07:54:48.226: INFO: Restart count of pod container-probe-2603/liveness-25ed6126-eecd-41b6-b488-e56252bb59c0 is now 5 (2m26.76126757s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar 20 07:54:48.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2603" for this suite.

• [SLOW TEST:148.824 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":356,"completed":124,"skipped":2298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:54:48.247: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 20 07:54:48.300: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:48.300: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:48.300: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:48.302: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 07:54:48.303: INFO: Node env016ar130-worker01 is running 0 daemon pod, expected 1
Mar 20 07:54:49.311: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:49.311: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:49.311: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:49.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 07:54:49.314: INFO: Node env016ar130-worker01 is running 0 daemon pod, expected 1
Mar 20 07:54:50.315: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:50.315: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:50.315: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:50.318: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 20 07:54:50.318: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 20 07:54:50.340: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:50.340: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:50.340: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:50.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 20 07:54:50.344: INFO: Node env016ar130-worker02 is running 0 daemon pod, expected 1
Mar 20 07:54:51.354: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:51.354: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:51.354: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:51.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 20 07:54:51.358: INFO: Node env016ar130-worker02 is running 0 daemon pod, expected 1
Mar 20 07:54:52.357: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:52.357: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:52.357: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 07:54:52.361: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 20 07:54:52.361: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4368, will wait for the garbage collector to delete the pods
Mar 20 07:54:52.435: INFO: Deleting DaemonSet.extensions daemon-set took: 13.139167ms
Mar 20 07:54:52.536: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.037468ms
Mar 20 07:54:54.741: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 07:54:54.741: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 20 07:54:54.743: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"814320"},"items":null}

Mar 20 07:54:54.744: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"814320"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Mar 20 07:54:54.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4368" for this suite.

• [SLOW TEST:6.517 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":356,"completed":125,"skipped":2326,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:54:54.764: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:54:54.789: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Creating first CR 
Mar 20 07:54:57.362: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-20T07:54:57Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-20T07:54:57Z]] name:name1 resourceVersion:814339 uid:b1f4cbd8-1836-4444-8dc7-b7d079dfc3ce] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar 20 07:55:07.386: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-20T07:55:07Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-20T07:55:07Z]] name:name2 resourceVersion:814401 uid:9aea9f34-6e4a-420c-9a5f-e17cd578bcd8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar 20 07:55:17.412: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-20T07:54:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-20T07:55:17Z]] name:name1 resourceVersion:814419 uid:b1f4cbd8-1836-4444-8dc7-b7d079dfc3ce] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar 20 07:55:27.433: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-20T07:55:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-20T07:55:27Z]] name:name2 resourceVersion:814444 uid:9aea9f34-6e4a-420c-9a5f-e17cd578bcd8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar 20 07:55:37.459: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-20T07:54:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-20T07:55:17Z]] name:name1 resourceVersion:814469 uid:b1f4cbd8-1836-4444-8dc7-b7d079dfc3ce] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar 20 07:55:47.483: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-20T07:55:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-20T07:55:27Z]] name:name2 resourceVersion:814489 uid:9aea9f34-6e4a-420c-9a5f-e17cd578bcd8] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 07:55:58.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-205" for this suite.

• [SLOW TEST:63.260 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":356,"completed":126,"skipped":2347,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:55:58.024: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:55:58.044: INFO: Creating pod...
Mar 20 07:56:00.071: INFO: Creating service...
Mar 20 07:56:00.084: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-1194/pods/agnhost/proxy?method=DELETE
Mar 20 07:56:00.089: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 20 07:56:00.089: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-1194/pods/agnhost/proxy?method=OPTIONS
Mar 20 07:56:00.093: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 20 07:56:00.093: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-1194/pods/agnhost/proxy?method=PATCH
Mar 20 07:56:00.097: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 20 07:56:00.097: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-1194/pods/agnhost/proxy?method=POST
Mar 20 07:56:00.100: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 20 07:56:00.100: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-1194/pods/agnhost/proxy?method=PUT
Mar 20 07:56:00.103: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 20 07:56:00.103: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-1194/services/e2e-proxy-test-service/proxy?method=DELETE
Mar 20 07:56:00.107: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 20 07:56:00.107: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-1194/services/e2e-proxy-test-service/proxy?method=OPTIONS
Mar 20 07:56:00.112: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 20 07:56:00.112: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-1194/services/e2e-proxy-test-service/proxy?method=PATCH
Mar 20 07:56:00.115: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 20 07:56:00.115: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-1194/services/e2e-proxy-test-service/proxy?method=POST
Mar 20 07:56:00.120: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 20 07:56:00.120: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-1194/services/e2e-proxy-test-service/proxy?method=PUT
Mar 20 07:56:00.122: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 20 07:56:00.122: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-1194/pods/agnhost/proxy?method=GET
Mar 20 07:56:00.126: INFO: http.Client request:GET StatusCode:301
Mar 20 07:56:00.126: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-1194/services/e2e-proxy-test-service/proxy?method=GET
Mar 20 07:56:00.130: INFO: http.Client request:GET StatusCode:301
Mar 20 07:56:00.130: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-1194/pods/agnhost/proxy?method=HEAD
Mar 20 07:56:00.132: INFO: http.Client request:HEAD StatusCode:301
Mar 20 07:56:00.132: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-1194/services/e2e-proxy-test-service/proxy?method=HEAD
Mar 20 07:56:00.134: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Mar 20 07:56:00.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1194" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","total":356,"completed":127,"skipped":2379,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:56:00.145: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
STEP: mirroring a new custom Endpoint
Mar 20 07:56:00.196: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Mar 20 07:56:02.221: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Mar 20 07:56:04.242: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:188
Mar 20 07:56:06.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-2511" for this suite.

• [SLOW TEST:6.115 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":356,"completed":128,"skipped":2402,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:56:06.261: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-6d837e3a-f409-47ef-a193-ed16ecbd2e25
STEP: Creating a pod to test consume secrets
Mar 20 07:56:06.300: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cc130e63-f53c-4d3a-bc01-08aeeb7d0d11" in namespace "projected-7260" to be "Succeeded or Failed"
Mar 20 07:56:06.302: INFO: Pod "pod-projected-secrets-cc130e63-f53c-4d3a-bc01-08aeeb7d0d11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.644734ms
Mar 20 07:56:08.315: INFO: Pod "pod-projected-secrets-cc130e63-f53c-4d3a-bc01-08aeeb7d0d11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014900333s
Mar 20 07:56:10.322: INFO: Pod "pod-projected-secrets-cc130e63-f53c-4d3a-bc01-08aeeb7d0d11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02223101s
STEP: Saw pod success
Mar 20 07:56:10.322: INFO: Pod "pod-projected-secrets-cc130e63-f53c-4d3a-bc01-08aeeb7d0d11" satisfied condition "Succeeded or Failed"
Mar 20 07:56:10.326: INFO: Trying to get logs from node env016ar130-worker02 pod pod-projected-secrets-cc130e63-f53c-4d3a-bc01-08aeeb7d0d11 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 20 07:56:10.368: INFO: Waiting for pod pod-projected-secrets-cc130e63-f53c-4d3a-bc01-08aeeb7d0d11 to disappear
Mar 20 07:56:10.371: INFO: Pod pod-projected-secrets-cc130e63-f53c-4d3a-bc01-08aeeb7d0d11 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Mar 20 07:56:10.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7260" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":129,"skipped":2425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:56:10.381: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5952
Mar 20 07:56:10.418: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar 20 07:56:12.430: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar 20 07:56:12.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-5952 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 20 07:56:12.627: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 20 07:56:12.627: INFO: stdout: "iptables"
Mar 20 07:56:12.627: INFO: proxyMode: iptables
Mar 20 07:56:12.639: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 20 07:56:12.644: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-5952
STEP: creating replication controller affinity-clusterip-timeout in namespace services-5952
I0320 07:56:12.662844      23 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5952, replica count: 3
I0320 07:56:15.714733      23 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 20 07:56:15.733: INFO: Creating new exec pod
Mar 20 07:56:18.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-5952 exec execpod-affinitybkgj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Mar 20 07:56:18.911: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar 20 07:56:18.911: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 07:56:18.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-5952 exec execpod-affinitybkgj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.236.253 80'
Mar 20 07:56:19.048: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.236.253 80\nConnection to 192.168.236.253 80 port [tcp/http] succeeded!\n"
Mar 20 07:56:19.048: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 07:56:19.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-5952 exec execpod-affinitybkgj2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.236.253:80/ ; done'
Mar 20 07:56:19.287: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n"
Mar 20 07:56:19.287: INFO: stdout: "\naffinity-clusterip-timeout-pxms9\naffinity-clusterip-timeout-pxms9\naffinity-clusterip-timeout-pxms9\naffinity-clusterip-timeout-pxms9\naffinity-clusterip-timeout-pxms9\naffinity-clusterip-timeout-pxms9\naffinity-clusterip-timeout-pxms9\naffinity-clusterip-timeout-pxms9\naffinity-clusterip-timeout-pxms9\naffinity-clusterip-timeout-pxms9\naffinity-clusterip-timeout-pxms9\naffinity-clusterip-timeout-pxms9\naffinity-clusterip-timeout-pxms9\naffinity-clusterip-timeout-pxms9\naffinity-clusterip-timeout-pxms9\naffinity-clusterip-timeout-pxms9"
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Received response from host: affinity-clusterip-timeout-pxms9
Mar 20 07:56:19.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-5952 exec execpod-affinitybkgj2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.236.253:80/'
Mar 20 07:56:19.437: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n"
Mar 20 07:56:19.437: INFO: stdout: "affinity-clusterip-timeout-pxms9"
Mar 20 07:56:39.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-5952 exec execpod-affinitybkgj2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.236.253:80/'
Mar 20 07:56:39.628: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.236.253:80/\n"
Mar 20 07:56:39.628: INFO: stdout: "affinity-clusterip-timeout-qtnxh"
Mar 20 07:56:39.628: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5952, will wait for the garbage collector to delete the pods
Mar 20 07:56:39.717: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.185039ms
Mar 20 07:56:39.817: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.59538ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 07:56:42.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5952" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:31.964 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":130,"skipped":2461,"failed":0}
SSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:56:42.345: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Mar 20 07:56:42.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7889" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":131,"skipped":2469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:56:42.396: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 07:56:42.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9160" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":356,"completed":132,"skipped":2523,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:56:42.426: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 07:56:42.457: INFO: Waiting up to 5m0s for pod "busybox-user-65534-427fd050-ba0e-4083-ada0-d99caa20db96" in namespace "security-context-test-3708" to be "Succeeded or Failed"
Mar 20 07:56:42.459: INFO: Pod "busybox-user-65534-427fd050-ba0e-4083-ada0-d99caa20db96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.494518ms
Mar 20 07:56:44.466: INFO: Pod "busybox-user-65534-427fd050-ba0e-4083-ada0-d99caa20db96": Phase="Running", Reason="", readiness=true. Elapsed: 2.009695399s
Mar 20 07:56:46.472: INFO: Pod "busybox-user-65534-427fd050-ba0e-4083-ada0-d99caa20db96": Phase="Running", Reason="", readiness=false. Elapsed: 4.015463202s
Mar 20 07:56:48.479: INFO: Pod "busybox-user-65534-427fd050-ba0e-4083-ada0-d99caa20db96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02236744s
Mar 20 07:56:48.479: INFO: Pod "busybox-user-65534-427fd050-ba0e-4083-ada0-d99caa20db96" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Mar 20 07:56:48.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3708" for this suite.

• [SLOW TEST:6.063 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:52
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":133,"skipped":2532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:56:48.490: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7568
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Mar 20 07:56:48.532: INFO: Found 0 stateful pods, waiting for 3
Mar 20 07:56:58.544: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 20 07:56:58.544: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 20 07:56:58.544: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 20 07:56:58.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-7568 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 20 07:56:58.718: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 20 07:56:58.718: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 20 07:56:58.718: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Mar 20 07:57:08.757: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 20 07:57:18.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-7568 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 20 07:57:18.918: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 20 07:57:18.918: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 20 07:57:18.918: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Mar 20 07:57:28.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-7568 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 20 07:57:29.160: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 20 07:57:29.160: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 20 07:57:29.160: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 20 07:57:39.206: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 20 07:57:49.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-7568 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 20 07:57:49.405: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 20 07:57:49.405: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 20 07:57:49.405: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 20 07:57:59.434: INFO: Deleting all statefulset in ns statefulset-7568
Mar 20 07:57:59.437: INFO: Scaling statefulset ss2 to 0
Mar 20 07:58:09.474: INFO: Waiting for statefulset status.replicas updated to 0
Mar 20 07:58:09.477: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar 20 07:58:09.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7568" for this suite.

• [SLOW TEST:81.012 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":356,"completed":134,"skipped":2555,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:58:09.502: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 20 07:58:09.538: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 20 07:59:09.652: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Mar 20 07:59:09.734: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 20 07:59:09.746: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 20 07:59:09.771: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 20 07:59:09.780: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 20 07:59:09.800: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 20 07:59:09.808: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Mar 20 07:59:25.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-647" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:76.449 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":356,"completed":135,"skipped":2561,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:59:25.951: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 07:59:25.980: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8870a435-ce29-4397-8b70-4256bfd8810a" in namespace "projected-4912" to be "Succeeded or Failed"
Mar 20 07:59:25.982: INFO: Pod "downwardapi-volume-8870a435-ce29-4397-8b70-4256bfd8810a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.854542ms
Mar 20 07:59:27.992: INFO: Pod "downwardapi-volume-8870a435-ce29-4397-8b70-4256bfd8810a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011782638s
Mar 20 07:59:30.006: INFO: Pod "downwardapi-volume-8870a435-ce29-4397-8b70-4256bfd8810a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025813484s
STEP: Saw pod success
Mar 20 07:59:30.006: INFO: Pod "downwardapi-volume-8870a435-ce29-4397-8b70-4256bfd8810a" satisfied condition "Succeeded or Failed"
Mar 20 07:59:30.010: INFO: Trying to get logs from node env016ar130-worker02 pod downwardapi-volume-8870a435-ce29-4397-8b70-4256bfd8810a container client-container: <nil>
STEP: delete the pod
Mar 20 07:59:30.042: INFO: Waiting for pod downwardapi-volume-8870a435-ce29-4397-8b70-4256bfd8810a to disappear
Mar 20 07:59:30.044: INFO: Pod downwardapi-volume-8870a435-ce29-4397-8b70-4256bfd8810a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar 20 07:59:30.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4912" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":136,"skipped":2602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 07:59:30.054: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 20 07:59:30.090: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 20 08:00:30.212: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:00:30.215: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Mar 20 08:00:32.331: INFO: found a healthy node: env016ar130-worker02
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:00:50.444: INFO: pods created so far: [1 1 1]
Mar 20 08:00:50.444: INFO: length of pods created so far: 3
Mar 20 08:00:52.460: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:188
Mar 20 08:00:59.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6580" for this suite.
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Mar 20 08:00:59.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8124" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:89.524 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":356,"completed":137,"skipped":2633,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:00:59.578: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 08:00:59.985: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 08:01:03.010: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 08:01:13.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3368" for this suite.
STEP: Destroying namespace "webhook-3368-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:13.648 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":356,"completed":138,"skipped":2662,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:01:13.226: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:01:13.261: INFO: Creating simple deployment test-new-deployment
Mar 20 08:01:13.274: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 20 08:01:15.320: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-4828  d1e8c047-ae63-4671-8c2f-ee218b99e3ba 816630 3 2023-03-20 08:01:13 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-20 08:01:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 08:01:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f08be8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-20 08:01:14 +0000 UTC,LastTransitionTime:2023-03-20 08:01:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-55df494869" has successfully progressed.,LastUpdateTime:2023-03-20 08:01:14 +0000 UTC,LastTransitionTime:2023-03-20 08:01:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 20 08:01:15.326: INFO: New ReplicaSet "test-new-deployment-55df494869" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-55df494869  deployment-4828  3b5989dd-c524-4708-b706-1d29d558af08 816633 2 2023-03-20 08:01:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment d1e8c047-ae63-4671-8c2f-ee218b99e3ba 0xc005f09027 0xc005f09028}] []  [{kube-controller-manager Update apps/v1 2023-03-20 08:01:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d1e8c047-ae63-4671-8c2f-ee218b99e3ba\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 08:01:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f090b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 20 08:01:15.331: INFO: Pod "test-new-deployment-55df494869-jp7pg" is available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-jp7pg test-new-deployment-55df494869- deployment-4828  9908d614-1b87-46df-9c13-b13450e1a83a 816622 0 2023-03-20 08:01:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/podIP:192.168.90.74/32 cni.projectcalico.org/podIPs:192.168.90.74/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.74"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.74"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-new-deployment-55df494869 3b5989dd-c524-4708-b706-1d29d558af08 0xc0089baaa7 0xc0089baaa8}] []  [{calico Update v1 2023-03-20 08:01:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 08:01:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b5989dd-c524-4708-b706-1d29d558af08\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-03-20 08:01:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-20 08:01:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.90.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hmdwk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hmdwk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:01:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:01:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:01:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:01:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.71,PodIP:192.168.90.74,StartTime:2023-03-20 08:01:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-20 08:01:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://fafac90c722e07d0f3a54ff477e233e67c12b6d5e46fd6272c048ee8f429b20e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.90.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 20 08:01:15.331: INFO: Pod "test-new-deployment-55df494869-v4h74" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-v4h74 test-new-deployment-55df494869- deployment-4828  0c925f2d-20cc-4434-981d-f3e8b7c6ebf6 816634 0 2023-03-20 08:01:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet test-new-deployment-55df494869 3b5989dd-c524-4708-b706-1d29d558af08 0xc0089bae30 0xc0089bae31}] []  [{kube-controller-manager Update v1 2023-03-20 08:01:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b5989dd-c524-4708-b706-1d29d558af08\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbvgv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbvgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:01:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar 20 08:01:15.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4828" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":356,"completed":139,"skipped":2672,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:01:15.340: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 08:01:15.368: INFO: Waiting up to 5m0s for pod "downwardapi-volume-09a2d1e8-4747-4f39-b76c-7f989523330c" in namespace "downward-api-9780" to be "Succeeded or Failed"
Mar 20 08:01:15.370: INFO: Pod "downwardapi-volume-09a2d1e8-4747-4f39-b76c-7f989523330c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.372753ms
Mar 20 08:01:17.380: INFO: Pod "downwardapi-volume-09a2d1e8-4747-4f39-b76c-7f989523330c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012314538s
Mar 20 08:01:19.393: INFO: Pod "downwardapi-volume-09a2d1e8-4747-4f39-b76c-7f989523330c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025347688s
STEP: Saw pod success
Mar 20 08:01:19.393: INFO: Pod "downwardapi-volume-09a2d1e8-4747-4f39-b76c-7f989523330c" satisfied condition "Succeeded or Failed"
Mar 20 08:01:19.396: INFO: Trying to get logs from node env016ar130-worker02 pod downwardapi-volume-09a2d1e8-4747-4f39-b76c-7f989523330c container client-container: <nil>
STEP: delete the pod
Mar 20 08:01:19.431: INFO: Waiting for pod downwardapi-volume-09a2d1e8-4747-4f39-b76c-7f989523330c to disappear
Mar 20 08:01:19.433: INFO: Pod downwardapi-volume-09a2d1e8-4747-4f39-b76c-7f989523330c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar 20 08:01:19.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9780" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":140,"skipped":2690,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:01:19.444: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar 20 08:01:30.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5460" for this suite.

• [SLOW TEST:11.104 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":356,"completed":141,"skipped":2693,"failed":0}
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:01:30.548: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar 20 08:01:30.589: INFO: The status of Pod pod-update-activedeadlineseconds-eb461dc0-1729-4d59-9f37-c7fe7b2ecd31 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:01:32.595: INFO: The status of Pod pod-update-activedeadlineseconds-eb461dc0-1729-4d59-9f37-c7fe7b2ecd31 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 20 08:01:33.121: INFO: Successfully updated pod "pod-update-activedeadlineseconds-eb461dc0-1729-4d59-9f37-c7fe7b2ecd31"
Mar 20 08:01:33.121: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-eb461dc0-1729-4d59-9f37-c7fe7b2ecd31" in namespace "pods-1161" to be "terminated due to deadline exceeded"
Mar 20 08:01:33.123: INFO: Pod "pod-update-activedeadlineseconds-eb461dc0-1729-4d59-9f37-c7fe7b2ecd31": Phase="Running", Reason="", readiness=true. Elapsed: 2.71472ms
Mar 20 08:01:35.136: INFO: Pod "pod-update-activedeadlineseconds-eb461dc0-1729-4d59-9f37-c7fe7b2ecd31": Phase="Running", Reason="", readiness=true. Elapsed: 2.01514873s
Mar 20 08:01:37.143: INFO: Pod "pod-update-activedeadlineseconds-eb461dc0-1729-4d59-9f37-c7fe7b2ecd31": Phase="Running", Reason="", readiness=false. Elapsed: 4.022795694s
Mar 20 08:01:39.161: INFO: Pod "pod-update-activedeadlineseconds-eb461dc0-1729-4d59-9f37-c7fe7b2ecd31": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.040467129s
Mar 20 08:01:39.161: INFO: Pod "pod-update-activedeadlineseconds-eb461dc0-1729-4d59-9f37-c7fe7b2ecd31" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar 20 08:01:39.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1161" for this suite.

• [SLOW TEST:8.628 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":356,"completed":142,"skipped":2693,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:01:39.176: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Mar 20 08:01:39.223: INFO: Waiting up to 5m0s for pod "downward-api-6719f0fa-d169-41d6-98f1-cde9f6fa780f" in namespace "downward-api-1028" to be "Succeeded or Failed"
Mar 20 08:01:39.225: INFO: Pod "downward-api-6719f0fa-d169-41d6-98f1-cde9f6fa780f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.718977ms
Mar 20 08:01:41.234: INFO: Pod "downward-api-6719f0fa-d169-41d6-98f1-cde9f6fa780f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011160981s
Mar 20 08:01:43.248: INFO: Pod "downward-api-6719f0fa-d169-41d6-98f1-cde9f6fa780f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025036707s
STEP: Saw pod success
Mar 20 08:01:43.248: INFO: Pod "downward-api-6719f0fa-d169-41d6-98f1-cde9f6fa780f" satisfied condition "Succeeded or Failed"
Mar 20 08:01:43.251: INFO: Trying to get logs from node env016ar130-worker02 pod downward-api-6719f0fa-d169-41d6-98f1-cde9f6fa780f container dapi-container: <nil>
STEP: delete the pod
Mar 20 08:01:43.266: INFO: Waiting for pod downward-api-6719f0fa-d169-41d6-98f1-cde9f6fa780f to disappear
Mar 20 08:01:43.268: INFO: Pod downward-api-6719f0fa-d169-41d6-98f1-cde9f6fa780f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Mar 20 08:01:43.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1028" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":356,"completed":143,"skipped":2726,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:01:43.276: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar 20 08:01:45.845: INFO: Successfully updated pod "adopt-release-kdw6j"
STEP: Checking that the Job readopts the Pod
Mar 20 08:01:45.845: INFO: Waiting up to 15m0s for pod "adopt-release-kdw6j" in namespace "job-2233" to be "adopted"
Mar 20 08:01:45.849: INFO: Pod "adopt-release-kdw6j": Phase="Running", Reason="", readiness=true. Elapsed: 3.528482ms
Mar 20 08:01:47.860: INFO: Pod "adopt-release-kdw6j": Phase="Running", Reason="", readiness=true. Elapsed: 2.014670779s
Mar 20 08:01:47.860: INFO: Pod "adopt-release-kdw6j" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar 20 08:01:48.378: INFO: Successfully updated pod "adopt-release-kdw6j"
STEP: Checking that the Job releases the Pod
Mar 20 08:01:48.378: INFO: Waiting up to 15m0s for pod "adopt-release-kdw6j" in namespace "job-2233" to be "released"
Mar 20 08:01:48.380: INFO: Pod "adopt-release-kdw6j": Phase="Running", Reason="", readiness=true. Elapsed: 2.372239ms
Mar 20 08:01:50.395: INFO: Pod "adopt-release-kdw6j": Phase="Running", Reason="", readiness=true. Elapsed: 2.016473963s
Mar 20 08:01:50.395: INFO: Pod "adopt-release-kdw6j" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Mar 20 08:01:50.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2233" for this suite.

• [SLOW TEST:7.131 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":356,"completed":144,"skipped":2733,"failed":0}
S
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:01:50.407: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:01:50.910: INFO: Checking APIGroup: apiregistration.k8s.io
Mar 20 08:01:50.912: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar 20 08:01:50.912: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar 20 08:01:50.912: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar 20 08:01:50.912: INFO: Checking APIGroup: apps
Mar 20 08:01:50.913: INFO: PreferredVersion.GroupVersion: apps/v1
Mar 20 08:01:50.913: INFO: Versions found [{apps/v1 v1}]
Mar 20 08:01:50.913: INFO: apps/v1 matches apps/v1
Mar 20 08:01:50.913: INFO: Checking APIGroup: events.k8s.io
Mar 20 08:01:50.915: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar 20 08:01:50.915: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Mar 20 08:01:50.915: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar 20 08:01:50.915: INFO: Checking APIGroup: authentication.k8s.io
Mar 20 08:01:50.916: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar 20 08:01:50.916: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar 20 08:01:50.916: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar 20 08:01:50.916: INFO: Checking APIGroup: authorization.k8s.io
Mar 20 08:01:50.917: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar 20 08:01:50.917: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar 20 08:01:50.917: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar 20 08:01:50.917: INFO: Checking APIGroup: autoscaling
Mar 20 08:01:50.918: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Mar 20 08:01:50.918: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Mar 20 08:01:50.918: INFO: autoscaling/v2 matches autoscaling/v2
Mar 20 08:01:50.918: INFO: Checking APIGroup: batch
Mar 20 08:01:50.919: INFO: PreferredVersion.GroupVersion: batch/v1
Mar 20 08:01:50.919: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Mar 20 08:01:50.919: INFO: batch/v1 matches batch/v1
Mar 20 08:01:50.919: INFO: Checking APIGroup: certificates.k8s.io
Mar 20 08:01:50.920: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar 20 08:01:50.920: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar 20 08:01:50.920: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar 20 08:01:50.920: INFO: Checking APIGroup: networking.k8s.io
Mar 20 08:01:50.922: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar 20 08:01:50.922: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar 20 08:01:50.922: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar 20 08:01:50.922: INFO: Checking APIGroup: policy
Mar 20 08:01:50.923: INFO: PreferredVersion.GroupVersion: policy/v1
Mar 20 08:01:50.923: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Mar 20 08:01:50.923: INFO: policy/v1 matches policy/v1
Mar 20 08:01:50.923: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar 20 08:01:50.923: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar 20 08:01:50.923: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar 20 08:01:50.923: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar 20 08:01:50.923: INFO: Checking APIGroup: storage.k8s.io
Mar 20 08:01:50.924: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar 20 08:01:50.924: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar 20 08:01:50.924: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar 20 08:01:50.924: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar 20 08:01:50.925: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar 20 08:01:50.925: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar 20 08:01:50.925: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar 20 08:01:50.925: INFO: Checking APIGroup: apiextensions.k8s.io
Mar 20 08:01:50.926: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar 20 08:01:50.926: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar 20 08:01:50.926: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar 20 08:01:50.926: INFO: Checking APIGroup: scheduling.k8s.io
Mar 20 08:01:50.927: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar 20 08:01:50.927: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar 20 08:01:50.927: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar 20 08:01:50.927: INFO: Checking APIGroup: coordination.k8s.io
Mar 20 08:01:50.929: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar 20 08:01:50.929: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar 20 08:01:50.929: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar 20 08:01:50.929: INFO: Checking APIGroup: node.k8s.io
Mar 20 08:01:50.930: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar 20 08:01:50.930: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Mar 20 08:01:50.930: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar 20 08:01:50.930: INFO: Checking APIGroup: discovery.k8s.io
Mar 20 08:01:50.931: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar 20 08:01:50.931: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Mar 20 08:01:50.931: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar 20 08:01:50.931: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar 20 08:01:50.932: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Mar 20 08:01:50.932: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar 20 08:01:50.932: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Mar 20 08:01:50.932: INFO: Checking APIGroup: ceph.rook.io
Mar 20 08:01:50.933: INFO: PreferredVersion.GroupVersion: ceph.rook.io/v1
Mar 20 08:01:50.933: INFO: Versions found [{ceph.rook.io/v1 v1}]
Mar 20 08:01:50.933: INFO: ceph.rook.io/v1 matches ceph.rook.io/v1
Mar 20 08:01:50.933: INFO: Checking APIGroup: crd.projectcalico.org
Mar 20 08:01:50.934: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar 20 08:01:50.934: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar 20 08:01:50.934: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Mar 20 08:01:50.934: INFO: Checking APIGroup: k8s.cni.cncf.io
Mar 20 08:01:50.934: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Mar 20 08:01:50.934: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Mar 20 08:01:50.935: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Mar 20 08:01:50.935: INFO: Checking APIGroup: monitoring.coreos.com
Mar 20 08:01:50.936: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Mar 20 08:01:50.936: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Mar 20 08:01:50.936: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Mar 20 08:01:50.936: INFO: Checking APIGroup: mutations.gatekeeper.sh
Mar 20 08:01:50.937: INFO: PreferredVersion.GroupVersion: mutations.gatekeeper.sh/v1
Mar 20 08:01:50.937: INFO: Versions found [{mutations.gatekeeper.sh/v1 v1} {mutations.gatekeeper.sh/v1beta1 v1beta1} {mutations.gatekeeper.sh/v1alpha1 v1alpha1}]
Mar 20 08:01:50.937: INFO: mutations.gatekeeper.sh/v1 matches mutations.gatekeeper.sh/v1
Mar 20 08:01:50.937: INFO: Checking APIGroup: templates.gatekeeper.sh
Mar 20 08:01:50.938: INFO: PreferredVersion.GroupVersion: templates.gatekeeper.sh/v1
Mar 20 08:01:50.938: INFO: Versions found [{templates.gatekeeper.sh/v1 v1} {templates.gatekeeper.sh/v1beta1 v1beta1} {templates.gatekeeper.sh/v1alpha1 v1alpha1}]
Mar 20 08:01:50.938: INFO: templates.gatekeeper.sh/v1 matches templates.gatekeeper.sh/v1
Mar 20 08:01:50.938: INFO: Checking APIGroup: config.gatekeeper.sh
Mar 20 08:01:50.939: INFO: PreferredVersion.GroupVersion: config.gatekeeper.sh/v1alpha1
Mar 20 08:01:50.939: INFO: Versions found [{config.gatekeeper.sh/v1alpha1 v1alpha1}]
Mar 20 08:01:50.939: INFO: config.gatekeeper.sh/v1alpha1 matches config.gatekeeper.sh/v1alpha1
Mar 20 08:01:50.939: INFO: Checking APIGroup: expansion.gatekeeper.sh
Mar 20 08:01:50.941: INFO: PreferredVersion.GroupVersion: expansion.gatekeeper.sh/v1alpha1
Mar 20 08:01:50.941: INFO: Versions found [{expansion.gatekeeper.sh/v1alpha1 v1alpha1}]
Mar 20 08:01:50.941: INFO: expansion.gatekeeper.sh/v1alpha1 matches expansion.gatekeeper.sh/v1alpha1
Mar 20 08:01:50.941: INFO: Checking APIGroup: externaldata.gatekeeper.sh
Mar 20 08:01:50.942: INFO: PreferredVersion.GroupVersion: externaldata.gatekeeper.sh/v1beta1
Mar 20 08:01:50.942: INFO: Versions found [{externaldata.gatekeeper.sh/v1beta1 v1beta1} {externaldata.gatekeeper.sh/v1alpha1 v1alpha1}]
Mar 20 08:01:50.942: INFO: externaldata.gatekeeper.sh/v1beta1 matches externaldata.gatekeeper.sh/v1beta1
Mar 20 08:01:50.942: INFO: Checking APIGroup: objectbucket.io
Mar 20 08:01:50.943: INFO: PreferredVersion.GroupVersion: objectbucket.io/v1alpha1
Mar 20 08:01:50.943: INFO: Versions found [{objectbucket.io/v1alpha1 v1alpha1}]
Mar 20 08:01:50.943: INFO: objectbucket.io/v1alpha1 matches objectbucket.io/v1alpha1
Mar 20 08:01:50.943: INFO: Checking APIGroup: status.gatekeeper.sh
Mar 20 08:01:50.944: INFO: PreferredVersion.GroupVersion: status.gatekeeper.sh/v1beta1
Mar 20 08:01:50.944: INFO: Versions found [{status.gatekeeper.sh/v1beta1 v1beta1}]
Mar 20 08:01:50.944: INFO: status.gatekeeper.sh/v1beta1 matches status.gatekeeper.sh/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:188
Mar 20 08:01:50.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-5405" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":356,"completed":145,"skipped":2734,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:01:50.958: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 08:01:51.419: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 08:01:54.449: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 08:01:54.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3327" for this suite.
STEP: Destroying namespace "webhook-3327-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":356,"completed":146,"skipped":2734,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:01:54.617: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-f1b332cc-21b9-4fde-907e-6afe9e64ba7b
STEP: Creating a pod to test consume configMaps
Mar 20 08:01:54.650: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0d15d425-acb4-44a8-92ec-f2083d51b169" in namespace "projected-3879" to be "Succeeded or Failed"
Mar 20 08:01:54.653: INFO: Pod "pod-projected-configmaps-0d15d425-acb4-44a8-92ec-f2083d51b169": Phase="Pending", Reason="", readiness=false. Elapsed: 2.309449ms
Mar 20 08:01:56.667: INFO: Pod "pod-projected-configmaps-0d15d425-acb4-44a8-92ec-f2083d51b169": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016445119s
Mar 20 08:01:58.673: INFO: Pod "pod-projected-configmaps-0d15d425-acb4-44a8-92ec-f2083d51b169": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022885243s
STEP: Saw pod success
Mar 20 08:01:58.673: INFO: Pod "pod-projected-configmaps-0d15d425-acb4-44a8-92ec-f2083d51b169" satisfied condition "Succeeded or Failed"
Mar 20 08:01:58.677: INFO: Trying to get logs from node env016ar130-worker02 pod pod-projected-configmaps-0d15d425-acb4-44a8-92ec-f2083d51b169 container agnhost-container: <nil>
STEP: delete the pod
Mar 20 08:01:58.697: INFO: Waiting for pod pod-projected-configmaps-0d15d425-acb4-44a8-92ec-f2083d51b169 to disappear
Mar 20 08:01:58.702: INFO: Pod pod-projected-configmaps-0d15d425-acb4-44a8-92ec-f2083d51b169 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar 20 08:01:58.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3879" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":147,"skipped":2742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:01:58.710: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 08:01:58.748: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6c22e7b5-93d4-4c65-9d22-3d21a40d2239" in namespace "projected-8560" to be "Succeeded or Failed"
Mar 20 08:01:58.751: INFO: Pod "downwardapi-volume-6c22e7b5-93d4-4c65-9d22-3d21a40d2239": Phase="Pending", Reason="", readiness=false. Elapsed: 3.558326ms
Mar 20 08:02:00.757: INFO: Pod "downwardapi-volume-6c22e7b5-93d4-4c65-9d22-3d21a40d2239": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008837711s
Mar 20 08:02:02.770: INFO: Pod "downwardapi-volume-6c22e7b5-93d4-4c65-9d22-3d21a40d2239": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022642985s
STEP: Saw pod success
Mar 20 08:02:02.770: INFO: Pod "downwardapi-volume-6c22e7b5-93d4-4c65-9d22-3d21a40d2239" satisfied condition "Succeeded or Failed"
Mar 20 08:02:02.777: INFO: Trying to get logs from node env016ar130-worker02 pod downwardapi-volume-6c22e7b5-93d4-4c65-9d22-3d21a40d2239 container client-container: <nil>
STEP: delete the pod
Mar 20 08:02:02.792: INFO: Waiting for pod downwardapi-volume-6c22e7b5-93d4-4c65-9d22-3d21a40d2239 to disappear
Mar 20 08:02:02.794: INFO: Pod downwardapi-volume-6c22e7b5-93d4-4c65-9d22-3d21a40d2239 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar 20 08:02:02.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8560" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":148,"skipped":2791,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:02:02.805: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar 20 08:02:18.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1838" for this suite.

• [SLOW TEST:16.199 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":356,"completed":149,"skipped":2806,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:02:19.004: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Mar 20 08:02:21.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3697" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":356,"completed":150,"skipped":2811,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:02:21.914: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap that has name configmap-test-emptyKey-12757369-58a1-44d9-b4e4-a7e07af62ae7
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Mar 20 08:02:21.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3117" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":356,"completed":151,"skipped":2812,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:02:21.956: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:02:21.991: INFO: The status of Pod server-envvars-da7d9ee6-8daf-4958-9c6a-605d488c103d is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:02:24.004: INFO: The status of Pod server-envvars-da7d9ee6-8daf-4958-9c6a-605d488c103d is Running (Ready = true)
Mar 20 08:02:24.037: INFO: Waiting up to 5m0s for pod "client-envvars-a794977e-8a09-4909-a1e0-291b47a9f8b0" in namespace "pods-8429" to be "Succeeded or Failed"
Mar 20 08:02:24.040: INFO: Pod "client-envvars-a794977e-8a09-4909-a1e0-291b47a9f8b0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.002255ms
Mar 20 08:02:26.046: INFO: Pod "client-envvars-a794977e-8a09-4909-a1e0-291b47a9f8b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008487951s
Mar 20 08:02:28.055: INFO: Pod "client-envvars-a794977e-8a09-4909-a1e0-291b47a9f8b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01717878s
STEP: Saw pod success
Mar 20 08:02:28.055: INFO: Pod "client-envvars-a794977e-8a09-4909-a1e0-291b47a9f8b0" satisfied condition "Succeeded or Failed"
Mar 20 08:02:28.058: INFO: Trying to get logs from node env016ar130-worker02 pod client-envvars-a794977e-8a09-4909-a1e0-291b47a9f8b0 container env3cont: <nil>
STEP: delete the pod
Mar 20 08:02:28.075: INFO: Waiting for pod client-envvars-a794977e-8a09-4909-a1e0-291b47a9f8b0 to disappear
Mar 20 08:02:28.079: INFO: Pod client-envvars-a794977e-8a09-4909-a1e0-291b47a9f8b0 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar 20 08:02:28.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8429" for this suite.

• [SLOW TEST:6.131 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":356,"completed":152,"skipped":2821,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:02:28.087: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2484
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2484
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2484
Mar 20 08:02:28.125: INFO: Found 0 stateful pods, waiting for 1
Mar 20 08:02:38.133: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 20 08:02:38.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-2484 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 20 08:02:38.305: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 20 08:02:38.305: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 20 08:02:38.305: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 20 08:02:38.309: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 20 08:02:48.321: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 20 08:02:48.321: INFO: Waiting for statefulset status.replicas updated to 0
Mar 20 08:02:48.341: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999556s
Mar 20 08:02:49.349: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994910059s
Mar 20 08:02:50.359: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988340918s
Mar 20 08:02:51.367: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.977969901s
Mar 20 08:02:52.380: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.970781116s
Mar 20 08:02:53.386: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.957433264s
Mar 20 08:02:54.397: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.951160468s
Mar 20 08:02:55.403: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.940059214s
Mar 20 08:02:56.409: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.934436875s
Mar 20 08:02:57.417: INFO: Verifying statefulset ss doesn't scale past 1 for another 928.187087ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2484
Mar 20 08:02:58.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-2484 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 20 08:02:58.586: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 20 08:02:58.586: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 20 08:02:58.586: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 20 08:02:58.592: INFO: Found 1 stateful pods, waiting for 3
Mar 20 08:03:08.611: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 20 08:03:08.611: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 20 08:03:08.611: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 20 08:03:08.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-2484 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 20 08:03:08.764: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 20 08:03:08.764: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 20 08:03:08.764: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 20 08:03:08.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-2484 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 20 08:03:08.922: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 20 08:03:08.922: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 20 08:03:08.922: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 20 08:03:08.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-2484 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 20 08:03:09.085: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 20 08:03:09.085: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 20 08:03:09.085: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 20 08:03:09.085: INFO: Waiting for statefulset status.replicas updated to 0
Mar 20 08:03:09.090: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 20 08:03:19.109: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 20 08:03:19.109: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 20 08:03:19.109: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 20 08:03:19.128: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998978s
Mar 20 08:03:20.135: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992765834s
Mar 20 08:03:21.143: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985330619s
Mar 20 08:03:22.153: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97709184s
Mar 20 08:03:23.163: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.966822496s
Mar 20 08:03:24.174: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.95723538s
Mar 20 08:03:25.180: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.946340909s
Mar 20 08:03:26.188: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.940226634s
Mar 20 08:03:27.195: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.932791496s
Mar 20 08:03:28.206: INFO: Verifying statefulset ss doesn't scale past 3 for another 925.322583ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2484
Mar 20 08:03:29.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-2484 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 20 08:03:29.380: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 20 08:03:29.380: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 20 08:03:29.380: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 20 08:03:29.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-2484 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 20 08:03:29.540: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 20 08:03:29.540: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 20 08:03:29.540: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 20 08:03:29.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-2484 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 20 08:03:29.706: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 20 08:03:29.706: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 20 08:03:29.706: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 20 08:03:29.706: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 20 08:03:39.734: INFO: Deleting all statefulset in ns statefulset-2484
Mar 20 08:03:39.738: INFO: Scaling statefulset ss to 0
Mar 20 08:03:39.756: INFO: Waiting for statefulset status.replicas updated to 0
Mar 20 08:03:39.758: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar 20 08:03:39.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2484" for this suite.

• [SLOW TEST:71.699 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":356,"completed":153,"skipped":2830,"failed":0}
SSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:03:39.787: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:188
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 20 08:03:39.830: INFO: starting watch
STEP: patching
STEP: updating
Mar 20 08:03:39.843: INFO: waiting for watch events with expected annotations
Mar 20 08:03:39.843: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:188
Mar 20 08:03:39.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-9441" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":356,"completed":154,"skipped":2836,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:03:39.865: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating api versions
Mar 20 08:03:39.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-3717 api-versions'
Mar 20 08:03:39.947: INFO: stderr: ""
Mar 20 08:03:39.947: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nceph.rook.io/v1\ncertificates.k8s.io/v1\nconfig.gatekeeper.sh/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nexpansion.gatekeeper.sh/v1alpha1\nexternaldata.gatekeeper.sh/v1alpha1\nexternaldata.gatekeeper.sh/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nk8s.cni.cncf.io/v1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmutations.gatekeeper.sh/v1\nmutations.gatekeeper.sh/v1alpha1\nmutations.gatekeeper.sh/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\nobjectbucket.io/v1alpha1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstatus.gatekeeper.sh/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplates.gatekeeper.sh/v1\ntemplates.gatekeeper.sh/v1alpha1\ntemplates.gatekeeper.sh/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 08:03:39.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3717" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":356,"completed":155,"skipped":2848,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:03:39.962: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:03:40.027: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"2a1182b9-a881-492d-9dae-06293f0d7644", Controller:(*bool)(0xc0018b185a), BlockOwnerDeletion:(*bool)(0xc0018b185b)}}
Mar 20 08:03:40.035: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"99f0e156-07d1-4128-b309-fd98b240c871", Controller:(*bool)(0xc00430af52), BlockOwnerDeletion:(*bool)(0xc00430af53)}}
Mar 20 08:03:40.047: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"989ca8d4-5217-498f-be8d-d918c354b369", Controller:(*bool)(0xc00430b23a), BlockOwnerDeletion:(*bool)(0xc00430b23b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Mar 20 08:03:45.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4268" for this suite.

• [SLOW TEST:5.104 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":356,"completed":156,"skipped":2870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:03:45.067: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 08:03:45.435: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 20 08:03:47.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 20, 8, 3, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 3, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 3, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 3, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 08:03:50.488: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 08:04:02.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4685" for this suite.
STEP: Destroying namespace "webhook-4685-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:17.598 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":356,"completed":157,"skipped":2955,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:04:02.666: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:04:02.702: INFO: created pod
Mar 20 08:04:02.702: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1988" to be "Succeeded or Failed"
Mar 20 08:04:02.705: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.188367ms
Mar 20 08:04:04.722: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020342958s
Mar 20 08:04:06.730: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027721309s
STEP: Saw pod success
Mar 20 08:04:06.730: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar 20 08:04:36.734: INFO: polling logs
Mar 20 08:04:36.766: INFO: Pod logs: 
I0320 08:04:03.738644       1 log.go:195] OK: Got token
I0320 08:04:03.738669       1 log.go:195] validating with in-cluster discovery
I0320 08:04:03.738907       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0320 08:04:03.738928       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1988:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679300042, NotBefore:1679299442, IssuedAt:1679299442, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1988", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d5f1f46c-5e37-4f08-8da5-40f75c9767e2"}}}
I0320 08:04:03.753349       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0320 08:04:03.763311       1 log.go:195] OK: Validated signature on JWT
I0320 08:04:03.763468       1 log.go:195] OK: Got valid claims from token!
I0320 08:04:03.763521       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1988:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679300042, NotBefore:1679299442, IssuedAt:1679299442, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1988", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d5f1f46c-5e37-4f08-8da5-40f75c9767e2"}}}

Mar 20 08:04:36.766: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Mar 20 08:04:36.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1988" for this suite.

• [SLOW TEST:34.122 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":356,"completed":158,"skipped":2974,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:04:36.788: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-0dd514ad-001d-4ce9-9228-d316fd5c6a54
STEP: Creating a pod to test consume configMaps
Mar 20 08:04:36.822: INFO: Waiting up to 5m0s for pod "pod-configmaps-e85ea6dc-14a9-4bd9-8893-680dedebd922" in namespace "configmap-2403" to be "Succeeded or Failed"
Mar 20 08:04:36.824: INFO: Pod "pod-configmaps-e85ea6dc-14a9-4bd9-8893-680dedebd922": Phase="Pending", Reason="", readiness=false. Elapsed: 2.195175ms
Mar 20 08:04:38.836: INFO: Pod "pod-configmaps-e85ea6dc-14a9-4bd9-8893-680dedebd922": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014139769s
Mar 20 08:04:40.848: INFO: Pod "pod-configmaps-e85ea6dc-14a9-4bd9-8893-680dedebd922": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026524031s
STEP: Saw pod success
Mar 20 08:04:40.848: INFO: Pod "pod-configmaps-e85ea6dc-14a9-4bd9-8893-680dedebd922" satisfied condition "Succeeded or Failed"
Mar 20 08:04:40.851: INFO: Trying to get logs from node env016ar130-worker02 pod pod-configmaps-e85ea6dc-14a9-4bd9-8893-680dedebd922 container agnhost-container: <nil>
STEP: delete the pod
Mar 20 08:04:40.878: INFO: Waiting for pod pod-configmaps-e85ea6dc-14a9-4bd9-8893-680dedebd922 to disappear
Mar 20 08:04:40.882: INFO: Pod pod-configmaps-e85ea6dc-14a9-4bd9-8893-680dedebd922 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar 20 08:04:40.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2403" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":159,"skipped":2989,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:04:40.891: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar 20 08:04:54.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5701" for this suite.

• [SLOW TEST:13.160 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":356,"completed":160,"skipped":3063,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:04:54.052: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Mar 20 08:05:04.722: INFO: 68 pods remaining
Mar 20 08:05:04.722: INFO: 68 pods has nil DeletionTimestamp
Mar 20 08:05:04.722: INFO: 
STEP: Gathering metrics
Mar 20 08:05:09.758: INFO: The status of Pod kube-controller-manager-env016ar130-master03 is Running (Ready = true)
Mar 20 08:05:09.843: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 20 08:05:09.843: INFO: Deleting pod "simpletest-rc-to-be-deleted-25t67" in namespace "gc-1462"
Mar 20 08:05:09.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-457vc" in namespace "gc-1462"
Mar 20 08:05:09.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hr4s" in namespace "gc-1462"
Mar 20 08:05:09.871: INFO: Deleting pod "simpletest-rc-to-be-deleted-69fqn" in namespace "gc-1462"
Mar 20 08:05:09.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-6f857" in namespace "gc-1462"
Mar 20 08:05:09.884: INFO: Deleting pod "simpletest-rc-to-be-deleted-72wbf" in namespace "gc-1462"
Mar 20 08:05:09.892: INFO: Deleting pod "simpletest-rc-to-be-deleted-7944t" in namespace "gc-1462"
Mar 20 08:05:09.900: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dj5n" in namespace "gc-1462"
Mar 20 08:05:09.910: INFO: Deleting pod "simpletest-rc-to-be-deleted-7r8pq" in namespace "gc-1462"
Mar 20 08:05:09.916: INFO: Deleting pod "simpletest-rc-to-be-deleted-8c6xn" in namespace "gc-1462"
Mar 20 08:05:09.927: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jdtg" in namespace "gc-1462"
Mar 20 08:05:09.936: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jm42" in namespace "gc-1462"
Mar 20 08:05:09.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-9k8ds" in namespace "gc-1462"
Mar 20 08:05:09.954: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6tg5" in namespace "gc-1462"
Mar 20 08:05:09.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbdml" in namespace "gc-1462"
Mar 20 08:05:09.969: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqn9m" in namespace "gc-1462"
Mar 20 08:05:09.977: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvss4" in namespace "gc-1462"
Mar 20 08:05:09.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-bx25j" in namespace "gc-1462"
Mar 20 08:05:09.995: INFO: Deleting pod "simpletest-rc-to-be-deleted-c29z2" in namespace "gc-1462"
Mar 20 08:05:10.001: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7npc" in namespace "gc-1462"
Mar 20 08:05:10.008: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgkvp" in namespace "gc-1462"
Mar 20 08:05:10.017: INFO: Deleting pod "simpletest-rc-to-be-deleted-cspnv" in namespace "gc-1462"
Mar 20 08:05:10.025: INFO: Deleting pod "simpletest-rc-to-be-deleted-czcff" in namespace "gc-1462"
Mar 20 08:05:10.032: INFO: Deleting pod "simpletest-rc-to-be-deleted-dblgz" in namespace "gc-1462"
Mar 20 08:05:10.041: INFO: Deleting pod "simpletest-rc-to-be-deleted-dg7tg" in namespace "gc-1462"
Mar 20 08:05:10.094: INFO: Deleting pod "simpletest-rc-to-be-deleted-dg8xv" in namespace "gc-1462"
Mar 20 08:05:10.105: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmr8x" in namespace "gc-1462"
Mar 20 08:05:10.114: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqfrb" in namespace "gc-1462"
Mar 20 08:05:10.129: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkt7m" in namespace "gc-1462"
Mar 20 08:05:10.140: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl9kd" in namespace "gc-1462"
Mar 20 08:05:10.151: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnk49" in namespace "gc-1462"
Mar 20 08:05:10.162: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6f68" in namespace "gc-1462"
Mar 20 08:05:10.177: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghbl7" in namespace "gc-1462"
Mar 20 08:05:10.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-gp7ng" in namespace "gc-1462"
Mar 20 08:05:10.203: INFO: Deleting pod "simpletest-rc-to-be-deleted-hc88q" in namespace "gc-1462"
Mar 20 08:05:10.211: INFO: Deleting pod "simpletest-rc-to-be-deleted-hm69v" in namespace "gc-1462"
Mar 20 08:05:10.225: INFO: Deleting pod "simpletest-rc-to-be-deleted-jgdg2" in namespace "gc-1462"
Mar 20 08:05:10.237: INFO: Deleting pod "simpletest-rc-to-be-deleted-k2ss2" in namespace "gc-1462"
Mar 20 08:05:10.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-k49hp" in namespace "gc-1462"
Mar 20 08:05:10.259: INFO: Deleting pod "simpletest-rc-to-be-deleted-k8sk4" in namespace "gc-1462"
Mar 20 08:05:10.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-kf8m2" in namespace "gc-1462"
Mar 20 08:05:10.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-khjhx" in namespace "gc-1462"
Mar 20 08:05:10.295: INFO: Deleting pod "simpletest-rc-to-be-deleted-kpf9w" in namespace "gc-1462"
Mar 20 08:05:10.305: INFO: Deleting pod "simpletest-rc-to-be-deleted-l6xtf" in namespace "gc-1462"
Mar 20 08:05:10.314: INFO: Deleting pod "simpletest-rc-to-be-deleted-l7665" in namespace "gc-1462"
Mar 20 08:05:10.325: INFO: Deleting pod "simpletest-rc-to-be-deleted-l7pmw" in namespace "gc-1462"
Mar 20 08:05:10.340: INFO: Deleting pod "simpletest-rc-to-be-deleted-l7r5h" in namespace "gc-1462"
Mar 20 08:05:10.351: INFO: Deleting pod "simpletest-rc-to-be-deleted-lcn2c" in namespace "gc-1462"
Mar 20 08:05:10.361: INFO: Deleting pod "simpletest-rc-to-be-deleted-ljhs2" in namespace "gc-1462"
Mar 20 08:05:10.371: INFO: Deleting pod "simpletest-rc-to-be-deleted-m8cq8" in namespace "gc-1462"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Mar 20 08:05:10.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1462" for this suite.

• [SLOW TEST:16.356 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":356,"completed":161,"skipped":3067,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:05:10.408: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Mar 20 08:05:10.503: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Mar 20 08:05:10.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6334" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":356,"completed":162,"skipped":3081,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:05:10.538: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Mar 20 08:05:10.608: INFO: Waiting up to 5m0s for pod "downward-api-22eb7e48-8a9e-496c-9562-789608fe197b" in namespace "downward-api-3195" to be "Succeeded or Failed"
Mar 20 08:05:10.613: INFO: Pod "downward-api-22eb7e48-8a9e-496c-9562-789608fe197b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.337859ms
Mar 20 08:05:12.627: INFO: Pod "downward-api-22eb7e48-8a9e-496c-9562-789608fe197b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019732766s
Mar 20 08:05:14.641: INFO: Pod "downward-api-22eb7e48-8a9e-496c-9562-789608fe197b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033320642s
Mar 20 08:05:16.646: INFO: Pod "downward-api-22eb7e48-8a9e-496c-9562-789608fe197b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038143603s
STEP: Saw pod success
Mar 20 08:05:16.646: INFO: Pod "downward-api-22eb7e48-8a9e-496c-9562-789608fe197b" satisfied condition "Succeeded or Failed"
Mar 20 08:05:16.684: INFO: Trying to get logs from node env016ar130-worker02 pod downward-api-22eb7e48-8a9e-496c-9562-789608fe197b container dapi-container: <nil>
STEP: delete the pod
Mar 20 08:05:16.704: INFO: Waiting for pod downward-api-22eb7e48-8a9e-496c-9562-789608fe197b to disappear
Mar 20 08:05:16.707: INFO: Pod downward-api-22eb7e48-8a9e-496c-9562-789608fe197b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Mar 20 08:05:16.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3195" for this suite.

• [SLOW TEST:6.176 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":356,"completed":163,"skipped":3090,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:05:16.714: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Mar 20 08:05:16.738: INFO: Pod name sample-pod: Found 0 pods out of 3
Mar 20 08:05:21.746: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Mar 20 08:05:21.748: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Mar 20 08:05:21.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9483" for this suite.

• [SLOW TEST:5.082 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":356,"completed":164,"skipped":3159,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:05:21.797: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1574
[It] should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Mar 20 08:05:21.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-2889 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 20 08:05:21.904: INFO: stderr: ""
Mar 20 08:05:21.904: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar 20 08:05:26.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-2889 get pod e2e-test-httpd-pod -o json'
Mar 20 08:05:27.057: INFO: stderr: ""
Mar 20 08:05:27.057: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.90.70/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.90.70/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"192.168.90.70\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"192.168.90.70\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2023-03-20T08:05:21Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2889\",\n        \"resourceVersion\": \"820816\",\n        \"uid\": \"e78aae73-c061-4aad-a1f1-f70713f425b0\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-f7xkp\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"env016ar130-worker01\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-f7xkp\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-20T08:05:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-20T08:05:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-20T08:05:23Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-20T08:05:21Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://572f3f20a0620b331126f3b635eb22e51c87541e0f282906a8c54d73f41ba31f\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-20T08:05:22Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.2.10.71\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.90.70\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.90.70\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-20T08:05:21Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 20 08:05:27.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-2889 replace -f -'
Mar 20 08:05:28.165: INFO: stderr: ""
Mar 20 08:05:28.165: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1578
Mar 20 08:05:28.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-2889 delete pods e2e-test-httpd-pod'
Mar 20 08:05:29.625: INFO: stderr: ""
Mar 20 08:05:29.625: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 08:05:29.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2889" for this suite.

• [SLOW TEST:7.840 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1571
    should update a single-container pod's image  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":356,"completed":165,"skipped":3170,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:05:29.636: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Mar 20 08:05:29.675: INFO: The status of Pod labelsupdate04fa99cf-4912-43c0-be33-90b2c9e2db7c is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:05:31.682: INFO: The status of Pod labelsupdate04fa99cf-4912-43c0-be33-90b2c9e2db7c is Running (Ready = true)
Mar 20 08:05:32.221: INFO: Successfully updated pod "labelsupdate04fa99cf-4912-43c0-be33-90b2c9e2db7c"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar 20 08:05:36.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5649" for this suite.

• [SLOW TEST:6.625 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":166,"skipped":3172,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:05:36.262: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Mar 20 08:05:56.467: INFO: EndpointSlice for Service endpointslice-3435/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Mar 20 08:06:06.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3435" for this suite.

• [SLOW TEST:30.236 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":356,"completed":167,"skipped":3187,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:06:06.499: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Mar 20 08:06:07.605: INFO: The status of Pod kube-controller-manager-env016ar130-master03 is Running (Ready = true)
Mar 20 08:06:07.663: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Mar 20 08:06:07.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7856" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":356,"completed":168,"skipped":3194,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:06:07.675: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 20 08:06:07.710: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar 20 08:06:07.714: INFO: starting watch
STEP: patching
STEP: updating
Mar 20 08:06:07.734: INFO: waiting for watch events with expected annotations
Mar 20 08:06:07.734: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Mar 20 08:06:07.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2777" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":356,"completed":169,"skipped":3202,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:06:07.787: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Mar 20 08:06:17.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6233" for this suite.

• [SLOW TEST:10.046 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":356,"completed":170,"skipped":3205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:06:17.834: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-secret-x9sc
STEP: Creating a pod to test atomic-volume-subpath
Mar 20 08:06:17.881: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-x9sc" in namespace "subpath-6514" to be "Succeeded or Failed"
Mar 20 08:06:17.885: INFO: Pod "pod-subpath-test-secret-x9sc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.217385ms
Mar 20 08:06:19.891: INFO: Pod "pod-subpath-test-secret-x9sc": Phase="Running", Reason="", readiness=true. Elapsed: 2.010051934s
Mar 20 08:06:21.898: INFO: Pod "pod-subpath-test-secret-x9sc": Phase="Running", Reason="", readiness=true. Elapsed: 4.01715906s
Mar 20 08:06:23.909: INFO: Pod "pod-subpath-test-secret-x9sc": Phase="Running", Reason="", readiness=true. Elapsed: 6.027556726s
Mar 20 08:06:25.932: INFO: Pod "pod-subpath-test-secret-x9sc": Phase="Running", Reason="", readiness=true. Elapsed: 8.050335456s
Mar 20 08:06:27.940: INFO: Pod "pod-subpath-test-secret-x9sc": Phase="Running", Reason="", readiness=true. Elapsed: 10.058856724s
Mar 20 08:06:29.953: INFO: Pod "pod-subpath-test-secret-x9sc": Phase="Running", Reason="", readiness=true. Elapsed: 12.071719713s
Mar 20 08:06:31.965: INFO: Pod "pod-subpath-test-secret-x9sc": Phase="Running", Reason="", readiness=true. Elapsed: 14.08386616s
Mar 20 08:06:33.979: INFO: Pod "pod-subpath-test-secret-x9sc": Phase="Running", Reason="", readiness=true. Elapsed: 16.097487128s
Mar 20 08:06:35.985: INFO: Pod "pod-subpath-test-secret-x9sc": Phase="Running", Reason="", readiness=true. Elapsed: 18.103689385s
Mar 20 08:06:37.995: INFO: Pod "pod-subpath-test-secret-x9sc": Phase="Running", Reason="", readiness=true. Elapsed: 20.114042794s
Mar 20 08:06:40.002: INFO: Pod "pod-subpath-test-secret-x9sc": Phase="Running", Reason="", readiness=false. Elapsed: 22.120803576s
Mar 20 08:06:42.008: INFO: Pod "pod-subpath-test-secret-x9sc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.127222266s
STEP: Saw pod success
Mar 20 08:06:42.009: INFO: Pod "pod-subpath-test-secret-x9sc" satisfied condition "Succeeded or Failed"
Mar 20 08:06:42.012: INFO: Trying to get logs from node env016ar130-worker02 pod pod-subpath-test-secret-x9sc container test-container-subpath-secret-x9sc: <nil>
STEP: delete the pod
Mar 20 08:06:42.039: INFO: Waiting for pod pod-subpath-test-secret-x9sc to disappear
Mar 20 08:06:42.043: INFO: Pod pod-subpath-test-secret-x9sc no longer exists
STEP: Deleting pod pod-subpath-test-secret-x9sc
Mar 20 08:06:42.043: INFO: Deleting pod "pod-subpath-test-secret-x9sc" in namespace "subpath-6514"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Mar 20 08:06:42.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6514" for this suite.

• [SLOW TEST:24.221 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","total":356,"completed":171,"skipped":3265,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:06:42.055: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 08:06:42.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9863" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":356,"completed":172,"skipped":3284,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:06:42.088: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-a91124a7-1e8f-402e-91ec-bfa0de99fc76
STEP: Creating a pod to test consume secrets
Mar 20 08:06:42.119: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-67e2db94-71ea-45bc-a9b7-957b95739751" in namespace "projected-4038" to be "Succeeded or Failed"
Mar 20 08:06:42.122: INFO: Pod "pod-projected-secrets-67e2db94-71ea-45bc-a9b7-957b95739751": Phase="Pending", Reason="", readiness=false. Elapsed: 3.039786ms
Mar 20 08:06:44.137: INFO: Pod "pod-projected-secrets-67e2db94-71ea-45bc-a9b7-957b95739751": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017862414s
Mar 20 08:06:46.145: INFO: Pod "pod-projected-secrets-67e2db94-71ea-45bc-a9b7-957b95739751": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025426874s
STEP: Saw pod success
Mar 20 08:06:46.145: INFO: Pod "pod-projected-secrets-67e2db94-71ea-45bc-a9b7-957b95739751" satisfied condition "Succeeded or Failed"
Mar 20 08:06:46.148: INFO: Trying to get logs from node env016ar130-worker01 pod pod-projected-secrets-67e2db94-71ea-45bc-a9b7-957b95739751 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 20 08:06:46.179: INFO: Waiting for pod pod-projected-secrets-67e2db94-71ea-45bc-a9b7-957b95739751 to disappear
Mar 20 08:06:46.183: INFO: Pod pod-projected-secrets-67e2db94-71ea-45bc-a9b7-957b95739751 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Mar 20 08:06:46.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4038" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":173,"skipped":3312,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:06:46.192: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Mar 20 08:06:46.251: INFO: PodSpec: initContainers in spec.initContainers
Mar 20 08:07:29.101: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-389e577c-3b4d-4eef-90bb-ccd0a0a32c4e", GenerateName:"", Namespace:"init-container-7364", SelfLink:"", UID:"31c67f0d-1f5b-46b6-b4b2-dde615246e36", ResourceVersion:"821635", Generation:0, CreationTimestamp:time.Date(2023, time.March, 20, 8, 6, 46, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"251252928"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.90.116/32", "cni.projectcalico.org/podIPs":"192.168.90.116/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.90.116\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.90.116\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 20, 8, 6, 46, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004226c78), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 20, 8, 6, 46, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004226ca8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 20, 8, 6, 46, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004226cd8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 20, 8, 6, 47, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004226d08), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-8r2cl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003913ca0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8r2cl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8r2cl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.7", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8r2cl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc008822b98), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"env016ar130-worker01", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000af6bd0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc008822c10)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc008822c30)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc008822c38), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc008822c3c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0029e0f20), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 20, 8, 6, 46, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 20, 8, 6, 46, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 20, 8, 6, 46, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 20, 8, 6, 46, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.2.10.71", PodIP:"192.168.90.116", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.90.116"}}, StartTime:time.Date(2023, time.March, 20, 8, 6, 46, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000af6cb0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000af6d20)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://790b0969dfc2b16386f6d15aeec3215720267ef942362e2be45644e4a75fbb28", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003913d40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003913d00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.7", ImageID:"", ContainerID:"", Started:(*bool)(0xc008822cbf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Mar 20 08:07:29.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7364" for this suite.

• [SLOW TEST:42.923 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":356,"completed":174,"skipped":3339,"failed":0}
SSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:07:29.115: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Mar 20 08:13:01.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7845" for this suite.

• [SLOW TEST:332.091 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":356,"completed":175,"skipped":3346,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:13:01.207: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:13:01.262: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 20 08:13:06.274: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 20 08:13:06.274: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 20 08:13:08.281: INFO: Creating deployment "test-rollover-deployment"
Mar 20 08:13:08.295: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 20 08:13:10.311: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 20 08:13:10.319: INFO: Ensure that both replica sets have 1 created replica
Mar 20 08:13:10.326: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 20 08:13:10.338: INFO: Updating deployment test-rollover-deployment
Mar 20 08:13:10.338: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 20 08:13:12.352: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 20 08:13:12.358: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 20 08:13:12.363: INFO: all replica sets need to contain the pod-template-hash label
Mar 20 08:13:12.363: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 13, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 13, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 13, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 13, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 20 08:13:14.371: INFO: all replica sets need to contain the pod-template-hash label
Mar 20 08:13:14.371: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 13, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 13, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 13, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 13, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 20 08:13:16.372: INFO: all replica sets need to contain the pod-template-hash label
Mar 20 08:13:16.372: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 13, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 13, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 13, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 13, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 20 08:13:18.378: INFO: all replica sets need to contain the pod-template-hash label
Mar 20 08:13:18.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 13, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 13, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 13, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 13, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 20 08:13:20.377: INFO: all replica sets need to contain the pod-template-hash label
Mar 20 08:13:20.377: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 13, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 13, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 13, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 13, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 20 08:13:22.372: INFO: 
Mar 20 08:13:22.372: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 20 08:13:22.379: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9261  fa0e144c-91d9-429f-bc1c-fafb75fb5eff 822590 2 2023-03-20 08:13:08 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-03-20 08:13:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 08:13:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005be5268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-20 08:13:08 +0000 UTC,LastTransitionTime:2023-03-20 08:13:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-779c67f4f8" has successfully progressed.,LastUpdateTime:2023-03-20 08:13:21 +0000 UTC,LastTransitionTime:2023-03-20 08:13:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 20 08:13:22.382: INFO: New ReplicaSet "test-rollover-deployment-779c67f4f8" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-779c67f4f8  deployment-9261  bec76c67-81fe-4ee0-b22d-b91bb4ba4ea8 822580 2 2023-03-20 08:13:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment fa0e144c-91d9-429f-bc1c-fafb75fb5eff 0xc005be5a57 0xc005be5a58}] []  [{kube-controller-manager Update apps/v1 2023-03-20 08:13:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa0e144c-91d9-429f-bc1c-fafb75fb5eff\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 08:13:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 779c67f4f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005be5b18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 20 08:13:22.382: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 20 08:13:22.382: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9261  67b6dacb-6097-4127-9f82-10eb56cf0cc0 822589 2 2023-03-20 08:13:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment fa0e144c-91d9-429f-bc1c-fafb75fb5eff 0xc005be5927 0xc005be5928}] []  [{e2e.test Update apps/v1 2023-03-20 08:13:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 08:13:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa0e144c-91d9-429f-bc1c-fafb75fb5eff\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-20 08:13:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005be59e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 20 08:13:22.382: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-87f8f6dcf  deployment-9261  c6515faa-0801-49cc-bb7e-3528eec53a9e 822522 2 2023-03-20 08:13:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment fa0e144c-91d9-429f-bc1c-fafb75fb5eff 0xc005be5b80 0xc005be5b81}] []  [{kube-controller-manager Update apps/v1 2023-03-20 08:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa0e144c-91d9-429f-bc1c-fafb75fb5eff\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 08:13:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 87f8f6dcf,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005be5c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 20 08:13:22.385: INFO: Pod "test-rollover-deployment-779c67f4f8-h6hmm" is available:
&Pod{ObjectMeta:{test-rollover-deployment-779c67f4f8-h6hmm test-rollover-deployment-779c67f4f8- deployment-9261  6f429bac-357c-48ce-91a6-337025735b34 822547 0 2023-03-20 08:13:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[cni.projectcalico.org/podIP:192.168.12.29/32 cni.projectcalico.org/podIPs:192.168.12.29/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.29"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.29"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rollover-deployment-779c67f4f8 bec76c67-81fe-4ee0-b22d-b91bb4ba4ea8 0xc0023acd47 0xc0023acd48}] []  [{calico Update v1 2023-03-20 08:13:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 08:13:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bec76c67-81fe-4ee0-b22d-b91bb4ba4ea8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-03-20 08:13:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-20 08:13:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42s6s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42s6s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:13:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:13:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:13:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:13:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.72,PodIP:192.168.12.29,StartTime:2023-03-20 08:13:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-20 08:13:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://75958f8e868d20bf19b6a882e8fd0ab0b4a6af5949306aaac0be71245779e5e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar 20 08:13:22.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9261" for this suite.

• [SLOW TEST:21.186 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":356,"completed":176,"skipped":3365,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:13:22.394: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-bb2f
STEP: Creating a pod to test atomic-volume-subpath
Mar 20 08:13:22.452: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bb2f" in namespace "subpath-1977" to be "Succeeded or Failed"
Mar 20 08:13:22.455: INFO: Pod "pod-subpath-test-configmap-bb2f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.04414ms
Mar 20 08:13:24.464: INFO: Pod "pod-subpath-test-configmap-bb2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011473279s
Mar 20 08:13:26.473: INFO: Pod "pod-subpath-test-configmap-bb2f": Phase="Running", Reason="", readiness=true. Elapsed: 4.020844663s
Mar 20 08:13:28.483: INFO: Pod "pod-subpath-test-configmap-bb2f": Phase="Running", Reason="", readiness=true. Elapsed: 6.030692196s
Mar 20 08:13:30.500: INFO: Pod "pod-subpath-test-configmap-bb2f": Phase="Running", Reason="", readiness=true. Elapsed: 8.047373421s
Mar 20 08:13:32.509: INFO: Pod "pod-subpath-test-configmap-bb2f": Phase="Running", Reason="", readiness=true. Elapsed: 10.056458469s
Mar 20 08:13:34.517: INFO: Pod "pod-subpath-test-configmap-bb2f": Phase="Running", Reason="", readiness=true. Elapsed: 12.064235598s
Mar 20 08:13:36.527: INFO: Pod "pod-subpath-test-configmap-bb2f": Phase="Running", Reason="", readiness=true. Elapsed: 14.074562576s
Mar 20 08:13:38.536: INFO: Pod "pod-subpath-test-configmap-bb2f": Phase="Running", Reason="", readiness=true. Elapsed: 16.083237929s
Mar 20 08:13:40.551: INFO: Pod "pod-subpath-test-configmap-bb2f": Phase="Running", Reason="", readiness=true. Elapsed: 18.098581376s
Mar 20 08:13:42.564: INFO: Pod "pod-subpath-test-configmap-bb2f": Phase="Running", Reason="", readiness=true. Elapsed: 20.111800541s
Mar 20 08:13:44.579: INFO: Pod "pod-subpath-test-configmap-bb2f": Phase="Running", Reason="", readiness=false. Elapsed: 22.126131351s
Mar 20 08:13:46.592: INFO: Pod "pod-subpath-test-configmap-bb2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.139917188s
STEP: Saw pod success
Mar 20 08:13:46.592: INFO: Pod "pod-subpath-test-configmap-bb2f" satisfied condition "Succeeded or Failed"
Mar 20 08:13:46.596: INFO: Trying to get logs from node env016ar130-worker01 pod pod-subpath-test-configmap-bb2f container test-container-subpath-configmap-bb2f: <nil>
STEP: delete the pod
Mar 20 08:13:46.626: INFO: Waiting for pod pod-subpath-test-configmap-bb2f to disappear
Mar 20 08:13:46.629: INFO: Pod pod-subpath-test-configmap-bb2f no longer exists
STEP: Deleting pod pod-subpath-test-configmap-bb2f
Mar 20 08:13:46.629: INFO: Deleting pod "pod-subpath-test-configmap-bb2f" in namespace "subpath-1977"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Mar 20 08:13:46.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1977" for this suite.

• [SLOW TEST:24.246 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","total":356,"completed":177,"skipped":3365,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:13:46.640: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Mar 20 08:13:48.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7585" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","total":356,"completed":178,"skipped":3375,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:13:48.706: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar 20 08:14:04.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3597" for this suite.

• [SLOW TEST:16.124 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":356,"completed":179,"skipped":3401,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:14:04.830: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:188
Mar 20 08:14:04.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7856" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":356,"completed":180,"skipped":3406,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:14:04.860: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 20 08:14:04.887: INFO: Waiting up to 5m0s for pod "pod-cdf4aab5-a09c-4576-8186-96bbaa127c9a" in namespace "emptydir-7970" to be "Succeeded or Failed"
Mar 20 08:14:04.889: INFO: Pod "pod-cdf4aab5-a09c-4576-8186-96bbaa127c9a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.965485ms
Mar 20 08:14:06.900: INFO: Pod "pod-cdf4aab5-a09c-4576-8186-96bbaa127c9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013238379s
Mar 20 08:14:08.904: INFO: Pod "pod-cdf4aab5-a09c-4576-8186-96bbaa127c9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017696735s
STEP: Saw pod success
Mar 20 08:14:08.904: INFO: Pod "pod-cdf4aab5-a09c-4576-8186-96bbaa127c9a" satisfied condition "Succeeded or Failed"
Mar 20 08:14:08.910: INFO: Trying to get logs from node env016ar130-worker01 pod pod-cdf4aab5-a09c-4576-8186-96bbaa127c9a container test-container: <nil>
STEP: delete the pod
Mar 20 08:14:08.925: INFO: Waiting for pod pod-cdf4aab5-a09c-4576-8186-96bbaa127c9a to disappear
Mar 20 08:14:08.927: INFO: Pod pod-cdf4aab5-a09c-4576-8186-96bbaa127c9a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar 20 08:14:08.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7970" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":181,"skipped":3420,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:14:08.934: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 08:14:08.966: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8301b31-35ef-4a1b-b7a2-7b8e3a504a68" in namespace "downward-api-4443" to be "Succeeded or Failed"
Mar 20 08:14:08.969: INFO: Pod "downwardapi-volume-a8301b31-35ef-4a1b-b7a2-7b8e3a504a68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.435276ms
Mar 20 08:14:10.977: INFO: Pod "downwardapi-volume-a8301b31-35ef-4a1b-b7a2-7b8e3a504a68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01033454s
Mar 20 08:14:12.986: INFO: Pod "downwardapi-volume-a8301b31-35ef-4a1b-b7a2-7b8e3a504a68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0194662s
STEP: Saw pod success
Mar 20 08:14:12.986: INFO: Pod "downwardapi-volume-a8301b31-35ef-4a1b-b7a2-7b8e3a504a68" satisfied condition "Succeeded or Failed"
Mar 20 08:14:12.990: INFO: Trying to get logs from node env016ar130-worker02 pod downwardapi-volume-a8301b31-35ef-4a1b-b7a2-7b8e3a504a68 container client-container: <nil>
STEP: delete the pod
Mar 20 08:14:13.022: INFO: Waiting for pod downwardapi-volume-a8301b31-35ef-4a1b-b7a2-7b8e3a504a68 to disappear
Mar 20 08:14:13.025: INFO: Pod downwardapi-volume-a8301b31-35ef-4a1b-b7a2-7b8e3a504a68 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar 20 08:14:13.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4443" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":182,"skipped":3426,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:14:13.034: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1215
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1215
STEP: Waiting until pod test-pod will start running in namespace statefulset-1215
STEP: Creating statefulset with conflicting port in namespace statefulset-1215
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1215
Mar 20 08:14:15.121: INFO: Observed stateful pod in namespace: statefulset-1215, name: ss-0, uid: 0b31aced-b980-4df4-9131-f862945ba864, status phase: Pending. Waiting for statefulset controller to delete.
Mar 20 08:14:15.136: INFO: Observed stateful pod in namespace: statefulset-1215, name: ss-0, uid: 0b31aced-b980-4df4-9131-f862945ba864, status phase: Failed. Waiting for statefulset controller to delete.
Mar 20 08:14:15.143: INFO: Observed stateful pod in namespace: statefulset-1215, name: ss-0, uid: 0b31aced-b980-4df4-9131-f862945ba864, status phase: Failed. Waiting for statefulset controller to delete.
Mar 20 08:14:15.147: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1215
STEP: Removing pod with conflicting port in namespace statefulset-1215
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1215 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 20 08:14:17.186: INFO: Deleting all statefulset in ns statefulset-1215
Mar 20 08:14:17.189: INFO: Scaling statefulset ss to 0
Mar 20 08:14:27.218: INFO: Waiting for statefulset status.replicas updated to 0
Mar 20 08:14:27.222: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar 20 08:14:27.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1215" for this suite.

• [SLOW TEST:14.217 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":356,"completed":183,"skipped":3434,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:14:27.252: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
Mar 20 08:14:27.275: INFO: Creating simple deployment test-deployment-2wzdk
Mar 20 08:14:27.284: INFO: new replicaset for deployment "test-deployment-2wzdk" is yet to be created
STEP: Getting /status
Mar 20 08:14:29.305: INFO: Deployment test-deployment-2wzdk has Conditions: [{Available True 2023-03-20 08:14:28 +0000 UTC 2023-03-20 08:14:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-20 08:14:28 +0000 UTC 2023-03-20 08:14:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2wzdk-688c4d6789" has successfully progressed.}]
STEP: updating Deployment Status
Mar 20 08:14:29.314: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 14, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 14, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 14, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 14, 27, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-2wzdk-688c4d6789\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Mar 20 08:14:29.316: INFO: Observed &Deployment event: ADDED
Mar 20 08:14:29.316: INFO: Observed Deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-20 08:14:27 +0000 UTC 2023-03-20 08:14:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2wzdk-688c4d6789"}
Mar 20 08:14:29.316: INFO: Observed &Deployment event: MODIFIED
Mar 20 08:14:29.316: INFO: Observed Deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-20 08:14:27 +0000 UTC 2023-03-20 08:14:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2wzdk-688c4d6789"}
Mar 20 08:14:29.316: INFO: Observed Deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-20 08:14:27 +0000 UTC 2023-03-20 08:14:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 20 08:14:29.316: INFO: Observed &Deployment event: MODIFIED
Mar 20 08:14:29.316: INFO: Observed Deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-20 08:14:27 +0000 UTC 2023-03-20 08:14:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 20 08:14:29.316: INFO: Observed Deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-20 08:14:27 +0000 UTC 2023-03-20 08:14:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2wzdk-688c4d6789" is progressing.}
Mar 20 08:14:29.316: INFO: Observed &Deployment event: MODIFIED
Mar 20 08:14:29.316: INFO: Observed Deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-20 08:14:28 +0000 UTC 2023-03-20 08:14:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 20 08:14:29.316: INFO: Observed Deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-20 08:14:28 +0000 UTC 2023-03-20 08:14:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2wzdk-688c4d6789" has successfully progressed.}
Mar 20 08:14:29.317: INFO: Observed &Deployment event: MODIFIED
Mar 20 08:14:29.317: INFO: Observed Deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-20 08:14:28 +0000 UTC 2023-03-20 08:14:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 20 08:14:29.317: INFO: Observed Deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-20 08:14:28 +0000 UTC 2023-03-20 08:14:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2wzdk-688c4d6789" has successfully progressed.}
Mar 20 08:14:29.317: INFO: Found Deployment test-deployment-2wzdk in namespace deployment-5741 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 20 08:14:29.317: INFO: Deployment test-deployment-2wzdk has an updated status
STEP: patching the Statefulset Status
Mar 20 08:14:29.317: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 20 08:14:29.321: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Mar 20 08:14:29.323: INFO: Observed &Deployment event: ADDED
Mar 20 08:14:29.323: INFO: Observed deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-20 08:14:27 +0000 UTC 2023-03-20 08:14:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2wzdk-688c4d6789"}
Mar 20 08:14:29.323: INFO: Observed &Deployment event: MODIFIED
Mar 20 08:14:29.323: INFO: Observed deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-20 08:14:27 +0000 UTC 2023-03-20 08:14:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2wzdk-688c4d6789"}
Mar 20 08:14:29.323: INFO: Observed deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-20 08:14:27 +0000 UTC 2023-03-20 08:14:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 20 08:14:29.323: INFO: Observed &Deployment event: MODIFIED
Mar 20 08:14:29.323: INFO: Observed deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-20 08:14:27 +0000 UTC 2023-03-20 08:14:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 20 08:14:29.323: INFO: Observed deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-20 08:14:27 +0000 UTC 2023-03-20 08:14:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2wzdk-688c4d6789" is progressing.}
Mar 20 08:14:29.323: INFO: Observed &Deployment event: MODIFIED
Mar 20 08:14:29.323: INFO: Observed deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-20 08:14:28 +0000 UTC 2023-03-20 08:14:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 20 08:14:29.323: INFO: Observed deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-20 08:14:28 +0000 UTC 2023-03-20 08:14:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2wzdk-688c4d6789" has successfully progressed.}
Mar 20 08:14:29.323: INFO: Observed &Deployment event: MODIFIED
Mar 20 08:14:29.323: INFO: Observed deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-20 08:14:28 +0000 UTC 2023-03-20 08:14:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 20 08:14:29.324: INFO: Observed deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-20 08:14:28 +0000 UTC 2023-03-20 08:14:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2wzdk-688c4d6789" has successfully progressed.}
Mar 20 08:14:29.324: INFO: Observed deployment test-deployment-2wzdk in namespace deployment-5741 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 20 08:14:29.324: INFO: Observed &Deployment event: MODIFIED
Mar 20 08:14:29.324: INFO: Found deployment test-deployment-2wzdk in namespace deployment-5741 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar 20 08:14:29.324: INFO: Deployment test-deployment-2wzdk has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 20 08:14:29.326: INFO: Deployment "test-deployment-2wzdk":
&Deployment{ObjectMeta:{test-deployment-2wzdk  deployment-5741  ef9edc9f-15dd-4258-995a-e226549b05a9 823040 1 2023-03-20 08:14:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2023-03-20 08:14:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 08:14:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-03-20 08:14:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005be4128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 20 08:14:29.329: INFO: New ReplicaSet "test-deployment-2wzdk-688c4d6789" of Deployment "test-deployment-2wzdk":
&ReplicaSet{ObjectMeta:{test-deployment-2wzdk-688c4d6789  deployment-5741  3a1279e6-6d74-466f-b107-bbeb876cb1dc 823035 1 2023-03-20 08:14:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-2wzdk ef9edc9f-15dd-4258-995a-e226549b05a9 0xc005be44f0 0xc005be44f1}] []  [{kube-controller-manager Update apps/v1 2023-03-20 08:14:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef9edc9f-15dd-4258-995a-e226549b05a9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 08:14:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 688c4d6789,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005be4598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 20 08:14:29.331: INFO: Pod "test-deployment-2wzdk-688c4d6789-g67fh" is available:
&Pod{ObjectMeta:{test-deployment-2wzdk-688c4d6789-g67fh test-deployment-2wzdk-688c4d6789- deployment-5741  9adf7856-8aaa-496b-9b7e-45f35c5e2e35 823034 0 2023-03-20 08:14:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[cni.projectcalico.org/podIP:192.168.90.97/32 cni.projectcalico.org/podIPs:192.168.90.97/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.97"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.97"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-2wzdk-688c4d6789 3a1279e6-6d74-466f-b107-bbeb876cb1dc 0xc005be4950 0xc005be4951}] []  [{calico Update v1 2023-03-20 08:14:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 08:14:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a1279e6-6d74-466f-b107-bbeb876cb1dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-03-20 08:14:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-20 08:14:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.90.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2j5qw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2j5qw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:14:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:14:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:14:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:14:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.71,PodIP:192.168.90.97,StartTime:2023-03-20 08:14:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-20 08:14:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4ef06ae644b8a79002eb8da2e56547559607fc5669b7c0d9f5c874f1a74b0db7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.90.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar 20 08:14:29.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5741" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":356,"completed":184,"skipped":3467,"failed":0}
SS
------------------------------
[sig-node] Containers 
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:14:29.341: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override arguments
Mar 20 08:14:29.369: INFO: Waiting up to 5m0s for pod "client-containers-3b5f6614-9bf6-4db1-8233-8011e0b72baf" in namespace "containers-3130" to be "Succeeded or Failed"
Mar 20 08:14:29.372: INFO: Pod "client-containers-3b5f6614-9bf6-4db1-8233-8011e0b72baf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.81353ms
Mar 20 08:14:31.381: INFO: Pod "client-containers-3b5f6614-9bf6-4db1-8233-8011e0b72baf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011457587s
Mar 20 08:14:33.387: INFO: Pod "client-containers-3b5f6614-9bf6-4db1-8233-8011e0b72baf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01714927s
STEP: Saw pod success
Mar 20 08:14:33.387: INFO: Pod "client-containers-3b5f6614-9bf6-4db1-8233-8011e0b72baf" satisfied condition "Succeeded or Failed"
Mar 20 08:14:33.390: INFO: Trying to get logs from node env016ar130-worker02 pod client-containers-3b5f6614-9bf6-4db1-8233-8011e0b72baf container agnhost-container: <nil>
STEP: delete the pod
Mar 20 08:14:33.405: INFO: Waiting for pod client-containers-3b5f6614-9bf6-4db1-8233-8011e0b72baf to disappear
Mar 20 08:14:33.407: INFO: Pod client-containers-3b5f6614-9bf6-4db1-8233-8011e0b72baf no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Mar 20 08:14:33.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3130" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","total":356,"completed":185,"skipped":3469,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:14:33.419: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2391
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2391
STEP: creating replication controller externalsvc in namespace services-2391
I0320 08:14:33.475328      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2391, replica count: 2
I0320 08:14:36.526596      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar 20 08:14:36.554: INFO: Creating new exec pod
Mar 20 08:14:38.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-2391 exec execpod8n49r -- /bin/sh -x -c nslookup clusterip-service.services-2391.svc.cluster.local'
Mar 20 08:14:38.752: INFO: stderr: "+ nslookup clusterip-service.services-2391.svc.cluster.local\n"
Mar 20 08:14:38.752: INFO: stdout: "Server:\t\t192.168.128.10\nAddress:\t192.168.128.10#53\n\nclusterip-service.services-2391.svc.cluster.local\tcanonical name = externalsvc.services-2391.svc.cluster.local.\nName:\texternalsvc.services-2391.svc.cluster.local\nAddress: 192.168.253.214\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2391, will wait for the garbage collector to delete the pods
Mar 20 08:14:38.814: INFO: Deleting ReplicationController externalsvc took: 8.961619ms
Mar 20 08:14:38.915: INFO: Terminating ReplicationController externalsvc pods took: 100.875949ms
Mar 20 08:14:41.240: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 08:14:41.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2391" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:7.838 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":356,"completed":186,"skipped":3471,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:14:41.257: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-d34d39af-e5b4-4781-8fea-b37414bbd4ca
STEP: Creating a pod to test consume secrets
Mar 20 08:14:41.288: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-80e3e81b-d4c9-4c1b-b54b-0ae98e33b41d" in namespace "projected-8065" to be "Succeeded or Failed"
Mar 20 08:14:41.291: INFO: Pod "pod-projected-secrets-80e3e81b-d4c9-4c1b-b54b-0ae98e33b41d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.9442ms
Mar 20 08:14:43.297: INFO: Pod "pod-projected-secrets-80e3e81b-d4c9-4c1b-b54b-0ae98e33b41d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008986761s
Mar 20 08:14:45.338: INFO: Pod "pod-projected-secrets-80e3e81b-d4c9-4c1b-b54b-0ae98e33b41d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049638178s
STEP: Saw pod success
Mar 20 08:14:45.338: INFO: Pod "pod-projected-secrets-80e3e81b-d4c9-4c1b-b54b-0ae98e33b41d" satisfied condition "Succeeded or Failed"
Mar 20 08:14:45.342: INFO: Trying to get logs from node env016ar130-worker02 pod pod-projected-secrets-80e3e81b-d4c9-4c1b-b54b-0ae98e33b41d container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 20 08:14:45.359: INFO: Waiting for pod pod-projected-secrets-80e3e81b-d4c9-4c1b-b54b-0ae98e33b41d to disappear
Mar 20 08:14:45.361: INFO: Pod pod-projected-secrets-80e3e81b-d4c9-4c1b-b54b-0ae98e33b41d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Mar 20 08:14:45.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8065" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":187,"skipped":3472,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:14:45.372: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test env composition
Mar 20 08:14:45.402: INFO: Waiting up to 5m0s for pod "var-expansion-21ca3b1e-9e33-4ef4-99c1-7b0c478e4d9e" in namespace "var-expansion-4803" to be "Succeeded or Failed"
Mar 20 08:14:45.406: INFO: Pod "var-expansion-21ca3b1e-9e33-4ef4-99c1-7b0c478e4d9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.2149ms
Mar 20 08:14:47.414: INFO: Pod "var-expansion-21ca3b1e-9e33-4ef4-99c1-7b0c478e4d9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012019309s
Mar 20 08:14:49.421: INFO: Pod "var-expansion-21ca3b1e-9e33-4ef4-99c1-7b0c478e4d9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018155481s
STEP: Saw pod success
Mar 20 08:14:49.421: INFO: Pod "var-expansion-21ca3b1e-9e33-4ef4-99c1-7b0c478e4d9e" satisfied condition "Succeeded or Failed"
Mar 20 08:14:49.424: INFO: Trying to get logs from node env016ar130-worker01 pod var-expansion-21ca3b1e-9e33-4ef4-99c1-7b0c478e4d9e container dapi-container: <nil>
STEP: delete the pod
Mar 20 08:14:49.439: INFO: Waiting for pod var-expansion-21ca3b1e-9e33-4ef4-99c1-7b0c478e4d9e to disappear
Mar 20 08:14:49.442: INFO: Pod var-expansion-21ca3b1e-9e33-4ef4-99c1-7b0c478e4d9e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar 20 08:14:49.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4803" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":356,"completed":188,"skipped":3486,"failed":0}
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:14:49.451: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-c2h5b in namespace proxy-8439
I0320 08:14:49.485358      23 runners.go:193] Created replication controller with name: proxy-service-c2h5b, namespace: proxy-8439, replica count: 1
I0320 08:14:50.536514      23 runners.go:193] proxy-service-c2h5b Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0320 08:14:51.537089      23 runners.go:193] proxy-service-c2h5b Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 20 08:14:51.549: INFO: setup took 2.081293648s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 20 08:14:51.556: INFO: (0) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 7.096885ms)
Mar 20 08:14:51.556: INFO: (0) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 7.280824ms)
Mar 20 08:14:51.557: INFO: (0) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 7.7929ms)
Mar 20 08:14:51.557: INFO: (0) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 8.10595ms)
Mar 20 08:14:51.557: INFO: (0) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 8.186558ms)
Mar 20 08:14:51.557: INFO: (0) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 8.01174ms)
Mar 20 08:14:51.558: INFO: (0) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 8.929108ms)
Mar 20 08:14:51.558: INFO: (0) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 8.926339ms)
Mar 20 08:14:51.559: INFO: (0) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 9.621238ms)
Mar 20 08:14:51.559: INFO: (0) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 10.271771ms)
Mar 20 08:14:51.560: INFO: (0) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 10.365523ms)
Mar 20 08:14:51.566: INFO: (0) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 16.80203ms)
Mar 20 08:14:51.566: INFO: (0) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 16.876597ms)
Mar 20 08:14:51.566: INFO: (0) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 17.062246ms)
Mar 20 08:14:51.566: INFO: (0) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 17.05074ms)
Mar 20 08:14:51.566: INFO: (0) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 17.118764ms)
Mar 20 08:14:51.571: INFO: (1) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 4.704976ms)
Mar 20 08:14:51.572: INFO: (1) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 5.230488ms)
Mar 20 08:14:51.572: INFO: (1) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 5.522849ms)
Mar 20 08:14:51.572: INFO: (1) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 5.49174ms)
Mar 20 08:14:51.573: INFO: (1) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 6.433603ms)
Mar 20 08:14:51.573: INFO: (1) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 6.903939ms)
Mar 20 08:14:51.573: INFO: (1) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 7.066752ms)
Mar 20 08:14:51.573: INFO: (1) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 7.019545ms)
Mar 20 08:14:51.573: INFO: (1) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 6.967848ms)
Mar 20 08:14:51.573: INFO: (1) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 6.954866ms)
Mar 20 08:14:51.574: INFO: (1) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 7.52022ms)
Mar 20 08:14:51.574: INFO: (1) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 7.790097ms)
Mar 20 08:14:51.574: INFO: (1) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 7.673442ms)
Mar 20 08:14:51.574: INFO: (1) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 7.712308ms)
Mar 20 08:14:51.574: INFO: (1) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 8.022307ms)
Mar 20 08:14:51.574: INFO: (1) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 7.859788ms)
Mar 20 08:14:51.579: INFO: (2) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 4.449202ms)
Mar 20 08:14:51.580: INFO: (2) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 5.381981ms)
Mar 20 08:14:51.580: INFO: (2) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 5.397106ms)
Mar 20 08:14:51.580: INFO: (2) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 5.551517ms)
Mar 20 08:14:51.580: INFO: (2) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 5.489143ms)
Mar 20 08:14:51.580: INFO: (2) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 5.782164ms)
Mar 20 08:14:51.580: INFO: (2) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 5.437919ms)
Mar 20 08:14:51.580: INFO: (2) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 5.540721ms)
Mar 20 08:14:51.580: INFO: (2) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 5.527827ms)
Mar 20 08:14:51.580: INFO: (2) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 5.586424ms)
Mar 20 08:14:51.580: INFO: (2) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 5.511737ms)
Mar 20 08:14:51.581: INFO: (2) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 6.514586ms)
Mar 20 08:14:51.581: INFO: (2) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 6.46815ms)
Mar 20 08:14:51.581: INFO: (2) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 6.566986ms)
Mar 20 08:14:51.581: INFO: (2) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 6.553239ms)
Mar 20 08:14:51.581: INFO: (2) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 6.672275ms)
Mar 20 08:14:51.586: INFO: (3) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.314954ms)
Mar 20 08:14:51.586: INFO: (3) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.41865ms)
Mar 20 08:14:51.586: INFO: (3) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 4.248221ms)
Mar 20 08:14:51.586: INFO: (3) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 4.356772ms)
Mar 20 08:14:51.586: INFO: (3) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 4.642443ms)
Mar 20 08:14:51.586: INFO: (3) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 4.642957ms)
Mar 20 08:14:51.586: INFO: (3) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 4.356195ms)
Mar 20 08:14:51.586: INFO: (3) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.269953ms)
Mar 20 08:14:51.586: INFO: (3) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 4.686466ms)
Mar 20 08:14:51.586: INFO: (3) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.50602ms)
Mar 20 08:14:51.586: INFO: (3) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 4.534527ms)
Mar 20 08:14:51.587: INFO: (3) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 5.738563ms)
Mar 20 08:14:51.588: INFO: (3) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 6.105518ms)
Mar 20 08:14:51.588: INFO: (3) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 6.399673ms)
Mar 20 08:14:51.588: INFO: (3) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 6.533475ms)
Mar 20 08:14:51.588: INFO: (3) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 6.280876ms)
Mar 20 08:14:51.592: INFO: (4) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 3.319813ms)
Mar 20 08:14:51.592: INFO: (4) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 3.521131ms)
Mar 20 08:14:51.593: INFO: (4) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 3.928176ms)
Mar 20 08:14:51.593: INFO: (4) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.471098ms)
Mar 20 08:14:51.593: INFO: (4) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 5.013664ms)
Mar 20 08:14:51.593: INFO: (4) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 5.176347ms)
Mar 20 08:14:51.593: INFO: (4) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 4.735791ms)
Mar 20 08:14:51.593: INFO: (4) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.548123ms)
Mar 20 08:14:51.593: INFO: (4) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 4.069987ms)
Mar 20 08:14:51.594: INFO: (4) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.525905ms)
Mar 20 08:14:51.594: INFO: (4) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 5.524194ms)
Mar 20 08:14:51.594: INFO: (4) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 4.727953ms)
Mar 20 08:14:51.594: INFO: (4) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 4.80103ms)
Mar 20 08:14:51.595: INFO: (4) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 6.107329ms)
Mar 20 08:14:51.595: INFO: (4) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 6.505482ms)
Mar 20 08:14:51.595: INFO: (4) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 6.483854ms)
Mar 20 08:14:51.600: INFO: (5) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.747802ms)
Mar 20 08:14:51.600: INFO: (5) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.540418ms)
Mar 20 08:14:51.600: INFO: (5) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 4.826579ms)
Mar 20 08:14:51.600: INFO: (5) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.594948ms)
Mar 20 08:14:51.600: INFO: (5) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 4.5907ms)
Mar 20 08:14:51.600: INFO: (5) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 4.695269ms)
Mar 20 08:14:51.601: INFO: (5) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.922885ms)
Mar 20 08:14:51.601: INFO: (5) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 5.061625ms)
Mar 20 08:14:51.601: INFO: (5) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 5.146159ms)
Mar 20 08:14:51.601: INFO: (5) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 5.109571ms)
Mar 20 08:14:51.601: INFO: (5) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 5.049165ms)
Mar 20 08:14:51.601: INFO: (5) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 5.522254ms)
Mar 20 08:14:51.602: INFO: (5) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 5.984175ms)
Mar 20 08:14:51.602: INFO: (5) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 6.076091ms)
Mar 20 08:14:51.602: INFO: (5) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 6.010976ms)
Mar 20 08:14:51.602: INFO: (5) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 6.14363ms)
Mar 20 08:14:51.605: INFO: (6) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 2.723051ms)
Mar 20 08:14:51.605: INFO: (6) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 3.409163ms)
Mar 20 08:14:51.605: INFO: (6) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 3.526918ms)
Mar 20 08:14:51.606: INFO: (6) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 3.83415ms)
Mar 20 08:14:51.606: INFO: (6) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.096932ms)
Mar 20 08:14:51.606: INFO: (6) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 4.36624ms)
Mar 20 08:14:51.606: INFO: (6) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 4.062978ms)
Mar 20 08:14:51.606: INFO: (6) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 4.184354ms)
Mar 20 08:14:51.606: INFO: (6) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 4.161181ms)
Mar 20 08:14:51.606: INFO: (6) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.168857ms)
Mar 20 08:14:51.606: INFO: (6) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 4.148624ms)
Mar 20 08:14:51.607: INFO: (6) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 5.093801ms)
Mar 20 08:14:51.607: INFO: (6) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 5.339949ms)
Mar 20 08:14:51.608: INFO: (6) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 5.960005ms)
Mar 20 08:14:51.608: INFO: (6) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 6.093045ms)
Mar 20 08:14:51.608: INFO: (6) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 5.973569ms)
Mar 20 08:14:51.610: INFO: (7) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 2.191566ms)
Mar 20 08:14:51.611: INFO: (7) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 2.550624ms)
Mar 20 08:14:51.612: INFO: (7) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 3.71375ms)
Mar 20 08:14:51.612: INFO: (7) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.002462ms)
Mar 20 08:14:51.612: INFO: (7) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 3.969638ms)
Mar 20 08:14:51.612: INFO: (7) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 3.910801ms)
Mar 20 08:14:51.612: INFO: (7) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 3.887937ms)
Mar 20 08:14:51.612: INFO: (7) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 3.945919ms)
Mar 20 08:14:51.612: INFO: (7) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.143979ms)
Mar 20 08:14:51.612: INFO: (7) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.213629ms)
Mar 20 08:14:51.613: INFO: (7) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 4.139799ms)
Mar 20 08:14:51.614: INFO: (7) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 5.645094ms)
Mar 20 08:14:51.614: INFO: (7) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 6.131728ms)
Mar 20 08:14:51.614: INFO: (7) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 6.143827ms)
Mar 20 08:14:51.614: INFO: (7) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 6.108614ms)
Mar 20 08:14:51.615: INFO: (7) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 6.286428ms)
Mar 20 08:14:51.617: INFO: (8) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 2.389637ms)
Mar 20 08:14:51.618: INFO: (8) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 3.044582ms)
Mar 20 08:14:51.618: INFO: (8) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 2.924417ms)
Mar 20 08:14:51.619: INFO: (8) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 4.005329ms)
Mar 20 08:14:51.619: INFO: (8) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 3.996139ms)
Mar 20 08:14:51.619: INFO: (8) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.079537ms)
Mar 20 08:14:51.619: INFO: (8) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.019456ms)
Mar 20 08:14:51.619: INFO: (8) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 4.157189ms)
Mar 20 08:14:51.619: INFO: (8) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 4.067872ms)
Mar 20 08:14:51.620: INFO: (8) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 4.959562ms)
Mar 20 08:14:51.620: INFO: (8) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 4.915017ms)
Mar 20 08:14:51.620: INFO: (8) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 4.929927ms)
Mar 20 08:14:51.620: INFO: (8) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 5.105306ms)
Mar 20 08:14:51.620: INFO: (8) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 5.215563ms)
Mar 20 08:14:51.620: INFO: (8) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 5.407746ms)
Mar 20 08:14:51.620: INFO: (8) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 5.542144ms)
Mar 20 08:14:51.624: INFO: (9) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 3.34786ms)
Mar 20 08:14:51.626: INFO: (9) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 5.207582ms)
Mar 20 08:14:51.626: INFO: (9) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 5.309413ms)
Mar 20 08:14:51.626: INFO: (9) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 5.178029ms)
Mar 20 08:14:51.626: INFO: (9) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 5.647788ms)
Mar 20 08:14:51.626: INFO: (9) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 5.004798ms)
Mar 20 08:14:51.626: INFO: (9) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 5.261827ms)
Mar 20 08:14:51.626: INFO: (9) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 5.043278ms)
Mar 20 08:14:51.626: INFO: (9) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 5.212152ms)
Mar 20 08:14:51.626: INFO: (9) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 5.358505ms)
Mar 20 08:14:51.626: INFO: (9) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 5.452387ms)
Mar 20 08:14:51.627: INFO: (9) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 6.436185ms)
Mar 20 08:14:51.627: INFO: (9) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 6.484706ms)
Mar 20 08:14:51.627: INFO: (9) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 6.278709ms)
Mar 20 08:14:51.627: INFO: (9) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 6.401173ms)
Mar 20 08:14:51.627: INFO: (9) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 6.263716ms)
Mar 20 08:14:51.632: INFO: (10) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.052848ms)
Mar 20 08:14:51.632: INFO: (10) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 4.221506ms)
Mar 20 08:14:51.632: INFO: (10) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 4.123752ms)
Mar 20 08:14:51.632: INFO: (10) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.144607ms)
Mar 20 08:14:51.632: INFO: (10) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 4.20092ms)
Mar 20 08:14:51.632: INFO: (10) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.076323ms)
Mar 20 08:14:51.632: INFO: (10) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 4.404861ms)
Mar 20 08:14:51.632: INFO: (10) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 4.19823ms)
Mar 20 08:14:51.632: INFO: (10) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 4.374305ms)
Mar 20 08:14:51.632: INFO: (10) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.423407ms)
Mar 20 08:14:51.632: INFO: (10) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 4.529033ms)
Mar 20 08:14:51.633: INFO: (10) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 5.872424ms)
Mar 20 08:14:51.634: INFO: (10) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 6.022458ms)
Mar 20 08:14:51.634: INFO: (10) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 6.061032ms)
Mar 20 08:14:51.634: INFO: (10) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 6.020587ms)
Mar 20 08:14:51.634: INFO: (10) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 6.257873ms)
Mar 20 08:14:51.639: INFO: (11) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 4.749419ms)
Mar 20 08:14:51.639: INFO: (11) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.929433ms)
Mar 20 08:14:51.639: INFO: (11) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 4.750892ms)
Mar 20 08:14:51.639: INFO: (11) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 4.803216ms)
Mar 20 08:14:51.639: INFO: (11) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.751303ms)
Mar 20 08:14:51.639: INFO: (11) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 5.347364ms)
Mar 20 08:14:51.639: INFO: (11) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.816895ms)
Mar 20 08:14:51.639: INFO: (11) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 4.782192ms)
Mar 20 08:14:51.639: INFO: (11) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 5.042474ms)
Mar 20 08:14:51.639: INFO: (11) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 5.011033ms)
Mar 20 08:14:51.640: INFO: (11) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 5.789961ms)
Mar 20 08:14:51.641: INFO: (11) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 6.408295ms)
Mar 20 08:14:51.641: INFO: (11) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 6.483507ms)
Mar 20 08:14:51.641: INFO: (11) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 6.95444ms)
Mar 20 08:14:51.641: INFO: (11) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 6.896656ms)
Mar 20 08:14:51.641: INFO: (11) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 7.152915ms)
Mar 20 08:14:51.646: INFO: (12) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 4.197588ms)
Mar 20 08:14:51.646: INFO: (12) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 3.990368ms)
Mar 20 08:14:51.646: INFO: (12) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 4.087957ms)
Mar 20 08:14:51.646: INFO: (12) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 4.367316ms)
Mar 20 08:14:51.646: INFO: (12) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.138392ms)
Mar 20 08:14:51.647: INFO: (12) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.874423ms)
Mar 20 08:14:51.647: INFO: (12) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 5.033859ms)
Mar 20 08:14:51.647: INFO: (12) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.992301ms)
Mar 20 08:14:51.647: INFO: (12) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 5.028268ms)
Mar 20 08:14:51.647: INFO: (12) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 5.229543ms)
Mar 20 08:14:51.647: INFO: (12) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 5.641858ms)
Mar 20 08:14:51.647: INFO: (12) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 5.81936ms)
Mar 20 08:14:51.648: INFO: (12) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 6.499206ms)
Mar 20 08:14:51.648: INFO: (12) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 6.79793ms)
Mar 20 08:14:51.649: INFO: (12) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 7.006724ms)
Mar 20 08:14:51.650: INFO: (12) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 8.017635ms)
Mar 20 08:14:51.654: INFO: (13) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 3.817928ms)
Mar 20 08:14:51.655: INFO: (13) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 4.882787ms)
Mar 20 08:14:51.655: INFO: (13) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 4.883859ms)
Mar 20 08:14:51.655: INFO: (13) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 4.728415ms)
Mar 20 08:14:51.655: INFO: (13) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.746702ms)
Mar 20 08:14:51.655: INFO: (13) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.726958ms)
Mar 20 08:14:51.655: INFO: (13) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 5.154703ms)
Mar 20 08:14:51.655: INFO: (13) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 4.979357ms)
Mar 20 08:14:51.655: INFO: (13) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 4.86794ms)
Mar 20 08:14:51.655: INFO: (13) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 4.924118ms)
Mar 20 08:14:51.656: INFO: (13) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 6.125982ms)
Mar 20 08:14:51.656: INFO: (13) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 6.609033ms)
Mar 20 08:14:51.656: INFO: (13) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 6.162953ms)
Mar 20 08:14:51.656: INFO: (13) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 6.601221ms)
Mar 20 08:14:51.656: INFO: (13) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 6.248567ms)
Mar 20 08:14:51.657: INFO: (13) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 6.582066ms)
Mar 20 08:14:51.661: INFO: (14) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 3.798671ms)
Mar 20 08:14:51.661: INFO: (14) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 3.738729ms)
Mar 20 08:14:51.661: INFO: (14) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 3.961153ms)
Mar 20 08:14:51.661: INFO: (14) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.025935ms)
Mar 20 08:14:51.661: INFO: (14) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 3.887792ms)
Mar 20 08:14:51.661: INFO: (14) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 3.748504ms)
Mar 20 08:14:51.661: INFO: (14) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.128167ms)
Mar 20 08:14:51.661: INFO: (14) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 3.970127ms)
Mar 20 08:14:51.661: INFO: (14) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 4.104456ms)
Mar 20 08:14:51.661: INFO: (14) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 3.836292ms)
Mar 20 08:14:51.662: INFO: (14) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 5.232732ms)
Mar 20 08:14:51.663: INFO: (14) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 5.950333ms)
Mar 20 08:14:51.663: INFO: (14) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 5.322902ms)
Mar 20 08:14:51.663: INFO: (14) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 5.886692ms)
Mar 20 08:14:51.663: INFO: (14) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 5.317322ms)
Mar 20 08:14:51.663: INFO: (14) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 5.479366ms)
Mar 20 08:14:51.667: INFO: (15) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 3.542607ms)
Mar 20 08:14:51.667: INFO: (15) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 3.675592ms)
Mar 20 08:14:51.667: INFO: (15) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 3.778725ms)
Mar 20 08:14:51.667: INFO: (15) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 3.811005ms)
Mar 20 08:14:51.667: INFO: (15) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.326505ms)
Mar 20 08:14:51.667: INFO: (15) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 3.873299ms)
Mar 20 08:14:51.667: INFO: (15) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 4.108725ms)
Mar 20 08:14:51.667: INFO: (15) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 3.835216ms)
Mar 20 08:14:51.667: INFO: (15) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 4.133431ms)
Mar 20 08:14:51.668: INFO: (15) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 4.772405ms)
Mar 20 08:14:51.668: INFO: (15) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 5.022155ms)
Mar 20 08:14:51.668: INFO: (15) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 5.270907ms)
Mar 20 08:14:51.668: INFO: (15) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 5.206331ms)
Mar 20 08:14:51.669: INFO: (15) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 5.618549ms)
Mar 20 08:14:51.669: INFO: (15) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 5.640714ms)
Mar 20 08:14:51.669: INFO: (15) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 5.712238ms)
Mar 20 08:14:51.673: INFO: (16) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 4.181003ms)
Mar 20 08:14:51.673: INFO: (16) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.131197ms)
Mar 20 08:14:51.673: INFO: (16) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 4.232398ms)
Mar 20 08:14:51.674: INFO: (16) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.529456ms)
Mar 20 08:14:51.674: INFO: (16) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 4.771897ms)
Mar 20 08:14:51.674: INFO: (16) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.685762ms)
Mar 20 08:14:51.674: INFO: (16) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 5.043488ms)
Mar 20 08:14:51.674: INFO: (16) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 4.994477ms)
Mar 20 08:14:51.674: INFO: (16) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 5.077232ms)
Mar 20 08:14:51.674: INFO: (16) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 5.079637ms)
Mar 20 08:14:51.674: INFO: (16) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 5.184934ms)
Mar 20 08:14:51.674: INFO: (16) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 5.174173ms)
Mar 20 08:14:51.675: INFO: (16) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 5.401892ms)
Mar 20 08:14:51.675: INFO: (16) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 5.628773ms)
Mar 20 08:14:51.675: INFO: (16) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 6.184089ms)
Mar 20 08:14:51.676: INFO: (16) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 6.810145ms)
Mar 20 08:14:51.681: INFO: (17) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.904403ms)
Mar 20 08:14:51.681: INFO: (17) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 5.110821ms)
Mar 20 08:14:51.681: INFO: (17) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 5.389059ms)
Mar 20 08:14:51.681: INFO: (17) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 5.351859ms)
Mar 20 08:14:51.681: INFO: (17) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 5.309418ms)
Mar 20 08:14:51.682: INFO: (17) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 5.299999ms)
Mar 20 08:14:51.682: INFO: (17) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 5.346513ms)
Mar 20 08:14:51.682: INFO: (17) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 5.428323ms)
Mar 20 08:14:51.682: INFO: (17) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 5.436487ms)
Mar 20 08:14:51.682: INFO: (17) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 5.575343ms)
Mar 20 08:14:51.682: INFO: (17) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 5.356815ms)
Mar 20 08:14:51.683: INFO: (17) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 6.554209ms)
Mar 20 08:14:51.683: INFO: (17) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 6.379811ms)
Mar 20 08:14:51.683: INFO: (17) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 6.453407ms)
Mar 20 08:14:51.683: INFO: (17) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 6.624602ms)
Mar 20 08:14:51.683: INFO: (17) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 6.568445ms)
Mar 20 08:14:51.687: INFO: (18) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 3.611791ms)
Mar 20 08:14:51.687: INFO: (18) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 3.694261ms)
Mar 20 08:14:51.687: INFO: (18) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 4.112411ms)
Mar 20 08:14:51.687: INFO: (18) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 3.973288ms)
Mar 20 08:14:51.687: INFO: (18) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 4.06471ms)
Mar 20 08:14:51.687: INFO: (18) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 4.175233ms)
Mar 20 08:14:51.687: INFO: (18) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.068954ms)
Mar 20 08:14:51.687: INFO: (18) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 4.003445ms)
Mar 20 08:14:51.687: INFO: (18) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 4.589748ms)
Mar 20 08:14:51.687: INFO: (18) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 4.314037ms)
Mar 20 08:14:51.687: INFO: (18) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 4.452075ms)
Mar 20 08:14:51.688: INFO: (18) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 5.479167ms)
Mar 20 08:14:51.689: INFO: (18) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 6.369959ms)
Mar 20 08:14:51.690: INFO: (18) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 6.561626ms)
Mar 20 08:14:51.690: INFO: (18) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 6.642661ms)
Mar 20 08:14:51.690: INFO: (18) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 6.494428ms)
Mar 20 08:14:51.692: INFO: (19) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 2.474289ms)
Mar 20 08:14:51.694: INFO: (19) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:462/proxy/: tls qux (200; 3.480556ms)
Mar 20 08:14:51.695: INFO: (19) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr/proxy/rewriteme">test</a> (200; 4.11943ms)
Mar 20 08:14:51.695: INFO: (19) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 3.953167ms)
Mar 20 08:14:51.695: INFO: (19) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:160/proxy/: foo (200; 3.991191ms)
Mar 20 08:14:51.696: INFO: (19) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">... (200; 4.613287ms)
Mar 20 08:14:51.696: INFO: (19) /api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/proxy-service-c2h5b-rhhlr:1080/proxy/rewriteme">test<... (200; 4.761087ms)
Mar 20 08:14:51.696: INFO: (19) /api/v1/namespaces/proxy-8439/pods/http:proxy-service-c2h5b-rhhlr:162/proxy/: bar (200; 5.072613ms)
Mar 20 08:14:51.696: INFO: (19) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:460/proxy/: tls baz (200; 4.733312ms)
Mar 20 08:14:51.696: INFO: (19) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname1/proxy/: tls baz (200; 5.884108ms)
Mar 20 08:14:51.696: INFO: (19) /api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/: <a href="/api/v1/namespaces/proxy-8439/pods/https:proxy-service-c2h5b-rhhlr:443/proxy/tlsrewritem... (200; 4.868274ms)
Mar 20 08:14:51.697: INFO: (19) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname1/proxy/: foo (200; 6.035624ms)
Mar 20 08:14:51.697: INFO: (19) /api/v1/namespaces/proxy-8439/services/https:proxy-service-c2h5b:tlsportname2/proxy/: tls qux (200; 6.317982ms)
Mar 20 08:14:51.697: INFO: (19) /api/v1/namespaces/proxy-8439/services/http:proxy-service-c2h5b:portname2/proxy/: bar (200; 6.408944ms)
Mar 20 08:14:51.697: INFO: (19) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname2/proxy/: bar (200; 6.895503ms)
Mar 20 08:14:51.697: INFO: (19) /api/v1/namespaces/proxy-8439/services/proxy-service-c2h5b:portname1/proxy/: foo (200; 7.041588ms)
STEP: deleting ReplicationController proxy-service-c2h5b in namespace proxy-8439, will wait for the garbage collector to delete the pods
Mar 20 08:14:51.759: INFO: Deleting ReplicationController proxy-service-c2h5b took: 8.861578ms
Mar 20 08:14:51.860: INFO: Terminating ReplicationController proxy-service-c2h5b pods took: 101.125092ms
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Mar 20 08:14:54.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8439" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":356,"completed":189,"skipped":3493,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:14:54.374: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service endpoint-test2 in namespace services-2602
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2602 to expose endpoints map[]
Mar 20 08:14:54.412: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Mar 20 08:14:55.421: INFO: successfully validated that service endpoint-test2 in namespace services-2602 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2602
Mar 20 08:14:55.438: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:14:57.445: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2602 to expose endpoints map[pod1:[80]]
Mar 20 08:14:57.460: INFO: successfully validated that service endpoint-test2 in namespace services-2602 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Mar 20 08:14:57.460: INFO: Creating new exec pod
Mar 20 08:15:00.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-2602 exec execpodscr4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar 20 08:15:00.678: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 20 08:15:00.678: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:15:00.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-2602 exec execpodscr4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.219.38 80'
Mar 20 08:15:00.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.219.38 80\nConnection to 192.168.219.38 80 port [tcp/http] succeeded!\n"
Mar 20 08:15:00.851: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-2602
Mar 20 08:15:00.865: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:15:02.880: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2602 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 20 08:15:02.894: INFO: successfully validated that service endpoint-test2 in namespace services-2602 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Mar 20 08:15:03.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-2602 exec execpodscr4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar 20 08:15:04.054: INFO: stderr: "+ + ncecho -v -t -w hostName 2\n endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 20 08:15:04.054: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:15:04.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-2602 exec execpodscr4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.219.38 80'
Mar 20 08:15:04.213: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.219.38 80\nConnection to 192.168.219.38 80 port [tcp/http] succeeded!\n"
Mar 20 08:15:04.213: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-2602
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2602 to expose endpoints map[pod2:[80]]
Mar 20 08:15:05.251: INFO: successfully validated that service endpoint-test2 in namespace services-2602 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Mar 20 08:15:06.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-2602 exec execpodscr4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar 20 08:15:06.417: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 20 08:15:06.417: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:15:06.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-2602 exec execpodscr4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.219.38 80'
Mar 20 08:15:06.568: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.219.38 80\nConnection to 192.168.219.38 80 port [tcp/http] succeeded!\n"
Mar 20 08:15:06.568: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-2602
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2602 to expose endpoints map[]
Mar 20 08:15:07.603: INFO: successfully validated that service endpoint-test2 in namespace services-2602 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 08:15:07.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2602" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:13.255 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":356,"completed":190,"skipped":3503,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:15:07.629: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 20 08:15:07.671: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 20 08:16:07.780: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:16:07.788: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:16:07.876: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Mar 20 08:16:07.879: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:188
Mar 20 08:16:07.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5914" for this suite.
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Mar 20 08:16:07.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7263" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:60.333 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":356,"completed":191,"skipped":3505,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:16:07.962: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod test-webserver-8409ac85-81c2-48fd-bb99-7032037b9108 in namespace container-probe-6497
Mar 20 08:16:10.003: INFO: Started pod test-webserver-8409ac85-81c2-48fd-bb99-7032037b9108 in namespace container-probe-6497
STEP: checking the pod's current state and verifying that restartCount is present
Mar 20 08:16:10.006: INFO: Initial restart count of pod test-webserver-8409ac85-81c2-48fd-bb99-7032037b9108 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar 20 08:20:11.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6497" for this suite.

• [SLOW TEST:243.306 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":192,"skipped":3532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:20:11.269: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 20 08:20:11.325: INFO: Waiting up to 5m0s for pod "pod-8fd3deaf-f64c-4291-a658-de5ff39f12c9" in namespace "emptydir-599" to be "Succeeded or Failed"
Mar 20 08:20:11.328: INFO: Pod "pod-8fd3deaf-f64c-4291-a658-de5ff39f12c9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.255376ms
Mar 20 08:20:13.335: INFO: Pod "pod-8fd3deaf-f64c-4291-a658-de5ff39f12c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010376004s
Mar 20 08:20:15.350: INFO: Pod "pod-8fd3deaf-f64c-4291-a658-de5ff39f12c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025445356s
STEP: Saw pod success
Mar 20 08:20:15.350: INFO: Pod "pod-8fd3deaf-f64c-4291-a658-de5ff39f12c9" satisfied condition "Succeeded or Failed"
Mar 20 08:20:15.354: INFO: Trying to get logs from node env016ar130-worker01 pod pod-8fd3deaf-f64c-4291-a658-de5ff39f12c9 container test-container: <nil>
STEP: delete the pod
Mar 20 08:20:15.386: INFO: Waiting for pod pod-8fd3deaf-f64c-4291-a658-de5ff39f12c9 to disappear
Mar 20 08:20:15.389: INFO: Pod pod-8fd3deaf-f64c-4291-a658-de5ff39f12c9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar 20 08:20:15.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-599" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":193,"skipped":3566,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:20:15.398: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:20:15.424: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Mar 20 08:20:20.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-9891 --namespace=crd-publish-openapi-9891 create -f -'
Mar 20 08:20:21.465: INFO: stderr: ""
Mar 20 08:20:21.465: INFO: stdout: "e2e-test-crd-publish-openapi-7502-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 20 08:20:21.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-9891 --namespace=crd-publish-openapi-9891 delete e2e-test-crd-publish-openapi-7502-crds test-cr'
Mar 20 08:20:21.544: INFO: stderr: ""
Mar 20 08:20:21.545: INFO: stdout: "e2e-test-crd-publish-openapi-7502-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 20 08:20:21.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-9891 --namespace=crd-publish-openapi-9891 apply -f -'
Mar 20 08:20:22.274: INFO: stderr: ""
Mar 20 08:20:22.274: INFO: stdout: "e2e-test-crd-publish-openapi-7502-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 20 08:20:22.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-9891 --namespace=crd-publish-openapi-9891 delete e2e-test-crd-publish-openapi-7502-crds test-cr'
Mar 20 08:20:22.337: INFO: stderr: ""
Mar 20 08:20:22.337: INFO: stdout: "e2e-test-crd-publish-openapi-7502-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 20 08:20:22.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-9891 explain e2e-test-crd-publish-openapi-7502-crds'
Mar 20 08:20:22.543: INFO: stderr: ""
Mar 20 08:20:22.544: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7502-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 08:20:27.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9891" for this suite.

• [SLOW TEST:12.066 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":356,"completed":194,"skipped":3566,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:20:27.464: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Mar 20 08:20:27.520: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Mar 20 08:20:30.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7028" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":356,"completed":195,"skipped":3590,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:20:30.896: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
STEP: create deployment with httpd image
Mar 20 08:20:30.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-4280 create -f -'
Mar 20 08:20:31.721: INFO: stderr: ""
Mar 20 08:20:31.721: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Mar 20 08:20:31.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-4280 diff -f -'
Mar 20 08:20:31.964: INFO: rc: 1
Mar 20 08:20:31.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-4280 delete -f -'
Mar 20 08:20:32.029: INFO: stderr: ""
Mar 20 08:20:32.029: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 08:20:32.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4280" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":356,"completed":196,"skipped":3592,"failed":0}
S
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:20:32.047: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Mar 20 08:20:58.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8870" for this suite.

• [SLOW TEST:26.414 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":356,"completed":197,"skipped":3593,"failed":0}
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:20:58.461: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 20 08:20:58.503: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 20 08:21:03.512: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Mar 20 08:21:04.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6048" for this suite.

• [SLOW TEST:6.096 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":356,"completed":198,"skipped":3593,"failed":0}
SSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:21:04.557: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-445cdac9-7428-4b21-ba66-d5c0f84220bc in namespace container-probe-498
Mar 20 08:21:06.617: INFO: Started pod busybox-445cdac9-7428-4b21-ba66-d5c0f84220bc in namespace container-probe-498
STEP: checking the pod's current state and verifying that restartCount is present
Mar 20 08:21:06.621: INFO: Initial restart count of pod busybox-445cdac9-7428-4b21-ba66-d5c0f84220bc is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar 20 08:25:07.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-498" for this suite.

• [SLOW TEST:243.424 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":199,"skipped":3598,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:25:07.982: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Mar 20 08:25:10.052: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Mar 20 08:25:12.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5093" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":356,"completed":200,"skipped":3643,"failed":0}
S
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:25:12.114: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-385fb6d7-07e2-4fe0-b750-9dad0b220934
STEP: Creating a pod to test consume secrets
Mar 20 08:25:12.157: INFO: Waiting up to 5m0s for pod "pod-secrets-70efccf4-a79a-49a9-b99b-b7a596116a3a" in namespace "secrets-7873" to be "Succeeded or Failed"
Mar 20 08:25:12.159: INFO: Pod "pod-secrets-70efccf4-a79a-49a9-b99b-b7a596116a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.63578ms
Mar 20 08:25:14.171: INFO: Pod "pod-secrets-70efccf4-a79a-49a9-b99b-b7a596116a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01430169s
Mar 20 08:25:16.186: INFO: Pod "pod-secrets-70efccf4-a79a-49a9-b99b-b7a596116a3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028998939s
STEP: Saw pod success
Mar 20 08:25:16.186: INFO: Pod "pod-secrets-70efccf4-a79a-49a9-b99b-b7a596116a3a" satisfied condition "Succeeded or Failed"
Mar 20 08:25:16.190: INFO: Trying to get logs from node env016ar130-worker01 pod pod-secrets-70efccf4-a79a-49a9-b99b-b7a596116a3a container secret-env-test: <nil>
STEP: delete the pod
Mar 20 08:25:16.230: INFO: Waiting for pod pod-secrets-70efccf4-a79a-49a9-b99b-b7a596116a3a to disappear
Mar 20 08:25:16.233: INFO: Pod pod-secrets-70efccf4-a79a-49a9-b99b-b7a596116a3a no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Mar 20 08:25:16.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7873" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":356,"completed":201,"skipped":3644,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:25:16.241: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-c1975438-37c4-4dd3-aa7c-e5c900124583
STEP: Creating a pod to test consume configMaps
Mar 20 08:25:16.340: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-092e359a-dff6-4808-8732-f9bf78cced09" in namespace "projected-5861" to be "Succeeded or Failed"
Mar 20 08:25:16.344: INFO: Pod "pod-projected-configmaps-092e359a-dff6-4808-8732-f9bf78cced09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.351708ms
Mar 20 08:25:18.353: INFO: Pod "pod-projected-configmaps-092e359a-dff6-4808-8732-f9bf78cced09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01312655s
Mar 20 08:25:20.368: INFO: Pod "pod-projected-configmaps-092e359a-dff6-4808-8732-f9bf78cced09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027834886s
STEP: Saw pod success
Mar 20 08:25:20.368: INFO: Pod "pod-projected-configmaps-092e359a-dff6-4808-8732-f9bf78cced09" satisfied condition "Succeeded or Failed"
Mar 20 08:25:20.371: INFO: Trying to get logs from node env016ar130-worker01 pod pod-projected-configmaps-092e359a-dff6-4808-8732-f9bf78cced09 container agnhost-container: <nil>
STEP: delete the pod
Mar 20 08:25:20.389: INFO: Waiting for pod pod-projected-configmaps-092e359a-dff6-4808-8732-f9bf78cced09 to disappear
Mar 20 08:25:20.392: INFO: Pod pod-projected-configmaps-092e359a-dff6-4808-8732-f9bf78cced09 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar 20 08:25:20.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5861" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":202,"skipped":3690,"failed":0}
S
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:25:20.403: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:25:20.432: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-6220
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:188
Mar 20 08:25:22.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-8761" for this suite.
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Mar 20 08:25:22.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6220" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":356,"completed":203,"skipped":3691,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:25:22.538: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replication controller my-hostname-basic-8f788751-1797-4010-af4a-bc9ea5831cd6
Mar 20 08:25:22.568: INFO: Pod name my-hostname-basic-8f788751-1797-4010-af4a-bc9ea5831cd6: Found 0 pods out of 1
Mar 20 08:25:27.582: INFO: Pod name my-hostname-basic-8f788751-1797-4010-af4a-bc9ea5831cd6: Found 1 pods out of 1
Mar 20 08:25:27.582: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-8f788751-1797-4010-af4a-bc9ea5831cd6" are running
Mar 20 08:25:27.585: INFO: Pod "my-hostname-basic-8f788751-1797-4010-af4a-bc9ea5831cd6-pl6gg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-20 08:25:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-20 08:25:23 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-20 08:25:23 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-20 08:25:22 +0000 UTC Reason: Message:}])
Mar 20 08:25:27.585: INFO: Trying to dial the pod
Mar 20 08:25:32.605: INFO: Controller my-hostname-basic-8f788751-1797-4010-af4a-bc9ea5831cd6: Got expected result from replica 1 [my-hostname-basic-8f788751-1797-4010-af4a-bc9ea5831cd6-pl6gg]: "my-hostname-basic-8f788751-1797-4010-af4a-bc9ea5831cd6-pl6gg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Mar 20 08:25:32.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9806" for this suite.

• [SLOW TEST:10.084 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":204,"skipped":3700,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:25:32.622: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-342
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 20 08:25:32.691: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 20 08:25:32.740: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:25:34.749: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 08:25:36.753: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 08:25:38.746: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 08:25:40.745: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 08:25:42.750: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 08:25:44.756: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 20 08:25:44.764: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 20 08:25:44.771: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 20 08:25:46.823: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 20 08:25:46.823: INFO: Going to poll 192.168.90.87 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 20 08:25:46.827: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.90.87 8081 | grep -v '^\s*$'] Namespace:pod-network-test-342 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 08:25:46.827: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 08:25:46.828: INFO: ExecWithOptions: Clientset creation
Mar 20 08:25:46.828: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/pod-network-test-342/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.90.87+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 20 08:25:47.932: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 20 08:25:47.932: INFO: Going to poll 192.168.12.7 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 20 08:25:47.938: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.12.7 8081 | grep -v '^\s*$'] Namespace:pod-network-test-342 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 08:25:47.938: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 08:25:47.939: INFO: ExecWithOptions: Clientset creation
Mar 20 08:25:47.939: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/pod-network-test-342/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.12.7+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 20 08:25:49.030: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 20 08:25:49.030: INFO: Going to poll 192.168.71.162 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 20 08:25:49.036: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.71.162 8081 | grep -v '^\s*$'] Namespace:pod-network-test-342 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 08:25:49.036: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 08:25:49.037: INFO: ExecWithOptions: Clientset creation
Mar 20 08:25:49.037: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/pod-network-test-342/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.71.162+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 20 08:25:50.127: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Mar 20 08:25:50.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-342" for this suite.

• [SLOW TEST:17.526 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":205,"skipped":3710,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:25:50.148: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 20 08:25:50.699: INFO: starting watch
STEP: patching
STEP: updating
Mar 20 08:25:50.722: INFO: waiting for watch events with expected annotations
Mar 20 08:25:50.723: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 08:25:50.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-1562" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":356,"completed":206,"skipped":3734,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:25:50.781: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:25:50.809: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar 20 08:25:50.825: INFO: The status of Pod pod-logs-websocket-88b65026-ef08-4844-840e-796ba028ebc5 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:25:52.839: INFO: The status of Pod pod-logs-websocket-88b65026-ef08-4844-840e-796ba028ebc5 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar 20 08:25:52.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5221" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":356,"completed":207,"skipped":3738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:25:52.903: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 20 08:25:56.989: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Mar 20 08:25:57.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3814" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":208,"skipped":3787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:25:57.017: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 08:25:57.055: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd2d5618-d19d-44af-9270-107b644a0bde" in namespace "projected-535" to be "Succeeded or Failed"
Mar 20 08:25:57.060: INFO: Pod "downwardapi-volume-fd2d5618-d19d-44af-9270-107b644a0bde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.794534ms
Mar 20 08:25:59.067: INFO: Pod "downwardapi-volume-fd2d5618-d19d-44af-9270-107b644a0bde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011994834s
Mar 20 08:26:01.081: INFO: Pod "downwardapi-volume-fd2d5618-d19d-44af-9270-107b644a0bde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025524172s
STEP: Saw pod success
Mar 20 08:26:01.081: INFO: Pod "downwardapi-volume-fd2d5618-d19d-44af-9270-107b644a0bde" satisfied condition "Succeeded or Failed"
Mar 20 08:26:01.084: INFO: Trying to get logs from node env016ar130-worker02 pod downwardapi-volume-fd2d5618-d19d-44af-9270-107b644a0bde container client-container: <nil>
STEP: delete the pod
Mar 20 08:26:01.110: INFO: Waiting for pod downwardapi-volume-fd2d5618-d19d-44af-9270-107b644a0bde to disappear
Mar 20 08:26:01.113: INFO: Pod downwardapi-volume-fd2d5618-d19d-44af-9270-107b644a0bde no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar 20 08:26:01.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-535" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":209,"skipped":3834,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:26:01.123: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating pod
Mar 20 08:26:01.173: INFO: The status of Pod pod-hostip-e82539c5-6e8d-431a-9003-f3069749dfb4 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:26:03.178: INFO: The status of Pod pod-hostip-e82539c5-6e8d-431a-9003-f3069749dfb4 is Running (Ready = true)
Mar 20 08:26:03.187: INFO: Pod pod-hostip-e82539c5-6e8d-431a-9003-f3069749dfb4 has hostIP: 10.2.10.71
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar 20 08:26:03.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2359" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":356,"completed":210,"skipped":3872,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:26:03.204: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:26:03.275: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar 20 08:26:03.290: INFO: The status of Pod pod-exec-websocket-74b9548c-2cf7-4726-954d-d06d6b83945c is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:26:05.300: INFO: The status of Pod pod-exec-websocket-74b9548c-2cf7-4726-954d-d06d6b83945c is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar 20 08:26:05.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7416" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":356,"completed":211,"skipped":3883,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:26:05.408: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar 20 08:26:05.452: INFO: The status of Pod pod-update-1feea2db-f1f9-44c3-82cc-d7c527f55cd4 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:26:07.466: INFO: The status of Pod pod-update-1feea2db-f1f9-44c3-82cc-d7c527f55cd4 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 20 08:26:08.001: INFO: Successfully updated pod "pod-update-1feea2db-f1f9-44c3-82cc-d7c527f55cd4"
STEP: verifying the updated pod is in kubernetes
Mar 20 08:26:08.009: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar 20 08:26:08.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7130" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":356,"completed":212,"skipped":3906,"failed":0}
S
------------------------------
[sig-architecture] Conformance Tests 
  should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:26:08.020: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename conformance-tests
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
STEP: Getting node addresses
Mar 20 08:26:08.053: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:188
Mar 20 08:26:08.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-8528" for this suite.
•{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","total":356,"completed":213,"skipped":3907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:26:08.075: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-4265
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 20 08:26:08.101: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 20 08:26:08.146: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:26:10.159: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 08:26:12.158: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 08:26:14.155: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 08:26:16.159: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 08:26:18.154: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 20 08:26:20.159: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 20 08:26:20.167: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 20 08:26:22.181: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 20 08:26:24.179: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 20 08:26:26.182: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 20 08:26:28.175: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 20 08:26:30.177: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 20 08:26:30.185: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 20 08:26:32.223: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 20 08:26:32.223: INFO: Breadth first check of 192.168.90.127 on host 10.2.10.71...
Mar 20 08:26:32.226: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.90.122:9080/dial?request=hostname&protocol=udp&host=192.168.90.127&port=8081&tries=1'] Namespace:pod-network-test-4265 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 08:26:32.226: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 08:26:32.227: INFO: ExecWithOptions: Clientset creation
Mar 20 08:26:32.227: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/pod-network-test-4265/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.90.122%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.90.127%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 20 08:26:32.335: INFO: Waiting for responses: map[]
Mar 20 08:26:32.335: INFO: reached 192.168.90.127 after 0/1 tries
Mar 20 08:26:32.335: INFO: Breadth first check of 192.168.12.20 on host 10.2.10.72...
Mar 20 08:26:32.340: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.90.122:9080/dial?request=hostname&protocol=udp&host=192.168.12.20&port=8081&tries=1'] Namespace:pod-network-test-4265 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 08:26:32.340: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 08:26:32.342: INFO: ExecWithOptions: Clientset creation
Mar 20 08:26:32.342: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/pod-network-test-4265/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.90.122%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.12.20%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 20 08:26:32.433: INFO: Waiting for responses: map[]
Mar 20 08:26:32.433: INFO: reached 192.168.12.20 after 0/1 tries
Mar 20 08:26:32.433: INFO: Breadth first check of 192.168.71.155 on host 10.2.10.73...
Mar 20 08:26:32.437: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.90.122:9080/dial?request=hostname&protocol=udp&host=192.168.71.155&port=8081&tries=1'] Namespace:pod-network-test-4265 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 08:26:32.437: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 08:26:32.438: INFO: ExecWithOptions: Clientset creation
Mar 20 08:26:32.438: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/pod-network-test-4265/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.90.122%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.71.155%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 20 08:26:32.540: INFO: Waiting for responses: map[]
Mar 20 08:26:32.540: INFO: reached 192.168.71.155 after 0/1 tries
Mar 20 08:26:32.540: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Mar 20 08:26:32.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4265" for this suite.

• [SLOW TEST:24.482 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":356,"completed":214,"skipped":3944,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:26:32.558: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1187.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1187.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1187.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1187.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 20 08:26:34.627: INFO: DNS probes using dns-1187/dns-test-3563182c-1d9b-4b1e-8f2f-8a9816bb87b0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar 20 08:26:34.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1187" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","total":356,"completed":215,"skipped":3946,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:26:34.654: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Indexed job
STEP: Ensuring job reaches completions
STEP: Ensuring pods with index for job exist
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Mar 20 08:26:44.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-519" for this suite.

• [SLOW TEST:10.065 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","total":356,"completed":216,"skipped":3957,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:26:44.719: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 08:26:44.760: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b4c69550-d554-4a21-bdd6-fa7d789fbe54" in namespace "downward-api-6708" to be "Succeeded or Failed"
Mar 20 08:26:44.764: INFO: Pod "downwardapi-volume-b4c69550-d554-4a21-bdd6-fa7d789fbe54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.306862ms
Mar 20 08:26:46.779: INFO: Pod "downwardapi-volume-b4c69550-d554-4a21-bdd6-fa7d789fbe54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019565171s
Mar 20 08:26:48.789: INFO: Pod "downwardapi-volume-b4c69550-d554-4a21-bdd6-fa7d789fbe54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029460748s
STEP: Saw pod success
Mar 20 08:26:48.789: INFO: Pod "downwardapi-volume-b4c69550-d554-4a21-bdd6-fa7d789fbe54" satisfied condition "Succeeded or Failed"
Mar 20 08:26:48.793: INFO: Trying to get logs from node env016ar130-worker02 pod downwardapi-volume-b4c69550-d554-4a21-bdd6-fa7d789fbe54 container client-container: <nil>
STEP: delete the pod
Mar 20 08:26:48.817: INFO: Waiting for pod downwardapi-volume-b4c69550-d554-4a21-bdd6-fa7d789fbe54 to disappear
Mar 20 08:26:48.821: INFO: Pod downwardapi-volume-b4c69550-d554-4a21-bdd6-fa7d789fbe54 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar 20 08:26:48.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6708" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":217,"skipped":3964,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:26:48.832: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Mar 20 08:26:50.914: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Mar 20 08:26:52.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8863" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":356,"completed":218,"skipped":3974,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:26:52.938: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:26:52.977: INFO: The status of Pod test-webserver-a0837ef1-ca92-4f2f-af15-54d7af9c67a1 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:26:54.992: INFO: The status of Pod test-webserver-a0837ef1-ca92-4f2f-af15-54d7af9c67a1 is Running (Ready = false)
Mar 20 08:26:56.990: INFO: The status of Pod test-webserver-a0837ef1-ca92-4f2f-af15-54d7af9c67a1 is Running (Ready = false)
Mar 20 08:26:58.983: INFO: The status of Pod test-webserver-a0837ef1-ca92-4f2f-af15-54d7af9c67a1 is Running (Ready = false)
Mar 20 08:27:00.991: INFO: The status of Pod test-webserver-a0837ef1-ca92-4f2f-af15-54d7af9c67a1 is Running (Ready = false)
Mar 20 08:27:02.983: INFO: The status of Pod test-webserver-a0837ef1-ca92-4f2f-af15-54d7af9c67a1 is Running (Ready = false)
Mar 20 08:27:04.985: INFO: The status of Pod test-webserver-a0837ef1-ca92-4f2f-af15-54d7af9c67a1 is Running (Ready = false)
Mar 20 08:27:06.993: INFO: The status of Pod test-webserver-a0837ef1-ca92-4f2f-af15-54d7af9c67a1 is Running (Ready = false)
Mar 20 08:27:08.988: INFO: The status of Pod test-webserver-a0837ef1-ca92-4f2f-af15-54d7af9c67a1 is Running (Ready = false)
Mar 20 08:27:10.990: INFO: The status of Pod test-webserver-a0837ef1-ca92-4f2f-af15-54d7af9c67a1 is Running (Ready = false)
Mar 20 08:27:12.984: INFO: The status of Pod test-webserver-a0837ef1-ca92-4f2f-af15-54d7af9c67a1 is Running (Ready = false)
Mar 20 08:27:14.985: INFO: The status of Pod test-webserver-a0837ef1-ca92-4f2f-af15-54d7af9c67a1 is Running (Ready = true)
Mar 20 08:27:14.988: INFO: Container started at 2023-03-20 08:26:53 +0000 UTC, pod became ready at 2023-03-20 08:27:13 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar 20 08:27:14.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4806" for this suite.

• [SLOW TEST:22.062 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":356,"completed":219,"skipped":4018,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:27:15.000: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Mar 20 08:27:19.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5473" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":220,"skipped":4062,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:27:19.097: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 08:27:19.131: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25d828e7-2f89-4749-aef0-83b44f206b8b" in namespace "downward-api-7464" to be "Succeeded or Failed"
Mar 20 08:27:19.134: INFO: Pod "downwardapi-volume-25d828e7-2f89-4749-aef0-83b44f206b8b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.09208ms
Mar 20 08:27:21.146: INFO: Pod "downwardapi-volume-25d828e7-2f89-4749-aef0-83b44f206b8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01546435s
Mar 20 08:27:23.156: INFO: Pod "downwardapi-volume-25d828e7-2f89-4749-aef0-83b44f206b8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02532842s
STEP: Saw pod success
Mar 20 08:27:23.156: INFO: Pod "downwardapi-volume-25d828e7-2f89-4749-aef0-83b44f206b8b" satisfied condition "Succeeded or Failed"
Mar 20 08:27:23.160: INFO: Trying to get logs from node env016ar130-worker02 pod downwardapi-volume-25d828e7-2f89-4749-aef0-83b44f206b8b container client-container: <nil>
STEP: delete the pod
Mar 20 08:27:23.181: INFO: Waiting for pod downwardapi-volume-25d828e7-2f89-4749-aef0-83b44f206b8b to disappear
Mar 20 08:27:23.186: INFO: Pod downwardapi-volume-25d828e7-2f89-4749-aef0-83b44f206b8b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar 20 08:27:23.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7464" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":221,"skipped":4069,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:27:23.196: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Mar 20 08:27:23.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-152" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":356,"completed":222,"skipped":4081,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:27:23.276: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3463.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3463.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3463.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3463.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 20 08:27:27.345: INFO: DNS probes using dns-test-74d1eb39-d0b4-4e28-8e9d-4abf55d5d450 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3463.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3463.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3463.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3463.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 20 08:27:31.407: INFO: File wheezy_udp@dns-test-service-3.dns-3463.svc.cluster.local from pod  dns-3463/dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 20 08:27:31.411: INFO: File jessie_udp@dns-test-service-3.dns-3463.svc.cluster.local from pod  dns-3463/dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 20 08:27:31.411: INFO: Lookups using dns-3463/dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 failed for: [wheezy_udp@dns-test-service-3.dns-3463.svc.cluster.local jessie_udp@dns-test-service-3.dns-3463.svc.cluster.local]

Mar 20 08:27:36.420: INFO: File wheezy_udp@dns-test-service-3.dns-3463.svc.cluster.local from pod  dns-3463/dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 20 08:27:36.425: INFO: File jessie_udp@dns-test-service-3.dns-3463.svc.cluster.local from pod  dns-3463/dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 20 08:27:36.425: INFO: Lookups using dns-3463/dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 failed for: [wheezy_udp@dns-test-service-3.dns-3463.svc.cluster.local jessie_udp@dns-test-service-3.dns-3463.svc.cluster.local]

Mar 20 08:27:41.417: INFO: File wheezy_udp@dns-test-service-3.dns-3463.svc.cluster.local from pod  dns-3463/dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 20 08:27:41.422: INFO: File jessie_udp@dns-test-service-3.dns-3463.svc.cluster.local from pod  dns-3463/dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 20 08:27:41.422: INFO: Lookups using dns-3463/dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 failed for: [wheezy_udp@dns-test-service-3.dns-3463.svc.cluster.local jessie_udp@dns-test-service-3.dns-3463.svc.cluster.local]

Mar 20 08:27:46.418: INFO: File wheezy_udp@dns-test-service-3.dns-3463.svc.cluster.local from pod  dns-3463/dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 20 08:27:46.422: INFO: File jessie_udp@dns-test-service-3.dns-3463.svc.cluster.local from pod  dns-3463/dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 20 08:27:46.422: INFO: Lookups using dns-3463/dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 failed for: [wheezy_udp@dns-test-service-3.dns-3463.svc.cluster.local jessie_udp@dns-test-service-3.dns-3463.svc.cluster.local]

Mar 20 08:27:51.420: INFO: File wheezy_udp@dns-test-service-3.dns-3463.svc.cluster.local from pod  dns-3463/dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 20 08:27:51.424: INFO: File jessie_udp@dns-test-service-3.dns-3463.svc.cluster.local from pod  dns-3463/dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 20 08:27:51.424: INFO: Lookups using dns-3463/dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 failed for: [wheezy_udp@dns-test-service-3.dns-3463.svc.cluster.local jessie_udp@dns-test-service-3.dns-3463.svc.cluster.local]

Mar 20 08:27:56.428: INFO: DNS probes using dns-test-0cc201ec-ea45-4d93-b7e7-f6b1b3118f11 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3463.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3463.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3463.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3463.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 20 08:28:00.578: INFO: DNS probes using dns-test-b02dc07a-5e00-4a00-9cdd-b9dad9040a63 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar 20 08:28:00.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3463" for this suite.

• [SLOW TEST:37.394 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":356,"completed":223,"skipped":4094,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:28:00.670: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 20 08:28:00.700: INFO: Waiting up to 5m0s for pod "pod-0218472a-867a-4365-b80c-11e67fca1983" in namespace "emptydir-416" to be "Succeeded or Failed"
Mar 20 08:28:00.702: INFO: Pod "pod-0218472a-867a-4365-b80c-11e67fca1983": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072803ms
Mar 20 08:28:02.717: INFO: Pod "pod-0218472a-867a-4365-b80c-11e67fca1983": Phase="Running", Reason="", readiness=false. Elapsed: 2.017441719s
Mar 20 08:28:04.731: INFO: Pod "pod-0218472a-867a-4365-b80c-11e67fca1983": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031237504s
STEP: Saw pod success
Mar 20 08:28:04.731: INFO: Pod "pod-0218472a-867a-4365-b80c-11e67fca1983" satisfied condition "Succeeded or Failed"
Mar 20 08:28:04.734: INFO: Trying to get logs from node env016ar130-worker02 pod pod-0218472a-867a-4365-b80c-11e67fca1983 container test-container: <nil>
STEP: delete the pod
Mar 20 08:28:04.753: INFO: Waiting for pod pod-0218472a-867a-4365-b80c-11e67fca1983 to disappear
Mar 20 08:28:04.757: INFO: Pod pod-0218472a-867a-4365-b80c-11e67fca1983 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar 20 08:28:04.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-416" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":224,"skipped":4099,"failed":0}
SSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:28:04.767: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
Mar 20 08:28:04.826: INFO: created test-event-1
Mar 20 08:28:04.834: INFO: created test-event-2
Mar 20 08:28:04.842: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Mar 20 08:28:04.846: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Mar 20 08:28:04.862: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Mar 20 08:28:04.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2313" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":356,"completed":225,"skipped":4103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:28:04.875: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Mar 20 08:28:04.919: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:28:06.933: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Mar 20 08:28:06.952: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:28:08.959: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Mar 20 08:28:08.973: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 20 08:28:08.979: INFO: Pod pod-with-prestop-http-hook still exists
Mar 20 08:28:10.979: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 20 08:28:10.991: INFO: Pod pod-with-prestop-http-hook still exists
Mar 20 08:28:12.980: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 20 08:28:12.983: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Mar 20 08:28:12.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7471" for this suite.

• [SLOW TEST:8.131 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":356,"completed":226,"skipped":4149,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:28:13.006: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-7a66a00c-6927-4c0b-b88f-8c46ce5204ee
STEP: Creating configMap with name cm-test-opt-upd-07208e73-c8c9-4fa5-9ec5-9863d552336b
STEP: Creating the pod
Mar 20 08:28:13.065: INFO: The status of Pod pod-configmaps-f1809dba-3365-4613-a73c-40dcb1588c20 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:28:15.083: INFO: The status of Pod pod-configmaps-f1809dba-3365-4613-a73c-40dcb1588c20 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-7a66a00c-6927-4c0b-b88f-8c46ce5204ee
STEP: Updating configmap cm-test-opt-upd-07208e73-c8c9-4fa5-9ec5-9863d552336b
STEP: Creating configMap with name cm-test-opt-create-757ac5a7-8d17-4a4e-a2f3-f02879732234
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar 20 08:28:17.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5550" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":227,"skipped":4162,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:28:17.183: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:28:17.228: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 20 08:28:22.245: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 20 08:28:22.245: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 20 08:28:22.268: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9981  d7955672-6149-4984-b22a-e08e1a4bcdf7 827138 1 2023-03-20 08:28:22 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2023-03-20 08:28:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00367f9d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar 20 08:28:22.271: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Mar 20 08:28:22.271: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar 20 08:28:22.271: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9981  db166f46-ad87-4888-bbb6-6d0ec039e995 827139 1 2023-03-20 08:28:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment d7955672-6149-4984-b22a-e08e1a4bcdf7 0xc00367fd57 0xc00367fd58}] []  [{e2e.test Update apps/v1 2023-03-20 08:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-20 08:28:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-20 08:28:22 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"d7955672-6149-4984-b22a-e08e1a4bcdf7\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00367fe18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 20 08:28:22.275: INFO: Pod "test-cleanup-controller-jpkjr" is available:
&Pod{ObjectMeta:{test-cleanup-controller-jpkjr test-cleanup-controller- deployment-9981  4ed88d83-1505-4f07-afeb-e460e598d97f 827127 0 2023-03-20 08:28:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:192.168.90.100/32 cni.projectcalico.org/podIPs:192.168.90.100/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.100"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.90.100"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-cleanup-controller db166f46-ad87-4888-bbb6-6d0ec039e995 0xc000f25617 0xc000f25618}] []  [{calico Update v1 2023-03-20 08:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-20 08:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"db166f46-ad87-4888-bbb6-6d0ec039e995\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-03-20 08:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-20 08:28:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.90.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rvpkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rvpkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env016ar130-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:28:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:28:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:28:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-20 08:28:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.10.71,PodIP:192.168.90.100,StartTime:2023-03-20 08:28:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-20 08:28:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://32a460af29709a45c65be90ad67019c99729e02059603de2e32427e8ea2358d6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.90.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar 20 08:28:22.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9981" for this suite.

• [SLOW TEST:5.106 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":356,"completed":228,"skipped":4169,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:28:22.289: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar 20 08:28:22.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4313" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":356,"completed":229,"skipped":4172,"failed":0}
SSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:28:22.341: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:28:22.376: INFO: The status of Pod busybox-readonly-fs19972b54-5c7d-459c-959f-79f0005fb9da is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:28:24.389: INFO: The status of Pod busybox-readonly-fs19972b54-5c7d-459c-959f-79f0005fb9da is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Mar 20 08:28:24.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6765" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":230,"skipped":4175,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:28:24.413: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 20 08:28:24.786: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 08:28:27.813: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:28:27.818: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 08:28:31.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6456" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:6.741 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":356,"completed":231,"skipped":4211,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:28:31.155: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 20 08:28:31.221: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar 20 08:28:31.226: INFO: starting watch
STEP: patching
STEP: updating
Mar 20 08:28:31.240: INFO: waiting for watch events with expected annotations
Mar 20 08:28:31.240: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Mar 20 08:28:31.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1482" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":356,"completed":232,"skipped":4233,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:28:31.274: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Mar 20 08:28:33.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5032" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":356,"completed":233,"skipped":4242,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:28:33.357: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5033
STEP: creating service affinity-nodeport in namespace services-5033
STEP: creating replication controller affinity-nodeport in namespace services-5033
I0320 08:28:33.397421      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5033, replica count: 3
I0320 08:28:36.449216      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 20 08:28:36.466: INFO: Creating new exec pod
Mar 20 08:28:39.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-5033 exec execpod-affinity2ds2x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Mar 20 08:28:39.689: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar 20 08:28:39.689: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:28:39.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-5033 exec execpod-affinity2ds2x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.155.85 80'
Mar 20 08:28:39.853: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.155.85 80\nConnection to 192.168.155.85 80 port [tcp/http] succeeded!\n"
Mar 20 08:28:39.853: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:28:39.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-5033 exec execpod-affinity2ds2x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.10.73 31187'
Mar 20 08:28:40.006: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.10.73 31187\nConnection to 10.2.10.73 31187 port [tcp/*] succeeded!\n"
Mar 20 08:28:40.006: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:28:40.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-5033 exec execpod-affinity2ds2x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.10.71 31187'
Mar 20 08:28:40.165: INFO: stderr: "+ + nc -v -t -wecho 2 10.2.10.71 31187\n hostName\nConnection to 10.2.10.71 31187 port [tcp/*] succeeded!\n"
Mar 20 08:28:40.165: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:28:40.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-5033 exec execpod-affinity2ds2x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.10.71:31187/ ; done'
Mar 20 08:28:40.401: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:31187/\n"
Mar 20 08:28:40.401: INFO: stdout: "\naffinity-nodeport-zsfxs\naffinity-nodeport-zsfxs\naffinity-nodeport-zsfxs\naffinity-nodeport-zsfxs\naffinity-nodeport-zsfxs\naffinity-nodeport-zsfxs\naffinity-nodeport-zsfxs\naffinity-nodeport-zsfxs\naffinity-nodeport-zsfxs\naffinity-nodeport-zsfxs\naffinity-nodeport-zsfxs\naffinity-nodeport-zsfxs\naffinity-nodeport-zsfxs\naffinity-nodeport-zsfxs\naffinity-nodeport-zsfxs\naffinity-nodeport-zsfxs"
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Received response from host: affinity-nodeport-zsfxs
Mar 20 08:28:40.401: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5033, will wait for the garbage collector to delete the pods
Mar 20 08:28:40.547: INFO: Deleting ReplicationController affinity-nodeport took: 67.019765ms
Mar 20 08:28:40.647: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.708081ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 08:28:42.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5033" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.643 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":234,"skipped":4280,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:28:43.000: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 20 08:28:43.043: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5777  d375dc95-1703-476c-8240-9d633aa0d8c8 827533 0 2023-03-20 08:28:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-20 08:28:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 20 08:28:43.043: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5777  d375dc95-1703-476c-8240-9d633aa0d8c8 827533 0 2023-03-20 08:28:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-20 08:28:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 20 08:28:43.053: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5777  d375dc95-1703-476c-8240-9d633aa0d8c8 827534 0 2023-03-20 08:28:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-20 08:28:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 20 08:28:43.053: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5777  d375dc95-1703-476c-8240-9d633aa0d8c8 827534 0 2023-03-20 08:28:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-20 08:28:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 20 08:28:43.063: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5777  d375dc95-1703-476c-8240-9d633aa0d8c8 827535 0 2023-03-20 08:28:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-20 08:28:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 20 08:28:43.063: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5777  d375dc95-1703-476c-8240-9d633aa0d8c8 827535 0 2023-03-20 08:28:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-20 08:28:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 20 08:28:43.069: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5777  d375dc95-1703-476c-8240-9d633aa0d8c8 827536 0 2023-03-20 08:28:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-20 08:28:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 20 08:28:43.069: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5777  d375dc95-1703-476c-8240-9d633aa0d8c8 827536 0 2023-03-20 08:28:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-20 08:28:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 20 08:28:43.075: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5777  5faa314d-3999-4972-a10a-5511e3bca532 827537 0 2023-03-20 08:28:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-03-20 08:28:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 20 08:28:43.076: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5777  5faa314d-3999-4972-a10a-5511e3bca532 827537 0 2023-03-20 08:28:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-03-20 08:28:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 20 08:28:53.085: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5777  5faa314d-3999-4972-a10a-5511e3bca532 827596 0 2023-03-20 08:28:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-03-20 08:28:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 20 08:28:53.085: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5777  5faa314d-3999-4972-a10a-5511e3bca532 827596 0 2023-03-20 08:28:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-03-20 08:28:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Mar 20 08:29:03.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5777" for this suite.

• [SLOW TEST:20.106 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":356,"completed":235,"skipped":4296,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:29:03.106: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test service account token: 
Mar 20 08:29:03.148: INFO: Waiting up to 5m0s for pod "test-pod-2294ae9f-e288-46be-bc87-81df645e596a" in namespace "svcaccounts-6045" to be "Succeeded or Failed"
Mar 20 08:29:03.151: INFO: Pod "test-pod-2294ae9f-e288-46be-bc87-81df645e596a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.702016ms
Mar 20 08:29:05.157: INFO: Pod "test-pod-2294ae9f-e288-46be-bc87-81df645e596a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008903711s
Mar 20 08:29:07.172: INFO: Pod "test-pod-2294ae9f-e288-46be-bc87-81df645e596a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023497242s
STEP: Saw pod success
Mar 20 08:29:07.172: INFO: Pod "test-pod-2294ae9f-e288-46be-bc87-81df645e596a" satisfied condition "Succeeded or Failed"
Mar 20 08:29:07.178: INFO: Trying to get logs from node env016ar130-worker02 pod test-pod-2294ae9f-e288-46be-bc87-81df645e596a container agnhost-container: <nil>
STEP: delete the pod
Mar 20 08:29:07.199: INFO: Waiting for pod test-pod-2294ae9f-e288-46be-bc87-81df645e596a to disappear
Mar 20 08:29:07.202: INFO: Pod test-pod-2294ae9f-e288-46be-bc87-81df645e596a no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Mar 20 08:29:07.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6045" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":356,"completed":236,"skipped":4320,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:29:07.212: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:29:07.240: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-1055e577-8aff-45a0-89e5-00a3aaa19264" in namespace "security-context-test-5957" to be "Succeeded or Failed"
Mar 20 08:29:07.244: INFO: Pod "busybox-privileged-false-1055e577-8aff-45a0-89e5-00a3aaa19264": Phase="Pending", Reason="", readiness=false. Elapsed: 3.663991ms
Mar 20 08:29:09.256: INFO: Pod "busybox-privileged-false-1055e577-8aff-45a0-89e5-00a3aaa19264": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016275746s
Mar 20 08:29:11.264: INFO: Pod "busybox-privileged-false-1055e577-8aff-45a0-89e5-00a3aaa19264": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024320973s
Mar 20 08:29:11.264: INFO: Pod "busybox-privileged-false-1055e577-8aff-45a0-89e5-00a3aaa19264" satisfied condition "Succeeded or Failed"
Mar 20 08:29:11.271: INFO: Got logs for pod "busybox-privileged-false-1055e577-8aff-45a0-89e5-00a3aaa19264": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Mar 20 08:29:11.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5957" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":237,"skipped":4335,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:29:11.285: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:29:11.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-1781 version'
Mar 20 08:29:11.373: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Mar 20 08:29:11.373: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.8\", GitCommit:\"fdc77503e954d1ee641c0e350481f7528e8d068b\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:38:19Z\", GoVersion:\"go1.18.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.4\nServer Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.8\", GitCommit:\"fdc77503e954d1ee641c0e350481f7528e8d068b\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:31:40Z\", GoVersion:\"go1.18.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 08:29:11.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1781" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":356,"completed":238,"skipped":4351,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:29:11.387: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Mar 20 08:29:11.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2045" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":239,"skipped":4366,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:29:11.430: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 20 08:29:11.965: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 08:29:15.005: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:29:15.010: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 08:29:18.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6117" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:6.809 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":356,"completed":240,"skipped":4388,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:29:18.240: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5675.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5675.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5675.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5675.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5675.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5675.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5675.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5675.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 20 08:29:20.307: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:20.311: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:20.315: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:20.319: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:20.323: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:20.326: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:20.329: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:20.332: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:20.332: INFO: Lookups using dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5675.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5675.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local jessie_udp@dns-test-service-2.dns-5675.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5675.svc.cluster.local]

Mar 20 08:29:25.338: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:25.343: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:25.347: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:25.350: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:25.354: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:25.357: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:25.360: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:25.363: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:25.363: INFO: Lookups using dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5675.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5675.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local jessie_udp@dns-test-service-2.dns-5675.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5675.svc.cluster.local]

Mar 20 08:29:30.339: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:30.343: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:30.346: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:30.350: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:30.354: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:30.357: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:30.363: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:30.367: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:30.367: INFO: Lookups using dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5675.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5675.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local jessie_udp@dns-test-service-2.dns-5675.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5675.svc.cluster.local]

Mar 20 08:29:35.338: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:35.342: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:35.346: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:35.349: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:35.352: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:35.356: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:35.359: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:35.363: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:35.363: INFO: Lookups using dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5675.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5675.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local jessie_udp@dns-test-service-2.dns-5675.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5675.svc.cluster.local]

Mar 20 08:29:40.340: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:40.344: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:40.349: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:40.353: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:40.356: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:40.360: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:40.363: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:40.367: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:40.367: INFO: Lookups using dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5675.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5675.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local jessie_udp@dns-test-service-2.dns-5675.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5675.svc.cluster.local]

Mar 20 08:29:45.338: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:45.343: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:45.347: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:45.351: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:45.354: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:45.357: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:45.360: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:45.364: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5675.svc.cluster.local from pod dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818: the server could not find the requested resource (get pods dns-test-bc448c9f-527a-482d-b372-d64166560818)
Mar 20 08:29:45.364: INFO: Lookups using dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5675.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5675.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5675.svc.cluster.local jessie_udp@dns-test-service-2.dns-5675.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5675.svc.cluster.local]

Mar 20 08:29:50.370: INFO: DNS probes using dns-5675/dns-test-bc448c9f-527a-482d-b372-d64166560818 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar 20 08:29:50.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5675" for this suite.

• [SLOW TEST:32.182 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":356,"completed":241,"skipped":4416,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:29:50.422: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-6381/configmap-test-953947af-1ad3-4f61-ba38-bc1d64c31ba3
STEP: Creating a pod to test consume configMaps
Mar 20 08:29:50.478: INFO: Waiting up to 5m0s for pod "pod-configmaps-f6f28b29-6976-4af5-81e0-a3c945d53e61" in namespace "configmap-6381" to be "Succeeded or Failed"
Mar 20 08:29:50.482: INFO: Pod "pod-configmaps-f6f28b29-6976-4af5-81e0-a3c945d53e61": Phase="Pending", Reason="", readiness=false. Elapsed: 3.333956ms
Mar 20 08:29:52.492: INFO: Pod "pod-configmaps-f6f28b29-6976-4af5-81e0-a3c945d53e61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013014134s
Mar 20 08:29:54.506: INFO: Pod "pod-configmaps-f6f28b29-6976-4af5-81e0-a3c945d53e61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02737637s
STEP: Saw pod success
Mar 20 08:29:54.506: INFO: Pod "pod-configmaps-f6f28b29-6976-4af5-81e0-a3c945d53e61" satisfied condition "Succeeded or Failed"
Mar 20 08:29:54.510: INFO: Trying to get logs from node env016ar130-worker02 pod pod-configmaps-f6f28b29-6976-4af5-81e0-a3c945d53e61 container env-test: <nil>
STEP: delete the pod
Mar 20 08:29:54.527: INFO: Waiting for pod pod-configmaps-f6f28b29-6976-4af5-81e0-a3c945d53e61 to disappear
Mar 20 08:29:54.531: INFO: Pod pod-configmaps-f6f28b29-6976-4af5-81e0-a3c945d53e61 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Mar 20 08:29:54.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6381" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":356,"completed":242,"skipped":4444,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:29:54.540: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 20 08:29:54.603: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:54.604: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:54.604: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:54.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:29:54.606: INFO: Node env016ar130-worker01 is running 0 daemon pod, expected 1
Mar 20 08:29:55.614: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:55.615: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:55.615: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:55.618: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:29:55.618: INFO: Node env016ar130-worker01 is running 0 daemon pod, expected 1
Mar 20 08:29:56.619: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:56.619: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:56.619: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:56.623: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 20 08:29:56.623: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 20 08:29:56.650: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:56.650: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:56.650: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:56.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 20 08:29:56.653: INFO: Node env016ar130-worker03 is running 0 daemon pod, expected 1
Mar 20 08:29:57.663: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:57.663: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:57.663: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:57.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 20 08:29:57.667: INFO: Node env016ar130-worker03 is running 0 daemon pod, expected 1
Mar 20 08:29:58.662: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:58.662: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:58.662: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:58.666: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 20 08:29:58.666: INFO: Node env016ar130-worker03 is running 0 daemon pod, expected 1
Mar 20 08:29:59.665: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:59.666: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:59.666: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:29:59.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 20 08:29:59.669: INFO: Node env016ar130-worker03 is running 0 daemon pod, expected 1
Mar 20 08:30:00.666: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:30:00.666: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:30:00.666: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:30:00.671: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 20 08:30:00.671: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2740, will wait for the garbage collector to delete the pods
Mar 20 08:30:00.736: INFO: Deleting DaemonSet.extensions daemon-set took: 9.13376ms
Mar 20 08:30:00.840: INFO: Terminating DaemonSet.extensions daemon-set pods took: 103.432601ms
Mar 20 08:30:03.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:30:03.650: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 20 08:30:03.654: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"828120"},"items":null}

Mar 20 08:30:03.658: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"828120"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Mar 20 08:30:03.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2740" for this suite.

• [SLOW TEST:9.146 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":356,"completed":243,"skipped":4447,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:30:03.687: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 08:30:03.729: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc1b0fe0-f3a6-4e9b-ab08-536295ab45a6" in namespace "projected-7576" to be "Succeeded or Failed"
Mar 20 08:30:03.733: INFO: Pod "downwardapi-volume-fc1b0fe0-f3a6-4e9b-ab08-536295ab45a6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.237881ms
Mar 20 08:30:05.745: INFO: Pod "downwardapi-volume-fc1b0fe0-f3a6-4e9b-ab08-536295ab45a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015568573s
Mar 20 08:30:07.754: INFO: Pod "downwardapi-volume-fc1b0fe0-f3a6-4e9b-ab08-536295ab45a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024422607s
STEP: Saw pod success
Mar 20 08:30:07.754: INFO: Pod "downwardapi-volume-fc1b0fe0-f3a6-4e9b-ab08-536295ab45a6" satisfied condition "Succeeded or Failed"
Mar 20 08:30:07.758: INFO: Trying to get logs from node env016ar130-worker01 pod downwardapi-volume-fc1b0fe0-f3a6-4e9b-ab08-536295ab45a6 container client-container: <nil>
STEP: delete the pod
Mar 20 08:30:07.791: INFO: Waiting for pod downwardapi-volume-fc1b0fe0-f3a6-4e9b-ab08-536295ab45a6 to disappear
Mar 20 08:30:07.795: INFO: Pod downwardapi-volume-fc1b0fe0-f3a6-4e9b-ab08-536295ab45a6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar 20 08:30:07.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7576" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":244,"skipped":4456,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:30:07.804: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:30:07.841: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 08:30:11.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9350" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":356,"completed":245,"skipped":4456,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:30:11.024: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-213a28c9-ac6c-4786-bbb8-8f44a3272422
STEP: Creating a pod to test consume configMaps
Mar 20 08:30:11.078: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c74d1ace-eab4-4c9e-a32f-444d447aebce" in namespace "projected-8346" to be "Succeeded or Failed"
Mar 20 08:30:11.082: INFO: Pod "pod-projected-configmaps-c74d1ace-eab4-4c9e-a32f-444d447aebce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009503ms
Mar 20 08:30:13.086: INFO: Pod "pod-projected-configmaps-c74d1ace-eab4-4c9e-a32f-444d447aebce": Phase="Running", Reason="", readiness=false. Elapsed: 2.008318962s
Mar 20 08:30:15.097: INFO: Pod "pod-projected-configmaps-c74d1ace-eab4-4c9e-a32f-444d447aebce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019685968s
STEP: Saw pod success
Mar 20 08:30:15.097: INFO: Pod "pod-projected-configmaps-c74d1ace-eab4-4c9e-a32f-444d447aebce" satisfied condition "Succeeded or Failed"
Mar 20 08:30:15.105: INFO: Trying to get logs from node env016ar130-worker01 pod pod-projected-configmaps-c74d1ace-eab4-4c9e-a32f-444d447aebce container agnhost-container: <nil>
STEP: delete the pod
Mar 20 08:30:15.138: INFO: Waiting for pod pod-projected-configmaps-c74d1ace-eab4-4c9e-a32f-444d447aebce to disappear
Mar 20 08:30:15.143: INFO: Pod pod-projected-configmaps-c74d1ace-eab4-4c9e-a32f-444d447aebce no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar 20 08:30:15.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8346" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":246,"skipped":4459,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:30:15.201: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Mar 20 08:30:15.247: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:30:17.258: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Mar 20 08:30:17.278: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:30:19.287: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 20 08:30:19.314: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 20 08:30:19.318: INFO: Pod pod-with-poststart-http-hook still exists
Mar 20 08:30:21.319: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 20 08:30:21.325: INFO: Pod pod-with-poststart-http-hook still exists
Mar 20 08:30:23.319: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 20 08:30:23.327: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Mar 20 08:30:23.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1950" for this suite.

• [SLOW TEST:8.144 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":356,"completed":247,"skipped":4474,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:30:23.345: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-f81413fc-7cc6-4bd2-b3c9-b02ec1daba8e
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar 20 08:30:25.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2983" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":248,"skipped":4476,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:30:25.437: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 08:30:25.747: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 20 08:30:27.766: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 20, 8, 30, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 30, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 30, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 30, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 08:30:30.801: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:30:30.811: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7277-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 08:30:33.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6985" for this suite.
STEP: Destroying namespace "webhook-6985-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.570 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":356,"completed":249,"skipped":4479,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:30:34.007: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-211c22f5-104a-46c8-a423-6a2139753dc5
STEP: Creating a pod to test consume configMaps
Mar 20 08:30:34.060: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cff462ce-3609-4dfa-8bd4-1cfb151c57e3" in namespace "projected-7909" to be "Succeeded or Failed"
Mar 20 08:30:34.063: INFO: Pod "pod-projected-configmaps-cff462ce-3609-4dfa-8bd4-1cfb151c57e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552441ms
Mar 20 08:30:36.068: INFO: Pod "pod-projected-configmaps-cff462ce-3609-4dfa-8bd4-1cfb151c57e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00822938s
Mar 20 08:30:38.075: INFO: Pod "pod-projected-configmaps-cff462ce-3609-4dfa-8bd4-1cfb151c57e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015202868s
STEP: Saw pod success
Mar 20 08:30:38.075: INFO: Pod "pod-projected-configmaps-cff462ce-3609-4dfa-8bd4-1cfb151c57e3" satisfied condition "Succeeded or Failed"
Mar 20 08:30:38.080: INFO: Trying to get logs from node env016ar130-worker02 pod pod-projected-configmaps-cff462ce-3609-4dfa-8bd4-1cfb151c57e3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 20 08:30:38.125: INFO: Waiting for pod pod-projected-configmaps-cff462ce-3609-4dfa-8bd4-1cfb151c57e3 to disappear
Mar 20 08:30:38.129: INFO: Pod pod-projected-configmaps-cff462ce-3609-4dfa-8bd4-1cfb151c57e3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar 20 08:30:38.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7909" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":250,"skipped":4490,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:30:38.143: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Mar 20 08:30:38.189: INFO: Waiting up to 5m0s for pod "downward-api-266befe2-11bd-413c-b377-7ba6cbe85ac0" in namespace "downward-api-2723" to be "Succeeded or Failed"
Mar 20 08:30:38.193: INFO: Pod "downward-api-266befe2-11bd-413c-b377-7ba6cbe85ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.888174ms
Mar 20 08:30:40.207: INFO: Pod "downward-api-266befe2-11bd-413c-b377-7ba6cbe85ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018327868s
Mar 20 08:30:42.223: INFO: Pod "downward-api-266befe2-11bd-413c-b377-7ba6cbe85ac0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033809213s
STEP: Saw pod success
Mar 20 08:30:42.223: INFO: Pod "downward-api-266befe2-11bd-413c-b377-7ba6cbe85ac0" satisfied condition "Succeeded or Failed"
Mar 20 08:30:42.227: INFO: Trying to get logs from node env016ar130-worker02 pod downward-api-266befe2-11bd-413c-b377-7ba6cbe85ac0 container dapi-container: <nil>
STEP: delete the pod
Mar 20 08:30:42.249: INFO: Waiting for pod downward-api-266befe2-11bd-413c-b377-7ba6cbe85ac0 to disappear
Mar 20 08:30:42.252: INFO: Pod downward-api-266befe2-11bd-413c-b377-7ba6cbe85ac0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Mar 20 08:30:42.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2723" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":356,"completed":251,"skipped":4538,"failed":0}
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:30:42.264: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Mar 20 08:30:46.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-523" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":356,"completed":252,"skipped":4545,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:30:46.351: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 08:30:46.392: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2a53028-d453-4601-9a8b-a8c4ef9ab3ad" in namespace "projected-9007" to be "Succeeded or Failed"
Mar 20 08:30:46.398: INFO: Pod "downwardapi-volume-a2a53028-d453-4601-9a8b-a8c4ef9ab3ad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.78091ms
Mar 20 08:30:48.408: INFO: Pod "downwardapi-volume-a2a53028-d453-4601-9a8b-a8c4ef9ab3ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015541703s
Mar 20 08:30:50.415: INFO: Pod "downwardapi-volume-a2a53028-d453-4601-9a8b-a8c4ef9ab3ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022643459s
STEP: Saw pod success
Mar 20 08:30:50.415: INFO: Pod "downwardapi-volume-a2a53028-d453-4601-9a8b-a8c4ef9ab3ad" satisfied condition "Succeeded or Failed"
Mar 20 08:30:50.419: INFO: Trying to get logs from node env016ar130-worker02 pod downwardapi-volume-a2a53028-d453-4601-9a8b-a8c4ef9ab3ad container client-container: <nil>
STEP: delete the pod
Mar 20 08:30:50.440: INFO: Waiting for pod downwardapi-volume-a2a53028-d453-4601-9a8b-a8c4ef9ab3ad to disappear
Mar 20 08:30:50.445: INFO: Pod downwardapi-volume-a2a53028-d453-4601-9a8b-a8c4ef9ab3ad no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar 20 08:30:50.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9007" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":253,"skipped":4565,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:30:50.458: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar 20 08:31:01.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5906" for this suite.

• [SLOW TEST:11.184 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":356,"completed":254,"skipped":4573,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:31:01.643: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:31:01.671: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8476
I0320 08:31:01.680551      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8476, replica count: 1
I0320 08:31:02.731611      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0320 08:31:03.732308      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 20 08:31:03.855: INFO: Created: latency-svc-95qvr
Mar 20 08:31:03.867: INFO: Got endpoints: latency-svc-95qvr [34.497067ms]
Mar 20 08:31:03.888: INFO: Created: latency-svc-q2b7w
Mar 20 08:31:03.891: INFO: Created: latency-svc-lp6jp
Mar 20 08:31:03.891: INFO: Created: latency-svc-lgggh
Mar 20 08:31:03.897: INFO: Created: latency-svc-gvkrn
Mar 20 08:31:03.897: INFO: Got endpoints: latency-svc-q2b7w [29.737558ms]
Mar 20 08:31:03.898: INFO: Got endpoints: latency-svc-lgggh [29.659262ms]
Mar 20 08:31:03.898: INFO: Got endpoints: latency-svc-lp6jp [29.75237ms]
Mar 20 08:31:03.904: INFO: Got endpoints: latency-svc-gvkrn [36.50906ms]
Mar 20 08:31:03.906: INFO: Created: latency-svc-bpdzv
Mar 20 08:31:03.911: INFO: Got endpoints: latency-svc-bpdzv [43.69073ms]
Mar 20 08:31:03.911: INFO: Created: latency-svc-bh86k
Mar 20 08:31:03.915: INFO: Created: latency-svc-42v7h
Mar 20 08:31:03.921: INFO: Got endpoints: latency-svc-bh86k [52.722233ms]
Mar 20 08:31:03.921: INFO: Got endpoints: latency-svc-42v7h [53.380098ms]
Mar 20 08:31:03.924: INFO: Created: latency-svc-pzxcl
Mar 20 08:31:03.930: INFO: Got endpoints: latency-svc-pzxcl [62.435411ms]
Mar 20 08:31:03.933: INFO: Created: latency-svc-mh4sw
Mar 20 08:31:03.938: INFO: Got endpoints: latency-svc-mh4sw [70.563041ms]
Mar 20 08:31:03.939: INFO: Created: latency-svc-79rpz
Mar 20 08:31:03.947: INFO: Got endpoints: latency-svc-79rpz [79.499476ms]
Mar 20 08:31:03.948: INFO: Created: latency-svc-n7nwd
Mar 20 08:31:03.953: INFO: Got endpoints: latency-svc-n7nwd [84.675982ms]
Mar 20 08:31:03.953: INFO: Created: latency-svc-gs7n8
Mar 20 08:31:03.960: INFO: Got endpoints: latency-svc-gs7n8 [92.544371ms]
Mar 20 08:31:03.961: INFO: Created: latency-svc-sgkcr
Mar 20 08:31:03.965: INFO: Created: latency-svc-6pgvs
Mar 20 08:31:03.966: INFO: Got endpoints: latency-svc-sgkcr [98.194446ms]
Mar 20 08:31:03.970: INFO: Created: latency-svc-n64jt
Mar 20 08:31:03.971: INFO: Got endpoints: latency-svc-6pgvs [103.330693ms]
Mar 20 08:31:03.975: INFO: Got endpoints: latency-svc-n64jt [106.803509ms]
Mar 20 08:31:03.977: INFO: Created: latency-svc-cbq9l
Mar 20 08:31:03.982: INFO: Got endpoints: latency-svc-cbq9l [84.506656ms]
Mar 20 08:31:04.020: INFO: Created: latency-svc-rdzft
Mar 20 08:31:04.024: INFO: Created: latency-svc-4lndt
Mar 20 08:31:04.028: INFO: Created: latency-svc-h2zvd
Mar 20 08:31:04.028: INFO: Created: latency-svc-pqpbg
Mar 20 08:31:04.032: INFO: Created: latency-svc-mzrdh
Mar 20 08:31:04.033: INFO: Created: latency-svc-kmtgw
Mar 20 08:31:04.033: INFO: Created: latency-svc-cqnn7
Mar 20 08:31:04.035: INFO: Created: latency-svc-wshzf
Mar 20 08:31:04.035: INFO: Created: latency-svc-xfjwg
Mar 20 08:31:04.035: INFO: Created: latency-svc-jqvjf
Mar 20 08:31:04.035: INFO: Created: latency-svc-lf4mw
Mar 20 08:31:04.036: INFO: Created: latency-svc-d429c
Mar 20 08:31:04.036: INFO: Created: latency-svc-8k2z5
Mar 20 08:31:04.040: INFO: Created: latency-svc-65gmj
Mar 20 08:31:04.040: INFO: Got endpoints: latency-svc-rdzft [142.192898ms]
Mar 20 08:31:04.040: INFO: Created: latency-svc-dfhft
Mar 20 08:31:04.044: INFO: Got endpoints: latency-svc-4lndt [96.85432ms]
Mar 20 08:31:04.047: INFO: Got endpoints: latency-svc-cqnn7 [94.2113ms]
Mar 20 08:31:04.047: INFO: Got endpoints: latency-svc-pqpbg [143.152222ms]
Mar 20 08:31:04.047: INFO: Got endpoints: latency-svc-h2zvd [125.602443ms]
Mar 20 08:31:04.052: INFO: Got endpoints: latency-svc-mzrdh [121.237295ms]
Mar 20 08:31:04.055: INFO: Got endpoints: latency-svc-xfjwg [73.156705ms]
Mar 20 08:31:04.055: INFO: Got endpoints: latency-svc-kmtgw [134.485535ms]
Mar 20 08:31:04.056: INFO: Created: latency-svc-tlmsc
Mar 20 08:31:04.056: INFO: Got endpoints: latency-svc-d429c [117.277421ms]
Mar 20 08:31:04.056: INFO: Got endpoints: latency-svc-lf4mw [80.939564ms]
Mar 20 08:31:04.060: INFO: Got endpoints: latency-svc-wshzf [88.734395ms]
Mar 20 08:31:04.064: INFO: Got endpoints: latency-svc-dfhft [167.000577ms]
Mar 20 08:31:04.065: INFO: Got endpoints: latency-svc-8k2z5 [98.639756ms]
Mar 20 08:31:04.065: INFO: Got endpoints: latency-svc-jqvjf [153.462909ms]
Mar 20 08:31:04.065: INFO: Got endpoints: latency-svc-65gmj [104.53637ms]
Mar 20 08:31:04.065: INFO: Got endpoints: latency-svc-tlmsc [25.52899ms]
Mar 20 08:31:04.065: INFO: Created: latency-svc-vknws
Mar 20 08:31:04.070: INFO: Got endpoints: latency-svc-vknws [25.737299ms]
Mar 20 08:31:04.071: INFO: Created: latency-svc-tr27z
Mar 20 08:31:04.073: INFO: Created: latency-svc-xpnpk
Mar 20 08:31:04.077: INFO: Created: latency-svc-mm79x
Mar 20 08:31:04.080: INFO: Created: latency-svc-sxl2c
Mar 20 08:31:04.082: INFO: Created: latency-svc-s954b
Mar 20 08:31:04.085: INFO: Created: latency-svc-rphsv
Mar 20 08:31:04.089: INFO: Created: latency-svc-2q7qh
Mar 20 08:31:04.093: INFO: Created: latency-svc-rnfzq
Mar 20 08:31:04.096: INFO: Created: latency-svc-t8p7s
Mar 20 08:31:04.099: INFO: Created: latency-svc-b88qt
Mar 20 08:31:04.103: INFO: Created: latency-svc-kc7xb
Mar 20 08:31:04.105: INFO: Created: latency-svc-cm2dk
Mar 20 08:31:04.109: INFO: Created: latency-svc-7k86k
Mar 20 08:31:04.112: INFO: Got endpoints: latency-svc-tr27z [64.458182ms]
Mar 20 08:31:04.115: INFO: Created: latency-svc-rr7pl
Mar 20 08:31:04.119: INFO: Created: latency-svc-6klnn
Mar 20 08:31:04.121: INFO: Created: latency-svc-h69jf
Mar 20 08:31:04.170: INFO: Got endpoints: latency-svc-xpnpk [123.046313ms]
Mar 20 08:31:04.184: INFO: Created: latency-svc-8vh5q
Mar 20 08:31:04.214: INFO: Got endpoints: latency-svc-mm79x [167.443097ms]
Mar 20 08:31:04.229: INFO: Created: latency-svc-gm5b8
Mar 20 08:31:04.265: INFO: Got endpoints: latency-svc-sxl2c [212.800079ms]
Mar 20 08:31:04.281: INFO: Created: latency-svc-qg6f8
Mar 20 08:31:04.315: INFO: Got endpoints: latency-svc-s954b [259.656044ms]
Mar 20 08:31:04.328: INFO: Created: latency-svc-6g45z
Mar 20 08:31:04.365: INFO: Got endpoints: latency-svc-rphsv [309.291394ms]
Mar 20 08:31:04.379: INFO: Created: latency-svc-pz5hk
Mar 20 08:31:04.414: INFO: Got endpoints: latency-svc-2q7qh [359.124938ms]
Mar 20 08:31:04.432: INFO: Created: latency-svc-z7njt
Mar 20 08:31:04.465: INFO: Got endpoints: latency-svc-rnfzq [409.007012ms]
Mar 20 08:31:04.480: INFO: Created: latency-svc-pc8zp
Mar 20 08:31:04.514: INFO: Got endpoints: latency-svc-t8p7s [454.318415ms]
Mar 20 08:31:04.527: INFO: Created: latency-svc-6lr96
Mar 20 08:31:04.565: INFO: Got endpoints: latency-svc-b88qt [499.557576ms]
Mar 20 08:31:04.577: INFO: Created: latency-svc-ktw9v
Mar 20 08:31:04.615: INFO: Got endpoints: latency-svc-kc7xb [550.320225ms]
Mar 20 08:31:04.644: INFO: Created: latency-svc-9fclj
Mar 20 08:31:04.675: INFO: Got endpoints: latency-svc-cm2dk [610.474248ms]
Mar 20 08:31:04.690: INFO: Created: latency-svc-lw8cr
Mar 20 08:31:04.714: INFO: Got endpoints: latency-svc-7k86k [649.740739ms]
Mar 20 08:31:04.729: INFO: Created: latency-svc-jb2dw
Mar 20 08:31:04.765: INFO: Got endpoints: latency-svc-rr7pl [700.493074ms]
Mar 20 08:31:04.778: INFO: Created: latency-svc-rdhml
Mar 20 08:31:04.814: INFO: Got endpoints: latency-svc-6klnn [743.895382ms]
Mar 20 08:31:04.828: INFO: Created: latency-svc-hrgwj
Mar 20 08:31:04.875: INFO: Got endpoints: latency-svc-h69jf [763.757233ms]
Mar 20 08:31:04.895: INFO: Created: latency-svc-94p4p
Mar 20 08:31:04.913: INFO: Got endpoints: latency-svc-8vh5q [742.686932ms]
Mar 20 08:31:04.924: INFO: Created: latency-svc-7jmd7
Mar 20 08:31:04.965: INFO: Got endpoints: latency-svc-gm5b8 [750.748851ms]
Mar 20 08:31:04.977: INFO: Created: latency-svc-bf5zb
Mar 20 08:31:05.015: INFO: Got endpoints: latency-svc-qg6f8 [750.080184ms]
Mar 20 08:31:05.029: INFO: Created: latency-svc-2pcxn
Mar 20 08:31:05.064: INFO: Got endpoints: latency-svc-6g45z [749.21989ms]
Mar 20 08:31:05.082: INFO: Created: latency-svc-sbvjc
Mar 20 08:31:05.115: INFO: Got endpoints: latency-svc-pz5hk [750.311717ms]
Mar 20 08:31:05.127: INFO: Created: latency-svc-pkxb9
Mar 20 08:31:05.164: INFO: Got endpoints: latency-svc-z7njt [749.675321ms]
Mar 20 08:31:05.176: INFO: Created: latency-svc-f827t
Mar 20 08:31:05.213: INFO: Got endpoints: latency-svc-pc8zp [748.38012ms]
Mar 20 08:31:05.224: INFO: Created: latency-svc-s4lcp
Mar 20 08:31:05.266: INFO: Got endpoints: latency-svc-6lr96 [751.665647ms]
Mar 20 08:31:05.279: INFO: Created: latency-svc-9h8qw
Mar 20 08:31:05.315: INFO: Got endpoints: latency-svc-ktw9v [750.198239ms]
Mar 20 08:31:05.329: INFO: Created: latency-svc-ddvhk
Mar 20 08:31:05.365: INFO: Got endpoints: latency-svc-9fclj [750.283907ms]
Mar 20 08:31:05.379: INFO: Created: latency-svc-rtnms
Mar 20 08:31:05.415: INFO: Got endpoints: latency-svc-lw8cr [739.542322ms]
Mar 20 08:31:05.430: INFO: Created: latency-svc-27p97
Mar 20 08:31:05.465: INFO: Got endpoints: latency-svc-jb2dw [750.482467ms]
Mar 20 08:31:05.480: INFO: Created: latency-svc-6lhnl
Mar 20 08:31:05.516: INFO: Got endpoints: latency-svc-rdhml [750.760084ms]
Mar 20 08:31:05.536: INFO: Created: latency-svc-qsdlt
Mar 20 08:31:05.565: INFO: Got endpoints: latency-svc-hrgwj [750.687191ms]
Mar 20 08:31:05.628: INFO: Got endpoints: latency-svc-94p4p [752.495193ms]
Mar 20 08:31:05.633: INFO: Created: latency-svc-4sg9n
Mar 20 08:31:05.643: INFO: Created: latency-svc-gmwfv
Mar 20 08:31:05.664: INFO: Got endpoints: latency-svc-7jmd7 [750.952681ms]
Mar 20 08:31:05.677: INFO: Created: latency-svc-z6zlm
Mar 20 08:31:05.715: INFO: Got endpoints: latency-svc-bf5zb [749.627938ms]
Mar 20 08:31:05.729: INFO: Created: latency-svc-r2wgx
Mar 20 08:31:05.766: INFO: Got endpoints: latency-svc-2pcxn [751.553396ms]
Mar 20 08:31:05.782: INFO: Created: latency-svc-fmmqf
Mar 20 08:31:05.815: INFO: Got endpoints: latency-svc-sbvjc [750.875618ms]
Mar 20 08:31:05.831: INFO: Created: latency-svc-bwdhq
Mar 20 08:31:05.865: INFO: Got endpoints: latency-svc-pkxb9 [749.665609ms]
Mar 20 08:31:05.884: INFO: Created: latency-svc-7g8tq
Mar 20 08:31:05.912: INFO: Got endpoints: latency-svc-f827t [748.144741ms]
Mar 20 08:31:05.923: INFO: Created: latency-svc-v84cx
Mar 20 08:31:05.963: INFO: Got endpoints: latency-svc-s4lcp [750.283699ms]
Mar 20 08:31:05.974: INFO: Created: latency-svc-jnvmz
Mar 20 08:31:06.015: INFO: Got endpoints: latency-svc-9h8qw [748.707217ms]
Mar 20 08:31:06.031: INFO: Created: latency-svc-mggcq
Mar 20 08:31:06.065: INFO: Got endpoints: latency-svc-ddvhk [749.652237ms]
Mar 20 08:31:06.080: INFO: Created: latency-svc-tfg7q
Mar 20 08:31:06.115: INFO: Got endpoints: latency-svc-rtnms [749.274913ms]
Mar 20 08:31:06.165: INFO: Got endpoints: latency-svc-27p97 [750.482153ms]
Mar 20 08:31:06.169: INFO: Created: latency-svc-gf58m
Mar 20 08:31:06.180: INFO: Created: latency-svc-9ghvp
Mar 20 08:31:06.215: INFO: Got endpoints: latency-svc-6lhnl [749.958316ms]
Mar 20 08:31:06.230: INFO: Created: latency-svc-rdcqp
Mar 20 08:31:06.264: INFO: Got endpoints: latency-svc-qsdlt [748.220836ms]
Mar 20 08:31:06.283: INFO: Created: latency-svc-qvvsc
Mar 20 08:31:06.313: INFO: Got endpoints: latency-svc-4sg9n [747.941407ms]
Mar 20 08:31:06.328: INFO: Created: latency-svc-7lvfq
Mar 20 08:31:06.366: INFO: Got endpoints: latency-svc-gmwfv [737.545951ms]
Mar 20 08:31:06.378: INFO: Created: latency-svc-mbfdn
Mar 20 08:31:06.415: INFO: Got endpoints: latency-svc-z6zlm [751.476306ms]
Mar 20 08:31:06.431: INFO: Created: latency-svc-2w5cx
Mar 20 08:31:06.465: INFO: Got endpoints: latency-svc-r2wgx [749.937517ms]
Mar 20 08:31:06.478: INFO: Created: latency-svc-jklqv
Mar 20 08:31:06.515: INFO: Got endpoints: latency-svc-fmmqf [748.379743ms]
Mar 20 08:31:06.528: INFO: Created: latency-svc-bjznl
Mar 20 08:31:06.565: INFO: Got endpoints: latency-svc-bwdhq [749.384839ms]
Mar 20 08:31:06.578: INFO: Created: latency-svc-v8dfz
Mar 20 08:31:06.615: INFO: Got endpoints: latency-svc-7g8tq [750.064558ms]
Mar 20 08:31:06.632: INFO: Created: latency-svc-dlksd
Mar 20 08:31:06.662: INFO: Got endpoints: latency-svc-v84cx [749.395016ms]
Mar 20 08:31:06.677: INFO: Created: latency-svc-6r4nf
Mar 20 08:31:06.714: INFO: Got endpoints: latency-svc-jnvmz [750.450088ms]
Mar 20 08:31:06.728: INFO: Created: latency-svc-fggks
Mar 20 08:31:06.761: INFO: Got endpoints: latency-svc-mggcq [746.130913ms]
Mar 20 08:31:06.775: INFO: Created: latency-svc-2mfgm
Mar 20 08:31:06.811: INFO: Got endpoints: latency-svc-tfg7q [746.369727ms]
Mar 20 08:31:06.822: INFO: Created: latency-svc-tpnbl
Mar 20 08:31:06.862: INFO: Got endpoints: latency-svc-gf58m [747.097292ms]
Mar 20 08:31:06.879: INFO: Created: latency-svc-zrh5p
Mar 20 08:31:06.912: INFO: Got endpoints: latency-svc-9ghvp [747.193279ms]
Mar 20 08:31:06.923: INFO: Created: latency-svc-c6xv8
Mar 20 08:31:06.964: INFO: Got endpoints: latency-svc-rdcqp [749.276725ms]
Mar 20 08:31:06.978: INFO: Created: latency-svc-z6dgg
Mar 20 08:31:07.012: INFO: Got endpoints: latency-svc-qvvsc [747.450909ms]
Mar 20 08:31:07.024: INFO: Created: latency-svc-7fb8q
Mar 20 08:31:07.065: INFO: Got endpoints: latency-svc-7lvfq [751.787564ms]
Mar 20 08:31:07.078: INFO: Created: latency-svc-cdtwx
Mar 20 08:31:07.115: INFO: Got endpoints: latency-svc-mbfdn [749.512569ms]
Mar 20 08:31:07.130: INFO: Created: latency-svc-rslxr
Mar 20 08:31:07.165: INFO: Got endpoints: latency-svc-2w5cx [749.564501ms]
Mar 20 08:31:07.182: INFO: Created: latency-svc-mzdsr
Mar 20 08:31:07.214: INFO: Got endpoints: latency-svc-jklqv [749.221687ms]
Mar 20 08:31:07.229: INFO: Created: latency-svc-dbgh9
Mar 20 08:31:07.262: INFO: Got endpoints: latency-svc-bjznl [747.558321ms]
Mar 20 08:31:07.274: INFO: Created: latency-svc-59m47
Mar 20 08:31:07.314: INFO: Got endpoints: latency-svc-v8dfz [749.289842ms]
Mar 20 08:31:07.329: INFO: Created: latency-svc-x9xs7
Mar 20 08:31:07.367: INFO: Got endpoints: latency-svc-dlksd [751.931453ms]
Mar 20 08:31:07.381: INFO: Created: latency-svc-fs69s
Mar 20 08:31:07.415: INFO: Got endpoints: latency-svc-6r4nf [752.648456ms]
Mar 20 08:31:07.430: INFO: Created: latency-svc-f8lws
Mar 20 08:31:07.467: INFO: Got endpoints: latency-svc-fggks [752.924409ms]
Mar 20 08:31:07.483: INFO: Created: latency-svc-n97pb
Mar 20 08:31:07.515: INFO: Got endpoints: latency-svc-2mfgm [754.141586ms]
Mar 20 08:31:07.529: INFO: Created: latency-svc-bn54c
Mar 20 08:31:07.564: INFO: Got endpoints: latency-svc-tpnbl [752.407146ms]
Mar 20 08:31:07.579: INFO: Created: latency-svc-mcf2h
Mar 20 08:31:07.615: INFO: Got endpoints: latency-svc-zrh5p [753.426269ms]
Mar 20 08:31:07.628: INFO: Created: latency-svc-f9wlz
Mar 20 08:31:07.664: INFO: Got endpoints: latency-svc-c6xv8 [751.434903ms]
Mar 20 08:31:07.686: INFO: Created: latency-svc-v9ds2
Mar 20 08:31:07.716: INFO: Got endpoints: latency-svc-z6dgg [751.711479ms]
Mar 20 08:31:07.730: INFO: Created: latency-svc-kjt9c
Mar 20 08:31:07.763: INFO: Got endpoints: latency-svc-7fb8q [751.611313ms]
Mar 20 08:31:07.779: INFO: Created: latency-svc-d48zz
Mar 20 08:31:07.815: INFO: Got endpoints: latency-svc-cdtwx [750.017318ms]
Mar 20 08:31:07.828: INFO: Created: latency-svc-qxhm9
Mar 20 08:31:07.865: INFO: Got endpoints: latency-svc-rslxr [749.431643ms]
Mar 20 08:31:07.896: INFO: Created: latency-svc-6qssc
Mar 20 08:31:07.913: INFO: Got endpoints: latency-svc-mzdsr [748.42305ms]
Mar 20 08:31:07.923: INFO: Created: latency-svc-rbpk4
Mar 20 08:31:07.962: INFO: Got endpoints: latency-svc-dbgh9 [748.100153ms]
Mar 20 08:31:07.979: INFO: Created: latency-svc-x4xpx
Mar 20 08:31:08.011: INFO: Got endpoints: latency-svc-59m47 [748.550538ms]
Mar 20 08:31:08.021: INFO: Created: latency-svc-8tqbg
Mar 20 08:31:08.065: INFO: Got endpoints: latency-svc-x9xs7 [750.83194ms]
Mar 20 08:31:08.079: INFO: Created: latency-svc-qt67g
Mar 20 08:31:08.115: INFO: Got endpoints: latency-svc-fs69s [747.68883ms]
Mar 20 08:31:08.128: INFO: Created: latency-svc-mbgjk
Mar 20 08:31:08.164: INFO: Got endpoints: latency-svc-f8lws [748.898377ms]
Mar 20 08:31:08.177: INFO: Created: latency-svc-lknmf
Mar 20 08:31:08.215: INFO: Got endpoints: latency-svc-n97pb [747.75336ms]
Mar 20 08:31:08.228: INFO: Created: latency-svc-wrxbd
Mar 20 08:31:08.265: INFO: Got endpoints: latency-svc-bn54c [749.811125ms]
Mar 20 08:31:08.278: INFO: Created: latency-svc-9j22l
Mar 20 08:31:08.317: INFO: Got endpoints: latency-svc-mcf2h [752.864712ms]
Mar 20 08:31:08.330: INFO: Created: latency-svc-5fnb9
Mar 20 08:31:08.365: INFO: Got endpoints: latency-svc-f9wlz [750.123349ms]
Mar 20 08:31:08.380: INFO: Created: latency-svc-xkdv8
Mar 20 08:31:08.415: INFO: Got endpoints: latency-svc-v9ds2 [750.937356ms]
Mar 20 08:31:08.435: INFO: Created: latency-svc-f76ql
Mar 20 08:31:08.464: INFO: Got endpoints: latency-svc-kjt9c [748.229758ms]
Mar 20 08:31:08.476: INFO: Created: latency-svc-4725p
Mar 20 08:31:08.516: INFO: Got endpoints: latency-svc-d48zz [752.496649ms]
Mar 20 08:31:08.530: INFO: Created: latency-svc-cdcw8
Mar 20 08:31:08.566: INFO: Got endpoints: latency-svc-qxhm9 [751.462315ms]
Mar 20 08:31:08.582: INFO: Created: latency-svc-2hxvk
Mar 20 08:31:08.644: INFO: Got endpoints: latency-svc-6qssc [779.120401ms]
Mar 20 08:31:08.658: INFO: Created: latency-svc-5fzfw
Mar 20 08:31:08.663: INFO: Got endpoints: latency-svc-rbpk4 [749.034474ms]
Mar 20 08:31:08.675: INFO: Created: latency-svc-2c4qn
Mar 20 08:31:08.715: INFO: Got endpoints: latency-svc-x4xpx [753.088738ms]
Mar 20 08:31:08.732: INFO: Created: latency-svc-tv9v8
Mar 20 08:31:08.767: INFO: Got endpoints: latency-svc-8tqbg [755.610985ms]
Mar 20 08:31:08.780: INFO: Created: latency-svc-2kzjs
Mar 20 08:31:08.814: INFO: Got endpoints: latency-svc-qt67g [749.496872ms]
Mar 20 08:31:08.828: INFO: Created: latency-svc-bjv7p
Mar 20 08:31:08.861: INFO: Got endpoints: latency-svc-mbgjk [746.30293ms]
Mar 20 08:31:08.881: INFO: Created: latency-svc-w295x
Mar 20 08:31:08.912: INFO: Got endpoints: latency-svc-lknmf [748.082843ms]
Mar 20 08:31:08.923: INFO: Created: latency-svc-scrm9
Mar 20 08:31:08.963: INFO: Got endpoints: latency-svc-wrxbd [747.958658ms]
Mar 20 08:31:08.976: INFO: Created: latency-svc-7mcl9
Mar 20 08:31:09.012: INFO: Got endpoints: latency-svc-9j22l [746.98009ms]
Mar 20 08:31:09.021: INFO: Created: latency-svc-ftcr2
Mar 20 08:31:09.065: INFO: Got endpoints: latency-svc-5fnb9 [748.071621ms]
Mar 20 08:31:09.080: INFO: Created: latency-svc-zfk7l
Mar 20 08:31:09.116: INFO: Got endpoints: latency-svc-xkdv8 [750.711596ms]
Mar 20 08:31:09.130: INFO: Created: latency-svc-6b85s
Mar 20 08:31:09.187: INFO: Got endpoints: latency-svc-f76ql [771.661582ms]
Mar 20 08:31:09.203: INFO: Created: latency-svc-9r7pl
Mar 20 08:31:09.213: INFO: Got endpoints: latency-svc-4725p [749.094665ms]
Mar 20 08:31:09.225: INFO: Created: latency-svc-95g8q
Mar 20 08:31:09.262: INFO: Got endpoints: latency-svc-cdcw8 [746.35176ms]
Mar 20 08:31:09.273: INFO: Created: latency-svc-gtl2d
Mar 20 08:31:09.313: INFO: Got endpoints: latency-svc-2hxvk [746.555241ms]
Mar 20 08:31:09.325: INFO: Created: latency-svc-99t5c
Mar 20 08:31:09.365: INFO: Got endpoints: latency-svc-5fzfw [720.962617ms]
Mar 20 08:31:09.416: INFO: Got endpoints: latency-svc-2c4qn [753.465548ms]
Mar 20 08:31:09.421: INFO: Created: latency-svc-9vgzg
Mar 20 08:31:09.430: INFO: Created: latency-svc-f24zb
Mar 20 08:31:09.466: INFO: Got endpoints: latency-svc-tv9v8 [750.394571ms]
Mar 20 08:31:09.481: INFO: Created: latency-svc-bbgwm
Mar 20 08:31:09.516: INFO: Got endpoints: latency-svc-2kzjs [748.98626ms]
Mar 20 08:31:09.531: INFO: Created: latency-svc-hvhzh
Mar 20 08:31:09.563: INFO: Got endpoints: latency-svc-bjv7p [748.727903ms]
Mar 20 08:31:09.578: INFO: Created: latency-svc-7j64g
Mar 20 08:31:09.615: INFO: Got endpoints: latency-svc-w295x [754.138818ms]
Mar 20 08:31:09.632: INFO: Created: latency-svc-nfc8h
Mar 20 08:31:09.665: INFO: Got endpoints: latency-svc-scrm9 [753.392747ms]
Mar 20 08:31:09.680: INFO: Created: latency-svc-qvbh5
Mar 20 08:31:09.715: INFO: Got endpoints: latency-svc-7mcl9 [752.020456ms]
Mar 20 08:31:09.731: INFO: Created: latency-svc-tslcn
Mar 20 08:31:09.766: INFO: Got endpoints: latency-svc-ftcr2 [753.558067ms]
Mar 20 08:31:09.782: INFO: Created: latency-svc-9cs2r
Mar 20 08:31:09.814: INFO: Got endpoints: latency-svc-zfk7l [749.07887ms]
Mar 20 08:31:09.830: INFO: Created: latency-svc-vgxjm
Mar 20 08:31:09.864: INFO: Got endpoints: latency-svc-6b85s [747.722119ms]
Mar 20 08:31:09.891: INFO: Created: latency-svc-hfjmc
Mar 20 08:31:09.912: INFO: Got endpoints: latency-svc-9r7pl [725.38433ms]
Mar 20 08:31:09.929: INFO: Created: latency-svc-7j8tj
Mar 20 08:31:09.962: INFO: Got endpoints: latency-svc-95g8q [749.076811ms]
Mar 20 08:31:09.978: INFO: Created: latency-svc-tcx9b
Mar 20 08:31:10.012: INFO: Got endpoints: latency-svc-gtl2d [749.547941ms]
Mar 20 08:31:10.024: INFO: Created: latency-svc-6lx9v
Mar 20 08:31:10.065: INFO: Got endpoints: latency-svc-99t5c [752.3201ms]
Mar 20 08:31:10.080: INFO: Created: latency-svc-zf48m
Mar 20 08:31:10.115: INFO: Got endpoints: latency-svc-9vgzg [750.04586ms]
Mar 20 08:31:10.176: INFO: Got endpoints: latency-svc-f24zb [760.179326ms]
Mar 20 08:31:10.181: INFO: Created: latency-svc-rzktj
Mar 20 08:31:10.191: INFO: Created: latency-svc-rfgkq
Mar 20 08:31:10.216: INFO: Got endpoints: latency-svc-bbgwm [749.911648ms]
Mar 20 08:31:10.231: INFO: Created: latency-svc-g8zqr
Mar 20 08:31:10.264: INFO: Got endpoints: latency-svc-hvhzh [748.55945ms]
Mar 20 08:31:10.276: INFO: Created: latency-svc-gfkgm
Mar 20 08:31:10.315: INFO: Got endpoints: latency-svc-7j64g [751.691257ms]
Mar 20 08:31:10.331: INFO: Created: latency-svc-7mqss
Mar 20 08:31:10.366: INFO: Got endpoints: latency-svc-nfc8h [750.758212ms]
Mar 20 08:31:10.381: INFO: Created: latency-svc-x78fr
Mar 20 08:31:10.415: INFO: Got endpoints: latency-svc-qvbh5 [749.74555ms]
Mar 20 08:31:10.429: INFO: Created: latency-svc-sw4zg
Mar 20 08:31:10.466: INFO: Got endpoints: latency-svc-tslcn [750.810917ms]
Mar 20 08:31:10.481: INFO: Created: latency-svc-r9nd5
Mar 20 08:31:10.515: INFO: Got endpoints: latency-svc-9cs2r [749.225847ms]
Mar 20 08:31:10.530: INFO: Created: latency-svc-6h67z
Mar 20 08:31:10.566: INFO: Got endpoints: latency-svc-vgxjm [751.656455ms]
Mar 20 08:31:10.580: INFO: Created: latency-svc-z6pnk
Mar 20 08:31:10.615: INFO: Got endpoints: latency-svc-hfjmc [751.056442ms]
Mar 20 08:31:10.629: INFO: Created: latency-svc-wrzfq
Mar 20 08:31:10.665: INFO: Got endpoints: latency-svc-7j8tj [753.047647ms]
Mar 20 08:31:10.726: INFO: Got endpoints: latency-svc-tcx9b [763.674754ms]
Mar 20 08:31:10.731: INFO: Created: latency-svc-489fg
Mar 20 08:31:10.738: INFO: Created: latency-svc-qbkj2
Mar 20 08:31:10.764: INFO: Got endpoints: latency-svc-6lx9v [752.605418ms]
Mar 20 08:31:10.779: INFO: Created: latency-svc-vhjnh
Mar 20 08:31:10.814: INFO: Got endpoints: latency-svc-zf48m [748.930045ms]
Mar 20 08:31:10.830: INFO: Created: latency-svc-l92dj
Mar 20 08:31:10.866: INFO: Got endpoints: latency-svc-rzktj [750.868402ms]
Mar 20 08:31:10.894: INFO: Created: latency-svc-g7vl7
Mar 20 08:31:10.915: INFO: Got endpoints: latency-svc-rfgkq [738.266844ms]
Mar 20 08:31:10.926: INFO: Created: latency-svc-456hl
Mar 20 08:31:10.963: INFO: Got endpoints: latency-svc-g8zqr [747.209223ms]
Mar 20 08:31:10.978: INFO: Created: latency-svc-zk8br
Mar 20 08:31:11.016: INFO: Got endpoints: latency-svc-gfkgm [751.130903ms]
Mar 20 08:31:11.027: INFO: Created: latency-svc-dcrfl
Mar 20 08:31:11.062: INFO: Got endpoints: latency-svc-7mqss [746.640797ms]
Mar 20 08:31:11.073: INFO: Created: latency-svc-bgqmx
Mar 20 08:31:11.116: INFO: Got endpoints: latency-svc-x78fr [749.359619ms]
Mar 20 08:31:11.133: INFO: Created: latency-svc-z8sv9
Mar 20 08:31:11.165: INFO: Got endpoints: latency-svc-sw4zg [750.076588ms]
Mar 20 08:31:11.181: INFO: Created: latency-svc-s9kvg
Mar 20 08:31:11.215: INFO: Got endpoints: latency-svc-r9nd5 [749.606359ms]
Mar 20 08:31:11.232: INFO: Created: latency-svc-cg6gh
Mar 20 08:31:11.265: INFO: Got endpoints: latency-svc-6h67z [750.145351ms]
Mar 20 08:31:11.278: INFO: Created: latency-svc-d6js7
Mar 20 08:31:11.315: INFO: Got endpoints: latency-svc-z6pnk [749.667695ms]
Mar 20 08:31:11.331: INFO: Created: latency-svc-jwkx6
Mar 20 08:31:11.362: INFO: Got endpoints: latency-svc-wrzfq [747.343493ms]
Mar 20 08:31:11.377: INFO: Created: latency-svc-fhnb5
Mar 20 08:31:11.415: INFO: Got endpoints: latency-svc-489fg [749.68156ms]
Mar 20 08:31:11.431: INFO: Created: latency-svc-7v2tm
Mar 20 08:31:11.463: INFO: Got endpoints: latency-svc-qbkj2 [736.714405ms]
Mar 20 08:31:11.475: INFO: Created: latency-svc-nh7xq
Mar 20 08:31:11.516: INFO: Got endpoints: latency-svc-vhjnh [751.413511ms]
Mar 20 08:31:11.533: INFO: Created: latency-svc-jf88s
Mar 20 08:31:11.565: INFO: Got endpoints: latency-svc-l92dj [750.317375ms]
Mar 20 08:31:11.579: INFO: Created: latency-svc-6br9c
Mar 20 08:31:11.614: INFO: Got endpoints: latency-svc-g7vl7 [748.156932ms]
Mar 20 08:31:11.687: INFO: Got endpoints: latency-svc-456hl [772.180514ms]
Mar 20 08:31:11.687: INFO: Created: latency-svc-8nmxh
Mar 20 08:31:11.709: INFO: Created: latency-svc-zn46z
Mar 20 08:31:11.713: INFO: Got endpoints: latency-svc-zk8br [750.178104ms]
Mar 20 08:31:11.763: INFO: Got endpoints: latency-svc-dcrfl [747.858388ms]
Mar 20 08:31:11.815: INFO: Got endpoints: latency-svc-bgqmx [753.232571ms]
Mar 20 08:31:11.863: INFO: Got endpoints: latency-svc-z8sv9 [746.993704ms]
Mar 20 08:31:11.912: INFO: Got endpoints: latency-svc-s9kvg [746.923805ms]
Mar 20 08:31:11.964: INFO: Got endpoints: latency-svc-cg6gh [748.692387ms]
Mar 20 08:31:12.013: INFO: Got endpoints: latency-svc-d6js7 [747.860705ms]
Mar 20 08:31:12.063: INFO: Got endpoints: latency-svc-jwkx6 [747.714759ms]
Mar 20 08:31:12.114: INFO: Got endpoints: latency-svc-fhnb5 [751.063488ms]
Mar 20 08:31:12.164: INFO: Got endpoints: latency-svc-7v2tm [748.995378ms]
Mar 20 08:31:12.214: INFO: Got endpoints: latency-svc-nh7xq [751.443884ms]
Mar 20 08:31:12.265: INFO: Got endpoints: latency-svc-jf88s [749.325712ms]
Mar 20 08:31:12.315: INFO: Got endpoints: latency-svc-6br9c [750.410601ms]
Mar 20 08:31:12.365: INFO: Got endpoints: latency-svc-8nmxh [751.209503ms]
Mar 20 08:31:12.416: INFO: Got endpoints: latency-svc-zn46z [729.201977ms]
Mar 20 08:31:12.416: INFO: Latencies: [25.52899ms 25.737299ms 29.659262ms 29.737558ms 29.75237ms 36.50906ms 43.69073ms 52.722233ms 53.380098ms 62.435411ms 64.458182ms 70.563041ms 73.156705ms 79.499476ms 80.939564ms 84.506656ms 84.675982ms 88.734395ms 92.544371ms 94.2113ms 96.85432ms 98.194446ms 98.639756ms 103.330693ms 104.53637ms 106.803509ms 117.277421ms 121.237295ms 123.046313ms 125.602443ms 134.485535ms 142.192898ms 143.152222ms 153.462909ms 167.000577ms 167.443097ms 212.800079ms 259.656044ms 309.291394ms 359.124938ms 409.007012ms 454.318415ms 499.557576ms 550.320225ms 610.474248ms 649.740739ms 700.493074ms 720.962617ms 725.38433ms 729.201977ms 736.714405ms 737.545951ms 738.266844ms 739.542322ms 742.686932ms 743.895382ms 746.130913ms 746.30293ms 746.35176ms 746.369727ms 746.555241ms 746.640797ms 746.923805ms 746.98009ms 746.993704ms 747.097292ms 747.193279ms 747.209223ms 747.343493ms 747.450909ms 747.558321ms 747.68883ms 747.714759ms 747.722119ms 747.75336ms 747.858388ms 747.860705ms 747.941407ms 747.958658ms 748.071621ms 748.082843ms 748.100153ms 748.144741ms 748.156932ms 748.220836ms 748.229758ms 748.379743ms 748.38012ms 748.42305ms 748.550538ms 748.55945ms 748.692387ms 748.707217ms 748.727903ms 748.898377ms 748.930045ms 748.98626ms 748.995378ms 749.034474ms 749.076811ms 749.07887ms 749.094665ms 749.21989ms 749.221687ms 749.225847ms 749.274913ms 749.276725ms 749.289842ms 749.325712ms 749.359619ms 749.384839ms 749.395016ms 749.431643ms 749.496872ms 749.512569ms 749.547941ms 749.564501ms 749.606359ms 749.627938ms 749.652237ms 749.665609ms 749.667695ms 749.675321ms 749.68156ms 749.74555ms 749.811125ms 749.911648ms 749.937517ms 749.958316ms 750.017318ms 750.04586ms 750.064558ms 750.076588ms 750.080184ms 750.123349ms 750.145351ms 750.178104ms 750.198239ms 750.283699ms 750.283907ms 750.311717ms 750.317375ms 750.394571ms 750.410601ms 750.450088ms 750.482153ms 750.482467ms 750.687191ms 750.711596ms 750.748851ms 750.758212ms 750.760084ms 750.810917ms 750.83194ms 750.868402ms 750.875618ms 750.937356ms 750.952681ms 751.056442ms 751.063488ms 751.130903ms 751.209503ms 751.413511ms 751.434903ms 751.443884ms 751.462315ms 751.476306ms 751.553396ms 751.611313ms 751.656455ms 751.665647ms 751.691257ms 751.711479ms 751.787564ms 751.931453ms 752.020456ms 752.3201ms 752.407146ms 752.495193ms 752.496649ms 752.605418ms 752.648456ms 752.864712ms 752.924409ms 753.047647ms 753.088738ms 753.232571ms 753.392747ms 753.426269ms 753.465548ms 753.558067ms 754.138818ms 754.141586ms 755.610985ms 760.179326ms 763.674754ms 763.757233ms 771.661582ms 772.180514ms 779.120401ms]
Mar 20 08:31:12.416: INFO: 50 %ile: 749.07887ms
Mar 20 08:31:12.416: INFO: 90 %ile: 752.605418ms
Mar 20 08:31:12.416: INFO: 99 %ile: 772.180514ms
Mar 20 08:31:12.416: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:188
Mar 20 08:31:12.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8476" for this suite.

• [SLOW TEST:10.799 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":356,"completed":255,"skipped":4609,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:31:12.442: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 20 08:31:12.468: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 20 08:31:12.474: INFO: Waiting for terminating namespaces to be deleted...
Mar 20 08:31:12.477: INFO: 
Logging pods the apiserver thinks is on node env016ar130-worker01 before test
Mar 20 08:31:12.497: INFO: calico-node-hdjmf from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container calico-node ready: true, restart count 0
Mar 20 08:31:12.497: INFO: config-update-8kkf9 from kube-system started at 2023-03-20 07:47:02 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container config-update ready: true, restart count 0
Mar 20 08:31:12.497: INFO: kube-multus-ds-w25rw from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container kube-multus ready: true, restart count 0
Mar 20 08:31:12.497: INFO: kube-proxy-sjh4k from kube-system started at 2023-03-16 12:25:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 20 08:31:12.497: INFO: static-lb-env016ar130-worker01 from kube-system started at 2023-03-16 12:25:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container static-lb ready: true, restart count 0
Mar 20 08:31:12.497: INFO: alertmanager-prometheus-kube-prometheus-alertmanager-0 from kubeops started at 2023-03-20 07:46:50 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container alertmanager ready: true, restart count 1
Mar 20 08:31:12.497: INFO: 	Container config-reloader ready: true, restart count 0
Mar 20 08:31:12.497: INFO: csi-cephfsplugin-fzjzg from kubeops started at 2023-03-20 07:46:45 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 08:31:12.497: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:31:12.497: INFO: csi-rbdplugin-m8z7v from kubeops started at 2023-03-20 07:46:45 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 08:31:12.497: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:31:12.497: INFO: filebeat-filebeat-7zl4k from kubeops started at 2023-03-20 07:46:45 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container filebeat ready: true, restart count 0
Mar 20 08:31:12.497: INFO: prometheus-prometheus-kube-prometheus-prometheus-0 from kubeops started at 2023-03-20 07:46:59 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container config-reloader ready: true, restart count 0
Mar 20 08:31:12.497: INFO: 	Container prometheus ready: true, restart count 0
Mar 20 08:31:12.497: INFO: prometheus-prometheus-node-exporter-hpppp from kubeops started at 2023-03-20 07:46:45 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container node-exporter ready: true, restart count 0
Mar 20 08:31:12.497: INFO: rook-ceph-crashcollector-env016ar130-worker01-766b6f9754-rt44x from kubeops started at 2023-03-20 07:46:45 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container ceph-crash ready: true, restart count 0
Mar 20 08:31:12.497: INFO: rook-ceph-mon-c-78d58bd46b-8n5tg from kubeops started at 2023-03-20 07:46:45 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:31:12.497: INFO: 	Container mon ready: true, restart count 0
Mar 20 08:31:12.497: INFO: rook-ceph-osd-0-c77fcd474-m5wsl from kubeops started at 2023-03-20 07:46:45 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:31:12.497: INFO: 	Container osd ready: true, restart count 0
Mar 20 08:31:12.497: INFO: rook-ceph-osd-prepare-env016ar130-worker01-n9m6f from kubeops started at 2023-03-20 07:46:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container provision ready: false, restart count 0
Mar 20 08:31:12.497: INFO: rook-discover-n7tql from kubeops started at 2023-03-20 07:46:45 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container rook-discover ready: true, restart count 0
Mar 20 08:31:12.497: INFO: sonobuoy from sonobuoy started at 2023-03-20 07:27:30 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 20 08:31:12.497: INFO: sonobuoy-e2e-job-5ef492aa84d840a2 from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container e2e ready: true, restart count 0
Mar 20 08:31:12.497: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 08:31:12.497: INFO: sonobuoy-systemd-logs-daemon-set-3a399369495241ac-j79q9 from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 08:31:12.497: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 20 08:31:12.497: INFO: svc-latency-rc-tj8sp from svc-latency-8476 started at 2023-03-20 08:31:01 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.497: INFO: 	Container svc-latency-rc ready: true, restart count 0
Mar 20 08:31:12.497: INFO: 
Logging pods the apiserver thinks is on node env016ar130-worker02 before test
Mar 20 08:31:12.518: INFO: calico-node-bphlv from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container calico-node ready: true, restart count 0
Mar 20 08:31:12.518: INFO: config-update-9h6xg from kube-system started at 2023-03-16 14:15:50 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container config-update ready: true, restart count 0
Mar 20 08:31:12.518: INFO: kube-multus-ds-9xqdf from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container kube-multus ready: true, restart count 0
Mar 20 08:31:12.518: INFO: kube-proxy-dk5kb from kube-system started at 2023-03-16 12:25:25 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 20 08:31:12.518: INFO: static-lb-env016ar130-worker02 from kube-system started at 2023-03-16 12:25:53 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container static-lb ready: true, restart count 0
Mar 20 08:31:12.518: INFO: csi-cephfsplugin-gd4zn from kubeops started at 2023-03-16 14:15:32 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:31:12.518: INFO: csi-cephfsplugin-provisioner-8f66f988-nr7c7 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (5 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 08:31:12.518: INFO: csi-rbdplugin-mkm94 from kubeops started at 2023-03-16 14:15:05 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:31:12.518: INFO: csi-rbdplugin-provisioner-7bb4c8b9c7-h8jb6 from kubeops started at 2023-03-16 15:03:27 +0000 UTC (5 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 08:31:12.518: INFO: filebeat-filebeat-pf2vz from kubeops started at 2023-03-16 14:15:59 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container filebeat ready: true, restart count 0
Mar 20 08:31:12.518: INFO: gatekeeper-audit-749874bd85-gbfdv from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container manager ready: true, restart count 0
Mar 20 08:31:12.518: INFO: gatekeeper-controller-manager-768fd8789c-s4xkm from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container manager ready: true, restart count 0
Mar 20 08:31:12.518: INFO: gatekeeper-controller-manager-768fd8789c-w7sxj from kubeops started at 2023-03-16 15:03:30 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container manager ready: true, restart count 0
Mar 20 08:31:12.518: INFO: gatekeeper-controller-manager-768fd8789c-wgpj5 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container manager ready: true, restart count 0
Mar 20 08:31:12.518: INFO: harbor-chartmuseum-7df8df844b-8fds2 from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container chartmuseum ready: true, restart count 0
Mar 20 08:31:12.518: INFO: harbor-jobservice-7b746648f6-x2kd8 from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container jobservice ready: true, restart count 1
Mar 20 08:31:12.518: INFO: harbor-notary-signer-c9db4c94d-fs46b from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container notary-signer ready: true, restart count 0
Mar 20 08:31:12.518: INFO: harbor-notary-signer-c9db4c94d-zqpzz from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container notary-signer ready: true, restart count 0
Mar 20 08:31:12.518: INFO: harbor-portal-dbbbb4456-7hb5f from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container portal ready: true, restart count 0
Mar 20 08:31:12.518: INFO: harbor-registry-b66c45c5f-lq8vr from kubeops started at 2023-03-16 14:14:52 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container registry ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container registryctl ready: true, restart count 0
Mar 20 08:31:12.518: INFO: harbor-trivy-1 from kubeops started at 2023-03-16 14:16:07 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container trivy ready: true, restart count 0
Mar 20 08:31:12.518: INFO: opensearch-cluster-master-0 from kubeops started at 2023-03-16 14:16:53 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container opensearch ready: true, restart count 0
Mar 20 08:31:12.518: INFO: opensearch-cluster-master-2 from kubeops started at 2023-03-17 12:48:10 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container opensearch ready: true, restart count 0
Mar 20 08:31:12.518: INFO: prometheus-prometheus-node-exporter-rxbn4 from kubeops started at 2023-03-16 14:15:38 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container node-exporter ready: true, restart count 0
Mar 20 08:31:12.518: INFO: rook-ceph-crashcollector-env016ar130-worker02-799c88cbcf-9ktc7 from kubeops started at 2023-03-16 15:03:28 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container ceph-crash ready: true, restart count 0
Mar 20 08:31:12.518: INFO: rook-ceph-mds-myfs-a-85c5fc87d-gj7d4 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container mds ready: true, restart count 0
Mar 20 08:31:12.518: INFO: rook-ceph-mgr-a-857696f864-4gjb2 from kubeops started at 2023-03-16 15:03:30 +0000 UTC (3 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container mgr ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container watch-active ready: true, restart count 0
Mar 20 08:31:12.518: INFO: rook-ceph-mon-a-cc8d49d77-sgj2c from kubeops started at 2023-03-16 14:15:59 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container mon ready: true, restart count 0
Mar 20 08:31:12.518: INFO: rook-ceph-osd-2-557dc55864-679xk from kubeops started at 2023-03-16 14:15:54 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container osd ready: true, restart count 0
Mar 20 08:31:12.518: INFO: rook-ceph-osd-prepare-env016ar130-worker02-w9vhn from kubeops started at 2023-03-20 07:46:55 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container provision ready: false, restart count 0
Mar 20 08:31:12.518: INFO: rook-ceph-tools-59d749577f-chcwh from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Mar 20 08:31:12.518: INFO: rook-discover-x9lwv from kubeops started at 2023-03-16 14:15:44 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container rook-discover ready: true, restart count 0
Mar 20 08:31:12.518: INFO: sonobuoy-systemd-logs-daemon-set-3a399369495241ac-swzbr from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.518: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 20 08:31:12.518: INFO: 
Logging pods the apiserver thinks is on node env016ar130-worker03 before test
Mar 20 08:31:12.552: INFO: calico-node-cpqql from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container calico-node ready: true, restart count 0
Mar 20 08:31:12.552: INFO: config-update-lj4n7 from kube-system started at 2023-03-16 12:35:49 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container config-update ready: true, restart count 0
Mar 20 08:31:12.552: INFO: kube-multus-ds-xcsml from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container kube-multus ready: true, restart count 0
Mar 20 08:31:12.552: INFO: kube-proxy-hxh8h from kube-system started at 2023-03-16 12:25:31 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 20 08:31:12.552: INFO: static-lb-env016ar130-worker03 from kube-system started at 2023-03-16 12:25:53 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container static-lb ready: true, restart count 0
Mar 20 08:31:12.552: INFO: csi-cephfsplugin-dpmqw from kubeops started at 2023-03-16 12:41:07 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 08:31:12.552: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:31:12.552: INFO: csi-cephfsplugin-provisioner-8f66f988-hg4ds from kubeops started at 2023-03-16 14:14:52 +0000 UTC (5 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 08:31:12.552: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 08:31:12.552: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 08:31:12.552: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 08:31:12.552: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 08:31:12.552: INFO: csi-rbdplugin-fhk4v from kubeops started at 2023-03-16 12:41:07 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 08:31:12.552: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:31:12.552: INFO: csi-rbdplugin-provisioner-7bb4c8b9c7-z7z8d from kubeops started at 2023-03-16 12:41:07 +0000 UTC (5 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 08:31:12.552: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 08:31:12.552: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 08:31:12.552: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 08:31:12.552: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 08:31:12.552: INFO: filebeat-filebeat-hbpls from kubeops started at 2023-03-16 12:56:38 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container filebeat ready: true, restart count 0
Mar 20 08:31:12.552: INFO: harbor-chartmuseum-7df8df844b-7zqxs from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container chartmuseum ready: true, restart count 0
Mar 20 08:31:12.552: INFO: harbor-chartmuseum-7df8df844b-hks64 from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container chartmuseum ready: true, restart count 0
Mar 20 08:31:12.552: INFO: harbor-core-d54995cdd-6c49r from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container core ready: true, restart count 0
Mar 20 08:31:12.552: INFO: harbor-core-d54995cdd-brrck from kubeops started at 2023-03-16 15:03:27 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container core ready: true, restart count 1
Mar 20 08:31:12.552: INFO: harbor-core-d54995cdd-n4l9h from kubeops started at 2023-03-16 15:03:28 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container core ready: true, restart count 0
Mar 20 08:31:12.552: INFO: harbor-jobservice-7b746648f6-cv6bz from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container jobservice ready: true, restart count 1
Mar 20 08:31:12.552: INFO: harbor-jobservice-7b746648f6-l24wk from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container jobservice ready: true, restart count 4
Mar 20 08:31:12.552: INFO: harbor-nginx-f8c975ff6-2bwcd from kubeops started at 2023-03-16 12:54:06 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container nginx ready: true, restart count 0
Mar 20 08:31:12.552: INFO: harbor-nginx-f8c975ff6-5hv2g from kubeops started at 2023-03-16 15:03:27 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container nginx ready: true, restart count 0
Mar 20 08:31:12.552: INFO: harbor-nginx-f8c975ff6-hllb9 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.552: INFO: 	Container nginx ready: true, restart count 0
Mar 20 08:31:12.552: INFO: harbor-notary-server-89fd67fc6-67cdb from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container notary-server ready: true, restart count 2
Mar 20 08:31:12.553: INFO: harbor-notary-server-89fd67fc6-7h95b from kubeops started at 2023-03-16 12:54:06 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container notary-server ready: true, restart count 0
Mar 20 08:31:12.553: INFO: harbor-notary-server-89fd67fc6-kn77l from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container notary-server ready: true, restart count 2
Mar 20 08:31:12.553: INFO: harbor-notary-signer-c9db4c94d-txwpf from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container notary-signer ready: true, restart count 2
Mar 20 08:31:12.553: INFO: harbor-portal-dbbbb4456-gqbx6 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container portal ready: true, restart count 0
Mar 20 08:31:12.553: INFO: harbor-portal-dbbbb4456-qgsgw from kubeops started at 2023-03-16 12:54:06 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container portal ready: true, restart count 0
Mar 20 08:31:12.553: INFO: harbor-registry-b66c45c5f-d5q94 from kubeops started at 2023-03-16 12:54:07 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container registry ready: true, restart count 0
Mar 20 08:31:12.553: INFO: 	Container registryctl ready: true, restart count 0
Mar 20 08:31:12.553: INFO: harbor-registry-b66c45c5f-fr9g4 from kubeops started at 2023-03-16 15:03:27 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container registry ready: true, restart count 0
Mar 20 08:31:12.553: INFO: 	Container registryctl ready: true, restart count 0
Mar 20 08:31:12.553: INFO: harbor-trivy-0 from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container trivy ready: true, restart count 0
Mar 20 08:31:12.553: INFO: harbor-trivy-2 from kubeops started at 2023-03-16 12:55:51 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container trivy ready: true, restart count 0
Mar 20 08:31:12.553: INFO: logstash-logstash-0 from kubeops started at 2023-03-16 12:56:41 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container logstash ready: true, restart count 1
Mar 20 08:31:12.553: INFO: opensearch-cluster-master-1 from kubeops started at 2023-03-16 12:56:36 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container opensearch ready: true, restart count 0
Mar 20 08:31:12.553: INFO: opensearch-dashboards-69f44df846-zmxg9 from kubeops started at 2023-03-16 12:56:36 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container dashboards ready: true, restart count 0
Mar 20 08:31:12.553: INFO: postgres-7f6cc6d46c-cdpmp from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container postgres ready: true, restart count 0
Mar 20 08:31:12.553: INFO: prometheus-grafana-5c58fc9dbb-gj98n from kubeops started at 2023-03-16 12:55:16 +0000 UTC (3 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container grafana ready: true, restart count 0
Mar 20 08:31:12.553: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Mar 20 08:31:12.553: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Mar 20 08:31:12.553: INFO: prometheus-kube-prometheus-operator-76b748b4b7-hgsmt from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Mar 20 08:31:12.553: INFO: prometheus-kube-state-metrics-85655df84d-qsk5t from kubeops started at 2023-03-16 12:55:16 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 20 08:31:12.553: INFO: prometheus-prometheus-node-exporter-sqpkb from kubeops started at 2023-03-16 12:55:16 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container node-exporter ready: true, restart count 0
Mar 20 08:31:12.553: INFO: redis-6fdbc8bc6c-cj4g2 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container redis ready: true, restart count 0
Mar 20 08:31:12.553: INFO: rook-ceph-crashcollector-env016ar130-worker03-68f58d9f4b-pbp4c from kubeops started at 2023-03-16 12:44:17 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container ceph-crash ready: true, restart count 0
Mar 20 08:31:12.553: INFO: rook-ceph-mds-myfs-b-767cb4cfc5-p5f4k from kubeops started at 2023-03-16 12:44:16 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:31:12.553: INFO: 	Container mds ready: true, restart count 0
Mar 20 08:31:12.553: INFO: rook-ceph-mgr-b-69f6d8d6c-8klhf from kubeops started at 2023-03-16 12:43:36 +0000 UTC (3 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:31:12.553: INFO: 	Container mgr ready: true, restart count 0
Mar 20 08:31:12.553: INFO: 	Container watch-active ready: true, restart count 0
Mar 20 08:31:12.553: INFO: rook-ceph-mon-b-7b5485875c-fjr22 from kubeops started at 2023-03-16 12:43:15 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:31:12.553: INFO: 	Container mon ready: true, restart count 0
Mar 20 08:31:12.553: INFO: rook-ceph-operator-f6f75855b-lt27v from kubeops started at 2023-03-16 12:39:01 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Mar 20 08:31:12.553: INFO: rook-ceph-osd-1-755cf9d57c-j6jcs from kubeops started at 2023-03-16 12:44:07 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:31:12.553: INFO: 	Container osd ready: true, restart count 0
Mar 20 08:31:12.553: INFO: rook-ceph-osd-prepare-env016ar130-worker03-8nc47 from kubeops started at 2023-03-20 07:46:58 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container provision ready: false, restart count 0
Mar 20 08:31:12.553: INFO: rook-discover-wj92q from kubeops started at 2023-03-16 12:39:43 +0000 UTC (1 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container rook-discover ready: true, restart count 0
Mar 20 08:31:12.553: INFO: sonobuoy-systemd-logs-daemon-set-3a399369495241ac-zkwlr from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 08:31:12.553: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 08:31:12.553: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f7357357-0cee-4c08-a833-cbd95eff5bee 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-f7357357-0cee-4c08-a833-cbd95eff5bee off the node env016ar130-worker01
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f7357357-0cee-4c08-a833-cbd95eff5bee
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Mar 20 08:31:16.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8732" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":356,"completed":256,"skipped":4626,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:31:16.700: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-bd161cbb-2784-44c3-91b0-55fbf2e5f2c2
STEP: Creating secret with name s-test-opt-upd-6e778947-5405-4cf5-93a7-05697205e0b7
STEP: Creating the pod
Mar 20 08:31:16.759: INFO: The status of Pod pod-projected-secrets-9cf27305-d227-4fd6-924b-f3ed298b6ef9 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:31:18.765: INFO: The status of Pod pod-projected-secrets-9cf27305-d227-4fd6-924b-f3ed298b6ef9 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-bd161cbb-2784-44c3-91b0-55fbf2e5f2c2
STEP: Updating secret s-test-opt-upd-6e778947-5405-4cf5-93a7-05697205e0b7
STEP: Creating secret with name s-test-opt-create-f736123c-5776-4301-b052-86efdc678d11
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Mar 20 08:31:20.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8217" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":257,"skipped":4666,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:31:20.867: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5293, will wait for the garbage collector to delete the pods
Mar 20 08:31:22.967: INFO: Deleting Job.batch foo took: 8.106283ms
Mar 20 08:31:23.468: INFO: Terminating Job.batch foo pods took: 500.994045ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Mar 20 08:31:55.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5293" for this suite.

• [SLOW TEST:34.925 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":356,"completed":258,"skipped":4686,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:31:55.792: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-projected-all-test-volume-418d8d04-7d50-4763-9730-2f2eafaba890
STEP: Creating secret with name secret-projected-all-test-volume-817a8f9d-c098-4737-9a92-a67572f5d949
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 20 08:31:55.855: INFO: Waiting up to 5m0s for pod "projected-volume-f0fe26d2-40eb-4795-a68c-cb2072e389e7" in namespace "projected-987" to be "Succeeded or Failed"
Mar 20 08:31:55.858: INFO: Pod "projected-volume-f0fe26d2-40eb-4795-a68c-cb2072e389e7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.367127ms
Mar 20 08:31:57.864: INFO: Pod "projected-volume-f0fe26d2-40eb-4795-a68c-cb2072e389e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009399403s
Mar 20 08:31:59.884: INFO: Pod "projected-volume-f0fe26d2-40eb-4795-a68c-cb2072e389e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028785377s
STEP: Saw pod success
Mar 20 08:31:59.884: INFO: Pod "projected-volume-f0fe26d2-40eb-4795-a68c-cb2072e389e7" satisfied condition "Succeeded or Failed"
Mar 20 08:31:59.888: INFO: Trying to get logs from node env016ar130-worker01 pod projected-volume-f0fe26d2-40eb-4795-a68c-cb2072e389e7 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 20 08:31:59.927: INFO: Waiting for pod projected-volume-f0fe26d2-40eb-4795-a68c-cb2072e389e7 to disappear
Mar 20 08:31:59.930: INFO: Pod projected-volume-f0fe26d2-40eb-4795-a68c-cb2072e389e7 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:188
Mar 20 08:31:59.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-987" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":356,"completed":259,"skipped":4693,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:31:59.942: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:32:00.021: INFO: Create a RollingUpdate DaemonSet
Mar 20 08:32:00.031: INFO: Check that daemon pods launch on every node of the cluster
Mar 20 08:32:00.036: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:00.036: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:00.036: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:00.040: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:32:00.040: INFO: Node env016ar130-worker01 is running 0 daemon pod, expected 1
Mar 20 08:32:01.049: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:01.049: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:01.049: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:01.052: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 20 08:32:01.052: INFO: Node env016ar130-worker01 is running 0 daemon pod, expected 1
Mar 20 08:32:02.048: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:02.048: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:02.049: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:02.053: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 20 08:32:02.053: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Mar 20 08:32:02.053: INFO: Update the DaemonSet to trigger a rollout
Mar 20 08:32:02.071: INFO: Updating DaemonSet daemon-set
Mar 20 08:32:05.099: INFO: Roll back the DaemonSet before rollout is complete
Mar 20 08:32:05.119: INFO: Updating DaemonSet daemon-set
Mar 20 08:32:05.119: INFO: Make sure DaemonSet rollback is complete
Mar 20 08:32:05.124: INFO: Wrong image for pod: daemon-set-rttth. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Mar 20 08:32:05.124: INFO: Pod daemon-set-rttth is not available
Mar 20 08:32:05.131: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:05.131: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:05.131: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:06.149: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:06.149: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:06.149: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:07.145: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:07.145: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:07.145: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:08.146: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:08.146: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:08.146: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:09.142: INFO: Pod daemon-set-vlmvc is not available
Mar 20 08:32:09.148: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:09.148: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:32:09.148: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5393, will wait for the garbage collector to delete the pods
Mar 20 08:32:09.252: INFO: Deleting DaemonSet.extensions daemon-set took: 7.847851ms
Mar 20 08:32:09.353: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.174958ms
Mar 20 08:32:11.663: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:32:11.663: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 20 08:32:11.667: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"830887"},"items":null}

Mar 20 08:32:11.670: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"830887"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Mar 20 08:32:11.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5393" for this suite.

• [SLOW TEST:11.759 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":356,"completed":260,"skipped":4697,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:32:11.702: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6429
[It] should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-6429
Mar 20 08:32:11.743: INFO: Found 0 stateful pods, waiting for 1
Mar 20 08:32:21.755: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 20 08:32:21.839: INFO: Deleting all statefulset in ns statefulset-6429
Mar 20 08:32:21.844: INFO: Scaling statefulset ss to 0
Mar 20 08:32:31.873: INFO: Waiting for statefulset status.replicas updated to 0
Mar 20 08:32:31.877: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar 20 08:32:31.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6429" for this suite.

• [SLOW TEST:20.220 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":356,"completed":261,"skipped":4717,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:32:31.928: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2193
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Mar 20 08:32:31.986: INFO: Found 0 stateful pods, waiting for 3
Mar 20 08:32:41.997: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 20 08:32:41.997: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 20 08:32:41.997: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Mar 20 08:32:42.039: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 20 08:32:52.095: INFO: Updating stateful set ss2
Mar 20 08:32:52.101: INFO: Waiting for Pod statefulset-2193/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Mar 20 08:33:02.166: INFO: Found 2 stateful pods, waiting for 3
Mar 20 08:33:12.180: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 20 08:33:12.180: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 20 08:33:12.180: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 20 08:33:12.220: INFO: Updating stateful set ss2
Mar 20 08:33:12.225: INFO: Waiting for Pod statefulset-2193/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Mar 20 08:33:22.271: INFO: Updating stateful set ss2
Mar 20 08:33:22.278: INFO: Waiting for StatefulSet statefulset-2193/ss2 to complete update
Mar 20 08:33:22.278: INFO: Waiting for Pod statefulset-2193/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 20 08:33:32.303: INFO: Deleting all statefulset in ns statefulset-2193
Mar 20 08:33:32.307: INFO: Scaling statefulset ss2 to 0
Mar 20 08:33:42.347: INFO: Waiting for statefulset status.replicas updated to 0
Mar 20 08:33:42.351: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar 20 08:33:42.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2193" for this suite.

• [SLOW TEST:70.453 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":356,"completed":262,"skipped":4762,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:33:42.381: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-e867a6c7-b230-4d5d-ad46-cc9c14555c39
STEP: Creating a pod to test consume secrets
Mar 20 08:33:42.427: INFO: Waiting up to 5m0s for pod "pod-secrets-a667c0bd-50e0-4f7e-8a06-91ac5d544ee1" in namespace "secrets-2906" to be "Succeeded or Failed"
Mar 20 08:33:42.431: INFO: Pod "pod-secrets-a667c0bd-50e0-4f7e-8a06-91ac5d544ee1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.809146ms
Mar 20 08:33:44.443: INFO: Pod "pod-secrets-a667c0bd-50e0-4f7e-8a06-91ac5d544ee1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016002684s
Mar 20 08:33:46.453: INFO: Pod "pod-secrets-a667c0bd-50e0-4f7e-8a06-91ac5d544ee1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02603808s
STEP: Saw pod success
Mar 20 08:33:46.453: INFO: Pod "pod-secrets-a667c0bd-50e0-4f7e-8a06-91ac5d544ee1" satisfied condition "Succeeded or Failed"
Mar 20 08:33:46.457: INFO: Trying to get logs from node env016ar130-worker01 pod pod-secrets-a667c0bd-50e0-4f7e-8a06-91ac5d544ee1 container secret-volume-test: <nil>
STEP: delete the pod
Mar 20 08:33:46.482: INFO: Waiting for pod pod-secrets-a667c0bd-50e0-4f7e-8a06-91ac5d544ee1 to disappear
Mar 20 08:33:46.485: INFO: Pod pod-secrets-a667c0bd-50e0-4f7e-8a06-91ac5d544ee1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar 20 08:33:46.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2906" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":263,"skipped":4767,"failed":0}
SS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:33:46.493: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Mar 20 08:33:46.565: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Mar 20 08:33:46.574: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 20 08:33:46.574: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Mar 20 08:33:46.590: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 20 08:33:46.590: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Mar 20 08:33:46.602: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 20 08:33:46.602: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Mar 20 08:33:53.661: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:188
Mar 20 08:33:53.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7268" for this suite.

• [SLOW TEST:7.204 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":356,"completed":264,"skipped":4769,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:33:53.697: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-256e8847-9274-4772-846b-0a0b540d8cb7
STEP: Creating a pod to test consume secrets
Mar 20 08:33:53.741: INFO: Waiting up to 5m0s for pod "pod-secrets-64e578e7-df1b-4646-a159-dc0ee9b605b1" in namespace "secrets-3932" to be "Succeeded or Failed"
Mar 20 08:33:53.744: INFO: Pod "pod-secrets-64e578e7-df1b-4646-a159-dc0ee9b605b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.449395ms
Mar 20 08:33:55.759: INFO: Pod "pod-secrets-64e578e7-df1b-4646-a159-dc0ee9b605b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018708164s
Mar 20 08:33:57.770: INFO: Pod "pod-secrets-64e578e7-df1b-4646-a159-dc0ee9b605b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02900128s
STEP: Saw pod success
Mar 20 08:33:57.770: INFO: Pod "pod-secrets-64e578e7-df1b-4646-a159-dc0ee9b605b1" satisfied condition "Succeeded or Failed"
Mar 20 08:33:57.774: INFO: Trying to get logs from node env016ar130-worker02 pod pod-secrets-64e578e7-df1b-4646-a159-dc0ee9b605b1 container secret-volume-test: <nil>
STEP: delete the pod
Mar 20 08:33:57.798: INFO: Waiting for pod pod-secrets-64e578e7-df1b-4646-a159-dc0ee9b605b1 to disappear
Mar 20 08:33:57.800: INFO: Pod pod-secrets-64e578e7-df1b-4646-a159-dc0ee9b605b1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar 20 08:33:57.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3932" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":265,"skipped":4790,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:33:57.810: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1305 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1305;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1305 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1305;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1305.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1305.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1305.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1305.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1305.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1305.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1305.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1305.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1305.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1305.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1305.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1305.svc;check="$$(dig +notcp +noall +answer +search 29.239.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.239.29_udp@PTR;check="$$(dig +tcp +noall +answer +search 29.239.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.239.29_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1305 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1305;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1305 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1305;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1305.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1305.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1305.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1305.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1305.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1305.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1305.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1305.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1305.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1305.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1305.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1305.svc;check="$$(dig +notcp +noall +answer +search 29.239.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.239.29_udp@PTR;check="$$(dig +tcp +noall +answer +search 29.239.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.239.29_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 20 08:34:01.913: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.917: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.921: INFO: Unable to read wheezy_udp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.924: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.927: INFO: Unable to read wheezy_udp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.931: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.934: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.937: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.955: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.958: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.961: INFO: Unable to read jessie_udp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.964: INFO: Unable to read jessie_tcp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.967: INFO: Unable to read jessie_udp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.970: INFO: Unable to read jessie_tcp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.973: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.976: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:01.987: INFO: Lookups using dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1305 wheezy_tcp@dns-test-service.dns-1305 wheezy_udp@dns-test-service.dns-1305.svc wheezy_tcp@dns-test-service.dns-1305.svc wheezy_udp@_http._tcp.dns-test-service.dns-1305.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1305.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1305 jessie_tcp@dns-test-service.dns-1305 jessie_udp@dns-test-service.dns-1305.svc jessie_tcp@dns-test-service.dns-1305.svc jessie_udp@_http._tcp.dns-test-service.dns-1305.svc jessie_tcp@_http._tcp.dns-test-service.dns-1305.svc]

Mar 20 08:34:06.995: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:06.999: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:07.003: INFO: Unable to read wheezy_udp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:07.007: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:07.011: INFO: Unable to read wheezy_udp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:07.014: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:07.017: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:07.021: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:07.036: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:07.039: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:07.042: INFO: Unable to read jessie_udp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:07.044: INFO: Unable to read jessie_tcp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:07.047: INFO: Unable to read jessie_udp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:07.050: INFO: Unable to read jessie_tcp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:07.053: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:07.056: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:07.068: INFO: Lookups using dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1305 wheezy_tcp@dns-test-service.dns-1305 wheezy_udp@dns-test-service.dns-1305.svc wheezy_tcp@dns-test-service.dns-1305.svc wheezy_udp@_http._tcp.dns-test-service.dns-1305.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1305.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1305 jessie_tcp@dns-test-service.dns-1305 jessie_udp@dns-test-service.dns-1305.svc jessie_tcp@dns-test-service.dns-1305.svc jessie_udp@_http._tcp.dns-test-service.dns-1305.svc jessie_tcp@_http._tcp.dns-test-service.dns-1305.svc]

Mar 20 08:34:11.996: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.000: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.004: INFO: Unable to read wheezy_udp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.007: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.011: INFO: Unable to read wheezy_udp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.015: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.019: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.022: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.041: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.044: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.048: INFO: Unable to read jessie_udp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.051: INFO: Unable to read jessie_tcp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.054: INFO: Unable to read jessie_udp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.057: INFO: Unable to read jessie_tcp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.059: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.062: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:12.076: INFO: Lookups using dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1305 wheezy_tcp@dns-test-service.dns-1305 wheezy_udp@dns-test-service.dns-1305.svc wheezy_tcp@dns-test-service.dns-1305.svc wheezy_udp@_http._tcp.dns-test-service.dns-1305.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1305.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1305 jessie_tcp@dns-test-service.dns-1305 jessie_udp@dns-test-service.dns-1305.svc jessie_tcp@dns-test-service.dns-1305.svc jessie_udp@_http._tcp.dns-test-service.dns-1305.svc jessie_tcp@_http._tcp.dns-test-service.dns-1305.svc]

Mar 20 08:34:16.995: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.000: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.004: INFO: Unable to read wheezy_udp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.009: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.012: INFO: Unable to read wheezy_udp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.016: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.020: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.023: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.040: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.043: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.047: INFO: Unable to read jessie_udp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.049: INFO: Unable to read jessie_tcp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.052: INFO: Unable to read jessie_udp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.055: INFO: Unable to read jessie_tcp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.059: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.061: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:17.074: INFO: Lookups using dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1305 wheezy_tcp@dns-test-service.dns-1305 wheezy_udp@dns-test-service.dns-1305.svc wheezy_tcp@dns-test-service.dns-1305.svc wheezy_udp@_http._tcp.dns-test-service.dns-1305.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1305.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1305 jessie_tcp@dns-test-service.dns-1305 jessie_udp@dns-test-service.dns-1305.svc jessie_tcp@dns-test-service.dns-1305.svc jessie_udp@_http._tcp.dns-test-service.dns-1305.svc jessie_tcp@_http._tcp.dns-test-service.dns-1305.svc]

Mar 20 08:34:21.997: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.002: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.006: INFO: Unable to read wheezy_udp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.010: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.014: INFO: Unable to read wheezy_udp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.018: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.021: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.025: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.041: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.044: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.047: INFO: Unable to read jessie_udp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.050: INFO: Unable to read jessie_tcp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.053: INFO: Unable to read jessie_udp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.056: INFO: Unable to read jessie_tcp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.059: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.062: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:22.074: INFO: Lookups using dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1305 wheezy_tcp@dns-test-service.dns-1305 wheezy_udp@dns-test-service.dns-1305.svc wheezy_tcp@dns-test-service.dns-1305.svc wheezy_udp@_http._tcp.dns-test-service.dns-1305.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1305.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1305 jessie_tcp@dns-test-service.dns-1305 jessie_udp@dns-test-service.dns-1305.svc jessie_tcp@dns-test-service.dns-1305.svc jessie_udp@_http._tcp.dns-test-service.dns-1305.svc jessie_tcp@_http._tcp.dns-test-service.dns-1305.svc]

Mar 20 08:34:26.993: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:26.999: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:27.011: INFO: Unable to read wheezy_udp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:27.023: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:27.027: INFO: Unable to read wheezy_udp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:27.031: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:27.035: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:27.039: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:27.057: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:27.060: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:27.063: INFO: Unable to read jessie_udp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:27.065: INFO: Unable to read jessie_tcp@dns-test-service.dns-1305 from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:27.068: INFO: Unable to read jessie_udp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:27.070: INFO: Unable to read jessie_tcp@dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:27.072: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:27.075: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1305.svc from pod dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712: the server could not find the requested resource (get pods dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712)
Mar 20 08:34:27.086: INFO: Lookups using dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1305 wheezy_tcp@dns-test-service.dns-1305 wheezy_udp@dns-test-service.dns-1305.svc wheezy_tcp@dns-test-service.dns-1305.svc wheezy_udp@_http._tcp.dns-test-service.dns-1305.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1305.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1305 jessie_tcp@dns-test-service.dns-1305 jessie_udp@dns-test-service.dns-1305.svc jessie_tcp@dns-test-service.dns-1305.svc jessie_udp@_http._tcp.dns-test-service.dns-1305.svc jessie_tcp@_http._tcp.dns-test-service.dns-1305.svc]

Mar 20 08:34:32.068: INFO: DNS probes using dns-1305/dns-test-f5d94607-6219-4bd4-8fd8-f0b36bf13712 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar 20 08:34:32.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1305" for this suite.

• [SLOW TEST:34.324 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":356,"completed":266,"skipped":4803,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:34:32.135: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 08:34:32.542: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 08:34:35.574: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:34:35.584: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2616-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 08:34:38.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1376" for this suite.
STEP: Destroying namespace "webhook-1376-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.712 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":356,"completed":267,"skipped":4803,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:34:38.847: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating cluster-info
Mar 20 08:34:38.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5584 cluster-info'
Mar 20 08:34:38.949: INFO: stderr: ""
Mar 20 08:34:38.949: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://192.168.128.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 08:34:38.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5584" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":356,"completed":268,"skipped":4820,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:34:38.958: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:34:38.997: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 20 08:34:39.007: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:34:39.007: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Mar 20 08:34:39.046: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:34:39.046: INFO: Node env016ar130-worker02 is running 0 daemon pod, expected 1
Mar 20 08:34:40.053: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:34:40.053: INFO: Node env016ar130-worker02 is running 0 daemon pod, expected 1
Mar 20 08:34:41.057: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 20 08:34:41.057: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 20 08:34:41.092: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 20 08:34:41.092: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Mar 20 08:34:42.099: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:34:42.099: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 20 08:34:42.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:34:42.118: INFO: Node env016ar130-worker02 is running 0 daemon pod, expected 1
Mar 20 08:34:43.125: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:34:43.125: INFO: Node env016ar130-worker02 is running 0 daemon pod, expected 1
Mar 20 08:34:44.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:34:44.128: INFO: Node env016ar130-worker02 is running 0 daemon pod, expected 1
Mar 20 08:34:45.125: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 20 08:34:45.125: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4823, will wait for the garbage collector to delete the pods
Mar 20 08:34:45.201: INFO: Deleting DaemonSet.extensions daemon-set took: 7.325225ms
Mar 20 08:34:45.301: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.370949ms
Mar 20 08:34:47.616: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:34:47.616: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 20 08:34:47.619: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"832107"},"items":null}

Mar 20 08:34:47.622: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"832107"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Mar 20 08:34:47.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4823" for this suite.

• [SLOW TEST:8.712 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":356,"completed":269,"skipped":4845,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:34:47.671: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 20 08:34:47.714: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  e2a0e2c5-f2c2-40dc-8652-137ea01187b6 832113 0 2023-03-20 08:34:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-03-20 08:34:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 20 08:34:47.714: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  e2a0e2c5-f2c2-40dc-8652-137ea01187b6 832114 0 2023-03-20 08:34:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-03-20 08:34:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 20 08:34:47.714: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  e2a0e2c5-f2c2-40dc-8652-137ea01187b6 832115 0 2023-03-20 08:34:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-03-20 08:34:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 20 08:34:57.788: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  e2a0e2c5-f2c2-40dc-8652-137ea01187b6 832164 0 2023-03-20 08:34:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-03-20 08:34:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 20 08:34:57.788: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  e2a0e2c5-f2c2-40dc-8652-137ea01187b6 832165 0 2023-03-20 08:34:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-03-20 08:34:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 20 08:34:57.788: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  e2a0e2c5-f2c2-40dc-8652-137ea01187b6 832166 0 2023-03-20 08:34:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-03-20 08:34:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Mar 20 08:34:57.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-599" for this suite.

• [SLOW TEST:10.128 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":356,"completed":270,"skipped":4851,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:34:57.799: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 20 08:34:57.841: INFO: Waiting up to 5m0s for pod "pod-8345fae8-ad9a-483f-ac05-b7494ea4d2bc" in namespace "emptydir-921" to be "Succeeded or Failed"
Mar 20 08:34:57.845: INFO: Pod "pod-8345fae8-ad9a-483f-ac05-b7494ea4d2bc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.38822ms
Mar 20 08:34:59.861: INFO: Pod "pod-8345fae8-ad9a-483f-ac05-b7494ea4d2bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019410443s
Mar 20 08:35:01.872: INFO: Pod "pod-8345fae8-ad9a-483f-ac05-b7494ea4d2bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030386043s
STEP: Saw pod success
Mar 20 08:35:01.872: INFO: Pod "pod-8345fae8-ad9a-483f-ac05-b7494ea4d2bc" satisfied condition "Succeeded or Failed"
Mar 20 08:35:01.875: INFO: Trying to get logs from node env016ar130-worker02 pod pod-8345fae8-ad9a-483f-ac05-b7494ea4d2bc container test-container: <nil>
STEP: delete the pod
Mar 20 08:35:01.894: INFO: Waiting for pod pod-8345fae8-ad9a-483f-ac05-b7494ea4d2bc to disappear
Mar 20 08:35:01.897: INFO: Pod pod-8345fae8-ad9a-483f-ac05-b7494ea4d2bc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar 20 08:35:01.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-921" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":271,"skipped":4865,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:35:01.905: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:35:01.963: INFO: The status of Pod pod-secrets-4125fed6-0dac-4b84-8f88-ee3c3c5cbe77 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:35:03.974: INFO: The status of Pod pod-secrets-4125fed6-0dac-4b84-8f88-ee3c3c5cbe77 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Mar 20 08:35:04.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5850" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":356,"completed":272,"skipped":4872,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:35:04.067: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Mar 20 08:37:00.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3137" for this suite.

• [SLOW TEST:116.096 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":356,"completed":273,"skipped":4921,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:37:00.164: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 20 08:37:00.189: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 20 08:37:00.197: INFO: Waiting for terminating namespaces to be deleted...
Mar 20 08:37:00.200: INFO: 
Logging pods the apiserver thinks is on node env016ar130-worker01 before test
Mar 20 08:37:00.221: INFO: replace-27988357-wxm7c from cronjob-3137 started at 2023-03-20 08:37:00 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container c ready: false, restart count 0
Mar 20 08:37:00.221: INFO: calico-node-hdjmf from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container calico-node ready: true, restart count 0
Mar 20 08:37:00.221: INFO: config-update-8kkf9 from kube-system started at 2023-03-20 07:47:02 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container config-update ready: true, restart count 0
Mar 20 08:37:00.221: INFO: kube-multus-ds-w25rw from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container kube-multus ready: true, restart count 0
Mar 20 08:37:00.221: INFO: kube-proxy-sjh4k from kube-system started at 2023-03-16 12:25:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 20 08:37:00.221: INFO: static-lb-env016ar130-worker01 from kube-system started at 2023-03-16 12:25:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container static-lb ready: true, restart count 0
Mar 20 08:37:00.221: INFO: alertmanager-prometheus-kube-prometheus-alertmanager-0 from kubeops started at 2023-03-20 07:46:50 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container alertmanager ready: true, restart count 1
Mar 20 08:37:00.221: INFO: 	Container config-reloader ready: true, restart count 0
Mar 20 08:37:00.221: INFO: csi-cephfsplugin-fzjzg from kubeops started at 2023-03-20 07:46:45 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 08:37:00.221: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:37:00.221: INFO: csi-rbdplugin-m8z7v from kubeops started at 2023-03-20 07:46:45 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 08:37:00.221: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:37:00.221: INFO: filebeat-filebeat-7zl4k from kubeops started at 2023-03-20 07:46:45 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container filebeat ready: true, restart count 0
Mar 20 08:37:00.221: INFO: prometheus-prometheus-kube-prometheus-prometheus-0 from kubeops started at 2023-03-20 07:46:59 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container config-reloader ready: true, restart count 0
Mar 20 08:37:00.221: INFO: 	Container prometheus ready: true, restart count 0
Mar 20 08:37:00.221: INFO: prometheus-prometheus-node-exporter-hpppp from kubeops started at 2023-03-20 07:46:45 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container node-exporter ready: true, restart count 0
Mar 20 08:37:00.221: INFO: rook-ceph-crashcollector-env016ar130-worker01-766b6f9754-rt44x from kubeops started at 2023-03-20 07:46:45 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container ceph-crash ready: true, restart count 0
Mar 20 08:37:00.221: INFO: rook-ceph-mon-c-78d58bd46b-8n5tg from kubeops started at 2023-03-20 07:46:45 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:37:00.221: INFO: 	Container mon ready: true, restart count 0
Mar 20 08:37:00.221: INFO: rook-ceph-osd-0-c77fcd474-m5wsl from kubeops started at 2023-03-20 07:46:45 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:37:00.221: INFO: 	Container osd ready: true, restart count 0
Mar 20 08:37:00.221: INFO: rook-ceph-osd-prepare-env016ar130-worker01-n9m6f from kubeops started at 2023-03-20 07:46:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container provision ready: false, restart count 0
Mar 20 08:37:00.221: INFO: rook-discover-n7tql from kubeops started at 2023-03-20 07:46:45 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container rook-discover ready: true, restart count 0
Mar 20 08:37:00.221: INFO: sonobuoy from sonobuoy started at 2023-03-20 07:27:30 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 20 08:37:00.221: INFO: sonobuoy-e2e-job-5ef492aa84d840a2 from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container e2e ready: true, restart count 0
Mar 20 08:37:00.221: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 08:37:00.221: INFO: sonobuoy-systemd-logs-daemon-set-3a399369495241ac-j79q9 from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.221: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 08:37:00.221: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 20 08:37:00.221: INFO: 
Logging pods the apiserver thinks is on node env016ar130-worker02 before test
Mar 20 08:37:00.246: INFO: replace-27988356-2c4q7 from cronjob-3137 started at 2023-03-20 08:36:00 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container c ready: true, restart count 0
Mar 20 08:37:00.246: INFO: calico-node-bphlv from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container calico-node ready: true, restart count 0
Mar 20 08:37:00.246: INFO: config-update-9h6xg from kube-system started at 2023-03-16 14:15:50 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container config-update ready: true, restart count 0
Mar 20 08:37:00.246: INFO: kube-multus-ds-9xqdf from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container kube-multus ready: true, restart count 0
Mar 20 08:37:00.246: INFO: kube-proxy-dk5kb from kube-system started at 2023-03-16 12:25:25 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 20 08:37:00.246: INFO: static-lb-env016ar130-worker02 from kube-system started at 2023-03-16 12:25:53 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container static-lb ready: true, restart count 0
Mar 20 08:37:00.246: INFO: csi-cephfsplugin-gd4zn from kubeops started at 2023-03-16 14:15:32 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:37:00.246: INFO: csi-cephfsplugin-provisioner-8f66f988-nr7c7 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (5 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 08:37:00.246: INFO: csi-rbdplugin-mkm94 from kubeops started at 2023-03-16 14:15:05 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:37:00.246: INFO: csi-rbdplugin-provisioner-7bb4c8b9c7-h8jb6 from kubeops started at 2023-03-16 15:03:27 +0000 UTC (5 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 08:37:00.246: INFO: filebeat-filebeat-pf2vz from kubeops started at 2023-03-16 14:15:59 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container filebeat ready: true, restart count 0
Mar 20 08:37:00.246: INFO: gatekeeper-audit-749874bd85-gbfdv from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container manager ready: true, restart count 0
Mar 20 08:37:00.246: INFO: gatekeeper-controller-manager-768fd8789c-s4xkm from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container manager ready: true, restart count 0
Mar 20 08:37:00.246: INFO: gatekeeper-controller-manager-768fd8789c-w7sxj from kubeops started at 2023-03-16 15:03:30 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container manager ready: true, restart count 0
Mar 20 08:37:00.246: INFO: gatekeeper-controller-manager-768fd8789c-wgpj5 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container manager ready: true, restart count 0
Mar 20 08:37:00.246: INFO: harbor-chartmuseum-7df8df844b-8fds2 from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container chartmuseum ready: true, restart count 0
Mar 20 08:37:00.246: INFO: harbor-jobservice-7b746648f6-x2kd8 from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container jobservice ready: true, restart count 1
Mar 20 08:37:00.246: INFO: harbor-notary-signer-c9db4c94d-fs46b from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container notary-signer ready: true, restart count 0
Mar 20 08:37:00.246: INFO: harbor-notary-signer-c9db4c94d-zqpzz from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container notary-signer ready: true, restart count 0
Mar 20 08:37:00.246: INFO: harbor-portal-dbbbb4456-7hb5f from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container portal ready: true, restart count 0
Mar 20 08:37:00.246: INFO: harbor-registry-b66c45c5f-lq8vr from kubeops started at 2023-03-16 14:14:52 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container registry ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container registryctl ready: true, restart count 0
Mar 20 08:37:00.246: INFO: harbor-trivy-1 from kubeops started at 2023-03-16 14:16:07 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container trivy ready: true, restart count 0
Mar 20 08:37:00.246: INFO: opensearch-cluster-master-0 from kubeops started at 2023-03-16 14:16:53 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container opensearch ready: true, restart count 0
Mar 20 08:37:00.246: INFO: opensearch-cluster-master-2 from kubeops started at 2023-03-17 12:48:10 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container opensearch ready: true, restart count 0
Mar 20 08:37:00.246: INFO: prometheus-prometheus-node-exporter-rxbn4 from kubeops started at 2023-03-16 14:15:38 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container node-exporter ready: true, restart count 0
Mar 20 08:37:00.246: INFO: rook-ceph-crashcollector-env016ar130-worker02-799c88cbcf-9ktc7 from kubeops started at 2023-03-16 15:03:28 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container ceph-crash ready: true, restart count 0
Mar 20 08:37:00.246: INFO: rook-ceph-mds-myfs-a-85c5fc87d-gj7d4 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container mds ready: true, restart count 0
Mar 20 08:37:00.246: INFO: rook-ceph-mgr-a-857696f864-4gjb2 from kubeops started at 2023-03-16 15:03:30 +0000 UTC (3 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container mgr ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container watch-active ready: true, restart count 0
Mar 20 08:37:00.246: INFO: rook-ceph-mon-a-cc8d49d77-sgj2c from kubeops started at 2023-03-16 14:15:59 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container mon ready: true, restart count 0
Mar 20 08:37:00.246: INFO: rook-ceph-osd-2-557dc55864-679xk from kubeops started at 2023-03-16 14:15:54 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container osd ready: true, restart count 0
Mar 20 08:37:00.246: INFO: rook-ceph-osd-prepare-env016ar130-worker02-w9vhn from kubeops started at 2023-03-20 07:46:55 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container provision ready: false, restart count 0
Mar 20 08:37:00.246: INFO: rook-ceph-tools-59d749577f-chcwh from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Mar 20 08:37:00.246: INFO: rook-discover-x9lwv from kubeops started at 2023-03-16 14:15:44 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container rook-discover ready: true, restart count 0
Mar 20 08:37:00.246: INFO: sonobuoy-systemd-logs-daemon-set-3a399369495241ac-swzbr from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.246: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 20 08:37:00.246: INFO: 
Logging pods the apiserver thinks is on node env016ar130-worker03 before test
Mar 20 08:37:00.276: INFO: calico-node-cpqql from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container calico-node ready: true, restart count 0
Mar 20 08:37:00.276: INFO: config-update-lj4n7 from kube-system started at 2023-03-16 12:35:49 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container config-update ready: true, restart count 0
Mar 20 08:37:00.276: INFO: kube-multus-ds-xcsml from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container kube-multus ready: true, restart count 0
Mar 20 08:37:00.276: INFO: kube-proxy-hxh8h from kube-system started at 2023-03-16 12:25:31 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 20 08:37:00.276: INFO: static-lb-env016ar130-worker03 from kube-system started at 2023-03-16 12:25:53 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container static-lb ready: true, restart count 0
Mar 20 08:37:00.276: INFO: csi-cephfsplugin-dpmqw from kubeops started at 2023-03-16 12:41:07 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:37:00.276: INFO: csi-cephfsplugin-provisioner-8f66f988-hg4ds from kubeops started at 2023-03-16 14:14:52 +0000 UTC (5 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 08:37:00.276: INFO: csi-rbdplugin-fhk4v from kubeops started at 2023-03-16 12:41:07 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:37:00.276: INFO: csi-rbdplugin-provisioner-7bb4c8b9c7-z7z8d from kubeops started at 2023-03-16 12:41:07 +0000 UTC (5 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 08:37:00.276: INFO: filebeat-filebeat-hbpls from kubeops started at 2023-03-16 12:56:38 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container filebeat ready: true, restart count 0
Mar 20 08:37:00.276: INFO: harbor-chartmuseum-7df8df844b-7zqxs from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container chartmuseum ready: true, restart count 0
Mar 20 08:37:00.276: INFO: harbor-chartmuseum-7df8df844b-hks64 from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container chartmuseum ready: true, restart count 0
Mar 20 08:37:00.276: INFO: harbor-core-d54995cdd-6c49r from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container core ready: true, restart count 0
Mar 20 08:37:00.276: INFO: harbor-core-d54995cdd-brrck from kubeops started at 2023-03-16 15:03:27 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container core ready: true, restart count 1
Mar 20 08:37:00.276: INFO: harbor-core-d54995cdd-n4l9h from kubeops started at 2023-03-16 15:03:28 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container core ready: true, restart count 0
Mar 20 08:37:00.276: INFO: harbor-jobservice-7b746648f6-cv6bz from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container jobservice ready: true, restart count 1
Mar 20 08:37:00.276: INFO: harbor-jobservice-7b746648f6-l24wk from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container jobservice ready: true, restart count 4
Mar 20 08:37:00.276: INFO: harbor-nginx-f8c975ff6-2bwcd from kubeops started at 2023-03-16 12:54:06 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container nginx ready: true, restart count 0
Mar 20 08:37:00.276: INFO: harbor-nginx-f8c975ff6-5hv2g from kubeops started at 2023-03-16 15:03:27 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container nginx ready: true, restart count 0
Mar 20 08:37:00.276: INFO: harbor-nginx-f8c975ff6-hllb9 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container nginx ready: true, restart count 0
Mar 20 08:37:00.276: INFO: harbor-notary-server-89fd67fc6-67cdb from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container notary-server ready: true, restart count 2
Mar 20 08:37:00.276: INFO: harbor-notary-server-89fd67fc6-7h95b from kubeops started at 2023-03-16 12:54:06 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container notary-server ready: true, restart count 0
Mar 20 08:37:00.276: INFO: harbor-notary-server-89fd67fc6-kn77l from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container notary-server ready: true, restart count 2
Mar 20 08:37:00.276: INFO: harbor-notary-signer-c9db4c94d-txwpf from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container notary-signer ready: true, restart count 2
Mar 20 08:37:00.276: INFO: harbor-portal-dbbbb4456-gqbx6 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container portal ready: true, restart count 0
Mar 20 08:37:00.276: INFO: harbor-portal-dbbbb4456-qgsgw from kubeops started at 2023-03-16 12:54:06 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container portal ready: true, restart count 0
Mar 20 08:37:00.276: INFO: harbor-registry-b66c45c5f-d5q94 from kubeops started at 2023-03-16 12:54:07 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container registry ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container registryctl ready: true, restart count 0
Mar 20 08:37:00.276: INFO: harbor-registry-b66c45c5f-fr9g4 from kubeops started at 2023-03-16 15:03:27 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container registry ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container registryctl ready: true, restart count 0
Mar 20 08:37:00.276: INFO: harbor-trivy-0 from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container trivy ready: true, restart count 0
Mar 20 08:37:00.276: INFO: harbor-trivy-2 from kubeops started at 2023-03-16 12:55:51 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container trivy ready: true, restart count 0
Mar 20 08:37:00.276: INFO: logstash-logstash-0 from kubeops started at 2023-03-16 12:56:41 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container logstash ready: true, restart count 1
Mar 20 08:37:00.276: INFO: opensearch-cluster-master-1 from kubeops started at 2023-03-16 12:56:36 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container opensearch ready: true, restart count 0
Mar 20 08:37:00.276: INFO: opensearch-dashboards-69f44df846-zmxg9 from kubeops started at 2023-03-16 12:56:36 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container dashboards ready: true, restart count 0
Mar 20 08:37:00.276: INFO: postgres-7f6cc6d46c-cdpmp from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container postgres ready: true, restart count 0
Mar 20 08:37:00.276: INFO: prometheus-grafana-5c58fc9dbb-gj98n from kubeops started at 2023-03-16 12:55:16 +0000 UTC (3 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container grafana ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Mar 20 08:37:00.276: INFO: prometheus-kube-prometheus-operator-76b748b4b7-hgsmt from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Mar 20 08:37:00.276: INFO: prometheus-kube-state-metrics-85655df84d-qsk5t from kubeops started at 2023-03-16 12:55:16 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 20 08:37:00.276: INFO: prometheus-prometheus-node-exporter-sqpkb from kubeops started at 2023-03-16 12:55:16 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container node-exporter ready: true, restart count 0
Mar 20 08:37:00.276: INFO: redis-6fdbc8bc6c-cj4g2 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container redis ready: true, restart count 0
Mar 20 08:37:00.276: INFO: rook-ceph-crashcollector-env016ar130-worker03-68f58d9f4b-pbp4c from kubeops started at 2023-03-16 12:44:17 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container ceph-crash ready: true, restart count 0
Mar 20 08:37:00.276: INFO: rook-ceph-mds-myfs-b-767cb4cfc5-p5f4k from kubeops started at 2023-03-16 12:44:16 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container mds ready: true, restart count 0
Mar 20 08:37:00.276: INFO: rook-ceph-mgr-b-69f6d8d6c-8klhf from kubeops started at 2023-03-16 12:43:36 +0000 UTC (3 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container mgr ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container watch-active ready: true, restart count 0
Mar 20 08:37:00.276: INFO: rook-ceph-mon-b-7b5485875c-fjr22 from kubeops started at 2023-03-16 12:43:15 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container mon ready: true, restart count 0
Mar 20 08:37:00.276: INFO: rook-ceph-operator-f6f75855b-lt27v from kubeops started at 2023-03-16 12:39:01 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Mar 20 08:37:00.276: INFO: rook-ceph-osd-1-755cf9d57c-j6jcs from kubeops started at 2023-03-16 12:44:07 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container osd ready: true, restart count 0
Mar 20 08:37:00.276: INFO: rook-ceph-osd-prepare-env016ar130-worker03-8nc47 from kubeops started at 2023-03-20 07:46:58 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container provision ready: false, restart count 0
Mar 20 08:37:00.276: INFO: rook-discover-wj92q from kubeops started at 2023-03-16 12:39:43 +0000 UTC (1 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container rook-discover ready: true, restart count 0
Mar 20 08:37:00.276: INFO: sonobuoy-systemd-logs-daemon-set-3a399369495241ac-zkwlr from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 08:37:00.276: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 08:37:00.276: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
STEP: verifying the node has the label node env016ar130-worker01
STEP: verifying the node has the label node env016ar130-worker02
STEP: verifying the node has the label node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod replace-27988356-2c4q7 requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod replace-27988357-wxm7c requesting resource cpu=0m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod calico-node-bphlv requesting resource cpu=250m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod calico-node-cpqql requesting resource cpu=250m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod calico-node-hdjmf requesting resource cpu=250m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod config-update-8kkf9 requesting resource cpu=0m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod config-update-9h6xg requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod config-update-lj4n7 requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod kube-multus-ds-9xqdf requesting resource cpu=100m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod kube-multus-ds-w25rw requesting resource cpu=100m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod kube-multus-ds-xcsml requesting resource cpu=100m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod kube-proxy-dk5kb requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod kube-proxy-hxh8h requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod kube-proxy-sjh4k requesting resource cpu=0m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod static-lb-env016ar130-worker01 requesting resource cpu=0m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod static-lb-env016ar130-worker02 requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod static-lb-env016ar130-worker03 requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod alertmanager-prometheus-kube-prometheus-alertmanager-0 requesting resource cpu=700m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod csi-cephfsplugin-dpmqw requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod csi-cephfsplugin-fzjzg requesting resource cpu=0m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod csi-cephfsplugin-gd4zn requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod csi-cephfsplugin-provisioner-8f66f988-hg4ds requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod csi-cephfsplugin-provisioner-8f66f988-nr7c7 requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod csi-rbdplugin-fhk4v requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod csi-rbdplugin-m8z7v requesting resource cpu=0m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod csi-rbdplugin-mkm94 requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod csi-rbdplugin-provisioner-7bb4c8b9c7-h8jb6 requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod csi-rbdplugin-provisioner-7bb4c8b9c7-z7z8d requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod filebeat-filebeat-7zl4k requesting resource cpu=100m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod filebeat-filebeat-hbpls requesting resource cpu=100m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod filebeat-filebeat-pf2vz requesting resource cpu=100m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod gatekeeper-audit-749874bd85-gbfdv requesting resource cpu=100m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod gatekeeper-controller-manager-768fd8789c-s4xkm requesting resource cpu=100m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod gatekeeper-controller-manager-768fd8789c-w7sxj requesting resource cpu=100m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod gatekeeper-controller-manager-768fd8789c-wgpj5 requesting resource cpu=100m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod harbor-chartmuseum-7df8df844b-7zqxs requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-chartmuseum-7df8df844b-8fds2 requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod harbor-chartmuseum-7df8df844b-hks64 requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-core-d54995cdd-6c49r requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-core-d54995cdd-brrck requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-core-d54995cdd-n4l9h requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-jobservice-7b746648f6-cv6bz requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-jobservice-7b746648f6-l24wk requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-jobservice-7b746648f6-x2kd8 requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod harbor-nginx-f8c975ff6-2bwcd requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-nginx-f8c975ff6-5hv2g requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-nginx-f8c975ff6-hllb9 requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-notary-server-89fd67fc6-67cdb requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-notary-server-89fd67fc6-7h95b requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-notary-server-89fd67fc6-kn77l requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-notary-signer-c9db4c94d-fs46b requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod harbor-notary-signer-c9db4c94d-txwpf requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-notary-signer-c9db4c94d-zqpzz requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod harbor-portal-dbbbb4456-7hb5f requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod harbor-portal-dbbbb4456-gqbx6 requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-portal-dbbbb4456-qgsgw requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-registry-b66c45c5f-d5q94 requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-registry-b66c45c5f-fr9g4 requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-registry-b66c45c5f-lq8vr requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod harbor-trivy-0 requesting resource cpu=200m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod harbor-trivy-1 requesting resource cpu=200m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod harbor-trivy-2 requesting resource cpu=200m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod logstash-logstash-0 requesting resource cpu=100m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod opensearch-cluster-master-0 requesting resource cpu=250m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod opensearch-cluster-master-1 requesting resource cpu=250m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod opensearch-cluster-master-2 requesting resource cpu=250m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod opensearch-dashboards-69f44df846-zmxg9 requesting resource cpu=200m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod postgres-7f6cc6d46c-cdpmp requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod prometheus-grafana-5c58fc9dbb-gj98n requesting resource cpu=100m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod prometheus-kube-prometheus-operator-76b748b4b7-hgsmt requesting resource cpu=100m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod prometheus-kube-state-metrics-85655df84d-qsk5t requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod prometheus-prometheus-kube-prometheus-prometheus-0 requesting resource cpu=1200m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod prometheus-prometheus-node-exporter-hpppp requesting resource cpu=0m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod prometheus-prometheus-node-exporter-rxbn4 requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod prometheus-prometheus-node-exporter-sqpkb requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod redis-6fdbc8bc6c-cj4g2 requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod rook-ceph-crashcollector-env016ar130-worker01-766b6f9754-rt44x requesting resource cpu=0m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod rook-ceph-crashcollector-env016ar130-worker02-799c88cbcf-9ktc7 requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod rook-ceph-crashcollector-env016ar130-worker03-68f58d9f4b-pbp4c requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod rook-ceph-mds-myfs-a-85c5fc87d-gj7d4 requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod rook-ceph-mds-myfs-b-767cb4cfc5-p5f4k requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod rook-ceph-mgr-a-857696f864-4gjb2 requesting resource cpu=500m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod rook-ceph-mgr-b-69f6d8d6c-8klhf requesting resource cpu=500m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod rook-ceph-mon-a-cc8d49d77-sgj2c requesting resource cpu=2000m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod rook-ceph-mon-b-7b5485875c-fjr22 requesting resource cpu=2000m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod rook-ceph-mon-c-78d58bd46b-8n5tg requesting resource cpu=2000m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod rook-ceph-operator-f6f75855b-lt27v requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod rook-ceph-osd-0-c77fcd474-m5wsl requesting resource cpu=2000m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod rook-ceph-osd-1-755cf9d57c-j6jcs requesting resource cpu=2000m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod rook-ceph-osd-2-557dc55864-679xk requesting resource cpu=2000m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod rook-ceph-tools-59d749577f-chcwh requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod rook-discover-n7tql requesting resource cpu=0m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod rook-discover-wj92q requesting resource cpu=0m on Node env016ar130-worker03
Mar 20 08:37:00.419: INFO: Pod rook-discover-x9lwv requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod sonobuoy requesting resource cpu=0m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod sonobuoy-e2e-job-5ef492aa84d840a2 requesting resource cpu=0m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod sonobuoy-systemd-logs-daemon-set-3a399369495241ac-j79q9 requesting resource cpu=0m on Node env016ar130-worker01
Mar 20 08:37:00.419: INFO: Pod sonobuoy-systemd-logs-daemon-set-3a399369495241ac-swzbr requesting resource cpu=0m on Node env016ar130-worker02
Mar 20 08:37:00.419: INFO: Pod sonobuoy-systemd-logs-daemon-set-3a399369495241ac-zkwlr requesting resource cpu=0m on Node env016ar130-worker03
STEP: Starting Pods to consume most of the cluster CPU.
Mar 20 08:37:00.419: INFO: Creating a pod which consumes cpu=455m on Node env016ar130-worker01
Mar 20 08:37:00.431: INFO: Creating a pod which consumes cpu=665m on Node env016ar130-worker02
Mar 20 08:37:00.438: INFO: Creating a pod which consumes cpu=630m on Node env016ar130-worker03
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-073b020c-e312-4e4e-91e3-9d13558bb4cb.174e13be9bfe503e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5092/filler-pod-073b020c-e312-4e4e-91e3-9d13558bb4cb to env016ar130-worker01]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-073b020c-e312-4e4e-91e3-9d13558bb4cb.174e13bebe4f6560], Reason = [AddedInterface], Message = [Add eth0 [192.168.90.82/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-073b020c-e312-4e4e-91e3-9d13558bb4cb.174e13bec677f45e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-073b020c-e312-4e4e-91e3-9d13558bb4cb.174e13bec8e6d802], Reason = [Created], Message = [Created container filler-pod-073b020c-e312-4e4e-91e3-9d13558bb4cb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-073b020c-e312-4e4e-91e3-9d13558bb4cb.174e13becf790e20], Reason = [Started], Message = [Started container filler-pod-073b020c-e312-4e4e-91e3-9d13558bb4cb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5f746151-b9ce-48d8-92cb-5ab6f489ef9e.174e13be9d14a791], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5092/filler-pod-5f746151-b9ce-48d8-92cb-5ab6f489ef9e to env016ar130-worker03]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5f746151-b9ce-48d8-92cb-5ab6f489ef9e.174e13bebd09e9b5], Reason = [AddedInterface], Message = [Add eth0 [192.168.71.178/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5f746151-b9ce-48d8-92cb-5ab6f489ef9e.174e13bec65f0b40], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5f746151-b9ce-48d8-92cb-5ab6f489ef9e.174e13bec8400438], Reason = [Created], Message = [Created container filler-pod-5f746151-b9ce-48d8-92cb-5ab6f489ef9e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5f746151-b9ce-48d8-92cb-5ab6f489ef9e.174e13becf11109e], Reason = [Started], Message = [Started container filler-pod-5f746151-b9ce-48d8-92cb-5ab6f489ef9e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d1f39784-5efb-47da-a0a9-cc90c165c14c.174e13be9c6b9cbb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5092/filler-pod-d1f39784-5efb-47da-a0a9-cc90c165c14c to env016ar130-worker02]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d1f39784-5efb-47da-a0a9-cc90c165c14c.174e13bebd2d4e61], Reason = [AddedInterface], Message = [Add eth0 [192.168.12.63/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d1f39784-5efb-47da-a0a9-cc90c165c14c.174e13bec4de8126], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d1f39784-5efb-47da-a0a9-cc90c165c14c.174e13bec6c38e6a], Reason = [Created], Message = [Created container filler-pod-d1f39784-5efb-47da-a0a9-cc90c165c14c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d1f39784-5efb-47da-a0a9-cc90c165c14c.174e13becd126229], Reason = [Started], Message = [Started container filler-pod-d1f39784-5efb-47da-a0a9-cc90c165c14c]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.174e13bf16411b92], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 Insufficient cpu. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.]
STEP: removing the label node off the node env016ar130-worker01
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node env016ar130-worker02
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node env016ar130-worker03
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Mar 20 08:37:03.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5092" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":356,"completed":274,"skipped":4935,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:37:03.590: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Mar 20 08:37:03.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5804" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":275,"skipped":4963,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:37:03.628: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-downwardapi-bdt5
STEP: Creating a pod to test atomic-volume-subpath
Mar 20 08:37:03.669: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-bdt5" in namespace "subpath-1991" to be "Succeeded or Failed"
Mar 20 08:37:03.671: INFO: Pod "pod-subpath-test-downwardapi-bdt5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.305297ms
Mar 20 08:37:05.685: INFO: Pod "pod-subpath-test-downwardapi-bdt5": Phase="Running", Reason="", readiness=true. Elapsed: 2.016336022s
Mar 20 08:37:07.698: INFO: Pod "pod-subpath-test-downwardapi-bdt5": Phase="Running", Reason="", readiness=true. Elapsed: 4.028648105s
Mar 20 08:37:09.709: INFO: Pod "pod-subpath-test-downwardapi-bdt5": Phase="Running", Reason="", readiness=true. Elapsed: 6.039576413s
Mar 20 08:37:11.719: INFO: Pod "pod-subpath-test-downwardapi-bdt5": Phase="Running", Reason="", readiness=true. Elapsed: 8.050053956s
Mar 20 08:37:13.731: INFO: Pod "pod-subpath-test-downwardapi-bdt5": Phase="Running", Reason="", readiness=true. Elapsed: 10.062393388s
Mar 20 08:37:15.742: INFO: Pod "pod-subpath-test-downwardapi-bdt5": Phase="Running", Reason="", readiness=true. Elapsed: 12.073406627s
Mar 20 08:37:17.756: INFO: Pod "pod-subpath-test-downwardapi-bdt5": Phase="Running", Reason="", readiness=true. Elapsed: 14.086859571s
Mar 20 08:37:19.768: INFO: Pod "pod-subpath-test-downwardapi-bdt5": Phase="Running", Reason="", readiness=true. Elapsed: 16.098919091s
Mar 20 08:37:21.782: INFO: Pod "pod-subpath-test-downwardapi-bdt5": Phase="Running", Reason="", readiness=true. Elapsed: 18.112601373s
Mar 20 08:37:23.791: INFO: Pod "pod-subpath-test-downwardapi-bdt5": Phase="Running", Reason="", readiness=true. Elapsed: 20.122326819s
Mar 20 08:37:25.802: INFO: Pod "pod-subpath-test-downwardapi-bdt5": Phase="Running", Reason="", readiness=false. Elapsed: 22.133331614s
Mar 20 08:37:27.814: INFO: Pod "pod-subpath-test-downwardapi-bdt5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.145041338s
STEP: Saw pod success
Mar 20 08:37:27.814: INFO: Pod "pod-subpath-test-downwardapi-bdt5" satisfied condition "Succeeded or Failed"
Mar 20 08:37:27.818: INFO: Trying to get logs from node env016ar130-worker01 pod pod-subpath-test-downwardapi-bdt5 container test-container-subpath-downwardapi-bdt5: <nil>
STEP: delete the pod
Mar 20 08:37:27.855: INFO: Waiting for pod pod-subpath-test-downwardapi-bdt5 to disappear
Mar 20 08:37:27.858: INFO: Pod pod-subpath-test-downwardapi-bdt5 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-bdt5
Mar 20 08:37:27.858: INFO: Deleting pod "pod-subpath-test-downwardapi-bdt5" in namespace "subpath-1991"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Mar 20 08:37:27.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1991" for this suite.

• [SLOW TEST:24.245 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","total":356,"completed":276,"skipped":4983,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:37:27.873: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar 20 08:37:39.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8109" for this suite.

• [SLOW TEST:11.200 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":356,"completed":277,"skipped":4999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:37:39.074: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:37:39.170: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 20 08:37:39.182: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:39.182: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:39.182: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:39.185: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:37:39.185: INFO: Node env016ar130-worker01 is running 0 daemon pod, expected 1
Mar 20 08:37:40.200: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:40.200: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:40.200: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:40.204: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:37:40.204: INFO: Node env016ar130-worker01 is running 0 daemon pod, expected 1
Mar 20 08:37:41.198: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:41.198: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:41.198: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:41.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 20 08:37:41.203: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 20 08:37:41.238: INFO: Wrong image for pod: daemon-set-5g9gq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 20 08:37:41.238: INFO: Wrong image for pod: daemon-set-9htjh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 20 08:37:41.238: INFO: Wrong image for pod: daemon-set-xcv4k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 20 08:37:41.242: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:41.242: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:41.242: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:42.253: INFO: Wrong image for pod: daemon-set-5g9gq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 20 08:37:42.253: INFO: Wrong image for pod: daemon-set-9htjh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 20 08:37:42.259: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:42.259: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:42.259: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:43.250: INFO: Wrong image for pod: daemon-set-5g9gq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 20 08:37:43.250: INFO: Wrong image for pod: daemon-set-9htjh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 20 08:37:43.255: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:43.255: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:43.256: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:44.249: INFO: Wrong image for pod: daemon-set-5g9gq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 20 08:37:44.249: INFO: Wrong image for pod: daemon-set-9htjh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 20 08:37:44.249: INFO: Pod daemon-set-wdbhc is not available
Mar 20 08:37:44.253: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:44.253: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:44.253: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:45.252: INFO: Wrong image for pod: daemon-set-9htjh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 20 08:37:45.258: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:45.258: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:45.258: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:46.253: INFO: Wrong image for pod: daemon-set-9htjh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 20 08:37:46.253: INFO: Pod daemon-set-bkpmk is not available
Mar 20 08:37:46.259: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:46.259: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:46.259: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:47.253: INFO: Wrong image for pod: daemon-set-9htjh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar 20 08:37:47.253: INFO: Pod daemon-set-bkpmk is not available
Mar 20 08:37:47.259: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:47.259: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:47.259: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:48.257: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:48.257: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:48.257: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:49.255: INFO: Pod daemon-set-5b69j is not available
Mar 20 08:37:49.264: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:49.264: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:49.264: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 20 08:37:49.270: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:49.270: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:49.270: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:49.274: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 20 08:37:49.274: INFO: Node env016ar130-worker01 is running 0 daemon pod, expected 1
Mar 20 08:37:50.289: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:50.290: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:50.290: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 08:37:50.294: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 20 08:37:50.294: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8877, will wait for the garbage collector to delete the pods
Mar 20 08:37:50.381: INFO: Deleting DaemonSet.extensions daemon-set took: 15.46174ms
Mar 20 08:37:50.481: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.186558ms
Mar 20 08:37:52.893: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 08:37:52.893: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 20 08:37:52.896: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"833049"},"items":null}

Mar 20 08:37:52.898: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"833049"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Mar 20 08:37:52.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8877" for this suite.

• [SLOW TEST:13.845 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":356,"completed":278,"skipped":5048,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:37:52.920: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's command
Mar 20 08:37:53.012: INFO: Waiting up to 5m0s for pod "var-expansion-6f142a9f-5a12-489d-bb5a-96b47b281bda" in namespace "var-expansion-2953" to be "Succeeded or Failed"
Mar 20 08:37:53.016: INFO: Pod "var-expansion-6f142a9f-5a12-489d-bb5a-96b47b281bda": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891184ms
Mar 20 08:37:55.030: INFO: Pod "var-expansion-6f142a9f-5a12-489d-bb5a-96b47b281bda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017540066s
Mar 20 08:37:57.043: INFO: Pod "var-expansion-6f142a9f-5a12-489d-bb5a-96b47b281bda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031269604s
STEP: Saw pod success
Mar 20 08:37:57.043: INFO: Pod "var-expansion-6f142a9f-5a12-489d-bb5a-96b47b281bda" satisfied condition "Succeeded or Failed"
Mar 20 08:37:57.047: INFO: Trying to get logs from node env016ar130-worker02 pod var-expansion-6f142a9f-5a12-489d-bb5a-96b47b281bda container dapi-container: <nil>
STEP: delete the pod
Mar 20 08:37:57.094: INFO: Waiting for pod var-expansion-6f142a9f-5a12-489d-bb5a-96b47b281bda to disappear
Mar 20 08:37:57.097: INFO: Pod var-expansion-6f142a9f-5a12-489d-bb5a-96b47b281bda no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar 20 08:37:57.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2953" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":356,"completed":279,"skipped":5076,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:37:57.105: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar 20 08:38:04.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4013" for this suite.

• [SLOW TEST:7.077 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":356,"completed":280,"skipped":5078,"failed":0}
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:38:04.182: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Mar 20 08:38:04.227: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:38:06.235: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Mar 20 08:38:06.255: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:38:08.264: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Mar 20 08:38:08.279: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 20 08:38:08.284: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 20 08:38:10.285: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 20 08:38:10.296: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 20 08:38:12.285: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 20 08:38:12.299: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Mar 20 08:38:12.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5710" for this suite.

• [SLOW TEST:8.171 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":356,"completed":281,"skipped":5083,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:38:12.353: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Mar 20 08:38:12.394: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 20 08:38:17.403: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Mar 20 08:38:17.407: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Mar 20 08:38:17.418: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Mar 20 08:38:17.420: INFO: Observed &ReplicaSet event: ADDED
Mar 20 08:38:17.420: INFO: Observed &ReplicaSet event: MODIFIED
Mar 20 08:38:17.420: INFO: Observed &ReplicaSet event: MODIFIED
Mar 20 08:38:17.420: INFO: Observed &ReplicaSet event: MODIFIED
Mar 20 08:38:17.420: INFO: Found replicaset test-rs in namespace replicaset-4934 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 20 08:38:17.420: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Mar 20 08:38:17.420: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 20 08:38:17.425: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Mar 20 08:38:17.426: INFO: Observed &ReplicaSet event: ADDED
Mar 20 08:38:17.426: INFO: Observed &ReplicaSet event: MODIFIED
Mar 20 08:38:17.427: INFO: Observed &ReplicaSet event: MODIFIED
Mar 20 08:38:17.427: INFO: Observed &ReplicaSet event: MODIFIED
Mar 20 08:38:17.427: INFO: Observed replicaset test-rs in namespace replicaset-4934 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 20 08:38:17.427: INFO: Observed &ReplicaSet event: MODIFIED
Mar 20 08:38:17.427: INFO: Found replicaset test-rs in namespace replicaset-4934 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar 20 08:38:17.427: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Mar 20 08:38:17.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4934" for this suite.

• [SLOW TEST:5.082 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":356,"completed":282,"skipped":5117,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:38:17.435: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Mar 20 08:38:17.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5034 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 20 08:38:17.533: INFO: stderr: ""
Mar 20 08:38:17.533: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Mar 20 08:38:17.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5034 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Mar 20 08:38:18.647: INFO: stderr: ""
Mar 20 08:38:18.647: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Mar 20 08:38:18.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5034 delete pods e2e-test-httpd-pod'
Mar 20 08:38:20.896: INFO: stderr: ""
Mar 20 08:38:20.896: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 08:38:20.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5034" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":356,"completed":283,"skipped":5118,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:38:20.912: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar 20 08:38:20.947: INFO: Waiting up to 5m0s for pod "downwardapi-volume-73f9247e-8451-4594-b330-9c2c0d962e8d" in namespace "projected-8226" to be "Succeeded or Failed"
Mar 20 08:38:20.950: INFO: Pod "downwardapi-volume-73f9247e-8451-4594-b330-9c2c0d962e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.039655ms
Mar 20 08:38:22.954: INFO: Pod "downwardapi-volume-73f9247e-8451-4594-b330-9c2c0d962e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007095908s
Mar 20 08:38:24.964: INFO: Pod "downwardapi-volume-73f9247e-8451-4594-b330-9c2c0d962e8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017293195s
STEP: Saw pod success
Mar 20 08:38:24.964: INFO: Pod "downwardapi-volume-73f9247e-8451-4594-b330-9c2c0d962e8d" satisfied condition "Succeeded or Failed"
Mar 20 08:38:24.968: INFO: Trying to get logs from node env016ar130-worker02 pod downwardapi-volume-73f9247e-8451-4594-b330-9c2c0d962e8d container client-container: <nil>
STEP: delete the pod
Mar 20 08:38:24.988: INFO: Waiting for pod downwardapi-volume-73f9247e-8451-4594-b330-9c2c0d962e8d to disappear
Mar 20 08:38:24.991: INFO: Pod downwardapi-volume-73f9247e-8451-4594-b330-9c2c0d962e8d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar 20 08:38:24.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8226" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":284,"skipped":5156,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:38:25.000: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 08:38:25.467: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 08:38:28.508: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar 20 08:38:28.539: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 08:38:28.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9636" for this suite.
STEP: Destroying namespace "webhook-9636-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":356,"completed":285,"skipped":5167,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:38:28.619: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Mar 20 08:40:00.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5972" for this suite.

• [SLOW TEST:92.066 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":356,"completed":286,"skipped":5186,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:40:00.685: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5281
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:40:00.757: INFO: Found 0 stateful pods, waiting for 1
Mar 20 08:40:10.773: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Mar 20 08:40:10.806: INFO: Found 1 stateful pods, waiting for 2
Mar 20 08:40:20.819: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 20 08:40:20.819: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 20 08:40:20.856: INFO: Deleting all statefulset in ns statefulset-5281
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar 20 08:40:20.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5281" for this suite.

• [SLOW TEST:20.201 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":356,"completed":287,"skipped":5205,"failed":0}
SSSSS
------------------------------
[sig-node] PodTemplates 
  should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:40:20.886: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a pod template
STEP: Replace a pod template
Mar 20 08:40:20.926: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Mar 20 08:40:20.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8422" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","total":356,"completed":288,"skipped":5210,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:40:20.941: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:79
Mar 20 08:40:20.963: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the sample API server.
Mar 20 08:40:21.767: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Mar 20 08:40:23.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-66955c4dd6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 20 08:40:25.845: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-66955c4dd6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 20 08:40:27.833: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-66955c4dd6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 20 08:40:29.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-66955c4dd6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 20 08:40:31.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-66955c4dd6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 20 08:40:33.825: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-66955c4dd6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 20 08:40:35.835: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-66955c4dd6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 20 08:40:37.831: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 20, 8, 40, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-66955c4dd6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 20 08:40:40.002: INFO: Waited 142.820715ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Mar 20 08:40:40.480: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:69
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:188
Mar 20 08:40:41.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1546" for this suite.

• [SLOW TEST:20.435 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":356,"completed":289,"skipped":5220,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:40:41.377: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a collection of services
Mar 20 08:40:41.439: INFO: Creating e2e-svc-a-rdwwx
Mar 20 08:40:41.451: INFO: Creating e2e-svc-b-c94ds
Mar 20 08:40:41.462: INFO: Creating e2e-svc-c-gx4cg
STEP: deleting service collection
Mar 20 08:40:41.496: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 08:40:41.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3571" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":356,"completed":290,"skipped":5237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:40:41.505: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar 20 08:40:41.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2502" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":356,"completed":291,"skipped":5284,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:40:41.576: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 20 08:40:45.656: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Mar 20 08:40:45.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7643" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":292,"skipped":5303,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:40:45.689: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-projected-db7c
STEP: Creating a pod to test atomic-volume-subpath
Mar 20 08:40:45.730: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-db7c" in namespace "subpath-9003" to be "Succeeded or Failed"
Mar 20 08:40:45.734: INFO: Pod "pod-subpath-test-projected-db7c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.11987ms
Mar 20 08:40:47.745: INFO: Pod "pod-subpath-test-projected-db7c": Phase="Running", Reason="", readiness=true. Elapsed: 2.015056687s
Mar 20 08:40:49.758: INFO: Pod "pod-subpath-test-projected-db7c": Phase="Running", Reason="", readiness=true. Elapsed: 4.028057943s
Mar 20 08:40:51.770: INFO: Pod "pod-subpath-test-projected-db7c": Phase="Running", Reason="", readiness=true. Elapsed: 6.040105715s
Mar 20 08:40:53.776: INFO: Pod "pod-subpath-test-projected-db7c": Phase="Running", Reason="", readiness=true. Elapsed: 8.045635273s
Mar 20 08:40:55.790: INFO: Pod "pod-subpath-test-projected-db7c": Phase="Running", Reason="", readiness=true. Elapsed: 10.059964927s
Mar 20 08:40:57.797: INFO: Pod "pod-subpath-test-projected-db7c": Phase="Running", Reason="", readiness=true. Elapsed: 12.067098726s
Mar 20 08:40:59.803: INFO: Pod "pod-subpath-test-projected-db7c": Phase="Running", Reason="", readiness=true. Elapsed: 14.072949749s
Mar 20 08:41:01.815: INFO: Pod "pod-subpath-test-projected-db7c": Phase="Running", Reason="", readiness=true. Elapsed: 16.084179263s
Mar 20 08:41:03.824: INFO: Pod "pod-subpath-test-projected-db7c": Phase="Running", Reason="", readiness=true. Elapsed: 18.093820651s
Mar 20 08:41:05.839: INFO: Pod "pod-subpath-test-projected-db7c": Phase="Running", Reason="", readiness=true. Elapsed: 20.108749658s
Mar 20 08:41:07.854: INFO: Pod "pod-subpath-test-projected-db7c": Phase="Running", Reason="", readiness=false. Elapsed: 22.123579574s
Mar 20 08:41:09.870: INFO: Pod "pod-subpath-test-projected-db7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.139124193s
STEP: Saw pod success
Mar 20 08:41:09.870: INFO: Pod "pod-subpath-test-projected-db7c" satisfied condition "Succeeded or Failed"
Mar 20 08:41:09.873: INFO: Trying to get logs from node env016ar130-worker01 pod pod-subpath-test-projected-db7c container test-container-subpath-projected-db7c: <nil>
STEP: delete the pod
Mar 20 08:41:09.901: INFO: Waiting for pod pod-subpath-test-projected-db7c to disappear
Mar 20 08:41:09.904: INFO: Pod pod-subpath-test-projected-db7c no longer exists
STEP: Deleting pod pod-subpath-test-projected-db7c
Mar 20 08:41:09.904: INFO: Deleting pod "pod-subpath-test-projected-db7c" in namespace "subpath-9003"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Mar 20 08:41:09.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9003" for this suite.

• [SLOW TEST:24.225 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","total":356,"completed":293,"skipped":5312,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:41:09.914: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service multi-endpoint-test in namespace services-4469
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4469 to expose endpoints map[]
Mar 20 08:41:09.984: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Mar 20 08:41:10.999: INFO: successfully validated that service multi-endpoint-test in namespace services-4469 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4469
Mar 20 08:41:11.020: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:41:13.027: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4469 to expose endpoints map[pod1:[100]]
Mar 20 08:41:13.043: INFO: successfully validated that service multi-endpoint-test in namespace services-4469 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4469
Mar 20 08:41:13.059: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:41:15.066: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4469 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 20 08:41:15.081: INFO: successfully validated that service multi-endpoint-test in namespace services-4469 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Mar 20 08:41:15.081: INFO: Creating new exec pod
Mar 20 08:41:20.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-4469 exec execpodpmcpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Mar 20 08:41:20.290: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar 20 08:41:20.290: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:41:20.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-4469 exec execpodpmcpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.188.235 80'
Mar 20 08:41:20.458: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.188.235 80\nConnection to 192.168.188.235 80 port [tcp/http] succeeded!\n"
Mar 20 08:41:20.458: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:41:20.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-4469 exec execpodpmcpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Mar 20 08:41:20.643: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar 20 08:41:20.643: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:41:20.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-4469 exec execpodpmcpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.188.235 81'
Mar 20 08:41:20.800: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.188.235 81\nConnection to 192.168.188.235 81 port [tcp/*] succeeded!\n"
Mar 20 08:41:20.800: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-4469
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4469 to expose endpoints map[pod2:[101]]
Mar 20 08:41:21.852: INFO: successfully validated that service multi-endpoint-test in namespace services-4469 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4469
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4469 to expose endpoints map[]
Mar 20 08:41:21.913: INFO: successfully validated that service multi-endpoint-test in namespace services-4469 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 08:41:21.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4469" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:12.029 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":356,"completed":294,"skipped":5320,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:41:21.944: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 20 08:41:21.967: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 20 08:41:21.978: INFO: Waiting for terminating namespaces to be deleted...
Mar 20 08:41:21.981: INFO: 
Logging pods the apiserver thinks is on node env016ar130-worker01 before test
Mar 20 08:41:21.999: INFO: calico-node-hdjmf from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container calico-node ready: true, restart count 0
Mar 20 08:41:21.999: INFO: config-update-8kkf9 from kube-system started at 2023-03-20 07:47:02 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container config-update ready: true, restart count 0
Mar 20 08:41:21.999: INFO: kube-multus-ds-w25rw from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container kube-multus ready: true, restart count 0
Mar 20 08:41:21.999: INFO: kube-proxy-sjh4k from kube-system started at 2023-03-16 12:25:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 20 08:41:21.999: INFO: static-lb-env016ar130-worker01 from kube-system started at 2023-03-16 12:25:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container static-lb ready: true, restart count 0
Mar 20 08:41:21.999: INFO: alertmanager-prometheus-kube-prometheus-alertmanager-0 from kubeops started at 2023-03-20 07:46:50 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container alertmanager ready: true, restart count 1
Mar 20 08:41:21.999: INFO: 	Container config-reloader ready: true, restart count 0
Mar 20 08:41:21.999: INFO: csi-cephfsplugin-fzjzg from kubeops started at 2023-03-20 07:46:45 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 08:41:21.999: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:41:21.999: INFO: csi-rbdplugin-m8z7v from kubeops started at 2023-03-20 07:46:45 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 08:41:21.999: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:41:21.999: INFO: filebeat-filebeat-7zl4k from kubeops started at 2023-03-20 07:46:45 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container filebeat ready: true, restart count 0
Mar 20 08:41:21.999: INFO: prometheus-prometheus-kube-prometheus-prometheus-0 from kubeops started at 2023-03-20 07:46:59 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container config-reloader ready: true, restart count 0
Mar 20 08:41:21.999: INFO: 	Container prometheus ready: true, restart count 0
Mar 20 08:41:21.999: INFO: prometheus-prometheus-node-exporter-hpppp from kubeops started at 2023-03-20 07:46:45 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container node-exporter ready: true, restart count 0
Mar 20 08:41:21.999: INFO: rook-ceph-crashcollector-env016ar130-worker01-766b6f9754-rt44x from kubeops started at 2023-03-20 07:46:45 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container ceph-crash ready: true, restart count 0
Mar 20 08:41:21.999: INFO: rook-ceph-mon-c-78d58bd46b-8n5tg from kubeops started at 2023-03-20 07:46:45 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:41:21.999: INFO: 	Container mon ready: true, restart count 0
Mar 20 08:41:21.999: INFO: rook-ceph-osd-0-c77fcd474-m5wsl from kubeops started at 2023-03-20 07:46:45 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:41:21.999: INFO: 	Container osd ready: true, restart count 0
Mar 20 08:41:21.999: INFO: rook-ceph-osd-prepare-env016ar130-worker01-n9m6f from kubeops started at 2023-03-20 07:46:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container provision ready: false, restart count 0
Mar 20 08:41:21.999: INFO: rook-discover-n7tql from kubeops started at 2023-03-20 07:46:45 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container rook-discover ready: true, restart count 0
Mar 20 08:41:21.999: INFO: sonobuoy from sonobuoy started at 2023-03-20 07:27:30 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 20 08:41:21.999: INFO: sonobuoy-e2e-job-5ef492aa84d840a2 from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container e2e ready: true, restart count 0
Mar 20 08:41:21.999: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 08:41:21.999: INFO: sonobuoy-systemd-logs-daemon-set-3a399369495241ac-j79q9 from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:21.999: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 08:41:21.999: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 20 08:41:21.999: INFO: 
Logging pods the apiserver thinks is on node env016ar130-worker02 before test
Mar 20 08:41:22.022: INFO: calico-node-bphlv from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container calico-node ready: true, restart count 0
Mar 20 08:41:22.022: INFO: config-update-9h6xg from kube-system started at 2023-03-16 14:15:50 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container config-update ready: true, restart count 0
Mar 20 08:41:22.022: INFO: kube-multus-ds-9xqdf from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container kube-multus ready: true, restart count 0
Mar 20 08:41:22.022: INFO: kube-proxy-dk5kb from kube-system started at 2023-03-16 12:25:25 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 20 08:41:22.022: INFO: static-lb-env016ar130-worker02 from kube-system started at 2023-03-16 12:25:53 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container static-lb ready: true, restart count 0
Mar 20 08:41:22.022: INFO: csi-cephfsplugin-gd4zn from kubeops started at 2023-03-16 14:15:32 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:41:22.022: INFO: csi-cephfsplugin-provisioner-8f66f988-nr7c7 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (5 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 08:41:22.022: INFO: csi-rbdplugin-mkm94 from kubeops started at 2023-03-16 14:15:05 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:41:22.022: INFO: csi-rbdplugin-provisioner-7bb4c8b9c7-h8jb6 from kubeops started at 2023-03-16 15:03:27 +0000 UTC (5 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 08:41:22.022: INFO: filebeat-filebeat-pf2vz from kubeops started at 2023-03-16 14:15:59 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container filebeat ready: true, restart count 0
Mar 20 08:41:22.022: INFO: gatekeeper-audit-749874bd85-gbfdv from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container manager ready: true, restart count 0
Mar 20 08:41:22.022: INFO: gatekeeper-controller-manager-768fd8789c-s4xkm from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container manager ready: true, restart count 0
Mar 20 08:41:22.022: INFO: gatekeeper-controller-manager-768fd8789c-w7sxj from kubeops started at 2023-03-16 15:03:30 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container manager ready: true, restart count 0
Mar 20 08:41:22.022: INFO: gatekeeper-controller-manager-768fd8789c-wgpj5 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container manager ready: true, restart count 0
Mar 20 08:41:22.022: INFO: harbor-chartmuseum-7df8df844b-8fds2 from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container chartmuseum ready: true, restart count 0
Mar 20 08:41:22.022: INFO: harbor-jobservice-7b746648f6-x2kd8 from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container jobservice ready: true, restart count 1
Mar 20 08:41:22.022: INFO: harbor-notary-signer-c9db4c94d-fs46b from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container notary-signer ready: true, restart count 0
Mar 20 08:41:22.022: INFO: harbor-notary-signer-c9db4c94d-zqpzz from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container notary-signer ready: true, restart count 0
Mar 20 08:41:22.022: INFO: harbor-portal-dbbbb4456-7hb5f from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container portal ready: true, restart count 0
Mar 20 08:41:22.022: INFO: harbor-registry-b66c45c5f-lq8vr from kubeops started at 2023-03-16 14:14:52 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container registry ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container registryctl ready: true, restart count 0
Mar 20 08:41:22.022: INFO: harbor-trivy-1 from kubeops started at 2023-03-16 14:16:07 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container trivy ready: true, restart count 0
Mar 20 08:41:22.022: INFO: opensearch-cluster-master-0 from kubeops started at 2023-03-16 14:16:53 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container opensearch ready: true, restart count 0
Mar 20 08:41:22.022: INFO: opensearch-cluster-master-2 from kubeops started at 2023-03-17 12:48:10 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container opensearch ready: true, restart count 0
Mar 20 08:41:22.022: INFO: prometheus-prometheus-node-exporter-rxbn4 from kubeops started at 2023-03-16 14:15:38 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container node-exporter ready: true, restart count 0
Mar 20 08:41:22.022: INFO: rook-ceph-crashcollector-env016ar130-worker02-799c88cbcf-9ktc7 from kubeops started at 2023-03-16 15:03:28 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container ceph-crash ready: true, restart count 0
Mar 20 08:41:22.022: INFO: rook-ceph-mds-myfs-a-85c5fc87d-gj7d4 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container mds ready: true, restart count 0
Mar 20 08:41:22.022: INFO: rook-ceph-mgr-a-857696f864-4gjb2 from kubeops started at 2023-03-16 15:03:30 +0000 UTC (3 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container mgr ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container watch-active ready: true, restart count 0
Mar 20 08:41:22.022: INFO: rook-ceph-mon-a-cc8d49d77-sgj2c from kubeops started at 2023-03-16 14:15:59 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container mon ready: true, restart count 0
Mar 20 08:41:22.022: INFO: rook-ceph-osd-2-557dc55864-679xk from kubeops started at 2023-03-16 14:15:54 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container osd ready: true, restart count 0
Mar 20 08:41:22.022: INFO: rook-ceph-osd-prepare-env016ar130-worker02-w9vhn from kubeops started at 2023-03-20 07:46:55 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container provision ready: false, restart count 0
Mar 20 08:41:22.022: INFO: rook-ceph-tools-59d749577f-chcwh from kubeops started at 2023-03-16 14:14:52 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Mar 20 08:41:22.022: INFO: rook-discover-x9lwv from kubeops started at 2023-03-16 14:15:44 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container rook-discover ready: true, restart count 0
Mar 20 08:41:22.022: INFO: execpodpmcpj from services-4469 started at 2023-03-20 08:41:15 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container agnhost-container ready: true, restart count 0
Mar 20 08:41:22.022: INFO: sonobuoy-systemd-logs-daemon-set-3a399369495241ac-swzbr from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:22.022: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 20 08:41:22.022: INFO: 
Logging pods the apiserver thinks is on node env016ar130-worker03 before test
Mar 20 08:41:22.050: INFO: calico-node-cpqql from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container calico-node ready: true, restart count 0
Mar 20 08:41:22.050: INFO: config-update-lj4n7 from kube-system started at 2023-03-16 12:35:49 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container config-update ready: true, restart count 0
Mar 20 08:41:22.050: INFO: kube-multus-ds-xcsml from kube-system started at 2023-03-16 12:35:14 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container kube-multus ready: true, restart count 0
Mar 20 08:41:22.050: INFO: kube-proxy-hxh8h from kube-system started at 2023-03-16 12:25:31 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 20 08:41:22.050: INFO: static-lb-env016ar130-worker03 from kube-system started at 2023-03-16 12:25:53 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container static-lb ready: true, restart count 0
Mar 20 08:41:22.050: INFO: csi-cephfsplugin-dpmqw from kubeops started at 2023-03-16 12:41:07 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:41:22.050: INFO: csi-cephfsplugin-provisioner-8f66f988-hg4ds from kubeops started at 2023-03-16 14:14:52 +0000 UTC (5 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 08:41:22.050: INFO: csi-rbdplugin-fhk4v from kubeops started at 2023-03-16 12:41:07 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container driver-registrar ready: true, restart count 0
Mar 20 08:41:22.050: INFO: csi-rbdplugin-provisioner-7bb4c8b9c7-z7z8d from kubeops started at 2023-03-16 12:41:07 +0000 UTC (5 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container csi-resizer ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar 20 08:41:22.050: INFO: filebeat-filebeat-hbpls from kubeops started at 2023-03-16 12:56:38 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container filebeat ready: true, restart count 0
Mar 20 08:41:22.050: INFO: harbor-chartmuseum-7df8df844b-7zqxs from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container chartmuseum ready: true, restart count 0
Mar 20 08:41:22.050: INFO: harbor-chartmuseum-7df8df844b-hks64 from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container chartmuseum ready: true, restart count 0
Mar 20 08:41:22.050: INFO: harbor-core-d54995cdd-6c49r from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container core ready: true, restart count 0
Mar 20 08:41:22.050: INFO: harbor-core-d54995cdd-brrck from kubeops started at 2023-03-16 15:03:27 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container core ready: true, restart count 1
Mar 20 08:41:22.050: INFO: harbor-core-d54995cdd-n4l9h from kubeops started at 2023-03-16 15:03:28 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container core ready: true, restart count 0
Mar 20 08:41:22.050: INFO: harbor-jobservice-7b746648f6-cv6bz from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container jobservice ready: true, restart count 1
Mar 20 08:41:22.050: INFO: harbor-jobservice-7b746648f6-l24wk from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container jobservice ready: true, restart count 4
Mar 20 08:41:22.050: INFO: harbor-nginx-f8c975ff6-2bwcd from kubeops started at 2023-03-16 12:54:06 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container nginx ready: true, restart count 0
Mar 20 08:41:22.050: INFO: harbor-nginx-f8c975ff6-5hv2g from kubeops started at 2023-03-16 15:03:27 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container nginx ready: true, restart count 0
Mar 20 08:41:22.050: INFO: harbor-nginx-f8c975ff6-hllb9 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container nginx ready: true, restart count 0
Mar 20 08:41:22.050: INFO: harbor-notary-server-89fd67fc6-67cdb from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container notary-server ready: true, restart count 2
Mar 20 08:41:22.050: INFO: harbor-notary-server-89fd67fc6-7h95b from kubeops started at 2023-03-16 12:54:06 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container notary-server ready: true, restart count 0
Mar 20 08:41:22.050: INFO: harbor-notary-server-89fd67fc6-kn77l from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container notary-server ready: true, restart count 2
Mar 20 08:41:22.050: INFO: harbor-notary-signer-c9db4c94d-txwpf from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container notary-signer ready: true, restart count 2
Mar 20 08:41:22.050: INFO: harbor-portal-dbbbb4456-gqbx6 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container portal ready: true, restart count 0
Mar 20 08:41:22.050: INFO: harbor-portal-dbbbb4456-qgsgw from kubeops started at 2023-03-16 12:54:06 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container portal ready: true, restart count 0
Mar 20 08:41:22.050: INFO: harbor-registry-b66c45c5f-d5q94 from kubeops started at 2023-03-16 12:54:07 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container registry ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container registryctl ready: true, restart count 0
Mar 20 08:41:22.050: INFO: harbor-registry-b66c45c5f-fr9g4 from kubeops started at 2023-03-16 15:03:27 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container registry ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container registryctl ready: true, restart count 0
Mar 20 08:41:22.050: INFO: harbor-trivy-0 from kubeops started at 2023-03-16 12:54:07 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container trivy ready: true, restart count 0
Mar 20 08:41:22.050: INFO: harbor-trivy-2 from kubeops started at 2023-03-16 12:55:51 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container trivy ready: true, restart count 0
Mar 20 08:41:22.050: INFO: logstash-logstash-0 from kubeops started at 2023-03-16 12:56:41 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container logstash ready: true, restart count 1
Mar 20 08:41:22.050: INFO: opensearch-cluster-master-1 from kubeops started at 2023-03-16 12:56:36 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container opensearch ready: true, restart count 0
Mar 20 08:41:22.050: INFO: opensearch-dashboards-69f44df846-zmxg9 from kubeops started at 2023-03-16 12:56:36 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container dashboards ready: true, restart count 0
Mar 20 08:41:22.050: INFO: postgres-7f6cc6d46c-cdpmp from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container postgres ready: true, restart count 0
Mar 20 08:41:22.050: INFO: prometheus-grafana-5c58fc9dbb-gj98n from kubeops started at 2023-03-16 12:55:16 +0000 UTC (3 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container grafana ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Mar 20 08:41:22.050: INFO: prometheus-kube-prometheus-operator-76b748b4b7-hgsmt from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Mar 20 08:41:22.050: INFO: prometheus-kube-state-metrics-85655df84d-qsk5t from kubeops started at 2023-03-16 12:55:16 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 20 08:41:22.050: INFO: prometheus-prometheus-node-exporter-sqpkb from kubeops started at 2023-03-16 12:55:16 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container node-exporter ready: true, restart count 0
Mar 20 08:41:22.050: INFO: redis-6fdbc8bc6c-cj4g2 from kubeops started at 2023-03-16 15:03:26 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container redis ready: true, restart count 0
Mar 20 08:41:22.050: INFO: rook-ceph-crashcollector-env016ar130-worker03-68f58d9f4b-pbp4c from kubeops started at 2023-03-16 12:44:17 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container ceph-crash ready: true, restart count 0
Mar 20 08:41:22.050: INFO: rook-ceph-mds-myfs-b-767cb4cfc5-p5f4k from kubeops started at 2023-03-16 12:44:16 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container mds ready: true, restart count 0
Mar 20 08:41:22.050: INFO: rook-ceph-mgr-b-69f6d8d6c-8klhf from kubeops started at 2023-03-16 12:43:36 +0000 UTC (3 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container mgr ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container watch-active ready: true, restart count 0
Mar 20 08:41:22.050: INFO: rook-ceph-mon-b-7b5485875c-fjr22 from kubeops started at 2023-03-16 12:43:15 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container mon ready: true, restart count 0
Mar 20 08:41:22.050: INFO: rook-ceph-operator-f6f75855b-lt27v from kubeops started at 2023-03-16 12:39:01 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Mar 20 08:41:22.050: INFO: rook-ceph-osd-1-755cf9d57c-j6jcs from kubeops started at 2023-03-16 12:44:07 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container log-collector ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container osd ready: true, restart count 0
Mar 20 08:41:22.050: INFO: rook-ceph-osd-prepare-env016ar130-worker03-8nc47 from kubeops started at 2023-03-20 07:46:58 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container provision ready: false, restart count 0
Mar 20 08:41:22.050: INFO: rook-discover-wj92q from kubeops started at 2023-03-16 12:39:43 +0000 UTC (1 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container rook-discover ready: true, restart count 0
Mar 20 08:41:22.050: INFO: sonobuoy-systemd-logs-daemon-set-3a399369495241ac-zkwlr from sonobuoy started at 2023-03-20 07:27:31 +0000 UTC (2 container statuses recorded)
Mar 20 08:41:22.050: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 20 08:41:22.050: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a34a5888-73c5-48fc-abcb-92785fcaeef9 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.2.10.71 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-a34a5888-73c5-48fc-abcb-92785fcaeef9 off the node env016ar130-worker01
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a34a5888-73c5-48fc-abcb-92785fcaeef9
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Mar 20 08:46:26.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1056" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:304.271 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":356,"completed":295,"skipped":5326,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:46:26.215: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
Mar 20 08:46:26.868: INFO: The status of Pod kube-controller-manager-env016ar130-master03 is Running (Ready = true)
Mar 20 08:46:26.931: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Mar 20 08:46:26.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3136" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":356,"completed":296,"skipped":5331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:46:26.944: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 20 08:46:26.973: INFO: Waiting up to 5m0s for pod "pod-5c43e861-246a-4e24-9171-1907cd54ba3e" in namespace "emptydir-7360" to be "Succeeded or Failed"
Mar 20 08:46:26.976: INFO: Pod "pod-5c43e861-246a-4e24-9171-1907cd54ba3e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.425253ms
Mar 20 08:46:28.985: INFO: Pod "pod-5c43e861-246a-4e24-9171-1907cd54ba3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01210418s
Mar 20 08:46:30.998: INFO: Pod "pod-5c43e861-246a-4e24-9171-1907cd54ba3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024999671s
STEP: Saw pod success
Mar 20 08:46:30.998: INFO: Pod "pod-5c43e861-246a-4e24-9171-1907cd54ba3e" satisfied condition "Succeeded or Failed"
Mar 20 08:46:31.002: INFO: Trying to get logs from node env016ar130-worker01 pod pod-5c43e861-246a-4e24-9171-1907cd54ba3e container test-container: <nil>
STEP: delete the pod
Mar 20 08:46:31.046: INFO: Waiting for pod pod-5c43e861-246a-4e24-9171-1907cd54ba3e to disappear
Mar 20 08:46:31.051: INFO: Pod pod-5c43e861-246a-4e24-9171-1907cd54ba3e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar 20 08:46:31.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7360" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":297,"skipped":5358,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:46:31.065: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in volume subpath
Mar 20 08:46:31.114: INFO: Waiting up to 5m0s for pod "var-expansion-774ee0e6-63e3-4e8a-bb72-1ed38d39d6da" in namespace "var-expansion-2763" to be "Succeeded or Failed"
Mar 20 08:46:31.121: INFO: Pod "var-expansion-774ee0e6-63e3-4e8a-bb72-1ed38d39d6da": Phase="Pending", Reason="", readiness=false. Elapsed: 7.258164ms
Mar 20 08:46:33.129: INFO: Pod "var-expansion-774ee0e6-63e3-4e8a-bb72-1ed38d39d6da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014618942s
Mar 20 08:46:35.135: INFO: Pod "var-expansion-774ee0e6-63e3-4e8a-bb72-1ed38d39d6da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021210735s
STEP: Saw pod success
Mar 20 08:46:35.135: INFO: Pod "var-expansion-774ee0e6-63e3-4e8a-bb72-1ed38d39d6da" satisfied condition "Succeeded or Failed"
Mar 20 08:46:35.139: INFO: Trying to get logs from node env016ar130-worker01 pod var-expansion-774ee0e6-63e3-4e8a-bb72-1ed38d39d6da container dapi-container: <nil>
STEP: delete the pod
Mar 20 08:46:35.157: INFO: Waiting for pod var-expansion-774ee0e6-63e3-4e8a-bb72-1ed38d39d6da to disappear
Mar 20 08:46:35.161: INFO: Pod var-expansion-774ee0e6-63e3-4e8a-bb72-1ed38d39d6da no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar 20 08:46:35.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2763" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":356,"completed":298,"skipped":5360,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:46:35.172: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Mar 20 08:46:35.221: INFO: The status of Pod annotationupdate79d55599-a5bd-42a2-83f2-154c8a9f918f is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:46:37.230: INFO: The status of Pod annotationupdate79d55599-a5bd-42a2-83f2-154c8a9f918f is Running (Ready = true)
Mar 20 08:46:37.766: INFO: Successfully updated pod "annotationupdate79d55599-a5bd-42a2-83f2-154c8a9f918f"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar 20 08:46:41.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3588" for this suite.

• [SLOW TEST:6.655 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":299,"skipped":5399,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:46:41.828: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Mar 20 08:46:45.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2998" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":356,"completed":300,"skipped":5436,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:46:45.719: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Mar 20 08:46:45.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8014" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":356,"completed":301,"skipped":5452,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:46:45.790: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Mar 20 08:46:45.828: INFO: Waiting up to 5m0s for pod "security-context-e1e2783b-3f35-492d-b679-79c989cb8121" in namespace "security-context-7206" to be "Succeeded or Failed"
Mar 20 08:46:45.831: INFO: Pod "security-context-e1e2783b-3f35-492d-b679-79c989cb8121": Phase="Pending", Reason="", readiness=false. Elapsed: 3.264928ms
Mar 20 08:46:47.845: INFO: Pod "security-context-e1e2783b-3f35-492d-b679-79c989cb8121": Phase="Running", Reason="", readiness=false. Elapsed: 2.01751555s
Mar 20 08:46:49.850: INFO: Pod "security-context-e1e2783b-3f35-492d-b679-79c989cb8121": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022523703s
STEP: Saw pod success
Mar 20 08:46:49.850: INFO: Pod "security-context-e1e2783b-3f35-492d-b679-79c989cb8121" satisfied condition "Succeeded or Failed"
Mar 20 08:46:49.853: INFO: Trying to get logs from node env016ar130-worker01 pod security-context-e1e2783b-3f35-492d-b679-79c989cb8121 container test-container: <nil>
STEP: delete the pod
Mar 20 08:46:49.873: INFO: Waiting for pod security-context-e1e2783b-3f35-492d-b679-79c989cb8121 to disappear
Mar 20 08:46:49.877: INFO: Pod security-context-e1e2783b-3f35-492d-b679-79c989cb8121 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Mar 20 08:46:49.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7206" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":302,"skipped":5487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:46:49.889: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-f000415e-d142-4fc5-a8d1-8425b67042ec
STEP: Creating a pod to test consume configMaps
Mar 20 08:46:49.930: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6ee90846-0f0b-43ae-8c5f-937933d8c501" in namespace "projected-2956" to be "Succeeded or Failed"
Mar 20 08:46:49.934: INFO: Pod "pod-projected-configmaps-6ee90846-0f0b-43ae-8c5f-937933d8c501": Phase="Pending", Reason="", readiness=false. Elapsed: 3.94572ms
Mar 20 08:46:51.947: INFO: Pod "pod-projected-configmaps-6ee90846-0f0b-43ae-8c5f-937933d8c501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01704949s
Mar 20 08:46:53.958: INFO: Pod "pod-projected-configmaps-6ee90846-0f0b-43ae-8c5f-937933d8c501": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027297804s
STEP: Saw pod success
Mar 20 08:46:53.958: INFO: Pod "pod-projected-configmaps-6ee90846-0f0b-43ae-8c5f-937933d8c501" satisfied condition "Succeeded or Failed"
Mar 20 08:46:53.962: INFO: Trying to get logs from node env016ar130-worker02 pod pod-projected-configmaps-6ee90846-0f0b-43ae-8c5f-937933d8c501 container agnhost-container: <nil>
STEP: delete the pod
Mar 20 08:46:54.002: INFO: Waiting for pod pod-projected-configmaps-6ee90846-0f0b-43ae-8c5f-937933d8c501 to disappear
Mar 20 08:46:54.006: INFO: Pod pod-projected-configmaps-6ee90846-0f0b-43ae-8c5f-937933d8c501 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar 20 08:46:54.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2956" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":303,"skipped":5516,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:46:54.017: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 20 08:46:58.100: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Mar 20 08:46:58.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3412" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":304,"skipped":5568,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:46:58.134: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2563
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2563
I0320 08:46:58.201161      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2563, replica count: 2
Mar 20 08:47:01.252: INFO: Creating new exec pod
I0320 08:47:01.252066      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 20 08:47:04.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-2563 exec execpodb25gs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 20 08:47:04.466: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 20 08:47:04.466: INFO: stdout: "externalname-service-ggl5f"
Mar 20 08:47:04.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-2563 exec execpodb25gs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.194.118 80'
Mar 20 08:47:04.596: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.194.118 80\nConnection to 192.168.194.118 80 port [tcp/http] succeeded!\n"
Mar 20 08:47:04.596: INFO: stdout: "externalname-service-4575q"
Mar 20 08:47:04.596: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 08:47:04.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2563" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:6.511 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":356,"completed":305,"skipped":5574,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:47:04.645: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption is created
Mar 20 08:47:04.688: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:47:06.693: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Mar 20 08:47:07.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5295" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":356,"completed":306,"skipped":5582,"failed":0}
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:47:07.741: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Mar 20 08:47:07.776: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Mar 20 08:47:12.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8461" for this suite.

• [SLOW TEST:5.031 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":356,"completed":307,"skipped":5588,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:47:12.773: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Mar 20 08:47:12.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-8041 create -f -'
Mar 20 08:47:13.874: INFO: stderr: ""
Mar 20 08:47:13.874: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 20 08:47:13.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-8041 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 20 08:47:13.948: INFO: stderr: ""
Mar 20 08:47:13.948: INFO: stdout: "update-demo-nautilus-drhrs update-demo-nautilus-s58vw "
Mar 20 08:47:13.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-8041 get pods update-demo-nautilus-drhrs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 20 08:47:14.017: INFO: stderr: ""
Mar 20 08:47:14.017: INFO: stdout: ""
Mar 20 08:47:14.017: INFO: update-demo-nautilus-drhrs is created but not running
Mar 20 08:47:19.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-8041 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 20 08:47:19.097: INFO: stderr: ""
Mar 20 08:47:19.097: INFO: stdout: "update-demo-nautilus-drhrs update-demo-nautilus-s58vw "
Mar 20 08:47:19.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-8041 get pods update-demo-nautilus-drhrs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 20 08:47:19.173: INFO: stderr: ""
Mar 20 08:47:19.173: INFO: stdout: "true"
Mar 20 08:47:19.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-8041 get pods update-demo-nautilus-drhrs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 20 08:47:19.239: INFO: stderr: ""
Mar 20 08:47:19.239: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar 20 08:47:19.239: INFO: validating pod update-demo-nautilus-drhrs
Mar 20 08:47:19.244: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 20 08:47:19.245: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 20 08:47:19.245: INFO: update-demo-nautilus-drhrs is verified up and running
Mar 20 08:47:19.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-8041 get pods update-demo-nautilus-s58vw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 20 08:47:19.309: INFO: stderr: ""
Mar 20 08:47:19.309: INFO: stdout: "true"
Mar 20 08:47:19.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-8041 get pods update-demo-nautilus-s58vw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 20 08:47:19.382: INFO: stderr: ""
Mar 20 08:47:19.382: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar 20 08:47:19.382: INFO: validating pod update-demo-nautilus-s58vw
Mar 20 08:47:19.388: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 20 08:47:19.388: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 20 08:47:19.388: INFO: update-demo-nautilus-s58vw is verified up and running
STEP: using delete to clean up resources
Mar 20 08:47:19.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-8041 delete --grace-period=0 --force -f -'
Mar 20 08:47:19.461: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 20 08:47:19.461: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 20 08:47:19.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-8041 get rc,svc -l name=update-demo --no-headers'
Mar 20 08:47:19.535: INFO: stderr: "No resources found in kubectl-8041 namespace.\n"
Mar 20 08:47:19.535: INFO: stdout: ""
Mar 20 08:47:19.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-8041 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 20 08:47:19.592: INFO: stderr: ""
Mar 20 08:47:19.592: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 08:47:19.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8041" for this suite.

• [SLOW TEST:6.842 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should create and stop a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":356,"completed":308,"skipped":5650,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:47:19.615: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:47:21.670: INFO: Deleting pod "var-expansion-2911f169-1a2e-42c2-99a3-6de60a97ce02" in namespace "var-expansion-5811"
Mar 20 08:47:21.687: INFO: Wait up to 5m0s for pod "var-expansion-2911f169-1a2e-42c2-99a3-6de60a97ce02" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar 20 08:47:25.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5811" for this suite.

• [SLOW TEST:6.106 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":356,"completed":309,"skipped":5760,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:47:25.721: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 20 08:47:25.801: INFO: Waiting up to 5m0s for pod "pod-5ec6123c-0dc0-495f-b902-bc609a510077" in namespace "emptydir-1476" to be "Succeeded or Failed"
Mar 20 08:47:25.804: INFO: Pod "pod-5ec6123c-0dc0-495f-b902-bc609a510077": Phase="Pending", Reason="", readiness=false. Elapsed: 3.071102ms
Mar 20 08:47:27.819: INFO: Pod "pod-5ec6123c-0dc0-495f-b902-bc609a510077": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017849384s
Mar 20 08:47:29.825: INFO: Pod "pod-5ec6123c-0dc0-495f-b902-bc609a510077": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024179026s
STEP: Saw pod success
Mar 20 08:47:29.825: INFO: Pod "pod-5ec6123c-0dc0-495f-b902-bc609a510077" satisfied condition "Succeeded or Failed"
Mar 20 08:47:29.829: INFO: Trying to get logs from node env016ar130-worker01 pod pod-5ec6123c-0dc0-495f-b902-bc609a510077 container test-container: <nil>
STEP: delete the pod
Mar 20 08:47:29.850: INFO: Waiting for pod pod-5ec6123c-0dc0-495f-b902-bc609a510077 to disappear
Mar 20 08:47:29.853: INFO: Pod pod-5ec6123c-0dc0-495f-b902-bc609a510077 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar 20 08:47:29.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1476" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":310,"skipped":5762,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:47:29.861: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Mar 20 08:47:29.963: INFO: observed Pod pod-test in namespace pods-9081 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar 20 08:47:29.967: INFO: observed Pod pod-test in namespace pods-9081 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:29 +0000 UTC  }]
Mar 20 08:47:29.983: INFO: observed Pod pod-test in namespace pods-9081 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:29 +0000 UTC  }]
Mar 20 08:47:30.565: INFO: observed Pod pod-test in namespace pods-9081 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:29 +0000 UTC  }]
Mar 20 08:47:30.592: INFO: observed Pod pod-test in namespace pods-9081 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:29 +0000 UTC  }]
Mar 20 08:47:31.840: INFO: Found Pod pod-test in namespace pods-9081 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:31 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:47:29 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Mar 20 08:47:31.896: INFO: observed event type MODIFIED
Mar 20 08:47:33.848: INFO: observed event type MODIFIED
Mar 20 08:47:34.145: INFO: observed event type MODIFIED
Mar 20 08:47:34.861: INFO: observed event type MODIFIED
Mar 20 08:47:34.868: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar 20 08:47:34.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9081" for this suite.

• [SLOW TEST:5.033 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":356,"completed":311,"skipped":5777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:47:34.895: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 20 08:47:34.943: INFO: Waiting up to 5m0s for pod "pod-be001458-8cdb-4729-b316-1a4a31e0ac94" in namespace "emptydir-2581" to be "Succeeded or Failed"
Mar 20 08:47:34.946: INFO: Pod "pod-be001458-8cdb-4729-b316-1a4a31e0ac94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.799502ms
Mar 20 08:47:36.953: INFO: Pod "pod-be001458-8cdb-4729-b316-1a4a31e0ac94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010191105s
Mar 20 08:47:38.960: INFO: Pod "pod-be001458-8cdb-4729-b316-1a4a31e0ac94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016818509s
STEP: Saw pod success
Mar 20 08:47:38.960: INFO: Pod "pod-be001458-8cdb-4729-b316-1a4a31e0ac94" satisfied condition "Succeeded or Failed"
Mar 20 08:47:38.963: INFO: Trying to get logs from node env016ar130-worker02 pod pod-be001458-8cdb-4729-b316-1a4a31e0ac94 container test-container: <nil>
STEP: delete the pod
Mar 20 08:47:38.983: INFO: Waiting for pod pod-be001458-8cdb-4729-b316-1a4a31e0ac94 to disappear
Mar 20 08:47:38.985: INFO: Pod pod-be001458-8cdb-4729-b316-1a4a31e0ac94 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar 20 08:47:38.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2581" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":312,"skipped":5819,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:47:38.995: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with configMap that has name projected-configmap-test-upd-2625e581-7147-413a-8241-e2cf60163f44
STEP: Creating the pod
Mar 20 08:47:39.053: INFO: The status of Pod pod-projected-configmaps-bf546a59-6b95-4983-b94d-fdef3e367fa4 is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:47:41.068: INFO: The status of Pod pod-projected-configmaps-bf546a59-6b95-4983-b94d-fdef3e367fa4 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-2625e581-7147-413a-8241-e2cf60163f44
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar 20 08:49:13.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3483" for this suite.

• [SLOW TEST:94.628 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":313,"skipped":5825,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:49:13.623: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-1688450e-574b-43ef-9577-ab60c33bf319
STEP: Creating a pod to test consume configMaps
Mar 20 08:49:13.682: INFO: Waiting up to 5m0s for pod "pod-configmaps-175a3b3d-574a-44fd-9c6a-0100b1e6f8ee" in namespace "configmap-6852" to be "Succeeded or Failed"
Mar 20 08:49:13.685: INFO: Pod "pod-configmaps-175a3b3d-574a-44fd-9c6a-0100b1e6f8ee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.154889ms
Mar 20 08:49:15.692: INFO: Pod "pod-configmaps-175a3b3d-574a-44fd-9c6a-0100b1e6f8ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010216807s
Mar 20 08:49:17.703: INFO: Pod "pod-configmaps-175a3b3d-574a-44fd-9c6a-0100b1e6f8ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021064077s
STEP: Saw pod success
Mar 20 08:49:17.703: INFO: Pod "pod-configmaps-175a3b3d-574a-44fd-9c6a-0100b1e6f8ee" satisfied condition "Succeeded or Failed"
Mar 20 08:49:17.707: INFO: Trying to get logs from node env016ar130-worker01 pod pod-configmaps-175a3b3d-574a-44fd-9c6a-0100b1e6f8ee container agnhost-container: <nil>
STEP: delete the pod
Mar 20 08:49:17.731: INFO: Waiting for pod pod-configmaps-175a3b3d-574a-44fd-9c6a-0100b1e6f8ee to disappear
Mar 20 08:49:17.735: INFO: Pod pod-configmaps-175a3b3d-574a-44fd-9c6a-0100b1e6f8ee no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar 20 08:49:17.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6852" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":314,"skipped":5837,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:49:17.744: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 20 08:49:18.137: INFO: Pod name wrapped-volume-race-fd766d8c-0673-4cfd-8239-778859fd51ac: Found 0 pods out of 5
Mar 20 08:49:23.147: INFO: Pod name wrapped-volume-race-fd766d8c-0673-4cfd-8239-778859fd51ac: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-fd766d8c-0673-4cfd-8239-778859fd51ac in namespace emptydir-wrapper-3906, will wait for the garbage collector to delete the pods
Mar 20 08:49:33.248: INFO: Deleting ReplicationController wrapped-volume-race-fd766d8c-0673-4cfd-8239-778859fd51ac took: 14.895222ms
Mar 20 08:49:33.348: INFO: Terminating ReplicationController wrapped-volume-race-fd766d8c-0673-4cfd-8239-778859fd51ac pods took: 100.235121ms
STEP: Creating RC which spawns configmap-volume pods
Mar 20 08:49:37.290: INFO: Pod name wrapped-volume-race-2ddeace2-9ad3-4ca9-a0af-7dfef1499e16: Found 0 pods out of 5
Mar 20 08:49:42.304: INFO: Pod name wrapped-volume-race-2ddeace2-9ad3-4ca9-a0af-7dfef1499e16: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2ddeace2-9ad3-4ca9-a0af-7dfef1499e16 in namespace emptydir-wrapper-3906, will wait for the garbage collector to delete the pods
Mar 20 08:49:52.412: INFO: Deleting ReplicationController wrapped-volume-race-2ddeace2-9ad3-4ca9-a0af-7dfef1499e16 took: 12.812048ms
Mar 20 08:49:52.513: INFO: Terminating ReplicationController wrapped-volume-race-2ddeace2-9ad3-4ca9-a0af-7dfef1499e16 pods took: 101.085676ms
STEP: Creating RC which spawns configmap-volume pods
Mar 20 08:49:55.062: INFO: Pod name wrapped-volume-race-f54b936e-605d-40c1-ad8d-5f4aa53e8d81: Found 0 pods out of 5
Mar 20 08:50:00.082: INFO: Pod name wrapped-volume-race-f54b936e-605d-40c1-ad8d-5f4aa53e8d81: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f54b936e-605d-40c1-ad8d-5f4aa53e8d81 in namespace emptydir-wrapper-3906, will wait for the garbage collector to delete the pods
Mar 20 08:50:12.200: INFO: Deleting ReplicationController wrapped-volume-race-f54b936e-605d-40c1-ad8d-5f4aa53e8d81 took: 18.586377ms
Mar 20 08:50:12.300: INFO: Terminating ReplicationController wrapped-volume-race-f54b936e-605d-40c1-ad8d-5f4aa53e8d81 pods took: 100.182353ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Mar 20 08:50:14.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3906" for this suite.

• [SLOW TEST:57.109 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":356,"completed":315,"skipped":5858,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:50:14.854: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:50:14.883: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties
Mar 20 08:50:19.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-3939 --namespace=crd-publish-openapi-3939 create -f -'
Mar 20 08:50:20.739: INFO: stderr: ""
Mar 20 08:50:20.739: INFO: stdout: "e2e-test-crd-publish-openapi-351-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 20 08:50:20.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-3939 --namespace=crd-publish-openapi-3939 delete e2e-test-crd-publish-openapi-351-crds test-foo'
Mar 20 08:50:20.814: INFO: stderr: ""
Mar 20 08:50:20.814: INFO: stdout: "e2e-test-crd-publish-openapi-351-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 20 08:50:20.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-3939 --namespace=crd-publish-openapi-3939 apply -f -'
Mar 20 08:50:21.597: INFO: stderr: ""
Mar 20 08:50:21.597: INFO: stdout: "e2e-test-crd-publish-openapi-351-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 20 08:50:21.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-3939 --namespace=crd-publish-openapi-3939 delete e2e-test-crd-publish-openapi-351-crds test-foo'
Mar 20 08:50:21.676: INFO: stderr: ""
Mar 20 08:50:21.676: INFO: stdout: "e2e-test-crd-publish-openapi-351-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values
Mar 20 08:50:21.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-3939 --namespace=crd-publish-openapi-3939 create -f -'
Mar 20 08:50:21.899: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar 20 08:50:21.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-3939 --namespace=crd-publish-openapi-3939 create -f -'
Mar 20 08:50:22.094: INFO: rc: 1
Mar 20 08:50:22.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-3939 --namespace=crd-publish-openapi-3939 apply -f -'
Mar 20 08:50:22.288: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties
Mar 20 08:50:22.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-3939 --namespace=crd-publish-openapi-3939 create -f -'
Mar 20 08:50:22.517: INFO: rc: 1
Mar 20 08:50:22.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-3939 --namespace=crd-publish-openapi-3939 apply -f -'
Mar 20 08:50:22.716: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar 20 08:50:22.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-3939 explain e2e-test-crd-publish-openapi-351-crds'
Mar 20 08:50:22.940: INFO: stderr: ""
Mar 20 08:50:22.940: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-351-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar 20 08:50:22.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-3939 explain e2e-test-crd-publish-openapi-351-crds.metadata'
Mar 20 08:50:23.142: INFO: stderr: ""
Mar 20 08:50:23.142: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-351-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     Deprecated: ClusterName is a legacy field that was always cleared by the\n     system and never used; it will be removed completely in 1.25.\n\n     The name in the go struct is changed to help clients detect accidental use.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 20 08:50:23.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-3939 explain e2e-test-crd-publish-openapi-351-crds.spec'
Mar 20 08:50:23.344: INFO: stderr: ""
Mar 20 08:50:23.344: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-351-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 20 08:50:23.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-3939 explain e2e-test-crd-publish-openapi-351-crds.spec.bars'
Mar 20 08:50:23.983: INFO: stderr: ""
Mar 20 08:50:23.983: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-351-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar 20 08:50:23.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=crd-publish-openapi-3939 explain e2e-test-crd-publish-openapi-351-crds.spec.bars2'
Mar 20 08:50:24.212: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 08:50:29.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3939" for this suite.

• [SLOW TEST:14.263 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":356,"completed":316,"skipped":5944,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:50:29.117: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-d10f744e-624e-4372-a29e-0398c16702e9
STEP: Creating a pod to test consume configMaps
Mar 20 08:50:29.157: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-078e26a1-797e-454f-b143-da6506cb88a4" in namespace "projected-3650" to be "Succeeded or Failed"
Mar 20 08:50:29.159: INFO: Pod "pod-projected-configmaps-078e26a1-797e-454f-b143-da6506cb88a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.143929ms
Mar 20 08:50:31.174: INFO: Pod "pod-projected-configmaps-078e26a1-797e-454f-b143-da6506cb88a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01657731s
Mar 20 08:50:33.181: INFO: Pod "pod-projected-configmaps-078e26a1-797e-454f-b143-da6506cb88a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02402186s
STEP: Saw pod success
Mar 20 08:50:33.181: INFO: Pod "pod-projected-configmaps-078e26a1-797e-454f-b143-da6506cb88a4" satisfied condition "Succeeded or Failed"
Mar 20 08:50:33.185: INFO: Trying to get logs from node env016ar130-worker01 pod pod-projected-configmaps-078e26a1-797e-454f-b143-da6506cb88a4 container agnhost-container: <nil>
STEP: delete the pod
Mar 20 08:50:33.212: INFO: Waiting for pod pod-projected-configmaps-078e26a1-797e-454f-b143-da6506cb88a4 to disappear
Mar 20 08:50:33.215: INFO: Pod pod-projected-configmaps-078e26a1-797e-454f-b143-da6506cb88a4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar 20 08:50:33.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3650" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":317,"skipped":5997,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:50:33.224: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pod templates
Mar 20 08:50:33.251: INFO: created test-podtemplate-1
Mar 20 08:50:33.257: INFO: created test-podtemplate-2
Mar 20 08:50:33.263: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Mar 20 08:50:33.266: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Mar 20 08:50:33.275: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Mar 20 08:50:33.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3133" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":356,"completed":318,"skipped":6008,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity 
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:50:33.285: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename csistoragecapacity
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/storage.k8s.io
STEP: getting /apis/storage.k8s.io/v1
STEP: creating
STEP: watching
Mar 20 08:50:33.320: INFO: starting watch
STEP: getting
STEP: listing in namespace
STEP: listing across namespaces
STEP: patching
STEP: updating
Mar 20 08:50:33.342: INFO: waiting for watch events with expected annotations in namespace
Mar 20 08:50:33.342: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:188
Mar 20 08:50:33.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-531" for this suite.
•{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","total":356,"completed":319,"skipped":6023,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:50:33.368: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Mar 20 08:50:43.452: INFO: The status of Pod kube-controller-manager-env016ar130-master03 is Running (Ready = true)
Mar 20 08:50:43.516: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Mar 20 08:50:43.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9190" for this suite.

• [SLOW TEST:10.161 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":356,"completed":320,"skipped":6048,"failed":0}
SSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:50:43.528: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pods
Mar 20 08:50:43.561: INFO: created test-pod-1
Mar 20 08:50:43.593: INFO: created test-pod-2
Mar 20 08:50:43.603: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running
Mar 20 08:50:43.603: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5270' to be running and ready
Mar 20 08:50:43.612: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 20 08:50:43.612: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 20 08:50:43.612: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 20 08:50:43.612: INFO: 0 / 3 pods in namespace 'pods-5270' are running and ready (0 seconds elapsed)
Mar 20 08:50:43.612: INFO: expected 0 pod replicas in namespace 'pods-5270', 0 are Running and Ready.
Mar 20 08:50:43.612: INFO: POD         NODE                  PHASE    GRACE  CONDITIONS
Mar 20 08:50:43.612: INFO: test-pod-1  env016ar130-worker02  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:50:43 +0000 UTC  }]
Mar 20 08:50:43.612: INFO: test-pod-2  env016ar130-worker01  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:50:43 +0000 UTC  }]
Mar 20 08:50:43.612: INFO: test-pod-3  env016ar130-worker01  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:50:43 +0000 UTC  }]
Mar 20 08:50:43.612: INFO: 
Mar 20 08:50:45.630: INFO: 3 / 3 pods in namespace 'pods-5270' are running and ready (2 seconds elapsed)
Mar 20 08:50:45.630: INFO: expected 0 pod replicas in namespace 'pods-5270', 0 are Running and Ready.
STEP: waiting for all pods to be deleted
Mar 20 08:50:45.651: INFO: Pod quantity 3 is different from expected quantity 0
Mar 20 08:50:46.660: INFO: Pod quantity 3 is different from expected quantity 0
Mar 20 08:50:47.660: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar 20 08:50:48.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5270" for this suite.

• [SLOW TEST:5.135 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":356,"completed":321,"skipped":6053,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:50:48.664: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar 20 08:50:48.685: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar 20 08:50:52.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-874" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":356,"completed":322,"skipped":6076,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:50:52.776: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Mar 20 08:50:52.819: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 20 08:51:52.936: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:51:52.939: INFO: Starting informer...
STEP: Starting pod...
Mar 20 08:51:53.159: INFO: Pod is running on env016ar130-worker01. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar 20 08:51:53.197: INFO: Pod wasn't evicted. Proceeding
Mar 20 08:51:53.197: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar 20 08:53:08.258: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:188
Mar 20 08:53:08.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1700" for this suite.

• [SLOW TEST:135.503 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":356,"completed":323,"skipped":6106,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:53:08.279: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service nodeport-test with type=NodePort in namespace services-1746
STEP: creating replication controller nodeport-test in namespace services-1746
I0320 08:53:08.328501      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1746, replica count: 2
Mar 20 08:53:11.379: INFO: Creating new exec pod
I0320 08:53:11.379214      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 20 08:53:14.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-1746 exec execpod8mwt2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Mar 20 08:53:14.568: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 20 08:53:14.568: INFO: stdout: "nodeport-test-xpjn9"
Mar 20 08:53:14.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-1746 exec execpod8mwt2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.215.243 80'
Mar 20 08:53:14.743: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.215.243 80\nConnection to 192.168.215.243 80 port [tcp/http] succeeded!\n"
Mar 20 08:53:14.743: INFO: stdout: "nodeport-test-kvf7z"
Mar 20 08:53:14.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-1746 exec execpod8mwt2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.10.72 31872'
Mar 20 08:53:14.911: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.10.72 31872\nConnection to 10.2.10.72 31872 port [tcp/*] succeeded!\n"
Mar 20 08:53:14.911: INFO: stdout: "nodeport-test-kvf7z"
Mar 20 08:53:14.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-1746 exec execpod8mwt2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.10.71 31872'
Mar 20 08:53:15.082: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.10.71 31872\nConnection to 10.2.10.71 31872 port [tcp/*] succeeded!\n"
Mar 20 08:53:15.082: INFO: stdout: "nodeport-test-xpjn9"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 08:53:15.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1746" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:6.823 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":356,"completed":324,"skipped":6116,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:53:15.103: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-3cd8fd04-5648-4179-b5b0-e274ff597c07
STEP: Creating a pod to test consume configMaps
Mar 20 08:53:15.142: INFO: Waiting up to 5m0s for pod "pod-configmaps-362119d8-7ce5-4505-98f4-1034bf8c0a97" in namespace "configmap-9274" to be "Succeeded or Failed"
Mar 20 08:53:15.146: INFO: Pod "pod-configmaps-362119d8-7ce5-4505-98f4-1034bf8c0a97": Phase="Pending", Reason="", readiness=false. Elapsed: 3.912273ms
Mar 20 08:53:17.160: INFO: Pod "pod-configmaps-362119d8-7ce5-4505-98f4-1034bf8c0a97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017867986s
Mar 20 08:53:19.172: INFO: Pod "pod-configmaps-362119d8-7ce5-4505-98f4-1034bf8c0a97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029979512s
STEP: Saw pod success
Mar 20 08:53:19.172: INFO: Pod "pod-configmaps-362119d8-7ce5-4505-98f4-1034bf8c0a97" satisfied condition "Succeeded or Failed"
Mar 20 08:53:19.176: INFO: Trying to get logs from node env016ar130-worker02 pod pod-configmaps-362119d8-7ce5-4505-98f4-1034bf8c0a97 container agnhost-container: <nil>
STEP: delete the pod
Mar 20 08:53:19.211: INFO: Waiting for pod pod-configmaps-362119d8-7ce5-4505-98f4-1034bf8c0a97 to disappear
Mar 20 08:53:19.215: INFO: Pod pod-configmaps-362119d8-7ce5-4505-98f4-1034bf8c0a97 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar 20 08:53:19.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9274" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":325,"skipped":6131,"failed":0}

------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:53:19.222: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Mar 20 08:53:19.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 create -f -'
Mar 20 08:53:20.063: INFO: stderr: ""
Mar 20 08:53:20.063: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 20 08:53:20.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 20 08:53:20.129: INFO: stderr: ""
Mar 20 08:53:20.129: INFO: stdout: "update-demo-nautilus-nqs7z "
STEP: Replicas for name=update-demo: expected=2 actual=1
Mar 20 08:53:25.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 20 08:53:25.200: INFO: stderr: ""
Mar 20 08:53:25.200: INFO: stdout: "update-demo-nautilus-nqs7z update-demo-nautilus-ttpv4 "
Mar 20 08:53:25.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods update-demo-nautilus-nqs7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 20 08:53:25.266: INFO: stderr: ""
Mar 20 08:53:25.266: INFO: stdout: "true"
Mar 20 08:53:25.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods update-demo-nautilus-nqs7z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 20 08:53:25.338: INFO: stderr: ""
Mar 20 08:53:25.338: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar 20 08:53:25.338: INFO: validating pod update-demo-nautilus-nqs7z
Mar 20 08:53:25.342: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 20 08:53:25.343: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 20 08:53:25.343: INFO: update-demo-nautilus-nqs7z is verified up and running
Mar 20 08:53:25.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods update-demo-nautilus-ttpv4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 20 08:53:25.409: INFO: stderr: ""
Mar 20 08:53:25.409: INFO: stdout: "true"
Mar 20 08:53:25.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods update-demo-nautilus-ttpv4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 20 08:53:25.469: INFO: stderr: ""
Mar 20 08:53:25.469: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar 20 08:53:25.469: INFO: validating pod update-demo-nautilus-ttpv4
Mar 20 08:53:25.474: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 20 08:53:25.474: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 20 08:53:25.474: INFO: update-demo-nautilus-ttpv4 is verified up and running
STEP: scaling down the replication controller
Mar 20 08:53:25.477: INFO: scanned /root for discovery docs: <nil>
Mar 20 08:53:25.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar 20 08:53:26.571: INFO: stderr: ""
Mar 20 08:53:26.571: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 20 08:53:26.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 20 08:53:26.657: INFO: stderr: ""
Mar 20 08:53:26.657: INFO: stdout: "update-demo-nautilus-nqs7z "
Mar 20 08:53:26.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods update-demo-nautilus-nqs7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 20 08:53:26.834: INFO: stderr: ""
Mar 20 08:53:26.834: INFO: stdout: "true"
Mar 20 08:53:26.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods update-demo-nautilus-nqs7z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 20 08:53:26.917: INFO: stderr: ""
Mar 20 08:53:26.917: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar 20 08:53:26.917: INFO: validating pod update-demo-nautilus-nqs7z
Mar 20 08:53:26.920: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 20 08:53:26.920: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 20 08:53:26.920: INFO: update-demo-nautilus-nqs7z is verified up and running
STEP: scaling up the replication controller
Mar 20 08:53:26.921: INFO: scanned /root for discovery docs: <nil>
Mar 20 08:53:26.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar 20 08:53:28.009: INFO: stderr: ""
Mar 20 08:53:28.009: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 20 08:53:28.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 20 08:53:28.081: INFO: stderr: ""
Mar 20 08:53:28.081: INFO: stdout: "update-demo-nautilus-nqs7z update-demo-nautilus-wklzw "
Mar 20 08:53:28.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods update-demo-nautilus-nqs7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 20 08:53:28.144: INFO: stderr: ""
Mar 20 08:53:28.144: INFO: stdout: "true"
Mar 20 08:53:28.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods update-demo-nautilus-nqs7z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 20 08:53:28.215: INFO: stderr: ""
Mar 20 08:53:28.215: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar 20 08:53:28.215: INFO: validating pod update-demo-nautilus-nqs7z
Mar 20 08:53:28.221: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 20 08:53:28.221: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 20 08:53:28.221: INFO: update-demo-nautilus-nqs7z is verified up and running
Mar 20 08:53:28.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods update-demo-nautilus-wklzw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 20 08:53:28.286: INFO: stderr: ""
Mar 20 08:53:28.286: INFO: stdout: ""
Mar 20 08:53:28.286: INFO: update-demo-nautilus-wklzw is created but not running
Mar 20 08:53:33.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 20 08:53:33.362: INFO: stderr: ""
Mar 20 08:53:33.362: INFO: stdout: "update-demo-nautilus-nqs7z update-demo-nautilus-wklzw "
Mar 20 08:53:33.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods update-demo-nautilus-nqs7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 20 08:53:33.437: INFO: stderr: ""
Mar 20 08:53:33.437: INFO: stdout: "true"
Mar 20 08:53:33.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods update-demo-nautilus-nqs7z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 20 08:53:33.505: INFO: stderr: ""
Mar 20 08:53:33.505: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar 20 08:53:33.505: INFO: validating pod update-demo-nautilus-nqs7z
Mar 20 08:53:33.512: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 20 08:53:33.512: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 20 08:53:33.512: INFO: update-demo-nautilus-nqs7z is verified up and running
Mar 20 08:53:33.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods update-demo-nautilus-wklzw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 20 08:53:33.578: INFO: stderr: ""
Mar 20 08:53:33.578: INFO: stdout: "true"
Mar 20 08:53:33.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods update-demo-nautilus-wklzw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 20 08:53:33.652: INFO: stderr: ""
Mar 20 08:53:33.652: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar 20 08:53:33.652: INFO: validating pod update-demo-nautilus-wklzw
Mar 20 08:53:33.657: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 20 08:53:33.657: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 20 08:53:33.657: INFO: update-demo-nautilus-wklzw is verified up and running
STEP: using delete to clean up resources
Mar 20 08:53:33.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 delete --grace-period=0 --force -f -'
Mar 20 08:53:33.732: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 20 08:53:33.732: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 20 08:53:33.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get rc,svc -l name=update-demo --no-headers'
Mar 20 08:53:33.806: INFO: stderr: "No resources found in kubectl-849 namespace.\n"
Mar 20 08:53:33.806: INFO: stdout: ""
Mar 20 08:53:33.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-849 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 20 08:53:33.866: INFO: stderr: ""
Mar 20 08:53:33.866: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 08:53:33.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-849" for this suite.

• [SLOW TEST:14.655 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should scale a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":356,"completed":326,"skipped":6131,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:53:33.880: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 20 08:53:33.943: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9594  9516e764-2229-4a2e-aa15-81226eb135e4 838845 0 2023-03-20 08:53:33 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2023-03-20 08:53:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 20 08:53:33.943: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9594  9516e764-2229-4a2e-aa15-81226eb135e4 838846 0 2023-03-20 08:53:33 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2023-03-20 08:53:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Mar 20 08:53:33.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9594" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":356,"completed":327,"skipped":6139,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:53:33.952: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Mar 20 08:53:36.000: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8338 PodName:var-expansion-9bdc78c1-14f3-475f-9740-8b0042231a00 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 08:53:36.000: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 08:53:36.001: INFO: ExecWithOptions: Clientset creation
Mar 20 08:53:36.001: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/var-expansion-8338/pods/var-expansion-9bdc78c1-14f3-475f-9740-8b0042231a00/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path
Mar 20 08:53:36.095: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8338 PodName:var-expansion-9bdc78c1-14f3-475f-9740-8b0042231a00 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 20 08:53:36.095: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
Mar 20 08:53:36.095: INFO: ExecWithOptions: Clientset creation
Mar 20 08:53:36.096: INFO: ExecWithOptions: execute(POST https://192.168.128.1:443/api/v1/namespaces/var-expansion-8338/pods/var-expansion-9bdc78c1-14f3-475f-9740-8b0042231a00/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value
Mar 20 08:53:36.707: INFO: Successfully updated pod "var-expansion-9bdc78c1-14f3-475f-9740-8b0042231a00"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Mar 20 08:53:36.712: INFO: Deleting pod "var-expansion-9bdc78c1-14f3-475f-9740-8b0042231a00" in namespace "var-expansion-8338"
Mar 20 08:53:36.746: INFO: Wait up to 5m0s for pod "var-expansion-9bdc78c1-14f3-475f-9740-8b0042231a00" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar 20 08:54:10.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8338" for this suite.

• [SLOW TEST:36.828 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":356,"completed":328,"skipped":6149,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:54:10.780: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-d17552ba-0068-484b-adf5-058aef612961
STEP: Creating a pod to test consume secrets
Mar 20 08:54:10.872: INFO: Waiting up to 5m0s for pod "pod-secrets-937dbea9-e21a-4a41-8b11-0df8f0209743" in namespace "secrets-1454" to be "Succeeded or Failed"
Mar 20 08:54:10.876: INFO: Pod "pod-secrets-937dbea9-e21a-4a41-8b11-0df8f0209743": Phase="Pending", Reason="", readiness=false. Elapsed: 4.233ms
Mar 20 08:54:12.884: INFO: Pod "pod-secrets-937dbea9-e21a-4a41-8b11-0df8f0209743": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012458392s
Mar 20 08:54:14.899: INFO: Pod "pod-secrets-937dbea9-e21a-4a41-8b11-0df8f0209743": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026563683s
STEP: Saw pod success
Mar 20 08:54:14.899: INFO: Pod "pod-secrets-937dbea9-e21a-4a41-8b11-0df8f0209743" satisfied condition "Succeeded or Failed"
Mar 20 08:54:14.901: INFO: Trying to get logs from node env016ar130-worker02 pod pod-secrets-937dbea9-e21a-4a41-8b11-0df8f0209743 container secret-volume-test: <nil>
STEP: delete the pod
Mar 20 08:54:14.916: INFO: Waiting for pod pod-secrets-937dbea9-e21a-4a41-8b11-0df8f0209743 to disappear
Mar 20 08:54:14.919: INFO: Pod pod-secrets-937dbea9-e21a-4a41-8b11-0df8f0209743 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar 20 08:54:14.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1454" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":329,"skipped":6152,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:54:14.928: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 20 08:54:15.339: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 20 08:54:18.371: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar 20 08:54:20.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=webhook-8033 attach --namespace=webhook-8033 to-be-attached-pod -i -c=container1'
Mar 20 08:54:20.521: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 08:54:20.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8033" for this suite.
STEP: Destroying namespace "webhook-8033-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.652 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":356,"completed":330,"skipped":6158,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:54:20.579: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:54:20.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5162 create -f -'
Mar 20 08:54:20.810: INFO: stderr: ""
Mar 20 08:54:20.810: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar 20 08:54:20.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5162 create -f -'
Mar 20 08:54:21.613: INFO: stderr: ""
Mar 20 08:54:21.613: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 20 08:54:22.625: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 20 08:54:22.625: INFO: Found 0 / 1
Mar 20 08:54:23.620: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 20 08:54:23.620: INFO: Found 1 / 1
Mar 20 08:54:23.620: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 20 08:54:23.624: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 20 08:54:23.624: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 20 08:54:23.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5162 describe pod agnhost-primary-nl2x8'
Mar 20 08:54:23.710: INFO: stderr: ""
Mar 20 08:54:23.710: INFO: stdout: "Name:         agnhost-primary-nl2x8\nNamespace:    kubectl-5162\nPriority:     0\nNode:         env016ar130-worker01/10.2.10.71\nStart Time:   Mon, 20 Mar 2023 08:54:20 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 192.168.90.82/32\n              cni.projectcalico.org/podIPs: 192.168.90.82/32\n              k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"192.168.90.82\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"192.168.90.82\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\nStatus:       Running\nIP:           192.168.90.82\nIPs:\n  IP:           192.168.90.82\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://dc3e86155d64169d6cc226fed86ca41e0dd1487a0ea563d0f73650cfa966e405\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 20 Mar 2023 08:54:21 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-k6zqv (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-k6zqv:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       2s    default-scheduler  Successfully assigned kubectl-5162/agnhost-primary-nl2x8 to env016ar130-worker01\n  Normal  AddedInterface  2s    multus             Add eth0 [192.168.90.82/32] from k8s-pod-network\n  Normal  Pulled          2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.39\" already present on machine\n  Normal  Created         2s    kubelet            Created container agnhost-primary\n  Normal  Started         2s    kubelet            Started container agnhost-primary\n"
Mar 20 08:54:23.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5162 describe rc agnhost-primary'
Mar 20 08:54:23.779: INFO: stderr: ""
Mar 20 08:54:23.779: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5162\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-nl2x8\n"
Mar 20 08:54:23.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5162 describe service agnhost-primary'
Mar 20 08:54:23.834: INFO: stderr: ""
Mar 20 08:54:23.834: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5162\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                192.168.162.138\nIPs:               192.168.162.138\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.90.82:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 20 08:54:23.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5162 describe node env016ar130-master01'
Mar 20 08:54:23.966: INFO: stderr: ""
Mar 20 08:54:23.966: INFO: stdout: "Name:               env016ar130-master01\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=env016ar130-master01\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.2.10.51/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 16 Mar 2023 12:17:14 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\n                    node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  env016ar130-master01\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 20 Mar 2023 08:54:15 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 16 Mar 2023 12:35:38 +0000   Thu, 16 Mar 2023 12:35:38 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 20 Mar 2023 08:52:05 +0000   Thu, 16 Mar 2023 12:17:12 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 20 Mar 2023 08:52:05 +0000   Thu, 16 Mar 2023 12:17:12 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 20 Mar 2023 08:52:05 +0000   Thu, 16 Mar 2023 12:17:12 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 20 Mar 2023 08:52:05 +0000   Thu, 16 Mar 2023 12:35:29 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.2.10.51\n  Hostname:    env016ar130-master01\nCapacity:\n  cpu:                4\n  ephemeral-storage:  13106Mi\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3825300Ki\n  pods:               110\nAllocatable:\n  cpu:                3\n  ephemeral-storage:  12368373330\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             1769775Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 f916814fecc44743b6011ac91f453a27\n  System UUID:                84343942-e730-5ca0-dc89-ed40c1f3db52\n  Boot ID:                    0cc94dc4-808e-47be-b33d-6c1becaeb955\n  Kernel Version:             4.18.0-305.19.1.el8_4.x86_64\n  OS Image:                   Red Hat Enterprise Linux 8.4 (Ootpa)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.6\n  Kubelet Version:            v1.24.8\n  Kube-Proxy Version:         v1.24.8\nPodCIDR:                      192.168.0.0/24\nPodCIDRs:                     192.168.0.0/24\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-sczk2                                          250m (8%)     0 (0%)      0 (0%)           0 (0%)         3d20h\n  kube-system                 etcd-env016ar130-master01                                  100m (3%)     0 (0%)      100Mi (5%)       0 (0%)         3d20h\n  kube-system                 kube-apiserver-env016ar130-master01                        250m (8%)     0 (0%)      0 (0%)           0 (0%)         3d20h\n  kube-system                 kube-controller-manager-env016ar130-master01               200m (6%)     0 (0%)      0 (0%)           0 (0%)         3d20h\n  kube-system                 kube-multus-ds-xnpgk                                       100m (3%)     100m (3%)   50Mi (2%)        50Mi (2%)      3d20h\n  kube-system                 kube-proxy-4fnxp                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d20h\n  kube-system                 kube-scheduler-env016ar130-master01                        100m (3%)     0 (0%)      0 (0%)           0 (0%)         3d20h\n  kube-system                 local-registry-env016ar130-master01                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d20h\n  kube-system                 static-lb-env016ar130-master01                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d20h\n  kubeops                     prometheus-prometheus-node-exporter-hsc9r                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d19h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-3a399369495241ac-zwfv5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         86m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                1 (33%)     100m (3%)\n  memory             150Mi (8%)  50Mi (2%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Mar 20 08:54:23.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=kubectl-5162 describe namespace kubectl-5162'
Mar 20 08:54:24.040: INFO: stderr: ""
Mar 20 08:54:24.040: INFO: stdout: "Name:         kubectl-5162\nLabels:       e2e-framework=kubectl\n              e2e-run=68c446fa-1a5b-4cef-a49f-90076ccfe47a\n              kubernetes.io/metadata.name=kubectl-5162\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar 20 08:54:24.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5162" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":356,"completed":331,"skipped":6175,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:54:24.054: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6885
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating stateful set ss in namespace statefulset-6885
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6885
Mar 20 08:54:24.158: INFO: Found 0 stateful pods, waiting for 1
Mar 20 08:54:34.169: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 20 08:54:34.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-6885 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 20 08:54:34.336: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 20 08:54:34.336: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 20 08:54:34.336: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 20 08:54:34.341: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 20 08:54:44.355: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 20 08:54:44.355: INFO: Waiting for statefulset status.replicas updated to 0
Mar 20 08:54:44.376: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
Mar 20 08:54:44.376: INFO: ss-0  env016ar130-worker01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:24 +0000 UTC  }]
Mar 20 08:54:44.376: INFO: 
Mar 20 08:54:44.376: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 20 08:54:45.390: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994511998s
Mar 20 08:54:46.401: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980593668s
Mar 20 08:54:47.407: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.970063881s
Mar 20 08:54:48.415: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.963421329s
Mar 20 08:54:49.426: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.955514637s
Mar 20 08:54:50.434: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.944840581s
Mar 20 08:54:51.444: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.936772542s
Mar 20 08:54:52.455: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.926730632s
Mar 20 08:54:53.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 915.916637ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6885
Mar 20 08:54:54.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-6885 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 20 08:54:54.629: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 20 08:54:54.629: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 20 08:54:54.629: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 20 08:54:54.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-6885 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 20 08:54:54.776: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 20 08:54:54.776: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 20 08:54:54.776: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 20 08:54:54.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-6885 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 20 08:54:54.933: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 20 08:54:54.933: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 20 08:54:54.933: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 20 08:54:54.939: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar 20 08:55:04.947: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 20 08:55:04.947: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 20 08:55:04.947: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 20 08:55:04.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-6885 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 20 08:55:05.097: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 20 08:55:05.097: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 20 08:55:05.097: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 20 08:55:05.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-6885 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 20 08:55:05.224: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 20 08:55:05.224: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 20 08:55:05.224: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 20 08:55:05.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=statefulset-6885 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 20 08:55:05.398: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 20 08:55:05.398: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 20 08:55:05.398: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 20 08:55:05.398: INFO: Waiting for statefulset status.replicas updated to 0
Mar 20 08:55:05.405: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 20 08:55:15.428: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 20 08:55:15.428: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 20 08:55:15.428: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 20 08:55:15.448: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
Mar 20 08:55:15.448: INFO: ss-0  env016ar130-worker01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:24 +0000 UTC  }]
Mar 20 08:55:15.448: INFO: ss-1  env016ar130-worker02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:44 +0000 UTC  }]
Mar 20 08:55:15.448: INFO: ss-2  env016ar130-worker03  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:44 +0000 UTC  }]
Mar 20 08:55:15.448: INFO: 
Mar 20 08:55:15.448: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 20 08:55:16.457: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
Mar 20 08:55:16.457: INFO: ss-0  env016ar130-worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:24 +0000 UTC  }]
Mar 20 08:55:16.457: INFO: ss-1  env016ar130-worker02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:44 +0000 UTC  }]
Mar 20 08:55:16.457: INFO: ss-2  env016ar130-worker03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-20 08:54:44 +0000 UTC  }]
Mar 20 08:55:16.457: INFO: 
Mar 20 08:55:16.457: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 20 08:55:17.464: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.986418529s
Mar 20 08:55:18.469: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.979195285s
Mar 20 08:55:19.477: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.973602529s
Mar 20 08:55:20.486: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.966733338s
Mar 20 08:55:21.496: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.957170303s
Mar 20 08:55:22.506: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.947767016s
Mar 20 08:55:23.517: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.937181897s
Mar 20 08:55:24.526: INFO: Verifying statefulset ss doesn't scale past 0 for another 926.936025ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6885
Mar 20 08:55:25.532: INFO: Scaling statefulset ss to 0
Mar 20 08:55:25.546: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 20 08:55:25.549: INFO: Deleting all statefulset in ns statefulset-6885
Mar 20 08:55:25.551: INFO: Scaling statefulset ss to 0
Mar 20 08:55:25.562: INFO: Waiting for statefulset status.replicas updated to 0
Mar 20 08:55:25.563: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar 20 08:55:25.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6885" for this suite.

• [SLOW TEST:61.532 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":356,"completed":332,"skipped":6184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:55:25.587: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Mar 20 08:55:25.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5486" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":356,"completed":333,"skipped":6207,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:55:25.646: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:55:25.676: INFO: The status of Pod busybox-host-aliasesc4c0113b-4d53-427f-9cb0-558c80fcabce is Pending, waiting for it to be Running (with Ready = true)
Mar 20 08:55:27.685: INFO: The status of Pod busybox-host-aliasesc4c0113b-4d53-427f-9cb0-558c80fcabce is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Mar 20 08:55:27.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1950" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":334,"skipped":6233,"failed":0}
SSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:55:27.704: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:55:27.726: INFO: Creating pod...
Mar 20 08:55:29.780: INFO: Creating service...
Mar 20 08:55:29.797: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-6422/pods/agnhost/proxy/some/path/with/DELETE
Mar 20 08:55:29.802: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 20 08:55:29.802: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-6422/pods/agnhost/proxy/some/path/with/GET
Mar 20 08:55:29.807: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 20 08:55:29.807: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-6422/pods/agnhost/proxy/some/path/with/HEAD
Mar 20 08:55:29.857: INFO: http.Client request:HEAD | StatusCode:200
Mar 20 08:55:29.857: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-6422/pods/agnhost/proxy/some/path/with/OPTIONS
Mar 20 08:55:29.860: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 20 08:55:29.860: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-6422/pods/agnhost/proxy/some/path/with/PATCH
Mar 20 08:55:29.863: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 20 08:55:29.863: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-6422/pods/agnhost/proxy/some/path/with/POST
Mar 20 08:55:29.867: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 20 08:55:29.867: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-6422/pods/agnhost/proxy/some/path/with/PUT
Mar 20 08:55:29.871: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 20 08:55:29.871: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-6422/services/test-service/proxy/some/path/with/DELETE
Mar 20 08:55:29.875: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 20 08:55:29.875: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-6422/services/test-service/proxy/some/path/with/GET
Mar 20 08:55:29.879: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 20 08:55:29.879: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-6422/services/test-service/proxy/some/path/with/HEAD
Mar 20 08:55:29.883: INFO: http.Client request:HEAD | StatusCode:200
Mar 20 08:55:29.883: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-6422/services/test-service/proxy/some/path/with/OPTIONS
Mar 20 08:55:29.887: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 20 08:55:29.887: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-6422/services/test-service/proxy/some/path/with/PATCH
Mar 20 08:55:29.891: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 20 08:55:29.891: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-6422/services/test-service/proxy/some/path/with/POST
Mar 20 08:55:29.895: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 20 08:55:29.895: INFO: Starting http.Client for https://192.168.128.1:443/api/v1/namespaces/proxy-6422/services/test-service/proxy/some/path/with/PUT
Mar 20 08:55:29.899: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Mar 20 08:55:29.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6422" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":356,"completed":335,"skipped":6238,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:55:29.908: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Mar 20 08:55:29.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3403" for this suite.
STEP: Destroying namespace "nspatchtest-b732efc5-35b2-4774-8d93-dbdcc47b7bfb-7573" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":356,"completed":336,"skipped":6244,"failed":0}
SSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:55:29.963: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:55:29.984: INFO: Endpoints addresses: [10.2.10.51 10.2.10.52 10.2.10.53] , ports: [6443]
Mar 20 08:55:29.984: INFO: EndpointSlices addresses: [10.2.10.51 10.2.10.52 10.2.10.53] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Mar 20 08:55:29.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8075" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":356,"completed":337,"skipped":6249,"failed":0}
SSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:55:29.989: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 08:55:30.021: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-b7ec6d33-4b5d-4f9b-9dbc-6eeba70804ec" in namespace "security-context-test-8540" to be "Succeeded or Failed"
Mar 20 08:55:30.024: INFO: Pod "busybox-readonly-false-b7ec6d33-4b5d-4f9b-9dbc-6eeba70804ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.718871ms
Mar 20 08:55:32.030: INFO: Pod "busybox-readonly-false-b7ec6d33-4b5d-4f9b-9dbc-6eeba70804ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008497548s
Mar 20 08:55:34.042: INFO: Pod "busybox-readonly-false-b7ec6d33-4b5d-4f9b-9dbc-6eeba70804ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020889771s
Mar 20 08:55:34.042: INFO: Pod "busybox-readonly-false-b7ec6d33-4b5d-4f9b-9dbc-6eeba70804ec" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Mar 20 08:55:34.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8540" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":356,"completed":338,"skipped":6254,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:55:34.053: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating server pod server in namespace prestop-5580
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5580
STEP: Deleting pre-stop pod
Mar 20 08:55:43.139: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:188
Mar 20 08:55:43.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5580" for this suite.

• [SLOW TEST:9.141 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":356,"completed":339,"skipped":6272,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:55:43.195: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-805
STEP: creating service affinity-clusterip in namespace services-805
STEP: creating replication controller affinity-clusterip in namespace services-805
I0320 08:55:43.287860      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-805, replica count: 3
I0320 08:55:46.338452      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 20 08:55:46.350: INFO: Creating new exec pod
Mar 20 08:55:49.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-805 exec execpod-affinity44nvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Mar 20 08:55:49.537: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar 20 08:55:49.537: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:55:49.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-805 exec execpod-affinity44nvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.169.215 80'
Mar 20 08:55:49.692: INFO: stderr: "+ + echo hostNamenc\n -v -t -w 2 192.168.169.215 80\nConnection to 192.168.169.215 80 port [tcp/http] succeeded!\n"
Mar 20 08:55:49.692: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:55:49.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-805 exec execpod-affinity44nvw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.169.215:80/ ; done'
Mar 20 08:55:49.935: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.169.215:80/\n"
Mar 20 08:55:49.935: INFO: stdout: "\naffinity-clusterip-zhw26\naffinity-clusterip-zhw26\naffinity-clusterip-zhw26\naffinity-clusterip-zhw26\naffinity-clusterip-zhw26\naffinity-clusterip-zhw26\naffinity-clusterip-zhw26\naffinity-clusterip-zhw26\naffinity-clusterip-zhw26\naffinity-clusterip-zhw26\naffinity-clusterip-zhw26\naffinity-clusterip-zhw26\naffinity-clusterip-zhw26\naffinity-clusterip-zhw26\naffinity-clusterip-zhw26\naffinity-clusterip-zhw26"
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Received response from host: affinity-clusterip-zhw26
Mar 20 08:55:49.935: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-805, will wait for the garbage collector to delete the pods
Mar 20 08:55:50.011: INFO: Deleting ReplicationController affinity-clusterip took: 6.22226ms
Mar 20 08:55:50.112: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.460274ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 08:55:52.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-805" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:8.940 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":340,"skipped":6292,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:55:52.135: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 20 08:55:52.162: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 20 08:56:52.277: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Mar 20 08:56:52.303: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 20 08:56:52.313: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 20 08:56:52.330: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 20 08:56:52.337: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 20 08:56:52.353: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 20 08:56:52.359: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Mar 20 08:57:06.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2131" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:74.376 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":356,"completed":341,"skipped":6297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:57:06.512: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-9cc3f5fc-e28d-47c9-b973-6c48613456fa
STEP: Creating a pod to test consume secrets
Mar 20 08:57:06.545: INFO: Waiting up to 5m0s for pod "pod-secrets-d70cc25b-f3e4-43da-bb6d-f0fa43f8b4d7" in namespace "secrets-1384" to be "Succeeded or Failed"
Mar 20 08:57:06.548: INFO: Pod "pod-secrets-d70cc25b-f3e4-43da-bb6d-f0fa43f8b4d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.874686ms
Mar 20 08:57:08.560: INFO: Pod "pod-secrets-d70cc25b-f3e4-43da-bb6d-f0fa43f8b4d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015339907s
Mar 20 08:57:10.573: INFO: Pod "pod-secrets-d70cc25b-f3e4-43da-bb6d-f0fa43f8b4d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027452609s
STEP: Saw pod success
Mar 20 08:57:10.573: INFO: Pod "pod-secrets-d70cc25b-f3e4-43da-bb6d-f0fa43f8b4d7" satisfied condition "Succeeded or Failed"
Mar 20 08:57:10.578: INFO: Trying to get logs from node env016ar130-worker01 pod pod-secrets-d70cc25b-f3e4-43da-bb6d-f0fa43f8b4d7 container secret-volume-test: <nil>
STEP: delete the pod
Mar 20 08:57:10.605: INFO: Waiting for pod pod-secrets-d70cc25b-f3e4-43da-bb6d-f0fa43f8b4d7 to disappear
Mar 20 08:57:10.608: INFO: Pod pod-secrets-d70cc25b-f3e4-43da-bb6d-f0fa43f8b4d7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar 20 08:57:10.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1384" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":342,"skipped":6334,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:57:10.617: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-6728
STEP: creating service affinity-nodeport-transition in namespace services-6728
STEP: creating replication controller affinity-nodeport-transition in namespace services-6728
I0320 08:57:10.652427      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6728, replica count: 3
I0320 08:57:13.704159      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 20 08:57:13.722: INFO: Creating new exec pod
Mar 20 08:57:16.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-6728 exec execpod-affinityzznr8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Mar 20 08:57:16.908: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar 20 08:57:16.908: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:57:16.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-6728 exec execpod-affinityzznr8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.139.125 80'
Mar 20 08:57:17.052: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.139.125 80\nConnection to 192.168.139.125 80 port [tcp/http] succeeded!\n"
Mar 20 08:57:17.052: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:57:17.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-6728 exec execpod-affinityzznr8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.10.73 32591'
Mar 20 08:57:17.212: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.10.73 32591\nConnection to 10.2.10.73 32591 port [tcp/*] succeeded!\n"
Mar 20 08:57:17.212: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:57:17.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-6728 exec execpod-affinityzznr8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.10.72 32591'
Mar 20 08:57:17.347: INFO: stderr: "+ + nc -vecho -t hostName -w\n 2 10.2.10.72 32591\nConnection to 10.2.10.72 32591 port [tcp/*] succeeded!\n"
Mar 20 08:57:17.347: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 20 08:57:17.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-6728 exec execpod-affinityzznr8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.10.71:32591/ ; done'
Mar 20 08:57:17.580: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n"
Mar 20 08:57:17.580: INFO: stdout: "\naffinity-nodeport-transition-shbf6\naffinity-nodeport-transition-shbf6\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-wk8wp\naffinity-nodeport-transition-shbf6\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-wk8wp\naffinity-nodeport-transition-shbf6\naffinity-nodeport-transition-shbf6\naffinity-nodeport-transition-shbf6\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-shbf6\naffinity-nodeport-transition-wk8wp"
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-shbf6
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-shbf6
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-wk8wp
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-shbf6
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-wk8wp
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-shbf6
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-shbf6
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-shbf6
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-shbf6
Mar 20 08:57:17.580: INFO: Received response from host: affinity-nodeport-transition-wk8wp
Mar 20 08:57:17.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-671188729 --namespace=services-6728 exec execpod-affinityzznr8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.10.71:32591/ ; done'
Mar 20 08:57:17.797: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.10.71:32591/\n"
Mar 20 08:57:17.797: INFO: stdout: "\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm\naffinity-nodeport-transition-6x9wm"
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Received response from host: affinity-nodeport-transition-6x9wm
Mar 20 08:57:17.797: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6728, will wait for the garbage collector to delete the pods
Mar 20 08:57:17.879: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.908526ms
Mar 20 08:57:17.979: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.648882ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar 20 08:57:20.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6728" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.899 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":343,"skipped":6335,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:57:20.516: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Mar 20 08:58:00.667: INFO: The status of Pod kube-controller-manager-env016ar130-master03 is Running (Ready = true)
Mar 20 08:58:00.752: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 20 08:58:00.752: INFO: Deleting pod "simpletest.rc-24ttz" in namespace "gc-1022"
Mar 20 08:58:00.764: INFO: Deleting pod "simpletest.rc-28zr6" in namespace "gc-1022"
Mar 20 08:58:00.777: INFO: Deleting pod "simpletest.rc-298f2" in namespace "gc-1022"
Mar 20 08:58:00.787: INFO: Deleting pod "simpletest.rc-2gzz4" in namespace "gc-1022"
Mar 20 08:58:00.797: INFO: Deleting pod "simpletest.rc-2msd6" in namespace "gc-1022"
Mar 20 08:58:00.803: INFO: Deleting pod "simpletest.rc-2x97w" in namespace "gc-1022"
Mar 20 08:58:00.810: INFO: Deleting pod "simpletest.rc-472dv" in namespace "gc-1022"
Mar 20 08:58:00.821: INFO: Deleting pod "simpletest.rc-498vf" in namespace "gc-1022"
Mar 20 08:58:00.830: INFO: Deleting pod "simpletest.rc-4b6tg" in namespace "gc-1022"
Mar 20 08:58:00.842: INFO: Deleting pod "simpletest.rc-4hrjc" in namespace "gc-1022"
Mar 20 08:58:00.881: INFO: Deleting pod "simpletest.rc-4q2rq" in namespace "gc-1022"
Mar 20 08:58:00.942: INFO: Deleting pod "simpletest.rc-52xxg" in namespace "gc-1022"
Mar 20 08:58:00.949: INFO: Deleting pod "simpletest.rc-55zm8" in namespace "gc-1022"
Mar 20 08:58:00.961: INFO: Deleting pod "simpletest.rc-5kgft" in namespace "gc-1022"
Mar 20 08:58:00.969: INFO: Deleting pod "simpletest.rc-64zhw" in namespace "gc-1022"
Mar 20 08:58:00.976: INFO: Deleting pod "simpletest.rc-6pfg2" in namespace "gc-1022"
Mar 20 08:58:00.987: INFO: Deleting pod "simpletest.rc-6qcgv" in namespace "gc-1022"
Mar 20 08:58:00.996: INFO: Deleting pod "simpletest.rc-6tfkh" in namespace "gc-1022"
Mar 20 08:58:01.005: INFO: Deleting pod "simpletest.rc-792kj" in namespace "gc-1022"
Mar 20 08:58:01.016: INFO: Deleting pod "simpletest.rc-79htt" in namespace "gc-1022"
Mar 20 08:58:01.027: INFO: Deleting pod "simpletest.rc-7gxs5" in namespace "gc-1022"
Mar 20 08:58:01.055: INFO: Deleting pod "simpletest.rc-7j6nv" in namespace "gc-1022"
Mar 20 08:58:01.068: INFO: Deleting pod "simpletest.rc-7vs8d" in namespace "gc-1022"
Mar 20 08:58:01.077: INFO: Deleting pod "simpletest.rc-8prfm" in namespace "gc-1022"
Mar 20 08:58:01.092: INFO: Deleting pod "simpletest.rc-8r9mz" in namespace "gc-1022"
Mar 20 08:58:01.108: INFO: Deleting pod "simpletest.rc-8t5gr" in namespace "gc-1022"
Mar 20 08:58:01.121: INFO: Deleting pod "simpletest.rc-9cztn" in namespace "gc-1022"
Mar 20 08:58:01.130: INFO: Deleting pod "simpletest.rc-9nnlm" in namespace "gc-1022"
Mar 20 08:58:01.140: INFO: Deleting pod "simpletest.rc-9qwx9" in namespace "gc-1022"
Mar 20 08:58:01.158: INFO: Deleting pod "simpletest.rc-9t6vq" in namespace "gc-1022"
Mar 20 08:58:01.169: INFO: Deleting pod "simpletest.rc-b228n" in namespace "gc-1022"
Mar 20 08:58:01.183: INFO: Deleting pod "simpletest.rc-b2vt7" in namespace "gc-1022"
Mar 20 08:58:01.191: INFO: Deleting pod "simpletest.rc-bgmmg" in namespace "gc-1022"
Mar 20 08:58:01.201: INFO: Deleting pod "simpletest.rc-blnct" in namespace "gc-1022"
Mar 20 08:58:01.213: INFO: Deleting pod "simpletest.rc-blwgx" in namespace "gc-1022"
Mar 20 08:58:01.227: INFO: Deleting pod "simpletest.rc-bp4vf" in namespace "gc-1022"
Mar 20 08:58:01.237: INFO: Deleting pod "simpletest.rc-bx4rs" in namespace "gc-1022"
Mar 20 08:58:01.254: INFO: Deleting pod "simpletest.rc-cdwzf" in namespace "gc-1022"
Mar 20 08:58:01.277: INFO: Deleting pod "simpletest.rc-cj5xs" in namespace "gc-1022"
Mar 20 08:58:01.334: INFO: Deleting pod "simpletest.rc-cjckj" in namespace "gc-1022"
Mar 20 08:58:01.386: INFO: Deleting pod "simpletest.rc-cppll" in namespace "gc-1022"
Mar 20 08:58:01.401: INFO: Deleting pod "simpletest.rc-crhp4" in namespace "gc-1022"
Mar 20 08:58:01.434: INFO: Deleting pod "simpletest.rc-crszx" in namespace "gc-1022"
Mar 20 08:58:01.445: INFO: Deleting pod "simpletest.rc-d7crt" in namespace "gc-1022"
Mar 20 08:58:01.459: INFO: Deleting pod "simpletest.rc-dbhlg" in namespace "gc-1022"
Mar 20 08:58:01.469: INFO: Deleting pod "simpletest.rc-dnkl9" in namespace "gc-1022"
Mar 20 08:58:01.481: INFO: Deleting pod "simpletest.rc-fptqj" in namespace "gc-1022"
Mar 20 08:58:01.491: INFO: Deleting pod "simpletest.rc-g46hs" in namespace "gc-1022"
Mar 20 08:58:01.508: INFO: Deleting pod "simpletest.rc-gg22v" in namespace "gc-1022"
Mar 20 08:58:01.531: INFO: Deleting pod "simpletest.rc-gql46" in namespace "gc-1022"
Mar 20 08:58:01.544: INFO: Deleting pod "simpletest.rc-hf2t6" in namespace "gc-1022"
Mar 20 08:58:01.563: INFO: Deleting pod "simpletest.rc-hj79h" in namespace "gc-1022"
Mar 20 08:58:01.585: INFO: Deleting pod "simpletest.rc-hnx82" in namespace "gc-1022"
Mar 20 08:58:01.605: INFO: Deleting pod "simpletest.rc-hxdts" in namespace "gc-1022"
Mar 20 08:58:01.629: INFO: Deleting pod "simpletest.rc-j28mv" in namespace "gc-1022"
Mar 20 08:58:01.644: INFO: Deleting pod "simpletest.rc-j2h6r" in namespace "gc-1022"
Mar 20 08:58:01.655: INFO: Deleting pod "simpletest.rc-jk8hr" in namespace "gc-1022"
Mar 20 08:58:01.664: INFO: Deleting pod "simpletest.rc-k7kpq" in namespace "gc-1022"
Mar 20 08:58:01.683: INFO: Deleting pod "simpletest.rc-kh4s4" in namespace "gc-1022"
Mar 20 08:58:01.693: INFO: Deleting pod "simpletest.rc-knlrs" in namespace "gc-1022"
Mar 20 08:58:01.703: INFO: Deleting pod "simpletest.rc-l2jrr" in namespace "gc-1022"
Mar 20 08:58:01.712: INFO: Deleting pod "simpletest.rc-lv26r" in namespace "gc-1022"
Mar 20 08:58:01.726: INFO: Deleting pod "simpletest.rc-lwgj2" in namespace "gc-1022"
Mar 20 08:58:01.748: INFO: Deleting pod "simpletest.rc-m26wc" in namespace "gc-1022"
Mar 20 08:58:01.762: INFO: Deleting pod "simpletest.rc-mwbdv" in namespace "gc-1022"
Mar 20 08:58:01.773: INFO: Deleting pod "simpletest.rc-nfxd4" in namespace "gc-1022"
Mar 20 08:58:01.795: INFO: Deleting pod "simpletest.rc-ng8xl" in namespace "gc-1022"
Mar 20 08:58:01.806: INFO: Deleting pod "simpletest.rc-nt4r4" in namespace "gc-1022"
Mar 20 08:58:01.840: INFO: Deleting pod "simpletest.rc-nwx46" in namespace "gc-1022"
Mar 20 08:58:01.856: INFO: Deleting pod "simpletest.rc-pl7nf" in namespace "gc-1022"
Mar 20 08:58:01.869: INFO: Deleting pod "simpletest.rc-q487b" in namespace "gc-1022"
Mar 20 08:58:01.888: INFO: Deleting pod "simpletest.rc-q79r7" in namespace "gc-1022"
Mar 20 08:58:01.912: INFO: Deleting pod "simpletest.rc-q7qb6" in namespace "gc-1022"
Mar 20 08:58:01.964: INFO: Deleting pod "simpletest.rc-qbrv7" in namespace "gc-1022"
Mar 20 08:58:02.009: INFO: Deleting pod "simpletest.rc-qclb8" in namespace "gc-1022"
Mar 20 08:58:02.062: INFO: Deleting pod "simpletest.rc-qw5zq" in namespace "gc-1022"
Mar 20 08:58:02.137: INFO: Deleting pod "simpletest.rc-r85cb" in namespace "gc-1022"
Mar 20 08:58:02.160: INFO: Deleting pod "simpletest.rc-rcwfs" in namespace "gc-1022"
Mar 20 08:58:02.208: INFO: Deleting pod "simpletest.rc-rdvbw" in namespace "gc-1022"
Mar 20 08:58:02.261: INFO: Deleting pod "simpletest.rc-rfv45" in namespace "gc-1022"
Mar 20 08:58:02.309: INFO: Deleting pod "simpletest.rc-rhcdx" in namespace "gc-1022"
Mar 20 08:58:02.364: INFO: Deleting pod "simpletest.rc-rtt66" in namespace "gc-1022"
Mar 20 08:58:02.409: INFO: Deleting pod "simpletest.rc-sfbpn" in namespace "gc-1022"
Mar 20 08:58:02.464: INFO: Deleting pod "simpletest.rc-sgcbx" in namespace "gc-1022"
Mar 20 08:58:02.509: INFO: Deleting pod "simpletest.rc-slk5x" in namespace "gc-1022"
Mar 20 08:58:02.557: INFO: Deleting pod "simpletest.rc-t8t9x" in namespace "gc-1022"
Mar 20 08:58:02.609: INFO: Deleting pod "simpletest.rc-vkn8n" in namespace "gc-1022"
Mar 20 08:58:02.660: INFO: Deleting pod "simpletest.rc-vv8r7" in namespace "gc-1022"
Mar 20 08:58:02.708: INFO: Deleting pod "simpletest.rc-wh66z" in namespace "gc-1022"
Mar 20 08:58:02.764: INFO: Deleting pod "simpletest.rc-wkpkl" in namespace "gc-1022"
Mar 20 08:58:02.810: INFO: Deleting pod "simpletest.rc-wmnmx" in namespace "gc-1022"
Mar 20 08:58:02.861: INFO: Deleting pod "simpletest.rc-wmrv8" in namespace "gc-1022"
Mar 20 08:58:02.909: INFO: Deleting pod "simpletest.rc-wz8mt" in namespace "gc-1022"
Mar 20 08:58:02.958: INFO: Deleting pod "simpletest.rc-x7wcw" in namespace "gc-1022"
Mar 20 08:58:03.006: INFO: Deleting pod "simpletest.rc-x9vfp" in namespace "gc-1022"
Mar 20 08:58:03.066: INFO: Deleting pod "simpletest.rc-xkhjc" in namespace "gc-1022"
Mar 20 08:58:03.123: INFO: Deleting pod "simpletest.rc-xljw4" in namespace "gc-1022"
Mar 20 08:58:03.158: INFO: Deleting pod "simpletest.rc-zfvgg" in namespace "gc-1022"
Mar 20 08:58:03.207: INFO: Deleting pod "simpletest.rc-zgvn6" in namespace "gc-1022"
Mar 20 08:58:03.261: INFO: Deleting pod "simpletest.rc-zkddc" in namespace "gc-1022"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Mar 20 08:58:03.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1022" for this suite.

• [SLOW TEST:42.886 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":356,"completed":344,"skipped":6341,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 08:58:03.403: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Mar 20 09:03:03.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8860" for this suite.

• [SLOW TEST:300.071 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":356,"completed":345,"skipped":6360,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 09:03:03.475: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 09:03:05.535: INFO: Deleting pod "var-expansion-135070ba-5199-4ae9-bf57-c09c23daa739" in namespace "var-expansion-8352"
Mar 20 09:03:05.543: INFO: Wait up to 5m0s for pod "var-expansion-135070ba-5199-4ae9-bf57-c09c23daa739" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar 20 09:03:09.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8352" for this suite.

• [SLOW TEST:6.098 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":356,"completed":346,"skipped":6379,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 09:03:09.573: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Mar 20 09:03:09.607: INFO: The status of Pod annotationupdate7efff9db-ae8c-417b-852c-d8c07398ac2a is Pending, waiting for it to be Running (with Ready = true)
Mar 20 09:03:11.619: INFO: The status of Pod annotationupdate7efff9db-ae8c-417b-852c-d8c07398ac2a is Running (Ready = true)
Mar 20 09:03:12.173: INFO: Successfully updated pod "annotationupdate7efff9db-ae8c-417b-852c-d8c07398ac2a"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar 20 09:03:16.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3788" for this suite.

• [SLOW TEST:6.652 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":347,"skipped":6388,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 09:03:16.225: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 09:03:16.271: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 20 09:03:21.277: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Mar 20 09:03:21.285: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Mar 20 09:03:21.294: INFO: observed ReplicaSet test-rs in namespace replicaset-6982 with ReadyReplicas 1, AvailableReplicas 1
Mar 20 09:03:21.302: INFO: observed ReplicaSet test-rs in namespace replicaset-6982 with ReadyReplicas 1, AvailableReplicas 1
Mar 20 09:03:21.312: INFO: observed ReplicaSet test-rs in namespace replicaset-6982 with ReadyReplicas 1, AvailableReplicas 1
Mar 20 09:03:21.323: INFO: observed ReplicaSet test-rs in namespace replicaset-6982 with ReadyReplicas 1, AvailableReplicas 1
Mar 20 09:03:22.305: INFO: observed ReplicaSet test-rs in namespace replicaset-6982 with ReadyReplicas 2, AvailableReplicas 2
Mar 20 09:03:22.639: INFO: observed Replicaset test-rs in namespace replicaset-6982 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Mar 20 09:03:22.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6982" for this suite.

• [SLOW TEST:6.427 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":356,"completed":348,"skipped":6390,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 09:03:22.653: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Mar 20 09:03:22.680: INFO: Waiting up to 5m0s for pod "downward-api-a1167fc1-d121-4d11-99d3-1baf5fd2a794" in namespace "downward-api-7693" to be "Succeeded or Failed"
Mar 20 09:03:22.682: INFO: Pod "downward-api-a1167fc1-d121-4d11-99d3-1baf5fd2a794": Phase="Pending", Reason="", readiness=false. Elapsed: 2.233057ms
Mar 20 09:03:24.697: INFO: Pod "downward-api-a1167fc1-d121-4d11-99d3-1baf5fd2a794": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017286428s
Mar 20 09:03:26.704: INFO: Pod "downward-api-a1167fc1-d121-4d11-99d3-1baf5fd2a794": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024548905s
STEP: Saw pod success
Mar 20 09:03:26.704: INFO: Pod "downward-api-a1167fc1-d121-4d11-99d3-1baf5fd2a794" satisfied condition "Succeeded or Failed"
Mar 20 09:03:26.708: INFO: Trying to get logs from node env016ar130-worker01 pod downward-api-a1167fc1-d121-4d11-99d3-1baf5fd2a794 container dapi-container: <nil>
STEP: delete the pod
Mar 20 09:03:26.732: INFO: Waiting for pod downward-api-a1167fc1-d121-4d11-99d3-1baf5fd2a794 to disappear
Mar 20 09:03:26.734: INFO: Pod downward-api-a1167fc1-d121-4d11-99d3-1baf5fd2a794 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Mar 20 09:03:26.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7693" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":356,"completed":349,"skipped":6415,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 09:03:26.742: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Mar 20 09:03:26.761: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar 20 09:03:33.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2785" for this suite.

• [SLOW TEST:6.656 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":356,"completed":350,"skipped":6416,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 09:03:33.398: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-2732fd5d-45d1-4c41-8b9b-9f96b7722a44
STEP: Creating a pod to test consume configMaps
Mar 20 09:03:33.433: INFO: Waiting up to 5m0s for pod "pod-configmaps-64cc4fcc-a588-4d13-bbad-9d6357c7f94e" in namespace "configmap-7224" to be "Succeeded or Failed"
Mar 20 09:03:33.436: INFO: Pod "pod-configmaps-64cc4fcc-a588-4d13-bbad-9d6357c7f94e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.616734ms
Mar 20 09:03:35.450: INFO: Pod "pod-configmaps-64cc4fcc-a588-4d13-bbad-9d6357c7f94e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016910919s
Mar 20 09:03:37.458: INFO: Pod "pod-configmaps-64cc4fcc-a588-4d13-bbad-9d6357c7f94e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024552281s
STEP: Saw pod success
Mar 20 09:03:37.458: INFO: Pod "pod-configmaps-64cc4fcc-a588-4d13-bbad-9d6357c7f94e" satisfied condition "Succeeded or Failed"
Mar 20 09:03:37.461: INFO: Trying to get logs from node env016ar130-worker01 pod pod-configmaps-64cc4fcc-a588-4d13-bbad-9d6357c7f94e container agnhost-container: <nil>
STEP: delete the pod
Mar 20 09:03:37.481: INFO: Waiting for pod pod-configmaps-64cc4fcc-a588-4d13-bbad-9d6357c7f94e to disappear
Mar 20 09:03:37.483: INFO: Pod pod-configmaps-64cc4fcc-a588-4d13-bbad-9d6357c7f94e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar 20 09:03:37.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7224" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":351,"skipped":6419,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 09:03:37.494: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 20 09:03:37.536: INFO: Waiting up to 5m0s for pod "pod-23e83ef9-157b-440b-979a-0f55112164ad" in namespace "emptydir-6207" to be "Succeeded or Failed"
Mar 20 09:03:37.541: INFO: Pod "pod-23e83ef9-157b-440b-979a-0f55112164ad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.006995ms
Mar 20 09:03:39.550: INFO: Pod "pod-23e83ef9-157b-440b-979a-0f55112164ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013780032s
Mar 20 09:03:41.557: INFO: Pod "pod-23e83ef9-157b-440b-979a-0f55112164ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020850794s
STEP: Saw pod success
Mar 20 09:03:41.557: INFO: Pod "pod-23e83ef9-157b-440b-979a-0f55112164ad" satisfied condition "Succeeded or Failed"
Mar 20 09:03:41.561: INFO: Trying to get logs from node env016ar130-worker02 pod pod-23e83ef9-157b-440b-979a-0f55112164ad container test-container: <nil>
STEP: delete the pod
Mar 20 09:03:41.593: INFO: Waiting for pod pod-23e83ef9-157b-440b-979a-0f55112164ad to disappear
Mar 20 09:03:41.596: INFO: Pod pod-23e83ef9-157b-440b-979a-0f55112164ad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar 20 09:03:41.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6207" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":352,"skipped":6443,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 09:03:41.605: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override command
Mar 20 09:03:41.630: INFO: Waiting up to 5m0s for pod "client-containers-9f8e6a40-d397-470f-b7a7-ff58103f8849" in namespace "containers-5503" to be "Succeeded or Failed"
Mar 20 09:03:41.633: INFO: Pod "client-containers-9f8e6a40-d397-470f-b7a7-ff58103f8849": Phase="Pending", Reason="", readiness=false. Elapsed: 2.118539ms
Mar 20 09:03:43.642: INFO: Pod "client-containers-9f8e6a40-d397-470f-b7a7-ff58103f8849": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011490479s
Mar 20 09:03:45.653: INFO: Pod "client-containers-9f8e6a40-d397-470f-b7a7-ff58103f8849": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022556204s
STEP: Saw pod success
Mar 20 09:03:45.653: INFO: Pod "client-containers-9f8e6a40-d397-470f-b7a7-ff58103f8849" satisfied condition "Succeeded or Failed"
Mar 20 09:03:45.657: INFO: Trying to get logs from node env016ar130-worker01 pod client-containers-9f8e6a40-d397-470f-b7a7-ff58103f8849 container agnhost-container: <nil>
STEP: delete the pod
Mar 20 09:03:45.675: INFO: Waiting for pod client-containers-9f8e6a40-d397-470f-b7a7-ff58103f8849 to disappear
Mar 20 09:03:45.678: INFO: Pod client-containers-9f8e6a40-d397-470f-b7a7-ff58103f8849 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Mar 20 09:03:45.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5503" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","total":356,"completed":353,"skipped":6459,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 09:03:45.687: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 20 09:03:45.741: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 09:03:45.741: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 09:03:45.741: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 09:03:45.744: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 09:03:45.744: INFO: Node env016ar130-worker01 is running 0 daemon pod, expected 1
Mar 20 09:03:46.753: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 09:03:46.753: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 09:03:46.753: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 09:03:46.757: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 09:03:46.757: INFO: Node env016ar130-worker01 is running 0 daemon pod, expected 1
Mar 20 09:03:47.756: INFO: DaemonSet pods can't tolerate node env016ar130-master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 09:03:47.757: INFO: DaemonSet pods can't tolerate node env016ar130-master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 09:03:47.757: INFO: DaemonSet pods can't tolerate node env016ar130-master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 20 09:03:47.761: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 20 09:03:47.761: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
Mar 20 09:03:47.765: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Mar 20 09:03:47.773: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Mar 20 09:03:47.776: INFO: Observed &DaemonSet event: ADDED
Mar 20 09:03:47.776: INFO: Observed &DaemonSet event: MODIFIED
Mar 20 09:03:47.776: INFO: Observed &DaemonSet event: MODIFIED
Mar 20 09:03:47.776: INFO: Observed &DaemonSet event: MODIFIED
Mar 20 09:03:47.776: INFO: Observed &DaemonSet event: MODIFIED
Mar 20 09:03:47.777: INFO: Found daemon set daemon-set in namespace daemonsets-3392 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 20 09:03:47.777: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Mar 20 09:03:47.783: INFO: Observed &DaemonSet event: ADDED
Mar 20 09:03:47.783: INFO: Observed &DaemonSet event: MODIFIED
Mar 20 09:03:47.783: INFO: Observed &DaemonSet event: MODIFIED
Mar 20 09:03:47.783: INFO: Observed &DaemonSet event: MODIFIED
Mar 20 09:03:47.784: INFO: Observed &DaemonSet event: MODIFIED
Mar 20 09:03:47.784: INFO: Observed daemon set daemon-set in namespace daemonsets-3392 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 20 09:03:47.784: INFO: Observed &DaemonSet event: MODIFIED
Mar 20 09:03:47.784: INFO: Found daemon set daemon-set in namespace daemonsets-3392 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar 20 09:03:47.784: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3392, will wait for the garbage collector to delete the pods
Mar 20 09:03:47.849: INFO: Deleting DaemonSet.extensions daemon-set took: 10.512977ms
Mar 20 09:03:47.950: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.430051ms
Mar 20 09:03:50.454: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 20 09:03:50.454: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 20 09:03:50.457: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"844521"},"items":null}

Mar 20 09:03:50.459: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"844521"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Mar 20 09:03:50.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3392" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":356,"completed":354,"skipped":6540,"failed":0}
S
------------------------------
[sig-node] Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 09:03:50.479: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Mar 20 09:03:52.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7904" for this suite.
•{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":356,"completed":355,"skipped":6541,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar 20 09:03:52.585: INFO: >>> kubeConfig: /tmp/kubeconfig-671188729
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-59554c0c-0cb8-4e41-9b0c-6dbf595313c4
STEP: Creating a pod to test consume configMaps
Mar 20 09:03:52.626: INFO: Waiting up to 5m0s for pod "pod-configmaps-598b6a41-c9b1-4ccf-bfe5-17f11919cf9c" in namespace "configmap-7177" to be "Succeeded or Failed"
Mar 20 09:03:52.628: INFO: Pod "pod-configmaps-598b6a41-c9b1-4ccf-bfe5-17f11919cf9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096747ms
Mar 20 09:03:54.642: INFO: Pod "pod-configmaps-598b6a41-c9b1-4ccf-bfe5-17f11919cf9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016338638s
Mar 20 09:03:56.655: INFO: Pod "pod-configmaps-598b6a41-c9b1-4ccf-bfe5-17f11919cf9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029027893s
STEP: Saw pod success
Mar 20 09:03:56.655: INFO: Pod "pod-configmaps-598b6a41-c9b1-4ccf-bfe5-17f11919cf9c" satisfied condition "Succeeded or Failed"
Mar 20 09:03:56.658: INFO: Trying to get logs from node env016ar130-worker01 pod pod-configmaps-598b6a41-c9b1-4ccf-bfe5-17f11919cf9c container agnhost-container: <nil>
STEP: delete the pod
Mar 20 09:03:56.674: INFO: Waiting for pod pod-configmaps-598b6a41-c9b1-4ccf-bfe5-17f11919cf9c to disappear
Mar 20 09:03:56.676: INFO: Pod pod-configmaps-598b6a41-c9b1-4ccf-bfe5-17f11919cf9c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar 20 09:03:56.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7177" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":356,"skipped":6614,"failed":0}
SSSMar 20 09:03:56.684: INFO: Running AfterSuite actions on all nodes
Mar 20 09:03:56.684: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func19.2
Mar 20 09:03:56.684: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Mar 20 09:03:56.684: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Mar 20 09:03:56.684: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Mar 20 09:03:56.684: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Mar 20 09:03:56.684: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Mar 20 09:03:56.684: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Mar 20 09:03:56.684: INFO: Running AfterSuite actions on node 1
Mar 20 09:03:56.684: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":356,"completed":356,"skipped":6617,"failed":0}

Ran 356 of 6973 Specs in 5782.413 seconds
SUCCESS! -- 356 Passed | 0 Failed | 0 Pending | 6617 Skipped
PASS

Ginkgo ran 1 suite in 1h36m23.991530657s
Test Suite Passed
